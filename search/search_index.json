{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HyperShift Self-Managed Azure Documentation","text":"<p>This documentation provides guidance for deploying HyperShift hosted clusters on self-managed Azure infrastructure.</p>"},{"location":"#what-is-hypershift","title":"What is HyperShift?","text":"<p>HyperShift enables you to deploy and manage OpenShift hosted control planes on a management cluster. For self-managed Azure deployments, this means:</p> <ul> <li>Reduced costs: Run multiple OpenShift control planes as pods on a shared management cluster</li> <li>Improved density: Host dozens of control planes on the same infrastructure</li> <li>Simplified management: Centralize control plane operations while isolating workload data planes</li> <li>Increased flexibility: Choose your own management cluster platform and customize networking</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Download the HyperShift CLI for your platform from GitHub Releases:</p> LinuxmacOS <pre><code># amd64\ncurl -LO https://github.com/hypershift-community/azure-dev-preview/releases/latest/download/hypershift-linux-amd64.tar.gz\ntar -xzf hypershift-linux-amd64.tar.gz\nsudo install -m 0755 hypershift /usr/local/bin/hypershift\n\n# arm64\ncurl -LO https://github.com/hypershift-community/azure-dev-preview/releases/latest/download/hypershift-linux-arm64.tar.gz\ntar -xzf hypershift-linux-arm64.tar.gz\nsudo install -m 0755 hypershift /usr/local/bin/hypershift\n</code></pre> <pre><code># Intel\ncurl -LO https://github.com/hypershift-community/azure-dev-preview/releases/latest/download/hypershift-darwin-amd64.tar.gz\ntar -xzf hypershift-darwin-amd64.tar.gz\nsudo install -m 0755 hypershift /usr/local/bin/hypershift\n\n# Apple Silicon\ncurl -LO https://github.com/hypershift-community/azure-dev-preview/releases/latest/download/hypershift-darwin-arm64.tar.gz\ntar -xzf hypershift-darwin-arm64.tar.gz\nsudo install -m 0755 hypershift /usr/local/bin/hypershift\n</code></pre> <p>Verify the installation:</p> <pre><code>hypershift version\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>New to HyperShift on Azure? Start with the Understanding guide to learn about the architecture and key concepts.</p>"},{"location":"#two-approaches","title":"Two Approaches","text":"<p>This documentation supports both step-by-step and single-shot workflows using Taskfile tasks:</p> Step-by-stepSingle Shot <p>Run each phase individually to understand what happens at each stage. Recommended for learning.</p> <pre><code>task azure:all        # Create Azure infrastructure\ntask mgmt:install     # Install HyperShift operator\ntask cluster:create   # Create hosted cluster\n</code></pre> <p>Best for: Learning the deployment process, understanding each phase, troubleshooting</p> <p>Run the complete deployment with one command.</p> <pre><code>task setup  # Orchestrates all phases automatically\n</code></pre> <p>Best for: Quick deployment, automation, when already familiar with the process</p> <p>All tasks use the same CLI commands (<code>az</code>, <code>ccoctl</code>, <code>hypershift</code>). The documentation shows the underlying commands in \"Under the hood\" sections for educational purposes.</p>"},{"location":"#documentation-sections","title":"Documentation Sections","text":"<ol> <li>Understanding - Learn HyperShift architecture and Azure-specific implementation</li> <li>Planning - Review prerequisites and plan your deployment</li> <li>Azure Setup - Create OIDC issuer, managed identities, and network infrastructure</li> <li>Management &amp; Hosted Clusters - Install HyperShift operator and create hosted clusters</li> <li>Reference - Troubleshooting guides and complete reference documentation</li> </ol>"},{"location":"#developer-preview","title":"Developer Preview","text":"<p>This is developer preview software. Expect frequent updates and documentation improvements. For issues or questions:</p> <ul> <li>Review the Troubleshooting guide</li> <li>Check HyperShift GitHub Issues</li> <li>Open an issue in this repository for preview-specific questions</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<p>Begin with Understanding HyperShift on Azure</p>"},{"location":"getting-started/","title":"Getting started","text":"<p>HyperShift is middleware for hosting OpenShift control planes at scale that solves for cost and time to provision, as well as portability across cloud service providers with strong separation of concerns between management and workloads. Clusters are fully compliant OpenShift Container Platform (OCP) clusters and are compatible with standard OCP and Kubernetes toolchains.</p> <p>This guide will lead you through the process of creating a new hosted cluster. Throughout the instructions, shell variables are used to indicate values that you should adjust to your own environment.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install the HyperShift CLI (<code>hypershift</code>) using Go 1.19:         <pre><code>git clone https://github.com/openshift/hypershift.git\ncd hypershift\nmake build\nsudo install -m 0755 bin/hypershift /usr/local/bin/hypershift\n</code></pre></li> <li>Admin access to an OpenShift cluster (version 4.12+) specified by the <code>KUBECONFIG</code> environment variable.</li> <li>The OpenShift CLI (<code>oc</code>) or Kubernetes CLI (<code>kubectl</code>). </li> <li>A valid pull secret file for the <code>quay.io/openshift-release-dev</code> repository. </li> <li>An AWS credentials file with permissions to create infrastructure for the cluster. </li> <li> <p>A Route53 public zone for cluster DNS records. To create a public zone:         <pre><code>BASE_DOMAIN=www.example.com\naws route53 create-hosted-zone --name $BASE_DOMAIN --caller-reference $(whoami)-$(date --rfc-3339=date)\n</code></pre></p> <p>Important</p> <p>To access applications in your guest clusters, the public zone must be routable. If the public zone exists, skip  this step. Otherwise, the public zone will affect the existing functions.</p> </li> <li> <p>An S3 bucket with public access to host OIDC discovery documents for your clusters. To create the bucket in us-east-1:         <pre><code>export BUCKET_NAME=your-bucket-name\naws s3api create-bucket --bucket $BUCKET_NAME\naws s3api delete-public-access-block --bucket $BUCKET_NAME\necho '{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": \"arn:aws:s3:::${BUCKET_NAME}/*\"\n    }\n  ]\n}' | envsubst &gt; policy.json\naws s3api put-bucket-policy --bucket $BUCKET_NAME --policy file://policy.json\n</code></pre></p> <p>To create the bucket in a region other than us-east-1:     <pre><code>export BUCKET_NAME=your-bucket-name\nREGION=us-east-2\naws s3api create-bucket --bucket $BUCKET_NAME \\\n  --create-bucket-configuration LocationConstraint=$REGION \\\n  --region $REGION\naws s3api delete-public-access-block --bucket $BUCKET_NAME\necho '{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": \"arn:aws:s3:::${BUCKET_NAME}/*\"\n    }\n  ]\n}' | envsubst &gt; policy.json\naws s3api put-bucket-policy --bucket $BUCKET_NAME --policy file://policy.json\n</code></pre></p> </li> </ol>"},{"location":"getting-started/#install-hypershift-operator","title":"Install HyperShift Operator","text":"<p>Install the HyperShift Operator into the management cluster, specifying the OIDC bucket, its region and credentials to access it (see Prerequisites):</p> <pre><code>REGION=us-east-1\nBUCKET_NAME=your-bucket-name\nAWS_CREDS=\"$HOME/.aws/credentials\"\n\nhypershift install \\\n  --oidc-storage-provider-s3-bucket-name $BUCKET_NAME \\\n  --oidc-storage-provider-s3-credentials $AWS_CREDS \\\n  --oidc-storage-provider-s3-region $REGION \\\n  --enable-defaulting-webhook true\n</code></pre> <p>Note</p> <p><code>enable-defaulting-webhook</code> is only for OCP version 4.14 and higher.</p>"},{"location":"getting-started/#create-a-hosted-cluster","title":"Create a Hosted Cluster","text":"<p>Create a new hosted cluster, specifying the domain of the public zone provided in the Prerequisites:</p> <pre><code>REGION=us-east-1\nCLUSTER_NAME=example\nBASE_DOMAIN=example.com\nAWS_CREDS=\"$HOME/.aws/credentials\"\nPULL_SECRET=\"$HOME/pull-secret\"\n\nhypershift create cluster aws \\\n  --name $CLUSTER_NAME \\\n  --node-pool-replicas=3 \\\n  --base-domain $BASE_DOMAIN \\\n  --pull-secret $PULL_SECRET \\\n  --aws-creds $AWS_CREDS \\\n  --region $REGION \\\n  --generate-ssh\n</code></pre> <p>Important</p> <p>The cluster name (<code>--name</code>) must be unique within the base domain to avoid unexpected and conflicting cluster management behavior.</p> <p>The cluster name must also adhere to the RFC1123 standard.</p> <p>Important</p> <p>You must include either flag, <code>release-image</code> or <code>release-stream</code>, when the <code>enable-defaulting-webhook</code> is not enabled on the installation of the HyperShift operator.</p> <p>Note</p> <p>A default NodePool will be created for the cluster with 3 replicas per the <code>--node-pool-replicas</code> flag. </p> <p>Note</p> <p>The default NodePool name will be a combination of your cluster name and zone name for  AWS (example, <code>example-us-east-1a</code>). For other providers, the default NodePool  name will be the same as the cluster name.</p> <p>Note</p> <p>The <code>--generate-ssh</code> flag is not strictly necessary but it will help in debugging why a node has not joined your cluster.</p> <p>After a few minutes, check the <code>hostedclusters</code> resources in the <code>clusters</code> namespace and when ready it will look similar to the following:</p> <pre><code>oc get --namespace clusters hostedclusters\nNAME      VERSION   KUBECONFIG                 PROGRESS    AVAILABLE   PROGRESSING   MESSAGE\nexample   4.12.0    example-admin-kubeconfig   Completed   True        False         The hosted control plane is available\n\noc get nodepools --namespace clusters\nNAME                 CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE\nexample-us-east-1a   example   2               2               False         False        4.12.0\n</code></pre> <p>Eventually the cluster's kubeconfig will become available and can be printed to standard out using the <code>hypershift</code> CLI:</p> <pre><code>hypershift create kubeconfig\n</code></pre>"},{"location":"getting-started/#create-additional-nodepools","title":"Create Additional NodePools","text":"<p>Create additional NodePools for a cluster by specifying a name, number of replicas and additional information such as instance type.</p> <pre><code>NODEPOOL_NAME=${CLUSTER_NAME}-work\nINSTANCE_TYPE=m5.2xlarge\nNODEPOOL_REPLICAS=2\n\nhypershift create nodepool aws \\\n  --cluster-name $CLUSTER_NAME \\\n  --namespace clusters \\\n  --name $NODEPOOL_NAME \\\n  --replicas $NODEPOOL_REPLICAS \\\n  --instance-type $INSTANCE_TYPE\n</code></pre> <p>Important</p> <p>The default infrastructure created for the cluster during Create a HostedCluster lives in a single availability zone. Any additional NodePool created for that cluster must be in the same availability zone and subnet.</p> <p>Check the status of the NodePool by listing <code>nodepool</code> resources in the <code>clusters</code> namespace:</p> <pre><code>oc get nodepools --namespace clusters\n</code></pre>"},{"location":"getting-started/#scale-a-nodepool","title":"Scale a NodePool","text":"<p>Manually scale a NodePool using the <code>oc scale</code> command:</p> <pre><code>NODEPOOL_NAME=${CLUSTER_NAME}-work\nNODEPOOL_REPLICAS=5\n\noc scale nodepool/$NODEPOOL_NAME \\\n  --namespace clusters \\\n  --replicas=$NODEPOOL_REPLICAS\n</code></pre> <p>Note</p> <p>See the Scale Down section of the NodePool lifecycle page for more details when scaling down NodePools.</p>"},{"location":"getting-started/#delete-a-hosted-cluster","title":"Delete a Hosted Cluster","text":"<p>To delete a Hosted Cluster:</p> <pre><code>hypershift destroy cluster aws \\\n  --name $CLUSTER_NAME \\\n  --aws-creds $AWS_CREDS\n</code></pre>"},{"location":"contribute/","title":"Contribute","text":"<p>Use these resources to contribute to HyperShift.</p> <ul> <li>Contributing guidelines (GitHub)</li> <li>Release Process</li> <li>Custom Images</li> <li>Onboard a Platform</li> <li>Run Tests</li> <li>Develop in Cluster</li> <li>Run hypershift-operator locally</li> <li>CPO Overrides</li> <li>Contribute to docs</li> <li>Pre-commit hook help</li> </ul>"},{"location":"contribute/branch-process/","title":"Branch process","text":""},{"location":"contribute/branch-process/#ocp-branching-tasks-for-the-hypershift-team","title":"OCP Branching Tasks for the HyperShift Team","text":"<p>These are a set of tasks we need to perform on every OCP branching. We need to:</p> <ol> <li>Update the HyperShift Repository to add the latest supported OCP version - Update Supported Version</li> <li>Update the base images in our Dockerfiles (if they are available at branching) - Update Dockerfiles</li> <li>Update the Renovate configuration to include the new release branch - Update Renovate</li> <li>Update the OpenShift Release repository to fix the step registry configuration files - OpenShift/Release</li> <li>Update TestGrid to include the new OCP version tests - TestGrid</li> </ol> <p>Danger</p> <p>If test platform are testing new OCP releases before the release is cut the hypershift test will fail and block payloads until:</p> <ul> <li>There are at least two accepted nightly payloads for the new release.</li> <li>The supported versions in the HyperShift repository are updated.</li> </ul>"},{"location":"contribute/branch-process/#hypershift-repository","title":"HyperShift Repository","text":""},{"location":"contribute/branch-process/#update-supported-version","title":"Update Supported Version","text":"<p>We need to add the latest supported version in the <code>hypershift</code> repository. We need to modify two files:</p> <ul> <li><code>support/supportedversion/version.go</code> which contains the variable called <code>LatestSupportedVersion</code>. This one contains, as you can imagine, the Latest supported version. We need to put the new version here.</li> <li><code>support/supportedversion/version_test.go</code> contains the tests to validate the Latest version. It should comply with the established contract to support 2 versions prior to the Latest.</li> </ul> <p>Example Supported Version Bump PR</p>"},{"location":"contribute/branch-process/#update-dockerfiles","title":"Update Dockerfiles","text":"<p>We also need to bump the base images in our Dockerfiles.</p> <p>Example Base Image Bump PR</p>"},{"location":"contribute/branch-process/#update-renovate-configuration","title":"Update Renovate Configuration","text":"<p>We need to add the new release branch to the Renovate configuration so that security updates are automatically applied to the release branch.</p> <p>Update <code>renovate.json</code> to include the new release branch in two places:</p> <ol> <li>Add the new branch to the <code>baseBranches</code> array at the top of the file</li> <li>Add the new branch to the <code>matchBaseBranches</code> array in the security-only Go updates package rule</li> </ol> <p>Note</p> <p>If any release branch has reached End of Life and is no longer supported, remove it from both locations in <code>renovate.json</code> to stop automated updates on that branch.</p> <p>Example change for release-4.21: <pre><code>{\n  \"baseBranches\": [\n    \"main\",\n    \"release-4.16\",\n    \"release-4.17\",\n    \"release-4.18\",\n    \"release-4.19\",\n    \"release-4.20\",\n    \"release-4.21\"\n  ],\n  \"packageRules\": [\n    {\n      \"description\": \"Enable security-only Go updates on release branches\",\n      \"matchManagers\": [\"gomod\"],\n      \"matchBaseBranches\": [\n        \"release-4.16\",\n        \"release-4.17\",\n        \"release-4.18\",\n        \"release-4.19\",\n        \"release-4.20\",\n        \"release-4.21\"\n      ],\n      \"matchUpdateTypes\": [\"patch\"]\n    }\n  ]\n}\n</code></pre></p>"},{"location":"contribute/branch-process/#openshiftrelease-repository","title":"Openshift/Release Repository","text":"<p>The Step registry config should be updated by Test Platform. However, the Test Platform is not aware of custom configurations of the different version for specific hypershift tests. So, we need to check over the Step registry config and make sure that the hypershift tests are correctly configured. Below is an example of the necessary changes to the Step registry config after test platform bumps:</p> <p>Example Release Repo PR</p> <p>We should also ensure that the latest release branch is using the Hypershift Operator and e2e from main.</p> <p>Example Release Branch PR</p>"},{"location":"contribute/branch-process/#update-testgrid","title":"Update TestGrid","text":"<p>We need to update TestGrid to include the new OCP version tests. </p> <p>Here is an Example PR to do that.</p>"},{"location":"contribute/contribute-docs/","title":"Contributing documentation","text":"<p>HyperShift's documentation is based on MkDocs with the Material theme and roughly follows the Di\u00e1taxis Framework for content organization and stylistic approach.</p> <p>The documentation site is built and published automatically to https://hypershift.pages.dev/.</p>"},{"location":"contribute/contribute-docs/#overview","title":"Overview","text":"<p>All documentation lives in the <code>docs</code> directory of the Git repository.</p> <p>All content should be Markdown files placed in the <code>docs/content</code> directory. The MkDocs configuration file contains all the MkDocs and Material theme configuration, including the navigation structure for the site.</p> <p>The <code>quay.io/hypershift/mkdocs-material:latest</code> image (Dockerfile) is published to provide an easy and portable way to run <code>mkdocs</code> fully configured to preview the site equivalent to the published site.</p> <p>Note</p> <p>The <code>README.md</code> file in the repository root is a minimal overview which quickly links users to the latest published documentation. Most content should go in the docs.</p> <p>Important</p> <p>The API reference is generated automatically. Do not edit it manually. See the API generation section section for details.</p>"},{"location":"contribute/contribute-docs/#preview-the-site-locally","title":"Preview the site locally","text":"<p>To start a live preview of the site which automatically rebuilds and refreshes in response to local content and configuration changes, run the following from the <code>docs</code> directory:</p> <pre><code>make serve-containerized\n</code></pre> <p>Visit the site at http://0.0.0.0:8000.</p> <p>Note</p> <p>The <code>serve-containerized</code> Make target runs the <code>quay.io/hypershift/mkdocs-material:latest</code> image with the local container runtime. Running <code>mkdocs</code> natively is possible but not supported.</p> <p>If you need more control over the local preview server, consult the Makefile as a guide to constructing your own local server command.</p>"},{"location":"contribute/contribute-docs/#generate-the-api-reference","title":"Generate the API reference","text":"<p>The API reference is automatically generated by the <code>gen-crd-api-reference-docs</code> tool.</p> <p>The <code>gen-crd-api-reference-docs</code> tool processes the HyperShift API Go type definitions and reads the Kubernetes Custom Resource Definition metadata associated with the API types. Then <code>gen-crd-api-reference-docs</code> executes a Go template which is provided with context about the processed Go packages. The output of template execution is the <code>docs/content/reference/api.md</code> file, which contains the API reference documentation content.</p> <ul> <li>To change documentation of specific API types, edit the API Go type definitions.</li> <li>To change the structure of the API reference page itself, edit the <code>gen-crd-api-reference-docs</code> Go templates.</li> </ul> <p>To run the API reference docs generator, run the following from the HyperShift Git repository root:</p> <pre><code>make api-docs\n</code></pre>"},{"location":"contribute/cpo-overrides/","title":"CPO Overrides Configuration Guide","text":""},{"location":"contribute/cpo-overrides/#overview","title":"Overview","text":"<p>The CPO (Control Plane Operator) overrides mechanism allows you to specify custom Control Plane Operator images for specific OpenShift versions and platforms. This is particularly useful for applying hotfixes and patches before they're officially released.</p>"},{"location":"contribute/cpo-overrides/#configuration-file-structure","title":"Configuration File Structure","text":"<p>The CPO overrides are configured using a YAML file with the following structure:</p> <pre><code>platforms:\n  aws:\n    overrides:\n      - version: \"4.17.9\"\n        cpoImage: \"quay.io/hypershift/control-plane-operator:4.17.9\"\n      - version: \"4.17.8\"\n        cpoImage: \"quay.io/hypershift/control-plane-operator:4.17.8\"\n    testing:\n      latest: \"quay.io/openshift-release-dev/ocp-release:4.17.9-x86_64\"\n      previous: \"quay.io/openshift-release-dev/ocp-release:4.17.8-x86_64\"\n  azure:\n    overrides:\n      - version: \"4.16.17\"\n        cpoImage: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1a50894aafa6b750bf890ef147a20699ff5b807e586d15506426a8a615580797\"\n      - version: \"4.16.18\"\n        cpoImage: \"quay.io/hypershift/hypershift-cpo:patch\"\n</code></pre>"},{"location":"contribute/cpo-overrides/#configuration-elements","title":"Configuration Elements","text":""},{"location":"contribute/cpo-overrides/#platforms-section","title":"Platforms Section","text":"<p>The top-level <code>platforms</code> section contains platform-specific configurations. Currently supported platforms:</p> <ul> <li><code>aws</code>: Amazon Web Services</li> <li><code>azure</code>: Microsoft Azure</li> </ul> <p>Each platform section is optional. If a platform is not configured, the default CPO image from the OpenShift release will be used.</p>"},{"location":"contribute/cpo-overrides/#platform-configuration","title":"Platform Configuration","text":"<p>Each platform can contain:</p>"},{"location":"contribute/cpo-overrides/#overrides-array","title":"<code>overrides</code> (Array)","text":"<p>A list of version-specific CPO image overrides:</p> <ul> <li><code>version</code> (string): The exact OpenShift version (e.g., \"4.17.9\", \"4.16.17\")</li> <li><code>cpoImage</code> (string): The full container image reference to use for this version</li> </ul>"},{"location":"contribute/cpo-overrides/#testing-object-optional","title":"<code>testing</code> (Object, Optional)","text":"<p>Configuration for automated testing:</p> <ul> <li><code>latest</code> (string): The latest OpenShift release image for testing</li> <li><code>previous</code> (string): The previous OpenShift release image for testing</li> </ul>"},{"location":"contribute/cpo-overrides/#configuration-examples","title":"Configuration Examples","text":""},{"location":"contribute/cpo-overrides/#basic-platform-specific-override","title":"Basic Platform-Specific Override","text":"<pre><code>platforms:\n  aws:\n    overrides:\n      - version: \"4.17.9\"\n        cpoImage: \"quay.io/myorg/custom-cpo-aws:4.17.9\"\n</code></pre>"},{"location":"contribute/cpo-overrides/#multiple-versions-and-platforms","title":"Multiple Versions and Platforms","text":"<pre><code>platforms:\n  aws:\n    overrides:\n      - version: \"4.17.9\"\n        cpoImage: \"quay.io/hypershift/control-plane-operator-aws:4.17.9\"\n      - version: \"4.17.8\"\n        cpoImage: \"quay.io/hypershift/control-plane-operator-aws:4.17.8\"\n      - version: \"4.16.15\"\n        cpoImage: \"quay.io/hotfix/cpo-aws:4.16.15-hotfix\"\n    testing:\n      latest: \"quay.io/openshift-release-dev/ocp-release:4.17.9-x86_64\"\n      previous: \"quay.io/openshift-release-dev/ocp-release:4.17.8-x86_64\"\n  azure:\n    overrides:\n      - version: \"4.17.9\"\n        cpoImage: \"quay.io/hypershift/control-plane-operator-azure:4.17.9\"\n      - version: \"4.16.20\"\n        cpoImage: \"quay.io/security-fix/cpo-azure:4.16.20-security\"\n</code></pre>"},{"location":"contribute/cpo-overrides/#how-it-works","title":"How It Works","text":""},{"location":"contribute/cpo-overrides/#image-resolution-process","title":"Image Resolution Process","text":"<ol> <li>Override Check: When a HostedCluster is created or updated, the system checks if CPO overrides are enabled</li> <li>Platform Lookup: The system looks up overrides for the specific platform (case-insensitive)</li> <li>Version Match: If the platform exists, it searches for an exact version match</li> <li>Fallback: If no override is found, the default CPO image from the OpenShift release is used</li> </ol>"},{"location":"contribute/cpo-overrides/#platform-handling","title":"Platform Handling","text":"<ul> <li>Platform names are case-insensitive (<code>AWS</code>, <code>aws</code>, <code>Aws</code> all work)</li> <li>Unknown platforms return empty string (graceful degradation)</li> <li>Missing platforms in configuration are handled safely (no panics)</li> </ul>"},{"location":"contribute/cpo-overrides/#enabling-cpo-overrides","title":"Enabling CPO Overrides","text":"<p>CPO overrides are controlled by an environment variable on the HyperShift operator:</p> <pre><code>export ENABLE_CPO_OVERRIDES=1\n</code></pre> <p>When this environment variable is set to <code>1</code>, the override system is activated. Without this variable, all overrides are ignored and default images are used.</p>"},{"location":"contribute/cpo-overrides/#file-locations","title":"File Locations","text":"<p>The override configuration files are embedded in the binary at build time:</p> <p><code>hypershift-operator/controlplaneoperator-overrides/assets/overrides.yaml</code></p>"},{"location":"contribute/cpo-overrides/#best-practices","title":"Best Practices","text":""},{"location":"contribute/cpo-overrides/#version-management","title":"Version Management","text":"<ul> <li>Use exact version strings (e.g., \"4.17.9\", not \"4.17.x\")</li> <li>Keep override lists sorted by version for maintainability</li> </ul>"},{"location":"contribute/cpo-overrides/#image-references","title":"Image References","text":"<ul> <li>Use full image references including registry and tag/digest</li> <li>Prefer digest references for production overrides for immutability</li> </ul>"},{"location":"contribute/cpo-overrides/#platform-configuration_1","title":"Platform Configuration","text":"<ul> <li>Only configure platforms that need overrides</li> <li>Keep platform-specific testing configurations separate</li> <li>Document the purpose of each override in comments</li> </ul>"},{"location":"contribute/cpo-overrides/#testing","title":"Testing","text":"<ul> <li>The test section allows specifying a pair of release images to use for a single hypershift e2e test run. Currently only a single pair can be specified per platform. If multiple releases need to be tested, create a clone of your override PR and specify a different pair of images to test in the clone. The clone shouldn't be merged, it should only be used for testing.</li> </ul>"},{"location":"contribute/cpo-overrides/#troubleshooting","title":"Troubleshooting","text":""},{"location":"contribute/cpo-overrides/#common-issues","title":"Common Issues","text":"<ol> <li>Override Not Applied</li> <li>Check if <code>ENABLE_CPO_OVERRIDES=1</code> is set</li> <li>Verify platform name matches exactly (case-insensitive)</li> <li> <p>Confirm version string is exact match</p> </li> <li> <p>Image Pull Failures</p> </li> <li>Verify image exists and is accessible</li> <li>Check image registry authentication</li> <li> <p>Validate image digest/tag is correct</p> </li> <li> <p>Platform Not Found</p> </li> <li>Returns empty string (safe fallback)</li> <li>Check platform configuration in YAML</li> <li>Verify platform name spelling</li> </ol>"},{"location":"contribute/cpo-overrides/#securityproduction-considerations","title":"Security/Production Considerations","text":"<ul> <li>Only use trusted image registries for CPO overrides</li> <li>Validate override images are signed and verified</li> <li>If multiple architectures are supported in the data plane, ensure that override image references point to multi-arch repositories that have all architectures supported in the data plane.</li> </ul>"},{"location":"contribute/custom-images/","title":"How to install HyperShift with a custom image","text":"<ol> <li> <p>Build and push a custom image build to your own repository.</p> <pre><code>  export QUAY_ACCOUNT=example\n\n  make build\n  make RUNTIME=podman IMG=quay.io/${QUAY_ACCOUNT}/hypershift:latest docker-build docker-push\n</code></pre> </li> <li> <p>Install HyperShift using the custom image:</p> <pre><code>  hypershift install \\\n    --oidc-storage-provider-s3-bucket-name $BUCKET_NAME \\\n    --oidc-storage-provider-s3-credentials $AWS_CREDS \\\n    --oidc-storage-provider-s3-region $REGION \\\n    --hypershift-image quay.io/${QUAY_ACCOUNT}/hypershift:latest \\\n</code></pre> </li> <li> <p>(Optional) If your repository is private, create a secret:</p> <pre><code>  oc create secret --namespace hypershift generic hypershift-operator-pull-secret \\\n    --from-file=.dockerconfig=/my/pull-secret --type=kubernetes.io/dockerconfig\n</code></pre> <p>Then update the operator ServiceAccount in the hypershift namespace:</p> <pre><code>  oc patch serviceaccount --namespace hypershift operator \\\n    -p '{\"imagePullSecrets\": [{\"name\": \"hypershift-operator-pull-secret\"}]}'\n</code></pre> </li> </ol>"},{"location":"contribute/develop_in_cluster/","title":"How to develop HyperShift components in-cluster","text":"<p>Sometimes when developing HyperShift components it's useful to iterate on new binary builds inside the cluster itself, especially when working on functionality that depends on the Kubernetes or cloud environment for one reason or another.</p> <p>Because such in-cluster build/image/publish/redeploy development workflows can be very tedious and slow, the HyperShift project includes a few tools and techniques to help make the feedback loop as fast as possible.</p> <p>This guide makes use of the ko tool to rapidly build lightweight images which are then published directly into an OCP cluster's internal registry. This approach has the following properties which can speed up development:</p> <ul> <li>No local container runtime required to build images, and image builds are   extremely fast.</li> <li>Resulting images are almost as small as the Go binary being published.</li> <li>Images are published directly into OCP's internal image registry, so images   are immediately available on or near the machines that will be pulling them.</li> </ul>"},{"location":"contribute/develop_in_cluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>An OCP 4.9+ cluster</li> <li>The <code>oc</code> CLI tool</li> <li>The ko CLI tool</li> </ul> <p>For this workflow, the OCP cluster must be configured to expose its internal image registry externally so the <code>ko</code> tool can publish to it.</p> <p>First, expose the cluster's image registry:</p> <pre><code>oc patch configs.imageregistry.operator.openshift.io/cluster --patch '{\"spec\":{\"defaultRoute\":true}}' --type=merge\n</code></pre> <p>Next, generate an authentication token for the registry and install it into the local Docker config file so that <code>ko</code> can push images into the registry. Be sure to replace <code>&lt;password&gt;</code> with your actual <code>kubeadmin</code> password.</p> <pre><code>oc login -u kubeadmin -p &lt;password&gt;\noc registry login --to=$HOME/.docker/config.json --skip-check --registry $(oc get routes --namespace openshift-image-registry default-route -o jsonpath='{.spec.host}')\n</code></pre> <p>Finally, configure OCP to allow any authenticated user to pull images from the internal registry. This will enable HyperShift component pods to pull the custom images you publish.</p> <pre><code>oc create clusterrolebinding authenticated-registry-viewer --clusterrole registry-viewer --group system:authenticated\n</code></pre>"},{"location":"contribute/develop_in_cluster/#build-and-publish-a-component","title":"Build and publish a component","text":"<p>To build and publish a given component into the OCP cluster from local source, use the <code>publish-ocp.sh</code> script. This tool uses <code>ko</code> to build and publish the image, and will output to stdout a single line containing the internal pullspec suitable for use by any HyperShift component deployment.</p> <p>For example, to build and publishing the <code>hypershift-operator</code>, run:</p> <pre><code>hack/publish-ocp.sh ./hypershift-operator\n</code></pre> <p>Here's what the output will look like:</p> <pre><code>2021/12/01 16:49:54 Using base gcr.io/distroless/static:nonroot for github.com/openshift/hypershift/hypershift-operator\n2021/12/01 16:49:55 Building github.com/openshift/hypershift/hypershift-operator for linux/amd64\n2021/12/01 16:50:02 Publishing default-route-openshift-image-registry.apps.dmace-7894.devcluster.openshift.com/hypershift/hypershift-operator-cd22693e35e87f2323fb625057793c02:latest\n2021/12/01 16:50:02 existing blob: sha256:250c06f7c38e52dc77e5c7586c3e40280dc7ff9bb9007c396e06d96736cf8542\n2021/12/01 16:50:02 existing blob: sha256:e8614d09b7bebabd9d8a450f44e88a8807c98a438a2ddd63146865286b132d1b\n2021/12/01 16:50:02 existing blob: sha256:cde5c5024aed9d3daaa3cd7b87fa21a66b10d2f2e8a1b9d339e2fb505cbde8c0\n2021/12/01 16:50:02 existing blob: sha256:a589e39bc5bc084dd0ec79f5492ff9dc2ac6dbbb5fb95eb200b319246a7b8207\n2021/12/01 16:50:03 default-route-openshift-image-registry.apps.dmace-7894.devcluster.openshift.com/hypershift/hypershift-operator-cd22693e35e87f2323fb625057793c02:latest: digest: sha256:20b0baf90c58a92a5e384eaa8d40cd47cc1c8cabce27bedccd7bbc2f54ca4c5b size: 953\n2021/12/01 16:50:03 Published default-route-openshift-image-registry.apps.dmace-7894.devcluster.openshift.com/hypershift/hypershift-operator-cd22693e35e87f2323fb625057793c02@sha256:20b0baf90c58a92a5e384eaa8d40cd47cc1c8cabce27bedccd7bbc2f54ca4c5b\nimage-registry.openshift-image-registry.svc:5000/hypershift/hypershift-operator-cd22693e35e87f2323fb625057793c02@sha256:20b0baf90c58a92a5e384eaa8d40cd47cc1c8cabce27bedccd7bbc2f54ca4c5b\n</code></pre> <p>The <code>publish-ocp.sh</code> script prints only the internal repo pullspec to stdout to make it easy to incorporate the script into pipelines.</p> <p>Note</p> <p>Notice on line 9 that public pullspec of the image is <code>default-route-openshift-image-registry.apps.dmace-7894.devcluster.openshift.com/hypershift/hypershift-operator-cd22...</code>. Pods in the cluster cannot pull the image using the public repo name because the host's certificate is likely self-signed, which would require additional configuration in the cluster to enable pods to pull it.</p> <p>Pods must reference the internal repo pullspec as printed to stdout on line 10: <code>image-registry.openshift-image-registry.svc:5000/hypershift/hypershift-operator-cd22...</code>.</p>"},{"location":"contribute/develop_in_cluster/#launch-a-custom-hypershift-operator-image-interactively","title":"Launch a custom <code>hypershift-operator</code> image interactively","text":"<p>To iterate on the <code>hypershift-operator</code> binary in-cluster interactively, first scale down the operator's deployment:</p> <pre><code>oc scale --replicas 0 --namespace hypershift deployments/operator\n</code></pre> <p>Alternatively, run the HyperShift CLI <code>install</code> command with the <code>--development</code> flag which sets up the deployment with zero replicas:</p> <pre><code>go run . install \\\n  --oidc-storage-provider-s3-bucket-name=$BUCKET_NAME \\\n  --oidc-storage-provider-s3-region=$BUCKET_REGION \\\n  --oidc-storage-provider-s3-credentials=$AWS_CREDS \\\n  --development\n</code></pre> <p>Now, you can build and publish the <code>hypershift-operator</code> image and run it interactively in a single shot using <code>publish-ocp.sh</code> together with the <code>oc debug</code> command:</p> <pre><code>oc debug --namespace hypershift deployments/operator --image $(hack/publish-ocp.sh ./hypershift-operator) -- \\\n  /ko-app/hypershift-operator run \\\n  --oidc-storage-provider-s3-region $BUCKET_REGION \\\n  --oidc-storage-provider-s3-bucket-name $BUCKET_NAME \\\n  --oidc-storage-provider-s3-credentials /etc/oidc-storage-provider-s3-creds/credentials \\\n  --namespace hypershift \\\n  --pod-name operator-debug\n</code></pre> <p>Note</p> <p>Make sure to replace <code>$BUCKET_NAME</code> and <code>$BUCKET_REGION</code> with the same values used to install HyperShift.</p> <p>Your latest code should be deployed and logs should soon begin streaming. Just press <code>ctrl-c</code> to terminate and delete the pod.</p> <p>Note</p> <p>See Use custom operator images to use your own registry.</p>"},{"location":"contribute/develop_in_cluster/#configure-a-hostedcluster-for-iterative-control-plane-development","title":"Configure a HostedCluster for iterative control plane development","text":"<p>To iterate on control plane components which are deployed and managed in a <code>HostedCluster</code> control plane namespace (e.g. the <code>control-plane-operator</code> or <code>ignition-server</code>), it's possible to configure the <code>HostedCluster</code> resource to scale down individual control plane components and facilitate various development workflows.</p> <p>The <code>hypershift.openshift.io/debug-deployments</code> annotation on a <code>HostedCluster</code> is used to configure individual control plane components as targets for development and debugging. The value of the annotation is a comma-delimited list of control plane deployment names. Any control plane component in the list will always be scaled to 0, enabling developers to replace the components with their own processes (inside or outside the cluster) while preserving the <code>Deployment</code> resources to use as templates for the replacement process environments.</p> <p>For example, to scale the <code>control-plane-operator</code> and <code>ignition-server</code> deployments to 0:</p> <pre><code>oc annotate -n clusters HostedCluster test-cluster hypershift.openshift.io/debug-deployments=control-plane-operator,ignition-server\n</code></pre> <p>Note</p> <p>Update the name of the HostedCluster to match your cluster.</p> <p>This will result in a <code>HostedCluster</code> like so:</p> <pre><code>apiVersion: hypershift.openshift.io/v1alpha1\nkind: HostedCluster\nmetadata:\n  annotations:\n    hypershift.openshift.io/debug-deployments: control-plane-operator,ignition-server\n  namespace: clusters\n  name: test\nspec:\n  release:\n    image: quay.io/openshift-release-dev/ocp-release:4.9.0-x86_64\n# &lt;remainder of resource omitted&gt;\n</code></pre> <p>To scale back up a given component's original deployment simply remove the component's deployment name from the list.</p> <p>The <code>hypershift.openshift.io/pod-security-admission-label-override</code> annotation may also need to be set in order to run debug pods locally.</p> <pre><code>oc annotate -n clusters HostedCluster test-cluster hypershift.openshift.io/pod-security-admission-label-override=baseline\n</code></pre>"},{"location":"contribute/develop_in_cluster/#launch-a-custom-control-plane-operator-image-interactively","title":"Launch a custom <code>control-plane-operator</code> image interactively","text":"<p>To iterate on the <code>control-plane-operator</code> binary in-cluster interactively, first configure the HostedCluster to scale down the <code>control-plane-operator</code> deployment.</p> <p>Now, you can build and publish the <code>control-plane-operator</code> image and run it interactively in a single shot using <code>publish-ocp.sh</code> together with the <code>oc debug</code> command. Be sure to replace <code>$NAMESPACE</code> with the namespace of the control plane that was deployed for the <code>HostedCluster</code>.</p> <pre><code>oc debug --namespace $NAMESPACE deployments/control-plane-operator --image $(hack/publish-ocp.sh ./control-plane-operator) -- /ko-app/control-plane-operator run\n</code></pre> <p>Your latest code should be deployed and logs should soon begin streaming. Just press <code>ctrl-c</code> to terminate and delete the pod.</p> <p>Note</p> <p>The default arguments to <code>control-plane-operator run</code> should be sufficient to get started.</p>"},{"location":"contribute/develop_in_cluster/#launch-a-custom-ignition-server-interactively","title":"Launch a custom <code>ignition-server</code> interactively","text":"<p>To iterate on the ignition server in-cluster interactively, first configure the HostedCluster to scale down the <code>ignition-server</code> deployment.</p> <p>Now, you can build and publish the <code>control-plane-operator</code> image and run the <code>ignition-server</code> command interactively in a single shot using <code>publish-ocp.sh</code> together with the <code>oc debug</code> command. Be sure to replace <code>$NAMESPACE</code> with the namespace of the control plane that was deployed for the <code>HostedCluster</code>.</p> <pre><code>oc debug --namespace $NAMESPACE deployments/ignition-server --image $(hack/publish-ocp.sh ./control-plane-operator) -- /ko-app/control-plane-operator ignition-server\n</code></pre> <p>Your latest code should be deployed and logs should soon begin streaming. Just press <code>ctrl-c</code> to terminate and delete the pod.</p> <p>Note</p> <p>The default arguments to <code>ignition-server</code> should be sufficient to get started.</p>"},{"location":"contribute/onboard-a-platform/","title":"How to extend HyperShift to support a new platform","text":"<p>A Platform represents a series of assumptions and choices that HyperShift makes about the environment where it's running, e.g AWS, IBMCloud, Kubevirt. The implementation of a new platform crosses multiple controllers.</p> <p>The HostedCluster controller requires an implementation of the Platform interface to shim a particular CAPI implementation and manage required cloud credentials.</p> <p>The NodePool controller requires an implementation of the machine template reconciliation.</p> <p>The ControlPlane Operator requires the following:</p> <ul> <li> <p>Implement cloud credentials</p> </li> <li> <p>Reconcile Kubernetes cloud provider config</p> </li> <li> <p>Reconcile the OCP Infrastructure CR</p> </li> <li> <p>Reconcile secret encryption (if your provider supports KMS)</p> </li> </ul>"},{"location":"contribute/onboard-a-platform/#end-to-end-testing","title":"End to end testing","text":"<p>The end-to-end tests require an implementation of the Cluster interface. As a starting point, check out the None-Platform implementation and its basic test</p>"},{"location":"contribute/onboard-a-platform/#supported-platforms","title":"Supported platforms","text":"<ul> <li>AWS.</li> </ul>"},{"location":"contribute/precommit-hook-help/","title":"General Help on Using precommit Hooks in the HyperShift Repo","text":"<p>precommit hooks are helpful in catching issues prior to any new code or pull request appearing in the HyperShift repo.  In the long run, the precommit hooks will help you save time by catching issues that would normally cause the <code>verify</code>  and <code>unit</code> tests fail on your pull request. The following sections will walk you through how to quickly install the  hooks, quickly uninstall the hooks, and how to bypass the hooks.</p>"},{"location":"contribute/precommit-hook-help/#installing-precommit-hooks","title":"Installing precommit hooks","text":"<p>Once you have precommit installed on your machine(see this for more info), it's quite simple to install the precommit hooks.</p> <pre><code>% pre-commit install\npre-commit installed at .git/hooks/pre-commit\npre-commit installed at .git/hooks/pre-push\n</code></pre> <p>The jobs ran at the pre-commit and pre-push stages are defined in the .golangci.yml file at the base of the HyperShift  repo.</p>"},{"location":"contribute/precommit-hook-help/#uninstalling-precommit-hooks","title":"Uninstalling precommit hooks","text":"<p>Sometimes it might be useful to turn off the precommit hooks briefly.</p> <pre><code>% pre-commit uninstall\npre-commit uninstalled\npre-push uninstalled\n</code></pre>"},{"location":"contribute/precommit-hook-help/#bypassing-precommit-hooks","title":"Bypassing precommit hooks","text":"<p>Sometimes you may want to bypass the precommit hooks on a <code>git push</code> command, for example, if you just updated something really minor, updating your local <code>main</code> branch, or just needed to rerun a <code>go mod tidy</code> command, etc. To ignore the <code>pre-push</code> hooks, just add the <code>--no-verify</code> flag to your command.</p> <pre><code>% git push --set-upstream origin remove-autorest --no-verify \n% git push -f --no-verify\n</code></pre>"},{"location":"contribute/release-process/","title":"HO/CPO Release process","text":"<p>Important</p> <p>This is a complex process that involves some changes in multiple repositories and will affect multiple teams daily basis work. Make sure you have multiple reviewers from Core dev team which could guide you in the full process.</p>"},{"location":"contribute/release-process/#preparing-a-release-in-openshifthypershift-repository","title":"Preparing a release in Openshift/Hypershift repository","text":""},{"location":"contribute/release-process/#bumping-release-version-and-generating-release-notes","title":"Bumping release version and generating Release Notes","text":"<p>The hypershift repo produces two different artifacts: Hypershift Operator (HO) and Control Plane Operator (CPO).</p> <p>The CPO release lifecycle is dictated by the OCP release payload.</p> <p>The HO has an independent release cadence. For consumer products:</p> <ul> <li>Our internal image build system builds from our latest commit in main several times a day.</li> <li>To roll out a new build we apply the following process:</li> <li>Create a git tag for the commit belonging to the image to be rolled out:<ul> <li><code>git co $commit-sha</code></li> <li><code>git tag v0.1.1</code></li> <li>Push against remote.</li> </ul> </li> <li>Generate release notes:<ul> <li><code>FROM=v0.1.0 TO=v0.1.1 make release</code></li> <li>Use the output to create the PR for bump the new image in the product gitOps repo. E.g.</li> </ul> </li> </ul> <p>This is a sample of how the release notes looks like added to the PR:</p> <pre><code>## area/control-plane-operator\n\n- [cpo: cno: follow image name change in release payload](https://github.com/openshift/hypershift/pull/2230)\n\n## area/hypershift-operator\n\n- [Added documentation around supported-versions configmap](https://github.com/openshift/hypershift/pull/2220)\n- [Add comment for BaseDomainPrefix](https://github.com/openshift/hypershift/pull/2219)\n- [Add condition to NodePool indicating whether a security group for it is available](https://github.com/openshift/hypershift/pull/2216)\n- [HOSTEDCP-827: Add root volume encryption e2e test](https://github.com/openshift/hypershift/pull/2192)\n- [fix(hypershift): reduce CAPI rbac access](https://github.com/openshift/hypershift/pull/2173)\n- [Validate Network Input for HostedCluster](https://github.com/openshift/hypershift/pull/2215)\n</code></pre>"},{"location":"contribute/run-hypershift-operator-locally/","title":"Run the hypershift-operator locally","text":""},{"location":"contribute/run-hypershift-operator-locally/#run-the-hypershift-operator-locally","title":"Run the HyperShift Operator locally","text":"<p>To run the HyperShift Operator locally, follow these steps:</p> <ol> <li>Ensure that the <code>KUBECONFIG</code> environment variable is set to a management cluster where HyperShift has not been installed yet.</li> </ol> <pre><code> export KUBECONFIG=\"/path/to/your/kubeconfig\"\n</code></pre> <ol> <li>Build HyperShift.</li> </ol> <p>Note</p> <p>`requires go v1.22+</p> <pre><code>  make build\n</code></pre> <ol> <li>Set the necessary environment variables</li> </ol> <pre><code>  export HYPERSHIFT_REGION=\"your-region\"\n  export HYPERSHIFT_BUCKET_NAME=\"your-bucket\"\n</code></pre> <p>Note</p> <p>`Consider setting HYPERSHIFT_REGION and HYPERSHIFT_BUCKET_NAME in your shell init script (e.g., $HOME/.bashrc).</p> <p>Note</p> <p>`Default values are provided for HYPERSHIFT_REGION and HYPERSHIFT_BUCKET_NAME so Step #4 will function without requiring you to export any values.</p> <ol> <li>Install HyperShift in development mode which causes the operator deployment to be deployment scaled to zero so that it doesn't conflict with your local operator process (see Prerequisites):</li> </ol> <pre><code>  make hypershift-install-aws-dev\n</code></pre> <ol> <li>Run the HyperShift operator locally.</li> </ol> <pre><code>  make run-operator-locally-aws-dev\n</code></pre>"},{"location":"contribute/run-tests/","title":"How to run all e2e tests","text":"<ol> <li>Install HyperShift.</li> <li>Run all tests:<pre><code>make e2e\nbin/test-e2e -test.v -test.timeout 0 \\\n--e2e.aws-credentials-file /my/aws-credentials \\\n--e2e.pull-secret-file /my/pull-secret \\\n--e2e.base-domain my-basedomain \\\n--e2e.aws-oidc-s3-bucket-name my-bucket-name\n</code></pre> </li> </ol> <p>Note if the s3 bucket is not in the <code>us-east-1</code> region then you need to add the <code>--e2e.aws-region</code> flag.</p>"},{"location":"contribute/run-tests/#how-to-run-e2e-tests-for-the-none-platform","title":"How to run e2e tests for the \"None\" platform","text":"<pre><code>    make e2e\n    bin/test-e2e -test.v -test.timeout 0 \\\n    --e2e.pull-secret-file /my/pull-secret \\\n    --e2e.base-domain my-basedomain -test.run='^TestNone.*'\n</code></pre>"},{"location":"how-to/","title":"How-to guides","text":"<p>This section of the HyperShift documentation contains pages that show how to do individual tasks. A how-to page shows how to do a single thing, typically by giving a short sequence of steps.</p>"},{"location":"how-to/distribute-hosted-cluster-workloads/","title":"Distribute Hosted Cluster workloads","text":""},{"location":"how-to/distribute-hosted-cluster-workloads/#topology","title":"Topology","text":"<p>HyperShift enables implementing the strategy to colocate and isolate pods for Hosted Clusters. As a management cluster operator you can leverage the following Node labels and taints:</p> <p><code>hypershift.openshift.io/control-plane: true</code></p> <p><code>hypershift.openshift.io/cluster: ${HostedControlPlane Namespace}</code></p> <ul> <li>Pods for a Hosted Cluster tolerate taints for \"control-plane\" and \"cluster\".</li> <li>Pods for a Hosted Cluster prefer to be scheduled into \"control-plane\" Nodes.</li> <li>Pods for a Hosted Cluster prefer to be scheduled into their own \"cluster\" Nodes.</li> </ul> <p>In addition:</p> <ul> <li>Pods for a Hosted Cluster prefer to be scheduled into the same Node.</li> <li>If the <code>ControllerAvailabilityPolicy</code> is <code>HighlyAvailable</code> Pods for each Deployment within a Hosted Cluster will require to be scheduled across different failure domains by setting <code>topology.kubernetes.io/zone</code> as the topology key.</li> <li>A HostedCluster can require their Pods to be scheduled into particular Nodes by setting <code>HostedCluster.spec.nodeSelector</code>. E.g <pre><code>  spec:\n    nodeSelector:\n      role.kubernetes.io/infra: \"\" \n</code></pre></li> </ul>"},{"location":"how-to/distribute-hosted-cluster-workloads/#custom-taints-and-tolerations","title":"Custom Taints and Tolerations","text":"<p>By default, pods for a Hosted Cluster tolerate the \"control-plane\" and \"cluster\" taints, however it is also possible to use custom taints on nodes and allow Hosted Clusters to tolerate those taints on a per Hosted Cluster basis by setting <code>HostedCluster.spec.tolerations</code>. E.g <pre><code>  spec:\n    tolerations:\n    - effect: NoSchedule\n      key: kubernetes.io/custom\n      operator: Exists\n</code></pre></p> <p>Tolerations can also be set on the Hosted Cluster while creating a cluster using the <code>--tolerations</code> hcp cli argument. E.g <pre><code>--toleration=\"key=kubernetes.io/custom,operator=Exists,effect=NoSchedule\"\n</code></pre></p> <p>Usage of custom tolerations in concert with nodeSelectors allows for fine granular control of Hosted Cluster pod placement on a per Hosted Cluster basis. This control allows for groups of Hosted Clusters to be colocated and isolated from other Hosted Clusters. It also allows for custom placement of Hosted Clusters within infra and master nodes.</p>"},{"location":"how-to/distribute-hosted-cluster-workloads/#scheduling-topology-options","title":"Scheduling Topology Options","text":"<p>Cluster Service Providers may choose how hosted control planes are isolated or co-located. The three different options are:</p> <ul> <li>Shared Everything</li> <li>Shared Nothing </li> <li>Dedicated Request Serving</li> </ul> <p>These options can be seen as a spectrum of isolation. Shared Everything is the least isolated, Dedicated Request Serving (Shared Some) and then Shared Nothing being the most isolated option.</p> <p>NOTE: Each hosted control plane can run Single Replica or Highly Available. If Highly Available, the control plane will be spread across failure domains via <code>topology.kubernetes.io/zone</code> as the topology key.</p>"},{"location":"how-to/distribute-hosted-cluster-workloads/#shared-everything","title":"Shared Everything","text":"<ul> <li>All hosted control plane pods are scheduled to any node that can run hosted control plane workloads.</li> <li>Nodes can be allocated specifically for control plane workloads by tainting and labeling them with <code>hypershift.openshift.io/control-plane: true</code>.</li> </ul>"},{"location":"how-to/distribute-hosted-cluster-workloads/#shared-nothing","title":"Shared Nothing","text":"<ul> <li>To confine nodes to a specific hosted cluster taint and label them with <code>hypershift.openshift.io/cluster</code> value.</li> <li>No other control plane pods will land on those nodes.</li> </ul>"},{"location":"how-to/distribute-hosted-cluster-workloads/#dedicated-request-serving","title":"Dedicated Request Serving","text":"<ul> <li>Two nodes in different zones are dedicated to a specific hosted cluster\u2019s front end serving components.</li> <li>The rest of the hosted cluster\u2019s control plane pods can co-exist with other clusters\u2019 control plane pods running on shared nodes.</li> <li>When running a Highly Available control plane, there will only be 2 replicas of request serving workloads instead of 3.</li> </ul> <p>NOTE: A HostedCluster must have:</p> <ul> <li><code>hypershift.openshift.io/topology: dedicated-request-serving-components</code> annotation to honor dedicated serving content workloads affinity opinions.</li> <li>nodeSelector set as <code>hypershift.openshift.io/control-plane: true</code> for it to be a hard requirement for workloads to be scheduled. Without it that label is a soft requirement meaning workloads will try to find any suitable node if there\u2019s none with this label.</li> </ul> <p></p>"},{"location":"how-to/distribute-hosted-cluster-workloads/#priority","title":"Priority","text":"<p>HyperShift leverages PriorityClasses for driving Priority and Preemption of their managed Pods. It will install four priority classes in a management cluster with the following order of priority from highest to lowest:</p> <ul> <li><code>hypershift-operator</code>: Hypershift operator pods</li> <li><code>hypershift-etcd</code>: Pods for etcd.</li> <li><code>hypershift-api-critical</code>: Pods that are required for API calls and resource admission to succeed. This includes pods like kube-apiserver, aggregated API servers, and webhooks.</li> <li><code>hypershift-control-plane</code>: pods in the HyperShift Control Plane that are not API critical but still need elevated priority. E.g Cluster Version Operator.</li> </ul>"},{"location":"how-to/feature-gates/","title":"Feature Gates","text":"<p>Feature gates in OpenShift allows to ensure everything new works together and optimizes for rapid evaluation in CI to promote without ever releasing a public preview. There are no guarantees that your fleet continues to be operational long term after you enable a feature gate.</p> <p>In the HCP context, there are multiple non exclusive scenarios where features might need to be gated:</p> <p>1 - A feature that impacts HO install E.g. New CRDs are required for CPOv2 (control plane operator version 2)</p> <p>2 - A feature that impacts the whole cluster fleet / HO E.g. Introduce fleet wide shared ingress to be validated in targeted environments</p> <p>3 - A feature that impacts individual clusters E.g. Introduce using CPOv2 for some HC to develop feedback</p> <p>4 - A feature that impacts API E.g. Introduce a new provider like Openstack E.g. Introduce a new field/feature like AWS tenancy</p> <p>5 - A feature specific for an OCP component Components honour existing standalone in-cluster OCP feature gate mechanisim</p>"},{"location":"how-to/feature-gates/#users","title":"Users","text":"<p>All the feature gates are grouped in a single TechPreviewNoUpgrade feature set. Current implementation exposes this --tech-preview-no-upgrade flag in the CLI at install time</p> <p><pre><code>hypershift install --help\n</code></pre> Will show among other flags: <pre><code>--tech-preview-no-upgrade                        If true, the HyperShift operator runs with TechPreviewNoUpgrade features enabled\n</code></pre></p> <p>In a follow up we'll consider to introduce support to also signal --tech-preview-no-upgrade at the HC level. Eventually support for at least 1, 2, 3 and 4 afromentioned scenarios will most likely converge into a single API.</p>"},{"location":"how-to/feature-gates/#devs","title":"Devs","text":"<p>We rely on openshift/api tooling for generating CRDs with openshift markers. See this PR as an example of a adding a field behind a feature gate.</p> <p>The currently ongoing implementation of feature gates for the controllers business logic relies on \"k8s.io/component-base/featuregate\". This enables devs to declare granular gates for their features. See this PR as an example.</p>"},{"location":"how-to/feature-gates/#promoting-a-feature-gated-api-field-and-feature-to-sable","title":"Promoting a feature gated API field and feature to sable","text":"<p>Generally speaking any new field should start by being feature gated. The minimum criteria for promotion is:</p> <ul> <li> <p>Provide clear context and analysis on the PR about how the field might impact the different GA products. This includes but it is not limited to ROSA, ARO, IBM Cloud and MCE (self hosted).</p> </li> <li> <p>Document the field with the expected behaviour for day 1 and day 2 changes.</p> </li> <li> <p>There is e2e test coverage for the feature that include day 2 changes of the field.</p> </li> <li> <p>There is e2e test coverage for on creation UX failure expectation via this e2e test</p> </li> <li> <p>There is e2e test coverage for day 2 on update UX failure expectations via this e2e test</p> </li> </ul> <p>In general we aim to adhere and converge with stand alone principles in openshift/api</p>"},{"location":"how-to/metrics-sets/","title":"Configure Metrics Sets","text":"<p>HyperShift creates ServiceMonitor resources in each control plane namespace that allow a Prometheus stack to scrape metrics from the control planes. ServiceMonitors use metrics relabelings to define which metrics are included or excluded from a particular component (etcd, Kube API server, etc) The number of metrics produced by control planes has a direct impact on resource requirements of the monitoring stack scraping them.</p> <p>Instead of producing a fixed number of metrics that apply to all situations, HyperShift allows configuration of a \"metrics set\" that identifies a set of metrics to produce per control plane.</p> <p>The following metrics sets are supported:</p> <ul> <li><code>Telemetry</code> - metrics needed for telemetry. This is the default and the smallest    set of metrics.</li> <li><code>SRE</code> - Configurable metrics set, intended to include necessary metrics to produce alerts and    allow troubleshooting of control plane components.</li> <li><code>All</code> - all the metrics produced by standalone OCP control plane components.</li> </ul> <p>The metrics set is configured by setting the <code>METRICS_SET</code> environment variable in the HyperShift operator deployment:</p> <pre><code>oc set env -n hypershift deployment/operator METRICS_SET=All\n</code></pre>"},{"location":"how-to/metrics-sets/#configuring-the-sre-metrics-set","title":"Configuring the SRE Metrics Set","text":"<p>When the SRE metrics set is specified, the HyperShift operator looks for a ConfigMap named <code>sre-metric-set</code> with a single key: <code>config</code>. The value of the <code>config</code> key should contain a set of RelabelConfigs organized by control plane component. An example of this configuration can be found in <code>support/metrics/testdata/sreconfig.yaml</code> in this repository.</p> <p>The following components can be specified:</p> <ul> <li>etcd</li> <li>kubeAPIServer</li> <li>kubeControllerManager</li> <li>openshiftAPIServer</li> <li>openshiftControllerManager</li> <li>openshiftRouteControllerManager</li> <li>cvo</li> <li>olm</li> <li>catalogOperator</li> <li>registryOperator</li> <li>nodeTuningOperator</li> <li>controlPlaneOperator</li> <li>hostedClusterConfigOperator</li> </ul>"},{"location":"how-to/pause-reconciliation/","title":"Pause Reconciliation","text":"<p>The <code>pausedUntil</code> field allows a hypershift administrator to pause reconciliation of a HostedCluster/HostedControlPlane pair. This is useful in operational scenarios like backing up and restoring an etcd database. It is also useful for debugging problems with HostedClusters/HostedControlPlanes.</p> <p>Pause reconciliation for a given <code>HostedCluster/HostedControlPlane</code> pair by populating the pausedUntil field of the resource:</p> <pre><code>#Can be boolean or RFC339 timestamp\n# PAUSED_UNTIL=\"true\"\nPAUSED_UNTIL=\"2022-03-03T03:28:48Z\"\nkubectl patch -n HOSTED_CLUSTERS_NAMESPACE hostedclusters/HOSTED_CLUSTER_NAME -p '{\"spec\":{\"pausedUntil\":\"'${PAUSED_UNTIL}'\"}}' --type=merge\n</code></pre> <p>Two formats are accepted: 1) If a RFC3339 formatted date is specified: reconciliation will be paused until that date. 2) If the boolean value of true is passed: reconciliation will be paused until the field is removed from the HostedCluster.</p> <p>All other values will not pause reconciliation.</p> <p>The associated downstream HostedControlPlane resource will have the pause reconciliation field added to it automatically. When the user removes the field from the HostedCluster or the specified date is passed: reconciliation will continue on the object</p> <p>The field can be removed with the following patch command: <pre><code>kubectl patch -n HOSTED_CLUSTERS_NAMESPACE hostedclusters/HOSTED_CLUSTER_NAME -p '{\"spec\":{\"pausedUntil\":null}}' --type=merge\n</code></pre></p>"},{"location":"how-to/per-hostedcluster-dashboard/","title":"Create Monitoring Dashboard per Hosted Cluster","text":"<p>The HyperShift operator can create/destroy a separate monitoring dashboard in the console of the management cluster for each HostedCluster that it manages. This functionality can be optionally enabled on installation of the HyperShift operator.</p>"},{"location":"how-to/per-hostedcluster-dashboard/#enable-monitoring-dashboards","title":"Enable Monitoring Dashboards","text":"<p>To enable monitoring dashboards, use the <code>--monitoring-dashboards</code> flag when running <code>hypershift install</code>. Alternatively, to enable monitoring dashboards in an existing installation, set the <code>MONITORING_DASHBOARDS</code> environment variable to <code>1</code> on the hypershift operator deployment:</p> <pre><code>oc set env deployment/operator -n hypershift MONITORING_DASHBOARDS=1\n</code></pre>"},{"location":"how-to/per-hostedcluster-dashboard/#dashboards","title":"Dashboards","text":"<p>When monitoring dashboards are enabled, the HyperShift operator creates a configmap named <code>cp-[NAMESPACE]-[NAME]</code> in the <code>openshift-config-managed</code> namespace (where NAMESPACE is the namespace of the HostedCluster and NAME is the name of the HostedCluster) for each HostedCluster that the operator manages. This results in a new dashboard getting added under Observe -&gt; Dashboards in the administrative console of the management cluster. When a HostedCluster is deleted, its corresponding dashboard is also deleted.</p>"},{"location":"how-to/per-hostedcluster-dashboard/#customize-monitoring-dashboards","title":"Customize Monitoring Dashboards","text":"<p>To generate per-cluster dashboards, the HyperShift operator uses a template stored in a ConfigMap named <code>monitoring-dashboard-template</code> in the operator namespace (<code>hypershift</code>). This template contains a set of grafana panels that contain the various metrics that should go on the dashboard. Edit the content of this ConfigMap to customize the dashboards. When a particular HostedCluster's dashboard is generated, the following strings will be replaced with values that correspond to the specific HostedCluster:</p> Name Description <code>__NAME__</code> The name of the HostedCluster <code>__NAMESPACE__</code> The namespace of the HostedCluster <code>__CONTROL_PLANE_NAMESPACE__</code> The namespace where the control plane pods of the HostedCluster are placed <code>__CLUSTER_ID__</code> The UUID of the HostedCluster (matches the <code>_id</code> label of HostedCluster metrics)"},{"location":"how-to/restart-control-plane-components/","title":"Restart Control Plane Components","text":"<p>The <code>hypershift.openshift.io/restart-date</code> annotation allows a hypershift administrator to restart all control plane components for a particular <code>HostedCluster</code>. This is useful for certificate rotation and in various development situations where a control plane restart is desired.</p> <p>Restart a control plane by annotating the <code>HostedCluster</code> resource:</p> <pre><code>oc annotate hostedcluster -n clusters example hypershift.openshift.io/restart-date=$(date --iso-8601=seconds)\n</code></pre> <p>The restart occurs whenever the value of this annotation changes. The <code>date</code> command in the example serves only as a source of a unique string.  The annotation is treated only as a string, not a timestamp.</p> <p>The list of components restarted are listed below:</p> <ul> <li>catalog-operator</li> <li>certified-operators-catalog</li> <li>cluster-api</li> <li>cluster-autoscaler</li> <li>cluster-policy-controller</li> <li>cluster-version-operator</li> <li>community-operators-catalog</li> <li>control-plane-operator</li> <li>hosted-cluster-config-operator</li> <li>ignition-server</li> <li>ingress-operator</li> <li>konnectivity-agent</li> <li>konnectivity-server</li> <li>kube-apiserver</li> <li>kube-controller-manager</li> <li>kube-scheduler</li> <li>machine-approver</li> <li>oauth-openshift</li> <li>olm-operator</li> <li>openshift-apiserver</li> <li>openshift-controller-manager</li> <li>openshift-oauth-apiserver</li> <li>packageserver</li> <li>redhat-marketplace-catalog</li> <li>redhat-operators-catalog</li> </ul>"},{"location":"how-to/troubleshooting-general/","title":"Troubleshooting","text":""},{"location":"how-to/troubleshooting-general/#general","title":"General","text":""},{"location":"how-to/troubleshooting-general/#dump-hostedcluster-resources-from-a-management-cluster","title":"Dump HostedCluster resources from a management cluster","text":"<p>To dump the relevant HostedCluster objects, we will need some prerequisites:</p> <ul> <li><code>cluster-admin</code> access to the management cluster</li> <li>The HostedCluster <code>name</code> and the <code>namespace</code> where the CR is deployed</li> <li>The Hypershift CLI</li> <li>The Openshift CLI</li> </ul> <p>Once we have these elements and the shell with the Kubeconfig loaded pointing to the management cluster, we will execute these commands:</p> <pre><code>CLUSTERNAME=\"samplecluster\"\nCLUSTERNS=\"clusters\"\n\nmkdir clusterDump-${CLUSTERNS}-${CLUSTERNAME}\nhypershift dump cluster \\\n    --name ${CLUSTERNAME} \\\n    --namespace ${CLUSTERNS} \\\n    --dump-guest-cluster \\\n    --artifact-dir clusterDump-${CLUSTERNS}-${CLUSTERNAME}\n</code></pre> <p>After some time, the output will show something like this:</p> <pre><code>2023-06-06T12:18:20+02:00   INFO    Archiving dump  {\"command\": \"tar\", \"args\": [\"-cvzf\", \"hypershift-dump.tar.gz\", \"cluster-scoped-resources\", \"event-filter.html\", \"namespaces\", \"network_logs\", \"timestamp\"]}\n2023-06-06T12:18:21+02:00   INFO    Successfully archived dump  {\"duration\": \"1.519376292s\"}\n</code></pre> <p>This dump contains artifacts that aid in troubleshooting issues with hosted control plane clusters.</p>"},{"location":"how-to/troubleshooting-general/#contents-from-dump-command","title":"Contents from Dump Command","text":"<p>The Management's Cluster's dump content:</p> <ul> <li>Cluster scoped resources: Basically nodes definitions of the management cluster.</li> <li>The dump compressed file: This is useful if you need to share the dump with other people</li> <li>Namespaced resources: This includes all the objects from all the relevant namespaces, like configmaps, services, events, logs, etc...</li> <li>Network logs: Includes the OVN northbound and southbound DBs and the statuses for each one.</li> <li>HostedClusters: Another level of dump, involves all the resources inside of the guest cluster.</li> </ul> <p>The Guest Cluster dump content:</p> <ul> <li>Cluster scoped resources: It contains al the cluster-wide objects, things like nodes, CRDs, etc...</li> <li>Namespaced resources: This includes all the objects from all the relevant namespaces, like configmaps, services, events, logs, etc...</li> </ul> <p>Note</p> <p>The dump will not contain any Secret object from the cluster, only references to the secret's names.</p>"},{"location":"how-to/troubleshooting-general/#impersonation-as-userservice-account","title":"Impersonation as user/service account","text":"<p>The dump command can be used with the flag <code>--as</code>, which works in the same way as the <code>oc</code> client. If you execute the command with the flag, the CLI will impersonate all the queries against the management cluster, using that username or service account.</p> <p>The service account should have enough permissions to query all the objects from the namespaces, so cluster-admin is recommended to make sure you have enough permissions. The service account should be located (or have permissions to query) at least the HostedControlPlane namespace.</p> <p>Note</p> <p>If your user/sa doesn't have enough permissions, the command will be executed and will dump only the objects you have permissions to get and during that process some <code>forbidden</code> errors will be raised.</p> <ul> <li>Impersonation Sample using a service account:</li> </ul> <pre><code>CLUSTERNAME=\"samplecluster\"\nCLUSTERNS=\"clusters\"\nSA=\"samplesa\"\nSA_NAMESPACE=\"default\"\n\nmkdir clusterDump-${CLUSTERNS}-${CLUSTERNAME}\nhypershift dump cluster \\\n    --name ${CLUSTERNAME} \\\n    --namespace ${CLUSTERNS} \\\n    --dump-guest-cluster \\\n    --as \"system:serviceaccount:${SA_NAMESPACE}:${SA}\" \\\n    --artifact-dir clusterDump-${CLUSTERNS}-${CLUSTERNAME}\n</code></pre> <ul> <li>Impersonation Sample using a user:</li> </ul> <pre><code>CLUSTERNAME=\"samplecluster\"\nCLUSTERNS=\"clusters\"\nCLUSTERUSER=\"cloud-admin\"\n\nmkdir clusterDump-${CLUSTERNS}-${CLUSTERNAME}\nhypershift dump cluster \\\n    --name ${CLUSTERNAME} \\\n    --namespace ${CLUSTERNS} \\\n    --dump-guest-cluster \\\n    --as \"${CLUSTERUSER}\" \\\n    --artifact-dir clusterDump-${CLUSTERNS}-${CLUSTERNAME}\n</code></pre>"},{"location":"how-to/troubleshooting-general/#how-to-view-the-ignition-payload","title":"How to view the ignition payload","text":"<ol> <li>Define the HCP namespace where the user-data secret is stored <pre><code>HCP_NAMESPACE=\"&lt;hcp-namespace&gt;\"\n</code></pre></li> <li>Find the user-data secret in the HCP namespace <pre><code>SECRET_NAME=$(oc get secret -n $HCP_NAMESPACE | grep user-data | awk '{print $1}')\n</code></pre></li> <li>Retrieve the secret and decode the value key <pre><code>USER_DATA_VALUE=$(oc get secret $SECRET_NAME -n $HCP_NAMESPACE -o jsonpath='{.data.value}' | base64 -d)\n</code></pre></li> <li> <p>Extract the bearer token and ignition server from the user-data value <pre><code>BEARER_TOKEN=$(echo $USER_DATA_VALUE | jq -r '.ignition.config.merge[0].httpHeaders[] | select(.name==\"Authorization\") | .value')\nIGNITION_SERVER=$(echo $USER_DATA_VALUE | jq -r '.ignition.config.merge[0].source')\n</code></pre></p> </li> <li> <p>Download the ignition payload from the ignition-server and save it to a file <pre><code>curl -k -H \"Authorization: $BEARER_TOKEN\" $IGNITION_SERVER -o ignition.json\n</code></pre></p> </li> </ol>"},{"location":"how-to/troubleshooting-general/#how-to-view-the-files-in-the-ignition-payload","title":"How to view the files in the ignition payload","text":"<p>Following on from the previous section and the example file in step 4, <code>ignition.json</code>, to view the files within the payload, execute the following command: <pre><code>% cat ignition.json | jq '.storage.files[].path'\n\"/usr/local/bin/nm-clean-initrd-state.sh\"\n\"/etc/NetworkManager/conf.d/01-ipv6.conf\"\n\"/etc/NetworkManager/conf.d/20-keyfiles.conf\"\n\"/etc/pki/ca-trust/source/anchors/openshift-config-user-ca-bundle.crt\"\n\"/etc/kubernetes/apiserver-url.env\"\n\"/etc/audit/rules.d/mco-audit-quiet-containers.rules\"\n\"/etc/tmpfiles.d/cleanup-cni.conf\"\n\"/usr/local/bin/configure-ovs.sh\"\n\"/etc/containers/storage.conf\"\n\"/etc/mco/proxy.env\"\n...\n</code></pre></p> <p>To view the specific contents of a file, execute the following command: <pre><code>% cat ignition.json | jq '.storage.files[] | select(.path==\"/etc/kubernetes/apiserver-url.env\")'\n{\n  \"overwrite\": true,\n  \"path\": \"/etc/kubernetes/apiserver-url.env\",\n  \"contents\": {\n    \"source\": \"data:,KUBERNETES_SERVICE_HOST%3D'52.150.32.156'%0AKUBERNETES_SERVICE_PORT%3D'7443'%0A\"\n  },\n  \"mode\": 420\n}\n</code></pre></p>"},{"location":"how-to/troubleshooting-general/#troubleshoot-by-provider","title":"Troubleshoot By Provider","text":"<p>If you have provider-scoped questions, please take a look at the troubleshooting section for the provider in the list below. We will keep adding more and more troubleshooting sections and updating the existent ones.</p> <ul> <li>AWS</li> <li>Azure</li> </ul>"},{"location":"how-to/upgrades/","title":"Upgrades","text":"<p>HyperShift enables the decoupling of upgrades between the Control Plane and Nodes.</p> <p>This allows there to be two separate procedures a Cluster Service Provider can take, giving them flexibility to manage the different components separately.</p> <p>Control Plane upgrades are driven by the HostedCluster, while Node upgrades are driven by its respective NodePool. Both the HostedCluster and NodePool expose a <code>.release</code> field where the OCP release image can be specified.</p> <p>For a cluster to keep fully operational during an upgrade process, Control Plane and Nodes upgrades need to be orchestrated while satisfying Kubernetes version skew policy at any time. The supported OCP versions are dictated by the running HyperShift Operator see here for more details on versioning.</p>"},{"location":"how-to/upgrades/#hostedcluster","title":"HostedCluster","text":"<p><code>.spec.release</code> dictates the version of the Control Plane.</p> <p>The HostedCluster propagates the intended <code>.spec.release</code> to the <code>HostedControlPlane.spec.release</code> and runs the appropriate Control Plane Operator version.</p> <p>The HostedControlPlane orchestrates the rollout of the new version of the Control Plane components along with any OCP component in the data plane through the new version of the cluster version operator (CVO). This includes resources like:</p> <ul> <li>the CVO itself</li> <li>cluster network operator (CNO)</li> <li>cluster ingress operator</li> <li>manifests for the kube API-server (KAS), scheduler, and manager</li> <li>machine approver</li> <li>autoscaler</li> <li>infra resources needed to enable ingress for control plane endpoints (KAS, ignition, konnectivity, etc.)</li> </ul> <p>The CVO also applies the payload for the <code>CLUSTER_PROFILE\": \"ibm-cloud-managed</code> into the hosted cluster.</p> <p>Traditionally, in standalone OCP, the CVO has been the sole source of truth for upgrades. In HyperShift, the responsibility is currently split between CPO and CVO. This enabled the flexibility and speed the HyperShift project needed for the CPO to support the management/guest cluster topology and the multiple customizations needed for manifests that otherwise would have needed to be segregated in the payload.</p> <p>HyperShift exposes available upgrades in HostedCluster.Status by bubbling up the status of the ClusterVersion resource inside a hosted cluster. This info is purely informational and doesn't determine upgradability, which is dictated by the <code>.spec.release</code> input in practice. This does result in the loss of some of the builtin features and guardrails from CVO like recommendations, allowed upgrade paths, risks, etc. However, this information is still available in the HostedCluster.Status field for consumers to read.</p>"},{"location":"how-to/upgrades/#nodepools","title":"NodePools","text":"<p><code>.spec.release</code> dictates the version of any particular NodePool.</p> <p>A NodePool will perform a Replace/InPlace rolling upgrade according to <code>.spec.management.upgradeType</code>. See NodePool Upgrades for details.</p>"},{"location":"how-to/agent/create-agent-cluster/","title":"Create an Agent cluster","text":"<p>This document explains how to create HostedClusters and NodePools using the Agent platform.</p> <p>The Agent platform uses the Infrastructure Operator (AKA Assisted Installer) to add worker nodes to a hosted cluster. For a primer on the Infrastructure Operator, see here.</p>"},{"location":"how-to/agent/create-agent-cluster/#overview","title":"Overview","text":"<p>When you create a HostedCluster with the Agent platform, HyperShift will install the Agent CAPI provider in the Hosted Control Plane (HCP) namespace.</p> <p>Upon scaling up a NodePool, a Machine will be created, and the CAPI provider will find a suitable Agent to match this Machine. Suitable means that the Agent is approved, is passing validations, is not currently bound (in use), and has the requirements specified on the NodePool Spec (e.g., minimum CPU/RAM, labels matching the label selector). You may monitor the installation of an Agent by checking its <code>Status</code> and <code>Conditions</code>.</p> <p>Upon scaling down a NodePool, Agents will be unbound from the corresponding cluster. However, you must boot them with the Discovery Image once again before reusing them.</p>"},{"location":"how-to/agent/create-agent-cluster/#install-hypershift-operator","title":"Install HyperShift Operator","text":"<p>Before installing the HyperShift operator we need to get the HyperShift CLI. We have two methods for getting the CLI installed in our system.</p>"},{"location":"how-to/agent/create-agent-cluster/#method-1-build-the-hypershift-cli","title":"Method 1 - Build the HyperShift CLI","text":"<p>Follow instructions for building the HyperShift CLI in Getting Started</p>"},{"location":"how-to/agent/create-agent-cluster/#method-2-extract-hypershift-cli-from-the-operator-image","title":"Method 2 - Extract HyperShift CLI from the Operator Image","text":"<p>INFO: We are using Podman in the example, same applies to Docker.</p> <pre><code>export HYPERSHIFT_RELEASE=4.11\n\npodman cp $(podman create --name hypershift --rm --pull always quay.io/hypershift/hypershift-operator:${HYPERSHIFT_RELEASE}):/usr/bin/hypershift /tmp/hypershift &amp;&amp; podman rm -f hypershift\n\nsudo install -m 0755 -o root -g root /tmp/hypershift /usr/local/bin/hypershift\n</code></pre>"},{"location":"how-to/agent/create-agent-cluster/#deploy-the-hypershift-operator","title":"Deploy the HyperShift Operator","text":"<p>With the CLI deployed, we can go ahead and deploy the operator:</p> <p>WARN: If we don't define the HyperShift image we want to use, by default the CLI will deploy <code>latest</code>. Usually you want to deploy the image matching the release of the OpenShift cluster where HyperShift will run.</p> <pre><code># This install latest\nhypershift install\n# You may want to run this instead\nhypershift install --hypershift-image quay.io/hypershift/hypershift-operator:4.11\n</code></pre> <p>You will see the operator running in the <code>hypershift</code> namespace:</p> <pre><code>oc -n hypershift get pods\n\nNAME                      READY   STATUS    RESTARTS   AGE\noperator-55fffbd6-whkxs   1/1     Running   0          61s\n</code></pre>"},{"location":"how-to/agent/create-agent-cluster/#install-assisted-service-and-hive-operators","title":"Install Assisted Service and Hive Operators","text":"<p>NOTE: If Red Hat Advanced Cluster Management (RHACM) is already installed, this can be skipped as the Infrastructure Operator and Hive Operator are dependencies of RHACM.</p> <p>We will leverage <code>tasty</code> to deploy the required operators easily.</p> <p>Install tasty:</p> <pre><code>curl -s -L https://github.com/karmab/tasty/releases/download/v0.4.0/tasty-linux-amd64 &gt; ./tasty\nsudo install -m 0755 -o root -g root ./tasty /usr/local/bin/tasty\n</code></pre> <p>Install the operators</p> <pre><code>tasty install assisted-service-operator hive-operator\n</code></pre>"},{"location":"how-to/agent/create-agent-cluster/#configure-agent-service","title":"Configure Agent Service","text":"<p>Create the <code>AgentServiceConfig</code> resource</p> <pre><code>export DB_VOLUME_SIZE=\"10Gi\"\nexport FS_VOLUME_SIZE=\"10Gi\"\nexport OCP_VERSION=\"4.11.5\"\nexport OCP_MAJMIN=${OCP_VERSION%.*}\nexport ARCH=\"x86_64\"\nexport OCP_RELEASE_VERSION=$(curl -s https://mirror.openshift.com/pub/openshift-v4/${ARCH}/clients/ocp/${OCP_VERSION}/release.txt | awk '/machine-os / { print $2 }')\nexport ISO_URL=\"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/${OCP_MAJMIN}/${OCP_VERSION}/rhcos-${OCP_VERSION}-${ARCH}-live.${ARCH}.iso\"\nexport ROOT_FS_URL=\"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/${OCP_MAJMIN}/${OCP_VERSION}/rhcos-${OCP_VERSION}-${ARCH}-live-rootfs.${ARCH}.img\"\n\nenvsubst &lt;&lt;\"EOF\" | oc apply -f -\napiVersion: agent-install.openshift.io/v1beta1\nkind: AgentServiceConfig\nmetadata:\n name: agent\nspec:\n  databaseStorage:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: ${DB_VOLUME_SIZE}\n  filesystemStorage:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: ${FS_VOLUME_SIZE}\n  osImages:\n    - openshiftVersion: \"${OCP_VERSION}\"\n      version: \"${OCP_RELEASE_VERSION}\"\n      url: \"${ISO_URL}\"\n      rootFSUrl: \"${ROOT_FS_URL}\"\n      cpuArchitecture: \"${ARCH}\"\nEOF\n</code></pre>"},{"location":"how-to/agent/create-agent-cluster/#configure-dns","title":"Configure DNS","text":"<p>The API Server for the Hosted Cluster is exposed a Service of type NodePort.</p> <p>A DNS entry must exist for <code>api.${HOSTED_CLUSTER_NAME}.${BASEDOMAIN}</code> pointing to destination where the API Server can be reached.</p> <p>This can be as simple as an A record pointing to one of the nodes in the management cluster (i.e. the cluster running the HCP).  It can also point to a load balancer deployed to redirect incoming traffic to the ingress pods.</p>"},{"location":"how-to/agent/create-agent-cluster/#example-dns-config","title":"Example DNS Config","text":"<pre><code>api.example.krnl.es.    IN A 192.168.122.20\napi.example.krnl.es.    IN A 192.168.122.21\napi.example.krnl.es.    IN A 192.168.122.22\napi-int.example.krnl.es.    IN A 192.168.122.20\napi-int.example.krnl.es.    IN A 192.168.122.21\napi-int.example.krnl.es.    IN A 192.168.122.22\n*.apps.example.krnl.es. IN A 192.168.122.23\n</code></pre>"},{"location":"how-to/agent/create-agent-cluster/#create-a-hosted-cluster","title":"Create a Hosted Cluster","text":"<p>WARN: Make sure you have a default storage class configured for your cluster, otherwise you may end up with pending PVCs.</p> <pre><code>export CLUSTERS_NAMESPACE=\"clusters\"\nexport HOSTED_CLUSTER_NAME=\"example\"\nexport HOSTED_CONTROL_PLANE_NAMESPACE=\"${CLUSTERS_NAMESPACE}-${HOSTED_CLUSTER_NAME}\"\nexport BASEDOMAIN=\"krnl.es\"\nexport PULL_SECRET_FILE=$PWD/pull-secret\nexport OCP_RELEASE=4.11.5-x86_64\nexport MACHINE_CIDR=192.168.122.0/24\n# Typically the namespace is created by the hypershift-operator\n# but agent cluster creation generates a capi-provider role that\n# needs the namespace to already exist\noc create ns ${HOSTED_CONTROL_PLANE_NAMESPACE}\n\nhypershift create cluster agent \\\n    --name=${HOSTED_CLUSTER_NAME} \\\n    --pull-secret=${PULL_SECRET_FILE} \\\n    --agent-namespace=${HOSTED_CONTROL_PLANE_NAMESPACE} \\\n    --base-domain=${BASEDOMAIN} \\\n    --api-server-address=api.${HOSTED_CLUSTER_NAME}.${BASEDOMAIN} \\\n    --release-image=quay.io/openshift-release-dev/ocp-release:${OCP_RELEASE}\n</code></pre> <p>After a few moments we should see our hosted control plane pods up and running:</p> <pre><code>oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get pods\n\nNAME                                             READY   STATUS    RESTARTS   AGE\ncapi-provider-7dcf5fc4c4-nr9sq                   1/1     Running   0          4m32s\ncatalog-operator-6cd867cc7-phb2q                 2/2     Running   0          2m50s\ncertified-operators-catalog-884c756c4-zdt64      1/1     Running   0          2m51s\ncluster-api-f75d86f8c-56wfz                      1/1     Running   0          4m32s\ncluster-autoscaler-7977864686-2rz4c              1/1     Running   0          4m13s\ncluster-network-operator-754cf4ffd6-lwfm2        1/1     Running   0          2m51s\ncluster-policy-controller-784f995d5-7cbrz        1/1     Running   0          2m51s\ncluster-version-operator-5c68f7f4f8-lqzcm        1/1     Running   0          2m51s\ncommunity-operators-catalog-58599d96cd-vpj2v     1/1     Running   0          2m51s\ncontrol-plane-operator-f6b4c8465-4k5dh           1/1     Running   0          4m32s\netcd-0                                           1/1     Running   0          4m13s\nhosted-cluster-config-operator-c4776f89f-dt46j   1/1     Running   0          2m51s\nignition-server-7cd8676fc5-hjx29                 1/1     Running   0          4m22s\ningress-operator-75484cdc8c-zhdz5                1/2     Running   0          2m51s\nkonnectivity-agent-c5485c9df-jsm9s               1/1     Running   0          4m13s\nkonnectivity-server-85dc754888-7z8vm             1/1     Running   0          4m13s\nkube-apiserver-db5fb5549-zlvpq                   3/3     Running   0          4m13s\nkube-controller-manager-5fbf7b7b7b-mrtjj         1/1     Running   0          90s\nkube-scheduler-776c59d757-kfhv6                  1/1     Running   0          3m12s\nmachine-approver-c6b947895-lkdbk                 1/1     Running   0          4m13s\noauth-openshift-787b87cff6-trvd6                 2/2     Running   0          87s\nolm-operator-69c4657864-hxwzk                    2/2     Running   0          2m50s\nopenshift-apiserver-67f9d9c5c7-c9bmv             2/2     Running   0          89s\nopenshift-controller-manager-5899fc8778-q89xh    1/1     Running   0          2m51s\nopenshift-oauth-apiserver-569c78c4d-568v8        1/1     Running   0          2m52s\npackageserver-ddfffb8d7-wlz6l                    2/2     Running   0          2m50s\nredhat-marketplace-catalog-7dd77d896-jtxkd       1/1     Running   0          2m51s\nredhat-operators-catalog-d66b5c965-qwhn7         1/1     Running   0          2m51s\n</code></pre>"},{"location":"how-to/agent/create-agent-cluster/#create-an-infraenv","title":"Create an InfraEnv","text":"<p>An InfraEnv is a environment to which hosts booting the live ISO can join as Agents.  In this case, the Agents will be created in the same namespace as our HostedControlPlane.</p> <pre><code>export SSH_PUB_KEY=$(cat $HOME/.ssh/id_rsa.pub)\n\nenvsubst &lt;&lt;\"EOF\" | oc apply -f -\napiVersion: agent-install.openshift.io/v1beta1\nkind: InfraEnv\nmetadata:\n  name: ${HOSTED_CLUSTER_NAME}\n  namespace: ${HOSTED_CONTROL_PLANE_NAMESPACE}\nspec:\n  pullSecretRef:\n    name: pull-secret\n  sshAuthorizedKey: ${SSH_PUB_KEY}\nEOF\n</code></pre> <p>This will generate a live ISO that allows machines (VMs or bare-metal) to join as Agents.</p> <pre><code>oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get InfraEnv ${HOSTED_CLUSTER_NAME} -ojsonpath=\"{.status.isoDownloadURL}\"\n</code></pre>"},{"location":"how-to/agent/create-agent-cluster/#adding-agents","title":"Adding Agents","text":"<p>You can add Agents by manually configuring the machine to boot with the live ISO or by using Metal3.</p>"},{"location":"how-to/agent/create-agent-cluster/#manual","title":"Manual","text":"<p>The live ISO may be downloaded and used to boot a node (bare-metal or VM).</p> <p>On boot, the node will communicate with the assisted-service and register as an Agent in the same namespace as the InfraEnv.</p> <p>Once each Agent is created, optionally set its installation_disk_id and hostname in the Spec. Then approve it to indicate that the Agent is ready for use.</p> <pre><code>oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agents\n\nNAME                                   CLUSTER   APPROVED   ROLE          STAGE\n86f7ac75-4fc4-4b36-8130-40fa12602218                        auto-assign\ne57a637f-745b-496e-971d-1abbf03341ba                        auto-assign\n</code></pre> <pre><code>oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} patch agent 86f7ac75-4fc4-4b36-8130-40fa12602218 -p '{\"spec\":{\"installation_disk_id\":\"/dev/sda\",\"approved\":true,\"hostname\":\"worker-0.example.krnl.es\"}}' --type merge\n\noc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} patch agent 23d0c614-2caa-43f5-b7d3-0b3564688baa -p '{\"spec\":{\"installation_disk_id\":\"/dev/sda\",\"approved\":true,\"hostname\":\"worker-1.example.krnl.es\"}}' --type merge\n</code></pre> <pre><code>oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agents\n\nNAME                                   CLUSTER   APPROVED   ROLE          STAGE\n86f7ac75-4fc4-4b36-8130-40fa12602218             true       auto-assign\ne57a637f-745b-496e-971d-1abbf03341ba             true       auto-assign\n</code></pre>"},{"location":"how-to/agent/create-agent-cluster/#metal3","title":"Metal3","text":"<p>We will leverage the Assisted Service and Hive to create the custom ISO as well as the Baremetal Operator to perform the installation.</p> <p>WARN: Since the <code>BaremetalHost</code> objects will be created outside the baremetal-operator namespace we need to configure the operator to watch all namespaces.</p> <pre><code>oc patch provisioning provisioning-configuration --type merge -p '{\"spec\":{\"watchAllNamespaces\": true }}'\n</code></pre> <p>INFO: This will trigger a restart of the <code>metal3</code> pod in the <code>openshift-machine-api</code> namespace.</p> <ul> <li>Wait until the <code>metal3</code> pod is ready again:</li> </ul> <pre><code>until oc wait -n openshift-machine-api $(oc get pods -n openshift-machine-api -l baremetal.openshift.io/cluster-baremetal-operator=metal3-state -o name) --for condition=containersready --timeout 10s &gt;/dev/null 2&gt;&amp;1 ; do sleep 1 ; done\n</code></pre> <p>Now we can go ahead and create our BaremetalHost objects. We will need to configure some variables required to be able to boot our bare-metal nodes.</p> <ul> <li><code>BMC_USERNAME</code>: Username to be used for connecting to the BMC.</li> <li><code>BMC_PASSWORD</code>: Password to be used for connecting to the BMC.</li> <li><code>BMC_IP</code>: IP used by Metal3 to connect to the BMC.</li> <li><code>WORKER_NAME</code>: Name of the BaremetalHost object (this will be used as hostname as well)</li> <li><code>BOOT_MAC_ADDRESS</code>: MAC address of the NIC connected to the MachineNetwork.</li> <li><code>UUID</code>: Redfish UUID, this is usually <code>1</code>. If using sushy-tools this will be a long UUID. If using iDrac this will be <code>System.Embedded.1</code>. You may need to check with the vendor.</li> <li><code>REDFISH_SCHEME</code>: The Redfish provider to use. If using hardware that uses a standard Redfish implementation you can set this to <code>redfish-virtualmedia</code>. iDRAC will use <code>idrac-virtualmedia</code>. iLO5 will use <code>ilo5-virtualmedia</code>. You may need to check with the vendor.</li> <li><code>REDFISH</code>: Redfish connection endpoint.</li> </ul> <pre><code>export BMC_USERNAME=$(echo -n \"root\" | base64 -w0)\nexport BMC_PASSWORD=$(echo -n \"calvin\" | base64 -w0)\nexport BMC_IP=\"192.168.124.228\"\nexport WORKER_NAME=\"ocp-worker-0\"\nexport BOOT_MAC_ADDRESS=\"aa:bb:cc:dd:ee:ff\"\nexport UUID=\"1\"\nexport REDFISH_SCHEME=\"redfish-virtualmedia\"\nexport REDFISH=\"${REDFISH_SCHEME}://${BMC_IP}/redfish/v1/Systems/${UUID}\"\n</code></pre> <p>With the required information ready, let's create the BaremetalHost. First we will create the BMC Secret:</p> <pre><code>envsubst &lt;&lt;\"EOF\" | oc apply -f -\napiVersion: v1\ndata:\n  password: ${BMC_PASSWORD}\n  username: ${BMC_USERNAME}\nkind: Secret\nmetadata:\n  name: ${WORKER_NAME}-bmc-secret\n  namespace: ${HOSTED_CONTROL_PLANE_NAMESPACE}\ntype: Opaque\nEOF\n</code></pre> <p>Second, we will create the BMH:</p> <p>INFO: <code>infraenvs.agent-install.openshift.io</code> label is used to specify which InfraEnv is used to boot the BMH. <code>bmac.agent-install.openshift.io/hostname</code> is used to manually set a hostname.</p> <p>In case you want to manually specify the installation disk you can make use of the rootDeviceHints in the BMH Spec. If rootDeviceHints are not provided, the agent will pick the installation disk that better suits the installation requirements.</p> <pre><code>envsubst &lt;&lt;\"EOF\" | oc apply -f -\napiVersion: metal3.io/v1alpha1\nkind: BareMetalHost\nmetadata:\n  name: ${WORKER_NAME}\n  namespace: ${HOSTED_CONTROL_PLANE_NAMESPACE}\n  labels:\n    infraenvs.agent-install.openshift.io: ${HOSTED_CLUSTER_NAME}\n  annotations:\n    inspect.metal3.io: disabled\n    bmac.agent-install.openshift.io/hostname: ${WORKER_NAME}\nspec:\n  automatedCleaningMode: disabled\n  bmc:\n    disableCertificateVerification: True\n    address: ${REDFISH}\n    credentialsName: ${WORKER_NAME}-bmc-secret\n  bootMACAddress: ${BOOT_MAC_ADDRESS}\n  online: true\nEOF\n</code></pre> <p>The Agent should be automatically approved, if not, make sure the <code>bootMACAddress</code> is correct.</p> <p>The BMH will be provisioned:</p> <pre><code>oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get bmh\n\nNAME           STATE          CONSUMER   ONLINE   ERROR   AGE\nocp-worker-0   provisioning              true             2m50s\n</code></pre> <p>BMH will reach <code>provisioned</code> state eventually.</p> <pre><code>oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get bmh\nNAME           STATE          CONSUMER   ONLINE   ERROR   AGE\nocp-worker-0   provisioned               true             72s\n</code></pre> <p>Provisioned means that the node was configured to boot from the virtualCD properly. It will take a few moments for the Agent to show up:</p> <pre><code>oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent\n\nNAME                                   CLUSTER   APPROVED   ROLE          STAGE\n4dac1ab2-7dd5-4894-a220-6a3473b67ee6             true       auto-assign  \n</code></pre> <p>As you can see it was auto-approved. We will repeat this with another two nodes.</p> <pre><code>oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent\n\nNAME                                   CLUSTER   APPROVED   ROLE          STAGE\n4dac1ab2-7dd5-4894-a220-6a3473b67ee6             true       auto-assign  \nd9198891-39f4-4930-a679-65fb142b108b             true       auto-assign\nda503cf1-a347-44f2-875c-4960ddb04091             true       auto-assign\n</code></pre>"},{"location":"how-to/agent/create-agent-cluster/#accessing-the-hostedcluster","title":"Accessing the HostedCluster","text":"<p>We have the HostedControlPlane running and the Agents ready to join the HostedCluster. Before we join the Agents let's access the HostedCluster.</p> <p>First, we need to generate the kubeconfig:</p> <pre><code>hypershift create kubeconfig --namespace ${CLUSTERS_NAMESPACE} --name ${HOSTED_CLUSTER_NAME} &gt; ${HOSTED_CLUSTER_NAME}.kubeconfig\n</code></pre> <p>If we access the cluster we will see that we don't have any nodes and that the ClusterVersion is trying to reconcile the OCP release:</p> <pre><code>oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get clusterversion,nodes\n\nNAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS\nclusterversion.config.openshift.io/version             False       True          8m6s    Unable to apply 4.11.5: some cluster operators have not yet rolled out\n</code></pre> <p>In order to get the cluster in a running state we need to add some nodes to it. Let's do it.</p>"},{"location":"how-to/agent/create-agent-cluster/#scale-the-nodepool","title":"Scale the NodePool","text":"<p>We add nodes to our HostedCluster by scaling the NodePool object. In this case we will start by scaling the NodePool object to two nodes:</p> <pre><code>oc -n ${CLUSTERS_NAMESPACE} scale nodepool ${NODEPOOL_NAME} --replicas 2\n</code></pre> <p>The ClusterAPI Agent provider will pick two agents randomly that will get assigned to the HostedCluster. These agents will go over different states and will finally join the HostedCluster as OpenShift nodes.</p> <p>INFO: States will be <code>binding</code> -&gt; <code>discoverying</code> -&gt; <code>insufficient</code> -&gt; <code>installing</code> -&gt; <code>installing-in-progress</code> -&gt; <code>added-to-existing-cluster</code></p> <pre><code>oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent\n\nNAME                                   CLUSTER         APPROVED   ROLE          STAGE\n4dac1ab2-7dd5-4894-a220-6a3473b67ee6   hypercluster1   true       auto-assign  \nd9198891-39f4-4930-a679-65fb142b108b                   true       auto-assign  \nda503cf1-a347-44f2-875c-4960ddb04091   hypercluster1   true       auto-assign\n\noc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent -o jsonpath='{range .items[*]}BMH: {@.metadata.labels.agent-install\\.openshift\\.io/bmh} Agent: {@.metadata.name} State: {@.status.debugInfo.state}{\"\\n\"}{end}'\n\nBMH: ocp-worker-2 Agent: 4dac1ab2-7dd5-4894-a220-6a3473b67ee6 State: binding\nBMH: ocp-worker-0 Agent: d9198891-39f4-4930-a679-65fb142b108b State: known-unbound\nBMH: ocp-worker-1 Agent: da503cf1-a347-44f2-875c-4960ddb04091 State: insufficient\n</code></pre> <p>Once the agents have reached the <code>added-to-existing-cluster</code> state, we should see the OpenShift nodes after a few moments:</p> <pre><code>oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get nodes\n\nNAME           STATUS   ROLES    AGE     VERSION\nocp-worker-1   Ready    worker   5m41s   v1.24.0+3882f8f\nocp-worker-2   Ready    worker   6m3s    v1.24.0+3882f8f\n</code></pre> <p>At this point some ClusterOperators will start to reconcile by adding workloads to the nodes.</p> <p>We can also see that two Machines were created when we scaled up the NodePool:</p> <pre><code>oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get machines\n\nNAME                            CLUSTER               NODENAME       PROVIDERID                                     PHASE     AGE   VERSION\nhypercluster1-c96b6f675-m5vch   hypercluster1-b2qhl   ocp-worker-1   agent://da503cf1-a347-44f2-875c-4960ddb04091   Running   15m   4.11.5\nhypercluster1-c96b6f675-tl42p   hypercluster1-b2qhl   ocp-worker-2   agent://4dac1ab2-7dd5-4894-a220-6a3473b67ee6   Running   15m   4.11.5\n</code></pre> <p>At some point the clusterversion reconcile will reach a point where only Ingress and Console cluster operators will be missing:</p> <pre><code>oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get clusterversion,co\n\nNAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS\nclusterversion.config.openshift.io/version             False       True          40m     Unable to apply 4.11.5: the cluster operator console has not yet successfully rolled out\n\nNAME                                                                           VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nclusteroperator.config.openshift.io/console                                    4.11.5    False       False         False      11m     RouteHealthAvailable: failed to GET route (https://console-openshift-console.apps.hypercluster1.domain.com): Get \"https://console-openshift-console.apps.hypercluster1.domain.com\": dial tcp 10.19.3.29:443: connect: connection refused\nclusteroperator.config.openshift.io/csi-snapshot-controller                    4.11.5    True        False         False      10m  \nclusteroperator.config.openshift.io/dns                                        4.11.5    True        False         False      9m16s  \nclusteroperator.config.openshift.io/image-registry                             4.11.5    True        False         False      9m5s  \nclusteroperator.config.openshift.io/ingress                                    4.11.5    True        False         True       39m     The \"default\" ingress controller reports Degraded=True: DegradedConditions: One or more other status conditions indicate a degraded state: CanaryChecksSucceeding=False (CanaryChecksRepetitiveFailures: Canary route checks for the default ingress controller are failing)\nclusteroperator.config.openshift.io/insights                                   4.11.5    True        False         False      11m  \nclusteroperator.config.openshift.io/kube-apiserver                             4.11.5    True        False         False      40m  \nclusteroperator.config.openshift.io/kube-controller-manager                    4.11.5    True        False         False      40m  \nclusteroperator.config.openshift.io/kube-scheduler                             4.11.5    True        False         False      40m  \nclusteroperator.config.openshift.io/kube-storage-version-migrator              4.11.5    True        False         False      10m  \nclusteroperator.config.openshift.io/monitoring                                 4.11.5    True        False         False      7m38s  \nclusteroperator.config.openshift.io/network                                    4.11.5    True        False         False      11m  \nclusteroperator.config.openshift.io/openshift-apiserver                        4.11.5    True        False         False      40m  \nclusteroperator.config.openshift.io/openshift-controller-manager               4.11.5    True        False         False      40m  \nclusteroperator.config.openshift.io/openshift-samples                          4.11.5    True        False         False      8m54s  \nclusteroperator.config.openshift.io/operator-lifecycle-manager                 4.11.5    True        False         False      40m  \nclusteroperator.config.openshift.io/operator-lifecycle-manager-catalog         4.11.5    True        False         False      40m  \nclusteroperator.config.openshift.io/operator-lifecycle-manager-packageserver   4.11.5    True        False         False      40m  \nclusteroperator.config.openshift.io/service-ca                                 4.11.5    True        False         False      11m  \nclusteroperator.config.openshift.io/storage                                    4.11.5    True        False         False      11m\n</code></pre> <p>Let's fix the Ingress.</p>"},{"location":"how-to/agent/create-agent-cluster/#handling-ingress","title":"Handling Ingress","text":"<p>Every OpenShift cluster comes set up with a default application ingress controller, which is expected have an external DNS record associated with it.</p> <p>For example, if a HyperShift cluster named <code>example</code> with the base domain <code>krnl.es</code> is created, then the wildcard domain <code>*.apps.example.krnl.es</code> is expected to be routable.</p>"},{"location":"how-to/agent/create-agent-cluster/#set-up-a-loadbalancer-and-wildcard-dns-record-for-the-apps","title":"Set up a LoadBalancer and wildcard DNS record for the <code>*.apps</code>.","text":"<p>This option requires deploying MetalLB, configuring a new LoadBalancer service that routes to the ingress deployment, as well as assigning a wildcard DNS entry to the LoadBalancer's IP address.</p>"},{"location":"how-to/agent/create-agent-cluster/#step-1-get-the-metallb-operator-deployed","title":"Step 1 - Get the MetalLB Operator Deployed","text":"<p>Set up MetalLB so that when you create a service of type LoadBalancer, MetalLB will add an external IP address for the service.</p> <pre><code>cat &lt;&lt;\"EOF\" | oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig apply -f -\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: metallb\n  labels:\n    openshift.io/cluster-monitoring: \"true\"\n  annotations:\n    workload.openshift.io/allowed: management\n---\napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: metallb-operator-operatorgroup\n  namespace: metallb\n---\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: metallb-operator\n  namespace: metallb\nspec:\n  channel: \"stable\"\n  name: metallb-operator\n  source: redhat-operators\n  sourceNamespace: openshift-marketplace\n</code></pre> <p>Once the operator is up and running, create the MetalLB instance:</p> <pre><code>cat &lt;&lt;\"EOF\" | oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig apply -f -\napiVersion: metallb.io/v1beta1\nkind: MetalLB\nmetadata:\n  name: metallb\n  namespace: metallb\nEOF\n</code></pre>"},{"location":"how-to/agent/create-agent-cluster/#step-2-get-the-metallb-operator-configured","title":"Step 2 - Get the MetalLB Operator Configured","text":"<p>We will create an <code>IPAddressPool</code> with a single IP address and L2Advertisement to advertise the LoadBalancer IPs provided by the <code>IPAddressPool</code> via L2. Since layer 2 mode relies on ARP and NDP, the IP address must be on the same subnet as the network used by the cluster nodes in order for the MetalLB to work. more information about metalLB configuration options is available here.</p> <p>WARN: Change <code>INGRESS_IP</code> env var to match your environments addressing.</p> <pre><code>export INGRESS_IP=192.168.122.23\n\nenvsubst &lt;&lt;\"EOF\" | oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig apply -f -\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: ingress-public-ip\n  namespace: metallb\nspec:\n  protocol: layer2\n  autoAssign: false\n  addresses:\n    - ${INGRESS_IP}-${INGRESS_IP}\n---\n\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: ingress-public-ip\n  namespace: metallb\nEOF\n</code></pre>"},{"location":"how-to/agent/create-agent-cluster/#step-3-get-the-openshift-router-exposed-via-metallb","title":"Step 3 - Get the OpenShift Router exposed via MetalLB","text":"<p>Set up the LoadBalancer Service that routes ingress traffic to the ingress deployment.</p> <pre><code>cat &lt;&lt;\"EOF\" | oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig apply -f -\nkind: Service\napiVersion: v1\nmetadata:\n  annotations:\n    metallb.universe.tf/address-pool: ingress-public-ip\n  name: metallb-ingress\n  namespace: openshift-ingress\nspec:\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 80\n    - name: https\n      protocol: TCP\n      port: 443\n      targetPort: 443\n  selector:\n    ingresscontroller.operator.openshift.io/deployment-ingresscontroller: default\n  type: LoadBalancer\nEOF\n</code></pre> <p>We already configured the wildcard record in our example DNS config:</p> <pre><code>*.apps.example.krnl.es. IN A 192.168.122.23\n</code></pre> <p>So we should be able to reach the OCP Console now:</p> <pre><code>curl -kI https://console-openshift-console.apps.example.krnl.es\n\nHTTP/1.1 200 OK\n</code></pre> <p>And if we check the clusterversion and clusteroperator we should have everything up and running now:</p> <pre><code>oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get clusterversion,co\n\nNAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS\nclusterversion.config.openshift.io/version   4.11.5    True        False         3m32s   Cluster version is 4.11.5\n\nNAME                                                                           VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nclusteroperator.config.openshift.io/console                                    4.11.5    True        False         False      3m50s  \nclusteroperator.config.openshift.io/csi-snapshot-controller                    4.11.5    True        False         False      25m  \nclusteroperator.config.openshift.io/dns                                        4.11.5    True        False         False      23m  \nclusteroperator.config.openshift.io/image-registry                             4.11.5    True        False         False      23m  \nclusteroperator.config.openshift.io/ingress                                    4.11.5    True        False         False      53m  \nclusteroperator.config.openshift.io/insights                                   4.11.5    True        False         False      25m  \nclusteroperator.config.openshift.io/kube-apiserver                             4.11.5    True        False         False      54m  \nclusteroperator.config.openshift.io/kube-controller-manager                    4.11.5    True        False         False      54m  \nclusteroperator.config.openshift.io/kube-scheduler                             4.11.5    True        False         False      54m  \nclusteroperator.config.openshift.io/kube-storage-version-migrator              4.11.5    True        False         False      25m  \nclusteroperator.config.openshift.io/monitoring                                 4.11.5    True        False         False      21m  \nclusteroperator.config.openshift.io/network                                    4.11.5    True        False         False      25m  \nclusteroperator.config.openshift.io/openshift-apiserver                        4.11.5    True        False         False      54m  \nclusteroperator.config.openshift.io/openshift-controller-manager               4.11.5    True        False         False      54m  \nclusteroperator.config.openshift.io/openshift-samples                          4.11.5    True        False         False      23m  \nclusteroperator.config.openshift.io/operator-lifecycle-manager                 4.11.5    True        False         False      54m  \nclusteroperator.config.openshift.io/operator-lifecycle-manager-catalog         4.11.5    True        False         False      54m  \nclusteroperator.config.openshift.io/operator-lifecycle-manager-packageserver   4.11.5    True        False         False      54m  \nclusteroperator.config.openshift.io/service-ca                                 4.11.5    True        False         False      25m  \nclusteroperator.config.openshift.io/storage                                    4.11.5    True        False         False      25m  \n</code></pre>"},{"location":"how-to/agent/create-agent-cluster/#enabling-node-auto-scaling-for-the-hosted-cluster","title":"Enabling Node Auto-Scaling for the Hosted Cluster","text":"<p>Auto-scaling can be enabled, if we choose to enable auto-scaling, when more capacity is require in our Hosted Cluster a new Agent will be installed (providing that we have spare agents). In order to enable auto-scaling we can run the following command:</p> <p>INFO: In this case the minimum nodes will be 2 and the maximum 5.</p> <pre><code>oc -n ${CLUSTERS_NAMESPACE} patch nodepool ${HOSTED_CLUSTER_NAME} --type=json -p '[{\"op\": \"remove\", \"path\": \"/spec/replicas\"},{\"op\":\"add\", \"path\": \"/spec/autoScaling\", \"value\": { \"max\": 5, \"min\": 2 }}]'\n</code></pre> <p>If 10 minutes passes without requiring the additional capacity the agent will be decommissioned and placed in the spare queue again.</p> <ol> <li> <p>Let's create a workload that requires a new node.</p> <pre><code>cat &lt;&lt;EOF | oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: reversewords\n  name: reversewords\n  namespace: default\nspec:\n  replicas: 40\n  selector:\n    matchLabels:\n      app: reversewords\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: reversewords\n    spec:\n      containers:\n      - image: quay.io/mavazque/reversewords:latest\n        name: reversewords\n        resources:\n          requests:\n            memory: 2Gi\nstatus: {}\nEOF\n</code></pre> </li> <li> <p>We will see the remaining agent starts getting deployed.</p> <p>INFO: The spare agent <code>d9198891-39f4-4930-a679-65fb142b108b</code> started getting provisioned.</p> <pre><code>oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent -o jsonpath='{range .items[*]}BMH: {@.metadata.labels.agent-install\\.openshift\\.io/bmh} Agent: {@.metadata.name} State: {@.status.debugInfo.state}{\"\\n\"}{end}'\n\nBMH: ocp-worker-2 Agent: 4dac1ab2-7dd5-4894-a220-6a3473b67ee6 State: added-to-existing-cluster\nBMH: ocp-worker-0 Agent: d9198891-39f4-4930-a679-65fb142b108b State: installing-in-progress\nBMH: ocp-worker-1 Agent: da503cf1-a347-44f2-875c-4960ddb04091 State: added-to-existing-cluster\n</code></pre> </li> <li> <p>If we check the nodes we will see a new one joined the cluster.</p> <p>INFO: We got ocp-worker-0 added to the cluster</p> <pre><code>oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get nodes\n\nNAME           STATUS   ROLES    AGE   VERSION\nocp-worker-0   Ready    worker   35s   v1.24.0+3882f8f\nocp-worker-1   Ready    worker   40m   v1.24.0+3882f8f\nocp-worker-2   Ready    worker   41m   v1.24.0+3882f8f\n</code></pre> </li> <li> <p>If we delete the workload and wait 10 minutes the node will be removed.</p> <pre><code>oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig -n default delete deployment reversewords\n</code></pre> </li> <li> <p>After 10 minutes.</p> <pre><code>oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get nodes\n\nNAME           STATUS   ROLES    AGE   VERSION\nocp-worker-1   Ready    worker   51m   v1.24.0+3882f8f\nocp-worker-2   Ready    worker   52m   v1.24.0+3882f8f\n</code></pre> <pre><code>oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent -o jsonpath='{range .items[*]}BMH: {@.metadata.labels.agent-install\\.openshift\\.io/bmh} Agent: {@.metadata.name} State: {@.status.debugInfo.state}{\"\\n\"}{end}'\n\nBMH: ocp-worker-2 Agent: 4dac1ab2-7dd5-4894-a220-6a3473b67ee6 State: added-to-existing-cluster\nBMH: ocp-worker-0 Agent: d9198891-39f4-4930-a679-65fb142b108b State: known-unbound\nBMH: ocp-worker-1 Agent: da503cf1-a347-44f2-875c-4960ddb04091 State: added-to-existing-cluster\n</code></pre> </li> </ol>"},{"location":"how-to/agent/create-heterogeneous-nodepools/","title":"Create Heterogeneous NodePools on Agent HostedClusters","text":"<p>This document explains how to create heterogeneous nodepools on agent platform.  Please refer to set up the env for agent cluster, this document only covers the things you need to configure to have heterogeneous nodepools.</p>"},{"location":"how-to/agent/create-heterogeneous-nodepools/#configure-agentserviceconfig-with-two-heterogeneous-architecture-os-images","title":"Configure AgentServiceConfig with two heterogeneous architecture OS images","text":"<pre><code>export DB_VOLUME_SIZE=\"10Gi\"\nexport FS_VOLUME_SIZE=\"10Gi\"\nexport OCP_VERSION=\"4.15.0\"\nexport OCP_MAJMIN=${OCP_VERSION%.*}\n\nexport ARCH_X86=\"x86_64\"\nexport OCP_RELEASE_VERSION_X86=$(curl -s https://mirror.openshift.com/pub/openshift-v4/${ARCH_X86}/clients/ocp/${OCP_VERSION}/release.txt | awk '/machine-os / { print $2 }')\nexport ISO_URL_X86=\"https://mirror.openshift.com/pub/openshift-v4/${ARCH_X86}/dependencies/rhcos/${OCP_MAJMIN}/${OCP_VERSION}/rhcos-${OCP_VERSION}-${ARCH_X86}-live.${ARCH_X86}.iso\"\nexport ROOT_FS_URL_X8=\"https://mirror.openshift.com/pub/openshift-v4/${ARCH_X86}/dependencies/rhcos/${OCP_MAJMIN}/${OCP_VERSION}/rhcos-${OCP_VERSION}-${ARCH_X86}-live-rootfs.${ARCH_X86}.img\"\n\nexport ARCH_PPC64LE=\"ppc64le\"\nexport OCP_RELEASE_VERSION_PPC64LE=$(curl -s https://mirror.openshift.com/pub/openshift-v4/${ARCH_PPC64LE}/clients/ocp/${OCP_VERSION}/release.txt | awk '/machine-os / { print $2 }')\nexport ISO_URL_PPC64LE=\"https://mirror.openshift.com/pub/openshift-v4/${ARCH_PPC64LE}/dependencies/rhcos/${OCP_MAJMIN}/${OCP_VERSION}/rhcos-${OCP_VERSION}-${ARCH_PPC64LE}-live.${ARCH_PPC64LE}.iso\"\nexport ROOT_FS_URL_PPC64LE=\"https://mirror.openshift.com/pub/openshift-v4/${ARCH_PPC64LE}/dependencies/rhcos/${OCP_MAJMIN}/${OCP_VERSION}/rhcos-${OCP_VERSION}-${ARCH_PPC64LE}-live-rootfs.${ARCH_PPC64LE}.img\"\n\nenvsubst &lt;&lt;\"EOF\" | oc apply -f -\napiVersion: agent-install.openshift.io/v1beta1\nkind: AgentServiceConfig\nmetadata:\n name: agent\nspec:\n  databaseStorage:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: ${DB_VOLUME_SIZE}\n  filesystemStorage:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: ${FS_VOLUME_SIZE}\n  osImages:\n    - openshiftVersion: \"${OCP_VERSION}\"\n      version: \"${OCP_RELEASE_VERSION_X86}\"\n      url: \"${ISO_URL_X86}\"\n      rootFSUrl: \"${ROOT_FS_URL_X8}\"\n      cpuArchitecture: \"${ARCH_X86}\"\n    - openshiftVersion: \"${OCP_VERSION}\"\n      version: \"${OCP_RELEASE_VERSION_PPC64LE}\"\n      url: \"${ISO_URL_PPC64LE}\"\n      rootFSUrl: \"${ROOT_FS_URL_PPC64LE}\"\n      cpuArchitecture: \"${ARCH_PPC64LE}\"\nEOF\n</code></pre> <p>The above configuration allows you to create ISO for both x86_64 and ppc64le architectures.</p>"},{"location":"how-to/agent/create-heterogeneous-nodepools/#configure-dns","title":"Configure DNS","text":"<p><code>*.apps.&lt;cluster_name&gt;</code> record can be pointed to either one of the worker node where ingress application is hosted, or if you are able to set up a load balancer on top of the worker nodes, point this record to this load balancer. When you are creating heterogeneous nodepool, please make sure the workers are reachable from each other or keep them in the same network.</p>"},{"location":"how-to/agent/create-heterogeneous-nodepools/#create-a-hosted-cluster","title":"Create a Hosted Cluster","text":"<p>Need to use multi arch release image while creating the cluster to use heterogeneous nodepools. Find the latest multi arch images from here <pre><code>export CLUSTERS_NAMESPACE=\"clusters\"\nexport HOSTED_CLUSTER_NAME=\"example\"\nexport HOSTED_CONTROL_PLANE_NAMESPACE=\"${CLUSTERS_NAMESPACE}-${HOSTED_CLUSTER_NAME}\"\nexport BASEDOMAIN=\"krnl.es\"\nexport PULL_SECRET_FILE=$PWD/pull-secret\nexport OCP_RELEASE=4.15.0-multi\nexport MACHINE_CIDR=192.168.122.0/24\n# Typically the namespace is created by the hypershift-operator \n# but agent cluster creation generates a capi-provider role that\n# needs the namespace to already exist\noc create ns ${HOSTED_CONTROL_PLANE_NAMESPACE}\n\nhypershift create cluster agent \\\n    --name=${HOSTED_CLUSTER_NAME} \\\n    --pull-secret=${PULL_SECRET_FILE} \\\n    --agent-namespace=${HOSTED_CONTROL_PLANE_NAMESPACE} \\\n    --base-domain=${BASEDOMAIN} \\\n    --api-server-address=api.${HOSTED_CLUSTER_NAME}.${BASEDOMAIN} \\\n    --release-image=quay.io/openshift-release-dev/ocp-release:${OCP_RELEASE}\n</code></pre></p>"},{"location":"how-to/agent/create-heterogeneous-nodepools/#create-heterogeneous-nodepool","title":"Create Heterogeneous NodePool","text":"<p>By default, the previous section creates a nodepool on <code>x86_64</code> architecture, to create heterogeneous nodepools, you can apply the below manifest with the architecture you want.</p> <p><pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: NodePool\nmetadata:\n  name: ${HOSTED_CLUSTER_NAME}\n  namespace: ${CLUSTERS_NAMESPACE}\nspec:\n  arch: ${ARCH_PPC64LE}\n  clusterName: ${HOSTED_CLUSTER_NAME}\n  management:\n    autoRepair: false\n    upgradeType: InPlace\n  nodeDrainTimeout: 0s\n  nodeVolumeDetachTimeout: 0s\n  platform:\n    agent:\n      agentLabelSelector:\n        matchLabels:\n          inventory.agent-install.openshift.io/cpu-architecture: ${ARCH_PPC64LE}\n    type: Agent\n  release:\n    image: quay.io/openshift-release-dev/ocp-release:${OCP_RELEASE}\n  replicas: 0\n</code></pre> This will create nodepool of architecture <code>ppc64le</code> with 0 replicas.</p> <p><pre><code>      agentLabelSelector:\n        matchLabels:\n          inventory.agent-install.openshift.io/cpu-architecture: ppc64le\n</code></pre> This selector block is used to select the agents which are matching given label. Which will ensure it will select only the agents from <code>ppc64le</code> arch when it scaled.</p>"},{"location":"how-to/agent/create-heterogeneous-nodepools/#create-infraenv","title":"Create Infraenv","text":"<p>For heterogeneous nodepools, you need to create infraenv for each architecture you are going to have in your HCP. i.e. for heterogeneous nodepools with x86_64 and ppc64le architecture, you will need to create two infraenvs with both architectures.</p> <p><pre><code>export SSH_PUB_KEY=$(cat $HOME/.ssh/id_rsa.pub)\n\nenvsubst &lt;&lt;\"EOF\" | oc apply -f -\napiVersion: agent-install.openshift.io/v1beta1\nkind: InfraEnv\nmetadata:\n  name: ${HOSTED_CLUSTER_NAME}-${ARCH_X86}\n  namespace: ${HOSTED_CONTROL_PLANE_NAMESPACE}\nspec:\n  cpuArchitecture: ${ARCH_X86}\n  pullSecretRef:\n    name: pull-secret\n  sshAuthorizedKey: ${SSH_PUB_KEY}\nEOF\n\nenvsubst &lt;&lt;\"EOF\" | oc apply -f -\napiVersion: agent-install.openshift.io/v1beta1\nkind: InfraEnv\nmetadata:\n  name: ${HOSTED_CLUSTER_NAME}-${ARCH_PPC64LE}\n  namespace: ${HOSTED_CONTROL_PLANE_NAMESPACE}\nspec:\n  cpuArchitecture: ${ARCH_PPC64LE}\n  pullSecretRef:\n    name: pull-secret\n  sshAuthorizedKey: ${SSH_PUB_KEY}\nEOF\n</code></pre> This will create two infraenvs with x86_64 &amp; ppc64le architectures. Before creating this, need to ensure respective architecture's OS image is added in <code>AgentServiceConfig</code></p> <p>After this you can use the above infraenvs to get the minimal iso and follow the create-agent-cluster.md to add them to your cluster as agents.</p>"},{"location":"how-to/agent/create-heterogeneous-nodepools/#scale-the-nodepool","title":"Scale the NodePool","text":"<p>Once your agents are approved, you can scale the nodepools. <code>agentLabelSelector</code> configured in nodepool ensures that only matching agents gets added to the cluster. This also aids in descaling the nodepool. To remove specific arch nodes from the cluster, you can descale the corresponding nodepool.</p>"},{"location":"how-to/agent/exposing-services-from-hcp/","title":"Exposing the Hosted Control Plane Services","text":"<p>To publish the services from the Hosted Control Plane, we need to understand the available strategies and their implications. Let's explore them.</p>"},{"location":"how-to/agent/exposing-services-from-hcp/#service-publishing-strategies","title":"Service Publishing Strategies","text":"<p>Let's delve into the motivations for each of the strategies.</p> <p>The NodePort strategy allows you to expose services without requiring a logical LoadBalancer like MetalLB or similar infrastructure. It is one of the simplest methods to implement. This strategy is supported by all services; however, the limitation arises in high availability (HA) environments where you will be pointing to one of the NodePorts instead of all three.</p> <p>The LoadBalancer strategy enables you to expose certain services through a load balancer. While not all services support this strategy, it is the preferred method for exposing the KubeApiServer, as it allows for a single entry point in an HA configuration without relying in the Ingress Controller of the Manamgement cluster.</p> <p>The Route strategy allows you to expose the HostedControlPlane services using the ingress of the Management OpenShift cluster. This strategy is supported by all services but kubeapi-server.</p>"},{"location":"how-to/agent/exposing-services-from-hcp/#nodeport","title":"NodePort","text":"<p>Exposing a service via NodePort is a method used in OpenShift to make a service accessible from outside the cluster. When you expose a service using NodePort, OpenShift allocates a port on each node in the cluster (if the cluster availability policy is set to HighlyAvailable). This port on each node is mapped to the port of the service, allowing external traffic to reach the service by accessing any node's IP address and the allocated NodePort.</p> <p>This is the default configuration when you use <code>Agent</code> and <code>None</code> provider platforms. The services relevant for on-premise platforms are:</p> <ul> <li>APIServer</li> <li>OAuthServer</li> <li>Konnectivity</li> <li>Ignition</li> </ul> <p>Note</p> <p>If any of the services are not relevant for your deployment, it is not necessary to specify them.</p> <p>Here is how it looks in the HostedCluster CR:</p> <pre><code>spec:\n...\n...\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: &lt;IP_ADDRESS&gt;\n        port: &lt;PORT&gt;\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: 10.103.101.101\n      type: NodePort\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: 10.103.101.101\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: 10.103.101.101\n      type: NodePort\n...\n...\n</code></pre>"},{"location":"how-to/agent/exposing-services-from-hcp/#route","title":"Route","text":"<p>OpenShift routes provide a way to expose services within the cluster to external clients. A route in OpenShift maps an external request (typically HTTP/HTTPS) to an internal service. The route specifies a hostname that external clients will use to access the service. OpenShift\u2019s router (based on HAProxy) will handle the traffic coming to this hostname.</p> <p>HostedControlPlanes operate in two domains: the Control Plane and the Data Plane. The Control Plane uses routes through the MGMT Cluster ingress to expose services for each of the HostedControlPlanes, and the routes are created in the HostedControlPlane namespace. For the Data Plane, the Ingress handles <code>*.apps.subdomain.tld</code> URLs, and all routes under this wildcard are directed to the Namespace by the OpenShift Router on the worker nodes.</p> <p>The usual configuration for the Hosted Cluster is similar to the LoadBalancer setup we will discuss next.</p>"},{"location":"how-to/agent/exposing-services-from-hcp/#loadbalancer","title":"LoadBalancer","text":"<p>The LoadBalancer strategy in OpenShift is used to expose services to external clients using an external load balancer. When you create a service of type LoadBalancer, Kubernetes interacts with the underlying cloud platform or appropriate LoadBalancer controllers to provision an external load balancer, which then routes traffic to the service's endpoints (pods).</p> <p>This is how looks like the most common configuration to expose the services from HCP side:</p> <pre><code>spec:\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      type: LoadBalancer\n  - service: OAuthServer\n    servicePublishingStrategy:\n      type: Route\n  - service: OIDC\n    servicePublishingStrategy:\n      type: Route\n      Route:\n        hostname: &lt;URL&gt;\n  - service: Konnectivity\n    servicePublishingStrate\n      type: Route\n  - service: Ignition\n    servicePublishingStrategy:\n      type: Route\n</code></pre> <p>If you wanna know more about how to expose the ingress service in the Data Plane side, please access the Recipes section to see how to do it with MetalLB.</p>"},{"location":"how-to/agent/global-pull-secret/","title":"Global Pull Secret for Hosted Control Planes","text":""},{"location":"how-to/agent/global-pull-secret/#overview","title":"Overview","text":"<p>The Global Pull Secret functionality enables Hosted Cluster administrators to include additional pull secrets for accessing container images from private registries without requiring assistance from the Management Cluster administrator. This feature allows you to merge your custom pull secret with the original HostedCluster pull secret, making it available to all nodes in the cluster.</p> <p>The implementation uses a DaemonSet approach that automatically detects when you create an <code>additional-pull-secret</code> in the <code>kube-system</code> namespace of your DataPlane (Hosted Cluster). The system then merges this secret with the original pull secret and deploys the merged result to all nodes via a DaemonSet that updates the kubelet configuration.</p> <p>Note</p> <p>This feature is designed to work autonomously - once you create the additional pull secret, the system automatically handles the rest without requiring Management Cluster administrator intervention.</p>"},{"location":"how-to/agent/global-pull-secret/#adding-your-pull-secret","title":"Adding your Pull Secret","text":"<p>Important</p> <p>All actions described in this section must be performed on the HostedCluster's workers (DataPlane), not on the Management Cluster.</p> <p>To use this functionality, follow these steps:</p>"},{"location":"how-to/agent/global-pull-secret/#1-create-your-additional-pull-secret","title":"1. Create your additional pull secret","text":"<p>Create a secret named <code>additional-pull-secret</code> in the <code>kube-system</code> namespace of your Hosted Cluster (DataPlane). The secret must contain a valid DockerConfigJSON format:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: additional-pull-secret\n  namespace: kube-system\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: &lt;base64-encoded-docker-config-json&gt;\n</code></pre>"},{"location":"how-to/agent/global-pull-secret/#2-example-dockerconfigjson-format","title":"2. Example DockerConfigJSON format","text":"<p>Your <code>.dockerconfigjson</code> should follow this structure:</p> <pre><code>{\n  \"auths\": {\n    \"registry.example.com\": {\n      \"auth\": \"base64-encoded-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"base64-encoded-credentials\"\n    }\n  }\n}\n</code></pre> <p>Using Namespace-Specific Registry Entries</p> <p>For registries like Quay.io that support organization/namespace-specific authentication, you can specify the full path in your registry entry (e.g., <code>quay.io/mycompany</code> instead of just <code>quay.io</code>). This allows you to provide different credentials for different namespaces within the same registry, and helps avoid conflicts with existing registry entries in the original pull secret.</p>"},{"location":"how-to/agent/global-pull-secret/#3-apply-the-secret","title":"3. Apply the secret","text":"<pre><code>kubectl apply -f additional-pull-secret.yaml\n</code></pre>"},{"location":"how-to/agent/global-pull-secret/#4-verification","title":"4. Verification","text":"<p>After creating the secret, the system will automatically:</p> <ol> <li>Validate the secret format</li> <li>Merge it with the original pull secret</li> <li>Deploy a DaemonSet to all nodes</li> <li>Update the kubelet configuration on each node</li> </ol> <p>You can verify the deployment by checking:</p> <pre><code># Check if the DaemonSet is running\nkubectl get daemonset global-pull-secret-syncer -n kube-system\n\n# Check the merged pull secret\nkubectl get secret global-pull-secret -n kube-system\n\n# Check DaemonSet pods\nkubectl get pods -n kube-system -l name=global-pull-secret-syncer\n</code></pre>"},{"location":"how-to/agent/global-pull-secret/#how-it-works","title":"How it works","text":"<p>The Global Pull Secret functionality operates through a multi-component system:</p>"},{"location":"how-to/agent/global-pull-secret/#automatic-detection","title":"Automatic Detection","text":"<ul> <li>The Hosted Cluster Config Operator (HCCO) continuously monitors the <code>kube-system</code> namespace</li> <li>When it detects the creation of <code>additional-pull-secret</code>, it triggers the reconciliation process</li> </ul>"},{"location":"how-to/agent/global-pull-secret/#validation-and-merging","title":"Validation and Merging","text":"<ul> <li>The system validates that your secret contains a proper DockerConfigJSON format</li> <li>It retrieves the original pull secret from the HostedControlPlane</li> <li>Your additional pull secret is merged with the original one</li> <li>If there are conflicting registry entries, the original pull secret takes precedence (the additional pull secret entry is ignored for conflicting registries)</li> <li>The system supports namespace-specific registry entries (e.g., <code>quay.io/namespace</code>) for better credential specificity</li> </ul>"},{"location":"how-to/agent/global-pull-secret/#deployment-process","title":"Deployment Process","text":"<ul> <li>A <code>global-pull-secret</code> is created in the <code>kube-system</code> namespace containing the merged result</li> <li>RBAC resources (ServiceAccount, Role, RoleBinding) are created for the DaemonSet in both <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>We use Role and RoleBinding in both namespaces to access secrets in <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>A DaemonSet named <code>global-pull-secret-syncer</code> is deployed to eligible nodes</li> </ul> <p>NodePool InPlace Strategy Restriction</p> <p>The Global Pull Secret DaemonSet is not deployed to nodes that belong to NodePools using the InPlace upgrade strategy. This restriction prevents conflicts between the DaemonSet's modifications to <code>/var/lib/kubelet/config.json</code> and the Machine Config Daemon (MCD) during InPlace upgrades.</p> <ul> <li>Nodes with Replace strategy: \u2705 Receive Global Pull Secret DaemonSet</li> <li>Nodes with InPlace strategy: \u274c Do not receive Global Pull Secret DaemonSet</li> </ul> <p>This ensures that MCD operations during InPlace upgrades do not fail due to unexpected changes in kubelet configuration files.</p>"},{"location":"how-to/agent/global-pull-secret/#node-level-synchronization","title":"Node-Level Synchronization","text":"<ul> <li>Each DaemonSet pod runs a controller that watches the secrets under kube-system namespace</li> <li>When changes are detected, it updates <code>/var/lib/kubelet/config.json</code> on the node</li> <li>The kubelet service is restarted via DBus to apply the new configuration</li> <li>If the restart fails after 3 attempts, the system rolls back the file changes</li> </ul>"},{"location":"how-to/agent/global-pull-secret/#automatic-cleanup","title":"Automatic Cleanup","text":"<ul> <li>If you delete the <code>additional-pull-secret</code>, the HCCO automatically removes the <code>global-pull-secret</code> secret</li> <li>The system reverts to using only the original pull secret from the HostedControlPlane</li> <li>The DaemonSet continues running but now syncs only the original pull secret to nodes</li> </ul>"},{"location":"how-to/agent/global-pull-secret/#registry-precedence-and-conflict-resolution","title":"Registry Precedence and Conflict Resolution","text":"<p>The Global Pull Secret system uses a specific precedence model when merging your additional pull secret with the original one:</p>"},{"location":"how-to/agent/global-pull-secret/#merge-behavior","title":"Merge Behavior","text":"<ul> <li>Original pull secret entries always take precedence over additional pull secret entries for the same registry</li> <li>If both secrets contain an entry for <code>quay.io</code>, the original pull secret's credentials will be used</li> <li>Your additional pull secret entries are only added if they don't conflict with existing entries</li> <li>Warnings are logged when conflicts are detected</li> </ul>"},{"location":"how-to/agent/global-pull-secret/#recommended-approach","title":"Recommended Approach","text":"<p>To avoid conflicts and ensure your credentials are used, consider these strategies:</p> <ol> <li>Use namespace-specific entries: Instead of <code>quay.io</code>, use <code>quay.io/your-namespace</code></li> <li>Target specific registries: Add entries only for registries not already in the original pull secret</li> <li>Check existing entries: Review what registries are already configured in the HostedControlPlane</li> </ol>"},{"location":"how-to/agent/global-pull-secret/#example-merge-scenario","title":"Example Merge Scenario","text":"<p>Original Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"original-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Your Additional Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"your-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"your-namespace-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Resulting Merged Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"original-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"your-namespace-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Note how the <code>quay.io</code> entry keeps the original credentials, but <code>quay.io/mycompany</code> is added from your additional secret.</p>"},{"location":"how-to/agent/global-pull-secret/#implementation-details","title":"Implementation details","text":"<p>The implementation consists of several key components working together:</p>"},{"location":"how-to/agent/global-pull-secret/#core-components","title":"Core Components","text":"<ol> <li>Global Pull Secret Controller (<code>globalps</code> package)</li> <li>Handles validation of user-provided pull secrets</li> <li>Manages the merging logic between original and additional pull secrets</li> <li>Creates and manages RBAC resources</li> <li>Deploys and manages the DaemonSet</li> <li> <p>Node eligibility assessment: Labels nodes from InPlace NodePools and configures DaemonSet scheduling restrictions</p> </li> <li> <p>Sync Global Pull Secret Command (<code>sync-global-pullsecret</code> package)</p> </li> <li>Runs as a DaemonSet on each node</li> <li>Watches for changes to the <code>global-pull-secret</code> in <code>kube-system</code> namespace</li> <li>Accesses the original <code>pull-secret</code> in <code>openshift-config</code> namespace</li> <li>Updates the kubelet configuration file</li> <li> <p>Manages kubelet service restarts via DBus</p> </li> <li> <p>Hosted Cluster Config Operator Integration</p> </li> <li>Monitors for the presence of <code>additional-pull-secret</code></li> <li>Orchestrates the entire process</li> <li>Handles cleanup when the secret is removed</li> </ol>"},{"location":"how-to/agent/global-pull-secret/#architecture-diagram","title":"Architecture Diagram","text":"graph TB     %% User Input     User[User creates additional-pull-secret] --&gt; |kube-system namespace| AdditionalPS[additional-pull-secret Secret]      %% HCCO Controller     HCCO[Hosted Cluster Config Operator] --&gt; |Watches kube-system secrets| GlobalPSController[Global Pull Secret Controller]     GlobalPSController --&gt; |Validates| AdditionalPS     GlobalPSController --&gt; |Gets original| OriginalPS[Original pull-secret from HCP]      %% Secret Processing     AdditionalPS --&gt; |Validates format| ValidatePS[Validate Additional Pull Secret]     OriginalPS --&gt; |Extracts data| OriginalPSData[Original Pull Secret Data]     ValidatePS --&gt; |Extracts data| AdditionalPSData[Additional Pull Secret Data]      %% Merge Process     OriginalPSData --&gt; MergeSecrets[Merge Pull Secrets]     AdditionalPSData --&gt; MergeSecrets     MergeSecrets --&gt; |Creates merged JSON| GlobalPSData[Global Pull Secret Data]      %% Secret Creation     GlobalPSData --&gt; |Creates in kube-system| GlobalPSSecret[global-pull-secret Secret]      %% RBAC Setup     GlobalPSController --&gt; |Creates RBAC| RBACSetup[Setup RBAC Resources]     RBACSetup --&gt; ServiceAccount[global-pull-secret-syncer ServiceAccount]     RBACSetup --&gt; KubeSystemRole[global-pull-secret-syncer Role in kube-system]     RBACSetup --&gt; KubeSystemRoleBinding[global-pull-secret-syncer RoleBinding in kube-system]     RBACSetup --&gt; OpenshiftConfigRole[global-pull-secret-syncer Role in openshift-config]     RBACSetup --&gt; OpenshiftConfigRoleBinding[global-pull-secret-syncer RoleBinding in openshift-config]      %% DaemonSet Deployment     GlobalPSController --&gt; |Deploys DaemonSet| DaemonSet[global-pull-secret-syncer DaemonSet]     DaemonSet --&gt; |Runs on each node| DaemonSetPod[DaemonSet Pod]      %% DaemonSet Pod Details     DaemonSetPod --&gt; |Mounts host paths| HostMounts[Host Path Mounts]     HostMounts --&gt; KubeletPath[\"/var/lib/kubelet\"]     HostMounts --&gt; DbusPath[\"/var/run/dbus\"]      %% Container Execution     DaemonSetPod --&gt; |Runs command| Container[control-plane-operator Container]     Container --&gt; |Executes| SyncCommand[sync-global-pullsecret command]      %% Sync Process     SyncCommand --&gt; |Watches global-pull-secret| SyncController[Global Pull Secret Reconciler]     SyncController --&gt; |Reads secret| ReadGlobalPS[Read global-pull-secret]     SyncController --&gt; |Reads original| ReadOriginalPS[Read original pull-secret]      %% File Update Process     ReadGlobalPS --&gt; |Gets data| GlobalPSBytes[Global Pull Secret Bytes]     ReadOriginalPS --&gt; |Gets data| OriginalPSBytes[Original Pull Secret Bytes]      %% Decision Logic     GlobalPSBytes --&gt; |If exists| UseGlobalPS[Use Global Pull Secret]     OriginalPSBytes --&gt; |If not exists| UseOriginalPS[Use Original Pull Secret]      %% File Update     UseGlobalPS --&gt; |Updates file| UpdateKubeletConfig[\"Update /var/lib/kubelet/config.json\"]     UseOriginalPS --&gt; |Updates file| UpdateKubeletConfig      %% Kubelet Restart     UpdateKubeletConfig --&gt; |Restarts kubelet| RestartKubelet[Restart kubelet.service via systemd]     RestartKubelet --&gt; |Via dbus| DbusConnection[DBus Connection]      %% Error Handling     UpdateKubeletConfig --&gt; |If restart fails| RollbackProcess[Rollback Process]     RollbackProcess --&gt; |Restore original| RestoreOriginal[Restore Original File Content]      %% Cleanup Process     GlobalPSController --&gt; |If additional PS deleted| CleanupProcess[Cleanup Process]     CleanupProcess --&gt; |Deletes global PS| DeleteGlobalPS[Delete global-pull-secret]     CleanupProcess --&gt; |Removes DaemonSet| RemoveDaemonSet[Remove DaemonSet]      %% Styling     classDef userInput fill:#e1f5fe     classDef controller fill:#f3e5f5     classDef secret fill:#e8f5e8     classDef process fill:#fff3e0     classDef daemonSet fill:#fce4ec     classDef fileSystem fill:#f1f8e9      class User,AdditionalPS userInput     class HCCO,GlobalPSController,SyncController controller     class OriginalPS,GlobalPSSecret,ServiceAccount,KubeSystemRole,KubeSystemRoleBinding,OpenshiftConfigRole,OpenshiftConfigRoleBinding secret     class ValidatePS,MergeSecrets,RBACSetup,UpdateKubeletConfig,RestartKubelet process     class DaemonSet,DaemonSetPod,Container daemonSet     class KubeletPath,DbusPath fileSystem"},{"location":"how-to/agent/global-pull-secret/#key-features","title":"Key Features","text":"<ul> <li>Security: Only watches specific secrets in <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>Robustness: Includes automatic rollback in case of failures</li> <li>Efficiency</li> <li>Only updates when there are actual changes</li> <li>The globalPullSecret implementation has their own controller so it cannot interfere with the HCCO reonciliation</li> <li>Security considerations: Uses specific RBAC for only the required resources in each namespace. The DaemonSet containers run in privileged mode due to the need to:</li> <li>Write to <code>/var/lib/kubelet/config.json</code> (kubelet configuration file)</li> <li>Connect to systemd via DBus for service management</li> <li>Restart kubelet.service, which requires root privileges</li> <li>Smart node targeting: Automatically excludes nodes from InPlace NodePools to prevent MCD conflicts</li> </ul>"},{"location":"how-to/agent/global-pull-secret/#inplace-nodepool-handling","title":"InPlace NodePool Handling","text":"<p>To prevent conflicts with Machine Config Daemon operations, the implementation includes intelligent node targeting:</p>"},{"location":"how-to/agent/global-pull-secret/#node-labeling-process","title":"Node Labeling Process","text":"<ol> <li>MachineSets Discovery: The controller queries the management cluster for MachineSets with InPlace-specific annotations (<code>hypershift.openshift.io/nodePoolTargetConfigVersion</code>)</li> <li>Machine Enumeration: For each InPlace MachineSets, it lists all associated Machines</li> <li>Node Identification: Maps Machine objects to their corresponding nodes via <code>machine.Status.NodeRef.Name</code></li> <li>Labeling: Applies <code>hypershift.openshift.io/nodepool-inplace-strategy=true</code> label to identified nodes</li> </ol>"},{"location":"how-to/agent/global-pull-secret/#daemonset-scheduling-configuration","title":"DaemonSet Scheduling Configuration","text":"<p>The DaemonSet uses NodeAffinity to exclude InPlace nodes:</p> <pre><code>spec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: hypershift.openshift.io/nodepool-inplace-strategy\n                operator: DoesNotExist\n</code></pre> <p>This ensures that: - Nodes without the label: \u2705 Are eligible for DaemonSet scheduling - Nodes with the label (any value): \u274c Are excluded from DaemonSet scheduling</p>"},{"location":"how-to/agent/global-pull-secret/#conflict-prevention-benefits","title":"Conflict Prevention Benefits","text":"<ul> <li>Prevents MCD failures: Avoids conflicts when MCD expects specific kubelet configuration during InPlace upgrades</li> <li>Maintains upgrade reliability: InPlace upgrade processes are not interrupted by Global Pull Secret modifications</li> <li>Automatic detection: No manual intervention required - the system automatically identifies and handles InPlace nodes</li> </ul>"},{"location":"how-to/agent/global-pull-secret/#error-handling","title":"Error Handling","text":"<p>The system includes comprehensive error handling:</p> <ul> <li>Validation errors: Invalid DockerConfigJSON format is caught early</li> <li>Restart failures: If kubelet restart fails after 3 attempts, the file is rolled back</li> <li>Resource cleanup: If the additional pull secret is deleted, the HCCO automatically removes the globalPullSecret</li> </ul> <p>This implementation provides a secure, autonomous solution that allows HostedCluster administrators to add private registry credentials without requiring Management Cluster administrator intervention.</p>"},{"location":"how-to/agent/other-sdn-providers/","title":"Other sdn providers","text":"<p>This document explains how to create a HostedCluster that runs an SDN provider different from OVNKubernetes. The document assumes that you already have the required infrastructure in place to create HostedClusters.</p> <p>Important</p> <p>The work described here is not supported. SDN providers must certify their software on HyperShift before it becomes a supported solution. The steps described here are just a technical reference for people who wants to try different SDN providers in HyperShift.</p> <p>Versions used while writing this doc:</p> <ul> <li>Management cluster running OpenShift <code>v4.14.5</code> and HyperShift Operator version <code>e87182ca75da37c74b371aa0f17aeaa41437561a</code>.</li> <li>HostedCluster release set to OpenShift <code>v4.14.10</code>.</li> </ul> <p>Important</p> <p>To configure a different CNI provider for the Hosted Cluster, you must adjust the <code>hostedcluster.spec.networking.networkType</code> to <code>Other</code>. By doing so, the Control Plane Operator will skip the deployment of the default CNI provider.</p>"},{"location":"how-to/agent/other-sdn-providers/#calico","title":"Calico","text":""},{"location":"how-to/agent/other-sdn-providers/#deployment","title":"Deployment","text":"<p>In this scenario we are using the Calico version v3.27.0 which is the last one at the time of this writing. The steps followed rely on the docs by Tigera to deploy Calico on OpenShift.</p> <ol> <li> <p>Create a <code>HostedCluster</code> and set its <code>HostedCluster.spec.networking.networkType</code> to <code>Other</code>.</p> </li> <li> <p>Wait for the HostedCluster's API to be ready. Once it's ready, get the admin kubeconfig.</p> </li> <li> <p>Eventually the compute nodes will show up in the cluster. Keep in mind since the SDN is not deployed yet, they will remain in <code>NotReady</code> state.</p> <pre><code>export KUBECONFIG=/path/to/hostedcluster/admin/kubeconfig\noc get nodes\n</code></pre> <pre><code>NAME             STATUS     ROLES    AGE     VERSION\nhosted-worker1   NotReady   worker   2m51s   v1.27.8+4fab27b\nhosted-worker2   NotReady   worker   2m52s   v1.27.8+4fab27b\n</code></pre> </li> <li> <p>Apply the yaml manifests provided by <code>Tigera</code> in the HostedCluster:</p> <pre><code>mkdir calico\nwget -qO- https://github.com/projectcalico/calico/releases/download/v3.27.0/ocp.tgz | tar xvz --strip-components=1 -C calico\ncd calico/\nls *crd*.yaml | xargs -n1 oc apply -f\nls 00* | xargs -n1 oc apply -f\nls 01* | xargs -n1 oc apply -f\nls 02* | xargs -n1 oc apply -f\n</code></pre> <pre><code>customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/apiservers.operator.tigera.io created\ncustomresourcedefinition.apiextensions.k8s.io/imagesets.operator.tigera.io created\ncustomresourcedefinition.apiextensions.k8s.io/installations.operator.tigera.io created\ncustomresourcedefinition.apiextensions.k8s.io/tigerastatuses.operator.tigera.io created\nnamespace/calico-apiserver created\nnamespace/calico-system created\nnamespace/tigera-operator created\napiserver.operator.tigera.io/default created\ninstallation.operator.tigera.io/default created\nconfigmap/calico-resources created\nclusterrolebinding.rbac.authorization.k8s.io/tigera-operator created\nclusterrole.rbac.authorization.k8s.io/tigera-operator created\nserviceaccount/tigera-operator created\ndeployment.apps/tigera-operator created\n</code></pre> </li> </ol>"},{"location":"how-to/agent/other-sdn-providers/#checks","title":"Checks","text":"<p>We should see the following pods running in the <code>tigera-operator</code> namespace:</p> <pre><code>oc -n tigera-operator get pods\n</code></pre> <pre><code>NAME                              READY   STATUS    RESTARTS   AGE\ntigera-operator-dc7c9647f-fvcvd   1/1     Running   0          2m1s\n</code></pre> <p>We should see the following pods running in the <code>calico-system</code> namespace:</p> <pre><code>oc -n calico-system get pods\n</code></pre> <pre><code>NAME                                       READY   STATUS    RESTARTS   AGE\ncalico-kube-controllers-69d6d5ff89-5ftcn   1/1     Running   0          2m1s\ncalico-node-6bzth                          1/1     Running   0          2m2s\ncalico-node-bl4b6                          1/1     Running   0          2m2s\ncalico-typha-6558c4c89d-mq2hw              1/1     Running   0          2m2s\ncsi-node-driver-l948w                      2/2     Running   0          2m1s\ncsi-node-driver-r6rgw                      2/2     Running   0          2m2s\n</code></pre> <p>We should see the following pods running in the <code>calico-apiserver</code> namespace:</p> <pre><code>oc -n calico-apiserver get pods\n</code></pre> <pre><code>NAME                                READY   STATUS    RESTARTS   AGE\ncalico-apiserver-7bfbf8fd7c-d75fw   1/1     Running   0          84s\ncalico-apiserver-7bfbf8fd7c-fqxjm   1/1     Running   0          84s\n</code></pre> <p>The nodes should've moved to <code>Ready</code> state:</p> <pre><code>oc get nodes\n</code></pre> <pre><code>NAME             STATUS   ROLES    AGE   VERSION\nhosted-worker1   Ready    worker   10m   v1.27.8+4fab27b\nhosted-worker2   Ready    worker   10m   v1.27.8+4fab27b\n</code></pre> <p>The HostedCluster deployment will continue, at this point the SDN is running.</p>"},{"location":"how-to/agent/other-sdn-providers/#cilium","title":"Cilium","text":""},{"location":"how-to/agent/other-sdn-providers/#deployment_1","title":"Deployment","text":"<p>In this scenario we are using the Cilium version v1.14.5 which is the last one at the time of this writing. The steps followed rely on the docs by Cilium project to deploy Cilium on OpenShift.</p> <ol> <li> <p>Create a <code>HostedCluster</code> and set its <code>HostedCluster.spec.networking.networkType</code> to <code>Other</code>.</p> </li> <li> <p>Wait for the HostedCluster's API to be ready. Once it's ready, get the admin kubeconfig.</p> </li> <li> <p>Eventually the compute nodes will show up in the cluster. Keep in mind since the SDN is not deployed yet, they will remain in <code>NotReady</code> state.</p> <pre><code>export KUBECONFIG=/path/to/hostedcluster/admin/kubeconfig\noc get nodes\n</code></pre> <pre><code>NAME             STATUS     ROLES    AGE     VERSION\nhosted-worker1   NotReady   worker   2m30s   v1.27.8+4fab27b\nhosted-worker2   NotReady   worker   2m33s   v1.27.8+4fab27b\n</code></pre> </li> <li> <p>Apply the yaml manifests provided by <code>Isovalent</code> in the HostedCluster:</p> <pre><code>#!/bin/bash\n\nversion=\"1.14.5\"\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-03-cilium-ciliumconfigs-crd.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00000-cilium-namespace.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00001-cilium-olm-serviceaccount.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00002-cilium-olm-deployment.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00003-cilium-olm-service.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00004-cilium-olm-leader-election-role.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00005-cilium-olm-role.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00006-leader-election-rolebinding.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00007-cilium-olm-rolebinding.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00008-cilium-cilium-olm-clusterrole.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00009-cilium-cilium-clusterrole.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00010-cilium-cilium-olm-clusterrolebinding.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00011-cilium-cilium-clusterrolebinding.yaml\n</code></pre> </li> <li> <p>Use the right configuration for each network stack</p> </li> </ol> IPv4IPv6Dual stack <pre><code>apiVersion: cilium.io/v1alpha1\nkind: CiliumConfig\nmetadata:\n  name: cilium\n  namespace: cilium\nspec:\n  debug:\n    enabled: true\n  k8s:\n    requireIPv4PodCIDR: true\n  logSystemLoad: true\n  bpf:\n    preallocateMaps: true\n  etcd:\n    leaseTTL: 30s\n  ipv4:\n    enabled: true\n  ipv6:\n    enabled: false\n  identityChangeGracePeriod: 0s\n  ipam:\n    mode: \"cluster-pool\"\n    operator:\n      clusterPoolIPv4PodCIDRList:\n        - \"10.128.0.0/14\"\n      clusterPoolIPv4MaskSize: \"23\"\n  nativeRoutingCIDR: \"10.128.0.0/14\"\n  endpointRoutes: {enabled: true}\n  clusterHealthPort: 9940\n  tunnelPort: 4789\n  cni:\n    binPath: \"/var/lib/cni/bin\"\n    confPath: \"/var/run/multus/cni/net.d\"\n    chainingMode: portmap\n  prometheus:\n    serviceMonitor: {enabled: false}\n  hubble:\n    tls: {enabled: false}\n  sessionAffinity: true\n</code></pre> <pre><code>oc apply -f ciliumconfig.yaml\n</code></pre> <pre><code>apiVersion: cilium.io/v1alpha1\nkind: CiliumConfig\nmetadata:\n  name: cilium\n  namespace: cilium\nspec:\n  debug:\n    enabled: true\n  k8s:\n    requireIPv6PodCIDR: true\n  logSystemLoad: true\n  bpf:\n    preallocateMaps: true\n  etcd:\n    leaseTTL: 30s\n  ipv4:\n    enabled: false\n  ipv6:\n    enabled: true\n  identityChangeGracePeriod: 0s\n  ipam:\n    mode: \"cluster-pool\"\n    operator:\n      clusterPoolIPv6PodCIDRList:\n        - \"fd01::/48\"\n      clusterPoolIPv6MaskSize: \"48\"\n  nativeRoutingCIDR: \"fd01::/48\"\n  endpointRoutes: {enabled: true}\n  clusterHealthPort: 9940\n  tunnelPort: 4789\n  cni:\n    binPath: \"/var/lib/cni/bin\"\n    confPath: \"/var/run/multus/cni/net.d\"\n    chainingMode: portmap\n  prometheus:\n    serviceMonitor: {enabled: false}\n  hubble:\n    tls: {enabled: false}\n  sessionAffinity: true\n</code></pre> <pre><code>oc apply -f ciliumconfig.yaml\n</code></pre> <pre><code>apiVersion: cilium.io/v1alpha1\nkind: CiliumConfig\nmetadata:\n  name: cilium\n  namespace: cilium\nspec:\n  debug:\n    enabled: true\n  k8s:\n    requireIPv4PodCIDR: true\n  logSystemLoad: true\n  bpf:\n    preallocateMaps: true\n  etcd:\n    leaseTTL: 30s\n  ipv4:\n    enabled: true\n  ipv6:\n    enabled: true\n  identityChangeGracePeriod: 0s\n  ipam:\n    mode: \"cluster-pool\"\n    operator:\n      clusterPoolIPv4PodCIDRList:\n        - \"10.128.0.0/14\"\n      clusterPoolIPv4MaskSize: \"23\"\n  nativeRoutingCIDR: \"10.128.0.0/14\"\n  endpointRoutes: {enabled: true}\n  clusterHealthPort: 9940\n  tunnelPort: 4789\n  cni:\n    binPath: \"/var/lib/cni/bin\"\n    confPath: \"/var/run/multus/cni/net.d\"\n    chainingMode: portmap\n  prometheus:\n    serviceMonitor: {enabled: false}\n  hubble:\n    tls: {enabled: false}\n  sessionAffinity: true\n</code></pre> <pre><code>oc apply -f ciliumconfig.yaml\n</code></pre> <p>Important</p> <p>Make sure you've changed the networking values according to your platform details <code>spec.ipam.operator.clusterPoolIPv4PodCIDRList</code>, <code>spec.ipam.operator.clusterPoolIPv4MaskSize</code> and <code>nativeRoutingCIDR</code> in IPv4 and <code>spec.ipam.operator.clusterPoolIPv6PodCIDRList</code>, <code>spec.ipam.operator.clusterPoolIPv6MaskSize</code> and <code>nativeRoutingCIDR</code> in IPv6 case.</p>"},{"location":"how-to/agent/other-sdn-providers/#checks_1","title":"Checks","text":"<p>This will be the output:</p> <pre><code>customresourcedefinition.apiextensions.k8s.io/ciliumconfigs.cilium.io created\nnamespace/cilium created\nserviceaccount/cilium-olm created\nWarning: would violate PodSecurity \"restricted:v1.24\": host namespaces (hostNetwork=true), hostPort (container \"operator\" uses hostPort 9443), allowPrivilegeEscalation != false (container \"operator\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"operator\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"operator\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"operator\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\ndeployment.apps/cilium-olm created\nservice/cilium-olm created\nrole.rbac.authorization.k8s.io/cilium-olm-leader-election created\nrole.rbac.authorization.k8s.io/cilium-olm created\nrolebinding.rbac.authorization.k8s.io/leader-election created\nrolebinding.rbac.authorization.k8s.io/cilium-olm created\nclusterrole.rbac.authorization.k8s.io/cilium-cilium-olm created\nclusterrole.rbac.authorization.k8s.io/cilium-cilium created\nclusterrolebinding.rbac.authorization.k8s.io/cilium-cilium-olm created\nclusterrolebinding.rbac.authorization.k8s.io/cilium-cilium created\nciliumconfig.cilium.io/cilium created\n</code></pre> <p>We should see the following pods running in the <code>cilium</code> namespace:</p> <pre><code>oc -n cilium get pods\n</code></pre> <pre><code>NAME                               READY   STATUS    RESTARTS   AGE\ncilium-ds5tr                       1/1     Running   0          106s\ncilium-olm-7c9cf7c948-txkvt        1/1     Running   0          2m36s\ncilium-operator-595594bf7d-gbnns   1/1     Running   0          106s\ncilium-operator-595594bf7d-mn5wc   1/1     Running   0          106s\ncilium-wzhdk                       1/1     Running   0          106s\n</code></pre> <p>The nodes should've moved to <code>Ready</code> state:</p> <pre><code>oc get nodes\n</code></pre> <pre><code>NAME             STATUS   ROLES    AGE   VERSION\nhosted-worker1   Ready    worker   8m    v1.27.8+4fab27b\nhosted-worker2   Ready    worker   8m    v1.27.8+4fab27b\n</code></pre> <p>The HostedCluster deployment will continue, at this point the SDN is running.</p>"},{"location":"how-to/agent/other-sdn-providers/#validation","title":"Validation","text":"<p>Additionally you have some conformance tests that could be deployed into the HostedCluster in order to validate if the Cilium SDN was deployed and working properly.</p> <p>In order for Cilium connectivity test pods to run on OpenShift, a simple custom SecurityContextConstraints object is required to allow hostPort/hostNetwork which the connectivity test pods relies on. it should only set allowHostPorts and allowHostNetwork without any other privileges.</p> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: security.openshift.io/v1\nkind: SecurityContextConstraints\nmetadata:\n  name: cilium-test\nallowHostPorts: true\nallowHostNetwork: true\nusers:\n  - system:serviceaccount:cilium-test:default\npriority: null\nreadOnlyRootFilesystem: false\nrunAsUser:\n  type: MustRunAsRange\nseLinuxContext:\n  type: MustRunAs\nvolumes: null\nallowHostDirVolumePlugin: false\nallowHostIPC: false\nallowHostPID: false\nallowPrivilegeEscalation: false\nallowPrivilegedContainer: false\nallowedCapabilities: null\ndefaultAddCapabilities: null\nrequiredDropCapabilities: null\ngroups: null\nEOF\n</code></pre> <pre><code>version=\"1.14.5\"\noc apply -n cilium-test -f https://raw.githubusercontent.com/cilium/cilium/${version}/examples/kubernetes/connectivity-check/connectivity-check.yaml\n</code></pre> <pre><code>oc get pod -n cilium-test\n</code></pre> <pre><code>NAME                                                     READY   STATUS    RESTARTS   AGE\necho-a-846dcb4-kq7zh                                     1/1     Running   0          23h\necho-b-58f67d5b86-5mrtx                                  1/1     Running   0          23h\necho-b-host-84d7468c8d-nf4vk                             1/1     Running   0          23h\nhost-to-b-multi-node-clusterip-b98ff785c-b9vgf           1/1     Running   0          23h\nhost-to-b-multi-node-headless-5c55d85dfc-5xjbc           1/1     Running   0          23h\npod-to-a-6b996b7675-46kkf                                1/1     Running   0          23h\npod-to-a-allowed-cnp-c958b55bf-6vskb                     1/1     Running   0          23h\npod-to-a-denied-cnp-6d9b8cbff5-lbrgp                     1/1     Running   0          23h\npod-to-b-intra-node-nodeport-5f9c4c866f-mhfs4            1/1     Running   0          23h\npod-to-b-multi-node-clusterip-7cb4bf5495-hmmtg           1/1     Running   0          23h\npod-to-b-multi-node-headless-68975fc557-sqbgq            1/1     Running   0          23h\npod-to-b-multi-node-nodeport-559c54c6fc-2rhvv            1/1     Running   0          23h\npod-to-external-1111-5c4cfd9497-6slss                    1/1     Running   0          23h\npod-to-external-fqdn-allow-google-cnp-7d65d9b747-w4cx5   1/1     Running   0          23h\n</code></pre>"},{"location":"how-to/automated-machine-management/","title":"Automated Machine Management","text":"<p>This section of the HyperShift documentation takes care of the Automated Machine management actions which could be triggered in order to help the customer with the daily basis duties. It includes the implications, caveats, limitations, etc...</p>"},{"location":"how-to/automated-machine-management/configure-machines/","title":"Configuring Machines in HyperShift","text":"<p>In standalone OpenShift, machine configuration is managed via resources like MachineConfig, KubeletConfig, and ContainerRuntimeConfig inside the cluster, and then applied to a set of machines via a MachineConfigPool. In HyperShift clusters, machine configuration is not managed from within a hosted cluster, but rather via a NodePool resource in the hosting cluster.</p> <p>The NodePool field <code>.spec.config</code> can be populated with a list of references to configmaps that contain MachineConfig, KubeletConfig, or ContainerRuntimeConfig manifests. This configuration is applied to all machines belonging to that NodePool.</p>"},{"location":"how-to/automated-machine-management/configure-machines/#creating-a-nodepool-with-custom-configuration","title":"Creating a NodePool with custom configuration","text":""},{"location":"how-to/automated-machine-management/configure-machines/#1-create-machineconfig-and-containerruntimeconfig-manifests","title":"1. Create MachineConfig and ContainerRuntimeConfig manifests","text":"<pre><code>cat &lt;&lt;EOF &gt; machine-config.yaml\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  name: example-machineconfig\nspec:\n  config:\n    storage:\n      files:\n        - path: /etc/custom.cfg\n          mode: 420\n          contents:\n            source: data:text/plain;charset=utf-8;base64,ZXhhbXBsZSBjb25maWd1cmF0aW9u\n  systemd:\n    units:\n      - name: example-service.service\n        enabled: true\n        contents: |\n          [Unit]\n          Description=Example Service\n\n          [Service]\n          ExecStart=/usr/local/bin/example-service\n          Restart=always\nEOF\n\ncat &lt;&lt;EOF &gt; cr-config.yaml\napiVersion: machineconfiguration.openshift.io/v1\nkind: ContainerRuntimeConfig\nmetadata:\n  name: example-container-runtime-config\nspec:\n  containerRuntimeConfig:\n    pidsLimit: 30\nEOF\n</code></pre>"},{"location":"how-to/automated-machine-management/configure-machines/#2-create-configmaps-for-every-configuration-resource","title":"2. Create ConfigMaps for every configuration resource","text":"<pre><code>oc create configmap example-machineconfig -n clusters --from-file config=machine-config.yaml \noc create configmap example-crc -n clusters --from-file config=cr-config.yaml\n</code></pre> <p>NOTE: the key in each ConfigMap must be <code>config</code></p>"},{"location":"how-to/automated-machine-management/configure-machines/#3-create-a-nodepool-that-references-these-configurations","title":"3. Create a NodePool that references these configurations","text":"<pre><code>cat &lt;&lt;EOF &gt; custom-nodepool.yaml\napiVersion: hypershift.openshift.io/v1beta1\nkind: NodePool\nmetadata:\n  name: custom-nodepool\n  namespace: clusters\nspec:\n  arch: amd64\n  clusterName: example-cluster\n  config:\n  - name: example-machineconfig\n  - name: example-crc\n  management:\n    autoRepair: true\n    replace:\n      rollingUpdate:\n        maxSurge: 1\n        maxUnavailable: 0\n      strategy: RollingUpdate\n    upgradeType: Replace\n  nodeDrainTimeout: 0s\n  nodeVolumeDetachTimeout: 0s\n  platform:\n    aws:\n      instanceProfile: example-profile\n      instanceType: m5.large\n      rootVolume:\n        size: 120\n        type: gp3\n    type: AWS\n  release:\n    image: quay.io/openshift-release-dev/ocp-release:4.14.6-x86_64\n  replicas: 2\n</code></pre> <pre><code>oc apply -f custom-nodepool.yaml\n</code></pre>"},{"location":"how-to/automated-machine-management/node-tuning/","title":"Node tuning","text":"<p>Manage node-level tuning with the Node Tuning Operator.</p>"},{"location":"how-to/automated-machine-management/node-tuning/#creating-a-simple-tuned-profile-for-setting-sysctl-settings","title":"Creating a simple TuneD profile for setting sysctl settings","text":"<p>If you would like to set some node-level tuning on the nodes in your hosted cluster, you can use the Node Tuning Operator. In HyperShift, node tuning can be configured by creating ConfigMaps which contain Tuned objects, and referencing these ConfigMaps in your NodePools.</p> <ol> <li> <p>Create a ConfigMap which contains a valid Tuned manifest and reference it in a NodePool. The example Tuned manifest below defines a profile which sets <code>vm.dirty_ratio</code> to 55, on Nodes which contain the Node label <code>tuned-1-node-label</code> with any value.</p> <p>Save the ConfigMap manifest in a file called <code>tuned-1.yaml</code>: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tuned-1\n  namespace: clusters\ndata:\n  tuning: |\n    apiVersion: tuned.openshift.io/v1\n    kind: Tuned\n    metadata:\n      name: tuned-1\n      namespace: openshift-cluster-node-tuning-operator\n    spec:\n      profile:\n      - data: |\n          [main]\n          summary=Custom OpenShift profile\n          include=openshift-node\n\n          [sysctl]\n          vm.dirty_ratio=\"55\"\n        name: tuned-1-profile\n      recommend:\n      - priority: 20\n        profile: tuned-1-profile\n</code></pre></p> <p>NOTE:  In the case where no labels are added to an entry in the <code>spec.recommend</code> section of the Tuned spec, NodePool based matching is assumed, so the highest priority profile in the <code>spec.recommend</code> section will be applied to Nodes in the pool. While more fine-grained Node label based matching is still possible by setting a label value in the Tuned <code>.spec.recommend.match</code>, users should be aware that Node labels will not persist during an upgrade, unless the NodePool <code>.spec.management.upgradeType</code> is set to <code>InPlace</code>.</p> <p>Create the ConfigMap in the management cluster: <pre><code>oc --kubeconfig=\"$MGMT_KUBECONFIG\" create -f tuned-1.yaml\n</code></pre></p> <p>Reference the ConfigMap in the NodePools <code>spec.tuningConfig</code> field, either by editing an existing NodePool or creating a new NodePool. In this example we assume we only have one NodePool called <code>nodepool-1</code>, containing 2 Nodes. <pre><code>apiVersion: hypershift.openshift.io/v1alpha1\nkind: NodePool\nmetadata:\n  ...\n  name: nodepool-1\n  namespace: clusters\n...\nspec:\n  ...\n  tuningConfig:\n  - name: tuned-1\nstatus:\n...\n</code></pre></p> <p>NOTE:  You may reference the same ConfigMap in multiple NodePools. In HyperShift, NTO will append a hash of the NodePool name and namespace to the name of the Tuneds to distinguish them. Outside of this case, users should be careful not to create multiple Tuned profiles of the same name in different Tuneds for the same hosted cluster.</p> </li> <li> <p>Now that the ConfigMap containing a Tuned manifest has been created and referenced in a NodePool, the Node Tuning Operator will sync the Tuned objects into the hosted cluster. You can check which Tuneds are defined and which profiles are set for each Node.</p> <p>List the Tuned objects in the hosted cluster: <pre><code>oc --kubeconfig=\"$HC_KUBECONFIG\" get Tuneds -n openshift-cluster-node-tuning-operator\n</code></pre></p> <p>Example output: <pre><code>NAME       AGE\ndefault    7m36s\nrendered   7m36s\ntuned-1    65s\n</code></pre></p> <p>List the Profiles in the hosted cluster: <pre><code>oc --kubeconfig=\"$HC_KUBECONFIG\" get Profiles -n openshift-cluster-node-tuning-operator\n</code></pre></p> <p>Example output: <pre><code>NAME                           TUNED            APPLIED   DEGRADED   AGE\nnodepool-1-worker-1            tuned-1-profile  True      False      7m43s\nnodepool-1-worker-2            tuned-1-profile  True      False      7m14s\n</code></pre></p> <p>As we can see, both worker nodes in the NodePool have the tuned-1-profile applied. Note that if no custom profiles are created, the <code>openshift-node</code> profile will be applied by default.</p> </li> <li> <p>To confirm the tuning was applied correctly, we can start a debug shell on a Node and check the sysctl values:     <pre><code>oc --kubeconfig=\"$HC_KUBECONFIG\" debug node/nodepool-1-worker-1 -- chroot /host sysctl vm.dirty_ratio\n</code></pre></p> <p>Example output: <pre><code>vm.dirty_ratio = 55\n</code></pre></p> </li> </ol>"},{"location":"how-to/automated-machine-management/node-tuning/#applying-tuning-which-requires-kernel-boot-parameters","title":"Applying tuning which requires kernel boot parameters","text":"<p>You can also use the Node Tuning Operator for more complex tuning which requires setting kernel boot parameters.  As an example, the following steps can be followed to create a NodePool with huge pages reserved.</p> <ol> <li> <p>Create the following ConfigMap which contains a Tuned object manifest for creating 10 hugepages of size 2M.</p> <p>Save this ConfigMap manifest in a file called <code>tuned-hugepages.yaml</code>: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tuned-hugepages\n  namespace: clusters\ndata:\n  tuning: |\n    apiVersion: tuned.openshift.io/v1\n    kind: Tuned\n    metadata:\n      name: hugepages\n      namespace: openshift-cluster-node-tuning-operator\n    spec:\n      profile:\n      - data: |\n          [main]\n          summary=Boot time configuration for hugepages\n          include=openshift-node\n          [bootloader]\n          cmdline_openshift_node_hugepages=hugepagesz=2M hugepages=50\n        name: openshift-node-hugepages\n      recommend:\n      - priority: 20\n        profile: openshift-node-hugepages\n</code></pre></p> <p>NOTE:  The <code>.spec.recommend.match</code> field is intentionally left blank. In this case this Tuned will be applied to all Nodes in the NodePool where this ConfigMap is referenced. It is advised to group Nodes with the same hardware configuration into the same NodePool. Not following this practice might result in TuneD operands calculating conflicting kernel parameters for two or more nodes sharing the same NodePool.</p> <p>Create the ConfigMap in the management cluster: <pre><code>oc --kubeconfig=\"$MGMT_KUBECONFIG\" create -f tuned-hugepages.yaml\n</code></pre></p> </li> <li> <p>Create a new NodePool manifest YAML file, customize the NodePools upgrade type, and reference the previously created ConfigMap in the <code>spec.tuningConfig</code> section before creating it in the management cluster.</p> <p>Create the NodePool manifest and save it in a file called <code>hugepages-nodepool.yaml</code>: <pre><code>NODEPOOL_NAME=hugepages-example\nINSTANCE_TYPE=m5.2xlarge\nNODEPOOL_REPLICAS=2\n\nhypershift create nodepool aws \\\n  --cluster-name $CLUSTER_NAME \\\n  --name $NODEPOOL_NAME \\\n  --replicas $NODEPOOL_REPLICAS \\\n  --instance-type $INSTANCE_TYPE \\\n  --render &gt; hugepages-nodepool.yaml\n</code></pre></p> <p>Edit <code>hugepages-nodepool.yaml</code>. Set <code>.spec.management.upgradeType</code> to <code>InPlace</code>, and set <code>.spec.tuningConfig</code> to reference the <code>tuned-hugepages</code> ConfigMap you created. <pre><code>apiVersion: hypershift.openshift.io/v1alpha1\nkind: NodePool\nmetadata:\n  name: hugepages-nodepool\n  namespace: clusters\n  ...\nspec:\n  management:\n    ...\n    upgradeType: InPlace\n  ...\n  tuningConfig:\n  - name: tuned-hugepages\n</code></pre></p> <p>NOTE:  Setting <code>.spec.management.upgradeType</code> to <code>InPlace</code> is recommended to avoid unnecessary Node recreations when applying the new MachineConfigs. With the <code>Replace</code> upgrade type, Nodes will be fully deleted and new nodes will replace them when applying the new kernel boot parameters that are calculated by the TuneD operand.</p> <p>Create the NodePool in the management cluster: <pre><code>oc --kubeconfig=\"$MGMT_KUBECONFIG\" create -f hugepages-nodepool.yaml\n</code></pre></p> </li> <li> <p>After the Nodes become available, the containerized TuneD daemon will calculate the required kernel boot parameters based on the applied TuneD profile. After the Nodes become <code>Ready</code> and reboot once to apply the generated MachineConfig, you can verify that the Tuned profile is applied and that the kernel boot parameters have been set.</p> <p>List the Tuned objects in the hosted cluster: <pre><code>oc --kubeconfig=\"$HC_KUBECONFIG\" get Tuneds -n openshift-cluster-node-tuning-operator\n</code></pre></p> <p>Example output: <pre><code>NAME                 AGE\ndefault              123m\nhugepages-8dfb1fed   1m23s\nrendered             123m\n</code></pre></p> <p>List the Profiles in the hosted cluster: <pre><code>oc --kubeconfig=\"$HC_KUBECONFIG\" get Profiles -n openshift-cluster-node-tuning-operator\n</code></pre></p> <p>Example output: <pre><code>NAME                           TUNED                      APPLIED   DEGRADED   AGE\nnodepool-1-worker-1            openshift-node             True      False      132m\nnodepool-1-worker-2            openshift-node             True      False      131m\nhugepages-nodepool-worker-1    openshift-node-hugepages   True      False      4m8s\nhugepages-nodepool-worker-2    openshift-node-hugepages   True      False      3m57s\n</code></pre></p> <p>Both worker nodes in the new NodePool have the <code>openshift-node-hugepages</code> profile applied.</p> </li> <li> <p>To confirm the tuning was applied correctly, we can start a debug shell on a Node and check <code>/proc/cmdline</code> <pre><code>oc --kubeconfig=\"$HC_KUBECONFIG\" debug node/nodepool-1-worker-1 -- chroot /host cat /proc/cmdline\n</code></pre></p> <p>Example output: <pre><code>BOOT_IMAGE=(hd0,gpt3)/ostree/rhcos-... hugepagesz=2M hugepages=50\n</code></pre></p> </li> </ol>"},{"location":"how-to/automated-machine-management/node-tuning/#how-to-debug-node-tuning-issues","title":"How to debug Node Tuning issues","text":"<p>If you face issues with Node Tuning, first check the Condition <code>ValidTuningConfig</code> in the NodePool that references your Tuned config. This reports any issue that may prevent the configuration load. <pre><code>- lastTransitionTime: \"2023-03-06T14:30:35Z\"\n  message: ConfigMap \"tuned\" not found\n  observedGeneration: 2\n  reason: ValidationFailed\n  status: \"False\"\n  type: ValidTuningConfig\n</code></pre></p> <p>If the NodePool condition shows no issues, it means that the configuration has been loaded and propagated to the NodePool. You can then check the status of the relevant <code>Profile</code> Custom Resource in your HostedCluster. In the conditions you should see if the configuration has been applied successfully and whether there are any outstanding Warning or Errors. An example can be seen below. <pre><code>status:\n  bootcmdline: \"\"\n  conditions:\n  - lastTransitionTime: \"2023-03-06T14:22:14Z\"\n    message: The TuneD daemon profile not yet applied, or application failed.\n    reason: Failed\n    status: \"False\"\n    type: Applied\n  - lastTransitionTime: \"2023-03-06T14:22:14Z\"\n    message: 'TuneD daemon issued one or more error message(s) during profile application.\n      TuneD stderr:  ERROR    tuned.daemon.controller: Failed to reload TuneD: Cannot\n      load profile(s) ''tuned-1-profile'': Cannot find profile ''openshift-node-notexistin''\n      in ''[''/etc/tuned'', ''/usr/lib/tuned'']''.'\n    reason: TunedError\n    status: \"True\"\n    type: Degraded\n  tunedProfile: tuned-1-profile\n</code></pre></p>"},{"location":"how-to/automated-machine-management/nodepool-lifecycle/","title":"NodePool lifecycle","text":"<p>NodePools represent homogeneous groups of Nodes with a common lifecycle management and upgrade cadence.</p>"},{"location":"how-to/automated-machine-management/nodepool-lifecycle/#upgrades-and-data-propagation","title":"Upgrades and data propagation","text":"<p>There are three main areas that will trigger rolling upgrades across the Nodes when they are changed:</p> <ul> <li>OCP Version dictated by <code>spec.release</code>.</li> <li>Machine configuration via <code>spec.config</code>, a knob for <code>machineconfiguration.openshift.io</code>.</li> <li>Platform specific changes via <code>.spec.platform</code>. Some fields might be immutable whereas other might allow changes e.g. aws instance type.</li> </ul> <p>Some cluster config changes (e.g. proxy, certs) may also trigger a rolling upgrade if the change needs to be propagated to the node.</p> <p>NodePools support two types of rolling upgrades: Replace and InPlace, specified via UpgradeType.</p> <p>Important</p> <p>You cannot switch the UpgradeType once the NodePool is created. You must specify UpgradeType during NodePool  creation. Modifying the field after the fact may cause nodes to become unmanaged.</p>"},{"location":"how-to/automated-machine-management/nodepool-lifecycle/#replace-upgrades","title":"Replace Upgrades","text":"<p>This will create new instances in the new version while removing old nodes in a rolling fashion. This is usually a good choice in cloud environments where this level of immutability is cost effective.</p>"},{"location":"how-to/automated-machine-management/nodepool-lifecycle/#inplace-upgrades","title":"InPlace Upgrades","text":"<p>This will directly perform updates to the Operating System of the existing instances. This is usually a good choice for environments where the infrastructure constraints are higher e.g. bare metal.</p> <p>When you are using in place upgrades, Platform specific changes will only affect upcoming new Nodes.</p>"},{"location":"how-to/automated-machine-management/nodepool-lifecycle/#data-propagation","title":"Data propagation","text":"<p>There some fields which will only propagate in place regardless of the upgrade strategy that is set. <code>.spec.nodeLabels</code> and <code>.spec.taints</code> will be propagated only to new upcoming machines.</p>"},{"location":"how-to/automated-machine-management/nodepool-lifecycle/#triggering-upgrades-examples","title":"Triggering Upgrades examples","text":""},{"location":"how-to/automated-machine-management/nodepool-lifecycle/#upgrading-to-a-new-ocp-version","title":"Upgrading to a new OCP version","text":"<p>These upgrades can be triggered via changing the <code>spec.release.image</code> of the NodePool. Note that you should only upgrade NodePools to the current version of the Hosted Control Plane.</p>"},{"location":"how-to/automated-machine-management/nodepool-lifecycle/#adding-a-new-machineconfig","title":"Adding a new MachineConfig","text":"<p>You can create a MachineConfig inside a ConfigMap in the management cluster as follows:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: ${CONFIGMAP_NAME}\n  namespace: clusters\ndata:\n  config: |\n    apiVersion: machineconfiguration.openshift.io/v1\n    kind: MachineConfig\n    metadata:\n      labels:\n        machineconfiguration.openshift.io/role: worker\n      name: ${MACHINECONFIG_NAME}\n    spec:\n      config:\n        ignition:\n          version: 3.2.0\n        storage:\n          files:\n          - contents:\n              source: data:...\n            mode: 420\n            overwrite: true\n            path: ${PATH}\n</code></pre> <p>Once that is applied to the cluster, you can specify that to the NodePool via:</p> <pre><code>spec:\n  config:\n    - name: ${CONFIGMAP_NAME}\n</code></pre>"},{"location":"how-to/automated-machine-management/nodepool-lifecycle/#scale-down","title":"Scale Down","text":"<p>Scaling a NodePool down will remove Nodes from the hosted cluster.</p>"},{"location":"how-to/automated-machine-management/nodepool-lifecycle/#scaling-to-zero","title":"Scaling To Zero","text":"<p>Node(s) can become stuck when removing all Nodes from a cluster (scaling NodePool(s) down to zero) because the Node(s) cannot be drained successfully from the cluster.</p> <p>Several conditions can prevent Node(s) from being drained successfully:</p> <ul> <li>The hosted cluster contains <code>PodDisruptionBudgets</code> that require at least </li> <li>The hosted cluster contains pods that use `PersistentVolumes``</li> </ul>"},{"location":"how-to/automated-machine-management/nodepool-lifecycle/#prevention","title":"Prevention","text":"<p>To prevent Nodes from becoming stuck when scaling down, set the <code>.spec.nodeDrainTimeout</code> and <code>.spec.nodeVolumeDetachTimeout</code> in the NodePool CR to a value greater than <code>0s</code>. </p> <p>This forces Nodes to be removed once the timeout specified in the field has been reached, regardless of whether the node can be drained or the volumes can be detached successfully.</p> <p>Note</p> <p>See the Hypershift API reference page for more details.</p>"},{"location":"how-to/automated-machine-management/performance-profiling/","title":"Manage node-level performance profiling with Performance Profile Controller","text":""},{"location":"how-to/automated-machine-management/performance-profiling/#intro","title":"Intro","text":"<p>The <code>Performance Profile controller</code>, formerly known as <code>Performance-Addon Operator</code>, is now part of <code>Node Tuning Operator</code>. Performance Profile controller allows you to optimize nodes in your hosted cluster for applications sensitive to CPU and network latency.</p> <p>In HyperShift, node profiling can be configured by creating ConfigMaps which contain <code>PerformanceProfile</code> objects, and referencing these ConfigMaps in your NodePools.</p>"},{"location":"how-to/automated-machine-management/performance-profiling/#steps","title":"Steps","text":"<ol> <li> <p>Create a ConfigMap which contains a valid <code>PerformanceProfile</code> manifest and reference it in a NodePool.    The example <code>PerformanceProfile</code> manifest defines several parameters like how to partition your CPUs into housekeeping and workload partitions, what topology policy to use, and more.    Save this ConfigMap as <code>perfprof-1.yaml</code>.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: perfprof-1\n  namespace: clusters\ndata:\n  tuning: |\n    apiVersion: performance.openshift.io/v2\n    kind: PerformanceProfile\n    metadata:\n      name: performance\n    spec:\n        additionalKernelArgs:\n        - nmi_watchdog=0\n        - audit=0\n        - mce=off\n        - processor.max_cstate=1\n        - intel_idle.max_cstate=0\n        - idle=poll\n        cpu:\n            isolated: \"1,3,5,7,9,11,13,15,17,19-39,41,43,45,47,49,51,53,55,57\"\n            reserved: \"0,2,4,6,8,10,12,14,16,18,40,42,44,46,48,50,52,54,56,58\"\n            offlined: \"59-79\"\n        nodeSelector:\n            node-role.kubernetes.io/worker-cnf: \"\"\n        numa:\n            topologyPolicy: single-numa-node\n        realTimeKernel:\n            enabled: true\n        workloadHints:\n            highPowerConsumption: true\n            realTime: true\n</code></pre> <p>NOTE: See PerformanceProfile Creator for help in creating this <code>PerformanceProfile</code> manifests.</p> </li> <li> <p>Create the ConfigMap in the management cluster:</p> <pre><code>oc --kubeconfig=\"$MGMT_KUBECONFIG\" create -f perfprof-1.yaml\n</code></pre> </li> <li> <p>Reference the ConfigMap in the NodePools <code>spec.tuningConfig</code> field, either by editing an existing NodePool or creating a new NodePool. In this example we assume we only have one NodePool called <code>nodepool-1</code>, containing 2 Nodes.</p> <pre><code>apiVersion: hypershift.openshift.io/v1alpha1\nkind: NodePool\nmetadata:\n  ...\n  name: nodepool-1\n  namespace: clusters\n...\nspec:\n  ...\n  tuningConfig:\n  - name: perfprof-1\nstatus:\n...\n</code></pre> <p>NOTE:  Remember one NodePool can only reference at most one <code>PerformanceProfile</code></p> </li> <li> <p>Now that the ConfigMap containing a <code>PerformanceProfile</code> manifest has been created and referenced in a NodePool, the Performance Profile Controller will handle the <code>PerformanceProfile</code> object and will create the following elements:</p> <ul> <li><code>MachineConfig</code>: This will be embedded into a ConfigMap that will be handled by NTO as any other ConfigMap with MachineConfig embedded (see this PR for further info)</li> <li><code>KubeletConfig</code>: This will be embedded into a ConfigMap that will be handled by NTO in a similar way as a ConfigMap with a MachineConfig embedded</li> <li><code>Tuned</code>: This will be embedded into a ConfigMap and so handled directly by the NTO.</li> <li><code>RuntimeClass</code>: This will be created directly inside the guest cluster.</li> </ul> </li> </ol>"},{"location":"how-to/automated-machine-management/scale-to-zero-dataplane/","title":"Scaling down data plane to Zero","text":""},{"location":"how-to/automated-machine-management/scale-to-zero-dataplane/#context-and-considerations","title":"Context and Considerations","text":"<p>The main reason to go through this scenario it's mainly to save resources and money, once you are not using an already created Hosted Control Plane.</p> <p>In order to continue with the next steps, we need to have in mind some considerations:</p> <ul> <li>This is a destructive action, all the workloads in the worker nodes will disappear.</li> <li>The Hosted Control Plane will stay up and running, and you can scale up the NodePool whenever you want.</li> <li>Some pods in the control plane will stay in \"Pending\" state.  </li> <li>Once you rescale the NodePool/s it will take time until they reach the fully Ready state.</li> <li>We will add an annotate to the nodes which will ensure the pod drainning does not happen. This we will save time and money and also we will avoid stuck pods.</li> </ul> <p>Now let's explain the workflow.</p>"},{"location":"how-to/automated-machine-management/scale-to-zero-dataplane/#workflow","title":"Workflow","text":""},{"location":"how-to/automated-machine-management/scale-to-zero-dataplane/#limitations-and-caveats","title":"Limitations and Caveats","text":"<p>To temporarily remove your data plane (Compute Nodes) in the HostedCluster all you need to do is scaling all NodePools down to zero. The default draining policy will protect any workloads, trying to move the pods to other available nodes. You can get around that policy deliberately by setting drainTimeout to a lower value, but this option has a caveat:</p> <ul> <li>Caveat: Changing drainTimeout in an existing NodePool will trigger a rolling update first and so the new value will only affect to the new created Machines. If you are in a situation where the intent is skip draining and don't want to assume the rolling upgrade triggered by changing the field, you can skip draining forcefully in a given Machine by setting <code>\"machine.cluster.x-k8s.io/exclude-node-draining=\"</code> annotation in the HostedCluster machines as we will follow in this document.</li> </ul> <p>This is a known limitation and we plane to prevent changing drainTimeout from triggering a rolling update in the future.</p> <p>NOTE: <code>machines.machine.cluster.x-k8s.io</code> are considered a lower level resource and any consumer interaction is recommended to happen via NodePool.</p>"},{"location":"how-to/automated-machine-management/scale-to-zero-dataplane/#procedure","title":"Procedure","text":"<p>This is a not difficult procedure but you need to be sure that you have set the right <code>KUBECONFIG</code> and you point to the correct context because maybe you already have some Kubeconfigs or contexts in the same file and we wanna work over the Management cluster.</p> <ul> <li>Set the right Kubeconfig/Context to work over the Management Cluster (The one where you have Hypershift installed)</li> <li>Now you need to annotate the Machine object from the Hosted Control Plane namespace with <code>\"machine.cluster.x-k8s.io/exclude-node-draining=\"</code>. This annotation will allow the Cluster API to avoid the Pod drainning for every Machine you wanna shutdown, that means, the AWS Instance deletion will be done instantly and Openshift will not try to drain the Pods and move them over other node.</li> </ul>  How to annotate the *Machine* objects   You can do it manually using this command: <pre><code>oc annotate -n &lt;HostedClusterNamespace&gt;-&lt;HostedClusterName&gt; machines --all \"machine.cluster.x-k8s.io/exclude-node-draining=\"\n</code></pre>  or execute this script:  <pre><code>#!/bin/bash\n\nfunction annotate_nodes() {\n    MACHINES=\"$(oc get machines -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name | wc -l)\"\n    if [[ ${MACHINES} -le 0 ]];then\n        echo \"There is not machines or machineSets in the Hosted ControlPlane namespace, exiting...\"\n        echo \"HC Namespace: ${HC_CLUSTER_NS}\"\n        echo \"HC Clusted Name: ${HC_CLUSTER_NAME}\"\n        exit 1\n    fi\n\n    echo \"Annotating Nodes to avoid Draining\"\n    oc annotate -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} machines --all \"machine.cluster.x-k8s.io/exclude-node-draining=\"\n    echo \"Nodes annotated!\"\n}\n\n\n## Fill these variables first\nexport KUBECONFIG=&lt;KubeconfigPath&gt;\nexport HC_CLUSTER_NS=&lt;HostedClusterNamespace&gt;\nexport HC_CLUSTER_NAME=&lt;HostedClusterName&gt;\n\nCHECK_NS=\"$(oc get ns -o name ${2})\"\nif [[ -z \"${CHECK_NS}\" ]];then\n    echo \"Namespace does not exists in the Management Cluster\"\n    exit 1\nfi\n\nCHECK_HC=\"$(oc get hc -n ${HC_CLUSTER_NS} -o name ${3})\"\nif [[ -z \"${CHECK_HC}\" ]];then\n    echo \"HC ${3} does not exists in the namespace ${2} of the Management Cluster\"\n    exit 1\nfi\n\nannotate_nodes\n</code></pre> <ul> <li>The next step it's basically scale down the NodePool associated to our HostedCluster. In order to identify it we need to execute an <code>oc get nodepool -n &lt;HostedCluster Namespace&gt;</code>, the one where you created the HostedCluster and NodePool on deployment time.</li> <li>To perform the scale down, you just need to grab the NodePool name from the last command and execute this other one <code>oc scale nodepool/&lt;NodePool Name&gt; --namespace &lt;HostedCluster Namespace&gt; --replicas=0</code></li> </ul>  How to scale down the nodes programmatically  <pre><code>#!/bin/bash\n\nfunction scale_down_pool() {\n\n    # Validated that the nodes in AWS Scale down instantly, they take sometime to disappear inside of Openshift\n    # but the draining is avoided for sure\n    echo \"Scalling down the nodes for ${HC_CLUSTER_NAME} cluster\"\n    NODEPOOLS=$(oc get nodepools -n ${HC_CLUSTER_NS} -o=jsonpath='{.items[?(@.spec.clusterName==\"'${HC_CLUSTER_NAME}'\")].metadata.name}')\n    oc scale nodepool/${NODEPOOLS} --namespace ${HC_CLUSTER_NS} --replicas=0\n    echo \"NodePool ${NODEPOOLS} scaled down!\"\n}\n\n## Fill these variables first\nexport KUBECONFIG=&lt;KubeconfigPath&gt;\nexport HC_CLUSTER_NS=&lt;HostedClusterNamespace&gt;\nexport HC_CLUSTER_NAME=&lt;HostedClusterName&gt;\n\nCHECK_NS=\"$(oc get ns -o name ${2})\"\nif [[ -z \"${CHECK_NS}\" ]];then\n    echo \"Namespace does not exists in the Management Cluster\"\n    exit 1\nfi\n\nCHECK_HC=\"$(oc get hc -n ${HC_CLUSTER_NS} -o name ${3})\"\nif [[ -z \"${CHECK_HC}\" ]];then\n    echo \"HC ${3} does not exists in the namespace ${2} of the Management Cluster\"\n    exit 1\nfi\n\nscale_down_nodepool\n</code></pre> <p>After these steps, you will see how the (in the AWS case) instances will be terminated instantly, but Openshift will take some time until the nodes get deleted because of the default timeouts set on the platforms.</p>"},{"location":"how-to/aws/create-aws-hosted-cluster-arm-workers/","title":"Create Arm NodePools on AWS HostedClusters","text":"<p>The <code>arch</code> field was added to the NodePool Spec in OCP 4.14. The <code>arch</code> field sets the required processor architecture for the NodePool (currently only supported on AWS).</p> <p>Note</p> <p>Currently, the only valid values for '--arch' are 'arm64' and 'amd64'. The HyperShift CLI will default to 'amd64' when the 'arch' field is not specified by the user.</p>"},{"location":"how-to/aws/create-aws-hosted-cluster-arm-workers/#creating-arm-nodepools-through-the-api","title":"Creating Arm NodePools Through the API","text":"<p>The HostedCluster custom resource (CR) must utilize a multi-arch manifested image. Multi-arch nightly images can be found at https://multi.ocp.releases.ci.openshift.org/.</p> <p>Here is an example of an OCP 4.14 multi-arch nightly image: <pre><code>% oc image info quay.io/openshift-release-dev/ocp-release-nightly@sha256:9b992c71f77501678c091e3dc77c7be066816562efe3d352be18128b8e8fce94 -a ~/pull-secrets.json\n\nerror: the image is a manifest list and contains multiple images - use --filter-by-os to select from:\n\n  OS            DIGEST\n  linux/amd64   sha256:c9dc4d07788ebc384a3d399a0e17f80b62a282b2513062a13ea702a811794a60\n  linux/ppc64le sha256:c59c99d6ff1fe7b85790e24166cfc448a3c2ac3ef3422fce3c7259e48d2c9aab\n  linux/s390x   sha256:07fcd16d5bee95196479b1e6b5b3b8064dd5359dac75e3d81f0bd4be6b8fe208\n  linux/arm64   sha256:1d93a6beccc83e2a4c56ecfc37e921fe73d8964247c1a3ec34c4d66f175d9b3d\n</code></pre></p> <p>The CPU architecture for a NodePool is specified through its CR spec: <code>spec.arch</code>. Here is an example: <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: NodePool\nmetadata:\n  creationTimestamp: null\n  name: hypershift-arm-us-east-1a\n  namespace: clusters\nspec:\n  arch: arm64\n  clusterName: hypershift-arm\n  management:\n    autoRepair: false\n    upgradeType: Replace\n  nodeDrainTimeout: 0s\n  nodeVolumeDetachTimeout: 0s\n  platform:\n    aws:\n      instanceProfile: hypershift-arm-2m289-worker\n      instanceType: m6g.large\n      rootVolume:\n        size: 120\n        type: gp3\n      securityGroups:\n      - id: sg-064ea63968d258493\n      subnet:\n        id: subnet-02c74cf1cf1e7413f\n    type: AWS\n  release:\n    image: quay.io/openshift-release-dev/ocp-release-nightly@sha256:390a33cebc940912a201a35ca03927ae5b058fbdae9626f7f4679786cab4fb1c\n  replicas: 3\nstatus:\n  replicas: 0\n</code></pre></p>"},{"location":"how-to/aws/create-aws-hosted-cluster-arm-workers/#creating-a-hostedcluster-with-arm-nodepool-through-hypershift-cli","title":"Creating a HostedCluster with Arm NodePool Through HyperShift CLI","text":"<p>Create a new cluster, specifying the <code>RELEASE_IMAGE</code> and <code>ARCH</code>:</p> <pre><code>REGION=us-east-1\nCLUSTER_NAME=example\nBASE_DOMAIN=example.com\nAWS_CREDS=\"$HOME/.aws/credentials\"\nPULL_SECRET=\"$HOME/pull-secret\"\nRELEASE_IMAGE=\"quay.io/openshift-release-dev/ocp-release-nightly@sha256:390a33cebc940912a201a35ca03927ae5b058fbdae9626f7f4679786cab4fb1c\"\nARCH=\"arm64\"\n\nhypershift create cluster aws \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas=3 \\\n--base-domain $BASE_DOMAIN \\\n--pull-secret $PULL_SECRET \\\n--aws-creds $AWS_CREDS \\\n--region $REGION \\\n--release-image $RELEASE_IMAGE \\\n--arch $ARCH \\\n--generate-ssh\n</code></pre> <p>The HostedCluster will spin up with an Arm NodePool. The default AWS Arm instance type is <code>m6g.large</code>.</p>"},{"location":"how-to/aws/create-aws-hosted-cluster-arm-workers/#creating-arm-nodepools-on-existing-hostedclusters-through-hypershift-cli","title":"Creating Arm NodePools on Existing HostedClusters Through HyperShift CLI","text":"<p>As long as a HostedCluster was created with a manifest listed image in the <code>--release-image</code>, Arm NodePools can be added to the HostedCluster:</p> <pre><code>CLUSTER_NAME=example\nNODE_POOLNAME=example-worker\nARCH=\"arm64\"\n\nhypershift create nodepool aws \\\n--cluster-name $CLUSTER_NAME \\\n--name $NODE_POOLNAME \\\n--replicas=3 \\\n--arch $ARCH \\\n</code></pre>"},{"location":"how-to/aws/create-aws-hosted-cluster-autonode/","title":"Create AutoNode hosted clusters","text":"<p>HyperShift AutoNode (Powered by Karpenter) is a feature that runs Karpenter management side as a control plane component while it watches <code>openshiftNodeClasses</code>, <code>nodePools.karpenter.sh</code>, and <code>nodeclaims.karpenter.sh</code> resources in the guest cluster.</p> <p>To create a hosted cluster with autoNode enabled the Hypershift Operator needs to be installed with the feature gate <code>--tech-preview-no-upgrade=true</code>.</p> <p>The Hypershift controller creates and manages the default <code>openshiftNodeClasses</code> in the hosted cluster allowing you to deploy workloads based in your environment with <code>NodePools</code>(<code>nodepools.karpenter.sh</code>).</p> <p>The following steps describes how to install OpenShift workload cluster on AWS with AutoNode feature by Karpenter.</p> <p>Note: <code>openshiftNodeClasses</code> is exposed for API consumers.</p>"},{"location":"how-to/aws/create-aws-hosted-cluster-autonode/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install the latest and greatest HyperShift CLI.<ul> <li>Make sure all prerequisites have been satisfied (Pull Secret, Hosted Zone, OIDC Bucket, etc)</li> </ul> </li> <li>Ensure that the AWS service-linked role for Spot is enabled in the account where the hosted cluster will be installed. This is a one-time setup per account.<ul> <li>You can verify if the role already exists using the following command: <pre><code>aws iam get-role --role-name AWSServiceRoleForEC2Spot\n</code></pre></li> <li>If the role does not exist, create it with: <pre><code>aws iam create-service-linked-role --aws-service-name spot.amazonaws.com\n</code></pre></li> </ul> </li> <li>Export environment variables, adjusting according to your setup: <pre><code># AWS config\nexport AWS_CREDS=\"$HOME/.aws/credentials\"\nexport AWS_REGION=us-east-1\n\n# OpenShift credentials and configuration\nexport CLUSTER_PREFIX=hcp-aws\nexport CLUSTER_BASE_DOMAIN=devcluster.openshift.com\nexport PULL_SECRET_FILE=\"${HOME}/.openshift/pull-secret-latest.json\"\nexport SSH_PUB_KEY_FILE=$HOME/.ssh/id_rsa.pub\n\n## S3 Bucket name hosting the OIDC discovery documents\n# You must have set it up, see Getting Started for more information:\n# https://hypershift-docs.netlify.app/getting-started/\nexport OIDC_BUCKET_NAME=\"${CLUSTER_PREFIX}-oidc\"\n</code></pre></li> </ul>"},{"location":"how-to/aws/create-aws-hosted-cluster-autonode/#install-the-hypershift-operator","title":"Install the Hypershift Operator","text":"<p>This section describes hands on steps to install the Hypershift Operator with AutoNode feature by enabling the feature gate <code>tech-preview-no-upgrade</code>. See the following documents for more information:</p> <ul> <li>Install HyperShift Operator</li> <li>Feature Gates</li> </ul> <p>Steps:</p> <ul> <li> <p>Install the operator: <pre><code>./hypershift install \\\n  --oidc-storage-provider-s3-bucket-name=\"${OIDC_BUCKET_NAME}\" \\\n  --oidc-storage-provider-s3-credentials=\"${AWS_CREDS}\" \\\n  --oidc-storage-provider-s3-region=\"${AWS_REGION}\" \\\n  --tech-preview-no-upgrade=true\n</code></pre></p> </li> <li> <p>Check if controller is running as expected:</p> </li> </ul> <pre><code>oc get all -n hypershift\n</code></pre>"},{"location":"how-to/aws/create-aws-hosted-cluster-autonode/#create-workload-cluster-with-autonode","title":"Create Workload Cluster with AutoNode","text":"<p>Create the workload cluster with HyperShift AutoNode.</p> <p>Choose the desired target release image name (release controller).</p> <p>Create a hosted cluster, enabling the flag <code>--auto-node</code>:</p> <pre><code>HOSTED_CLUSTER_NAME=${CLUSTER_PREFIX}-wl\nOCP_RELEASE_IMAGE=&lt;CHANGE_ME_TO_LATEST_RELEASE_IMAGE&gt;\n# Example of image: quay.io/openshift-release-dev/ocp-release:4.19.0-rc.5-x86_64\n\n./hypershift create cluster aws \\\n  --name=\"${HOSTED_CLUSTER_NAME}\" \\\n  --region=\"${AWS_REGION}\" \\\n  --zones=\"${AWS_REGION}a\" \\\n  --node-pool-replicas=1 \\\n  --base-domain=\"${CLUSTER_BASE_DOMAIN}\" \\\n  --pull-secret=\"${PULL_SECRET_FILE}\" \\\n  --aws-creds=\"${AWS_CREDS}\" \\\n  --ssh-key=\"${SSH_PUB_KEY_FILE}\" \\\n  --release-image=\"${OCP_RELEASE_IMAGE}\" \\\n  --auto-node=true\n</code></pre> <p>Check the cluster information:</p> <pre><code>oc get --namespace clusters hostedclusters\noc get --namespace clusters nodepools\n</code></pre> <p>When completed, extract the credentials for workload cluster:</p> <pre><code>./hypershift create kubeconfig --name ${HOSTED_CLUSTER_NAME} &gt; kubeconfig-${HOSTED_CLUSTER_NAME}\n\n# kubeconfig for workload cluster\nexport KUBECONFIG=$PWD/kubeconfig-${HOSTED_CLUSTER_NAME}\n</code></pre> <p>Check the managed <code>openshiftNodeClasses</code> object created in the workload cluster:</p> <pre><code>oc get openshiftNodeClasses\n</code></pre> <p>Now you are ready to use AutoNode feature by setting the Karpenter configuration to fit your workloads.</p>"},{"location":"how-to/aws/create-aws-hosted-cluster-autonode/#deploy-sample-workloads","title":"Deploy Sample Workloads","text":"<p>This section provides examples to getting started exploring HyperShift AutoNode.</p>"},{"location":"how-to/aws/create-aws-hosted-cluster-autonode/#using-autonode-with-a-simple-web-app","title":"Using AutoNode with a Simple Web App","text":"<p>This example demonstrates how to use the AutoNode feature by creating a NodePool to fit the sample application. The sample application selects the instance type <code>t3.large</code>, matching the <code>node.kubernetes.io/instance-type</code> selector in the <code>NodePool</code>.</p> <p>Create a Karpenter NodePool with the configuration for the workload:</p> <pre><code>cat &lt;&lt; EOF | oc apply -f -\napiVersion: karpenter.sh/v1\nkind: NodePool\nmetadata:\n name: spot-and-gpu\nspec:\n  disruption:\n    budgets:\n      - nodes: 10%\n    consolidateAfter: 30s\n    consolidationPolicy: WhenEmptyOrUnderutilized\n  weight: 10\n  template:\n    spec:\n      expireAfter: 336h\n      terminationGracePeriod: 24h0m0s\n      nodeClassRef:\n        group: karpenter.k8s.aws\n        kind: EC2NodeClass\n        name: default\n      requirements:\n      - key: karpenter.sh/capacity-type\n        operator: In\n        values: [\"spot\"]\n      - key: karpenter.sh/instance-family\n        operator: In\n        values: [\"g4dn\", \"m5\", \"m6i\", \"c5\", \"c6i\", \"t3\"]\n      - key: karpenter.sh/instance-size\n        operator: In\n        values: [\"large\", \"xlarge\", \"2xlarge\"]\nEOF\n</code></pre> <p>Create a Sample App deployment:</p> <p>This section demonstrates how to deploy sample applications to test and scale Karpenter's AutoNode feature. By creating workloads with specific resource requirements, you can observe how Karpenter provisions nodes dynamically to meet the demands of your applications.</p> <pre><code>cat &lt;&lt; EOF | oc apply -f -\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n name: web-app\nspec:\n replicas: 0\n selector:\n   matchLabels:\n     app: web-app\n template:\n   metadata:\n     labels:\n       app: web-app\n   spec:\n     affinity:\n       podAntiAffinity:\n         requiredDuringSchedulingIgnoredDuringExecution:\n           - labelSelector:\n               matchLabels:\n                 app: web-app\n             topologyKey: \"kubernetes.io/hostname\"   \n     securityContext:\n       runAsUser: 1000\n       runAsGroup: 3000\n       fsGroup: 2000\n     containers:\n     - image: public.ecr.aws/eks-distro/kubernetes/pause:3.2\n       name: web-app\n       resources:\n         requests:\n           cpu: \"1\"\n           memory: 256M\n       securityContext:\n         allowPrivilegeEscalation: false\n     nodeSelector:\n       node.kubernetes.io/instance-type: \"t3.large\"\nEOF\n</code></pre> <p>Scale the application:</p> <pre><code>oc scale --replicas=1 deployment.apps/web-app\n</code></pre> <p>Monitor and Debug Node Provisioning with AutoNode:</p> <p>Monitor the <code>nodeClaims</code> objects to track the provisioning of nodes by AutoNode. These objects provide detailed insights into the lifecycle of nodes, including their current state and any associated events. Use the following command to continuously watch the <code>nodeClaims</code>:</p> <p>Provisioning Time</p> <p>The provisioning process for an instance to become a node may take approximately 10 minutes. While waiting, monitor the progress using the provided commands to ensure the process completes successfully.</p> <pre><code>oc get nodeclaims --watch\n</code></pre> <p>To investigate a specific <code>nodeClaim</code> in detail, use the following command:</p> <pre><code>oc describe nodeclaim &lt;nodeClaimName&gt;\n</code></pre> <p>This will provide comprehensive information about the selected <code>nodeClaim</code>, helping you debug and confirm that nodes are being provisioned and functioning as expected.</p> <p>Verify Node Join Status:</p> <p>Ensure that the node has successfully joined the cluster. Use the following command to check:</p> <pre><code>oc get nodes -l karpenter.sh/nodepool=spot-and-gpu\n</code></pre> <p>This command filters the nodes associated with the <code>spot-and-gpu</code> NodePool, allowing you to confirm that the AutoNode feature is functioning as expected.</p> <p>Verify Application Scheduling:</p> <p>Ensure that the application has been successfully scheduled onto the newly provisioned node. Use the following command to check the status of the pods associated with the application:</p> <pre><code>oc get pods -l app=web-app -o wide\n</code></pre> <p>This command filters the pods by the label <code>app=web-app</code>, allowing you to confirm that the application is running on the expected node provisioned by AutoNode.</p>"},{"location":"how-to/aws/create-aws-hosted-cluster-multiple-zones/","title":"Create AWS Hosted Cluster in Multiple Zones","text":""},{"location":"how-to/aws/create-aws-hosted-cluster-multiple-zones/#prerequisites","title":"Prerequisites","text":"<p>Complete the Prerequisites and Before you begin.</p>"},{"location":"how-to/aws/create-aws-hosted-cluster-multiple-zones/#creating-the-hosted-cluster","title":"Creating the Hosted Cluster","text":"<p>Create a new cluster, specifying the <code>BASE_DOMAIN</code> of the public zone provided in the Prerequisites:</p> <pre><code>REGION=us-east-1\nZONES=us-east-1a,us-east-1b\nCLUSTER_NAME=example\nBASE_DOMAIN=example.com\nAWS_CREDS=\"$HOME/.aws/credentials\"\nPULL_SECRET=\"$HOME/pull-secret\"\n\nhypershift create cluster aws \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas=3 \\\n--base-domain $BASE_DOMAIN \\\n--pull-secret $PULL_SECRET \\\n--aws-creds $AWS_CREDS \\\n--region $REGION \\\n--zones $ZONES\n</code></pre> <p>Important</p> <p>The <code>--zones</code> flag must specify Availability Zones (AZs) within the region specified by the <code>--region</code> flag</p> <p>The <code>--zones</code> flag is also available on the  <code>hypershift create infra aws</code> command used to create infrastructure separately.</p> <p>The following per-zone infrastructure is created for all specified zones:</p> <ul> <li>Public subnet</li> <li>Private subnet</li> <li>NAT gateway</li> <li>Private route table (Public route table is shared across public subnets)</li> </ul> <p>Two <code>NodePool</code> resources are created, one for each zone.  The <code>NodePool</code> name is suffixed by the zone name.  The private subnet for zone is set in <code>spec.platform.aws.subnet.id</code>.</p>"},{"location":"how-to/aws/create-aws-hosted-cluster-secret-credentials/","title":"Create an AWS Hosted Cluster using a credential secret","text":""},{"location":"how-to/aws/create-aws-hosted-cluster-secret-credentials/#creating-the-credential-secret","title":"Creating the credential secret","text":"<p>If using the Multi-cluster Engine, the console provides a credential page that allows you to create the AWS credential secret.</p> <p>To manually create the secret, apply the following secret yaml:</p> <pre><code>apiVersion: v1\nkind: Secret\ntype: Opaque\nmetadata:\n  name: my-aws-credentials\n  namespace: clusters\n  labels:\n    cluster.open-cluster-management.io/credentials: \"\"\n    cluster.open-cluster-management.io/type: aws\nstringData:\n  aws_access_key_id: &lt;value&gt;\n  aws_secret_access_key: &lt;value&gt;\n  baseDomain: &lt;value&gt;\n  pullSecret: &lt;value&gt;\n  ssh-privatekey: &lt;value&gt;\n  ssh-publickey: &lt;value&gt;\n</code></pre> <p>Important</p> <p>The required parameters when using a secret to create a cluster are <code>--secret-creds &lt;SECRET_NAME&gt; --namespace &lt;NAMESPACE_NAME&gt;</code>. </p> <p>If <code>--namespace</code> is not included, then the \"clusters\" namespace will used.</p> <p>Note</p> <p>The labels on this secret allow it to be displayed by the multi-cluster engine console.</p>"},{"location":"how-to/aws/create-aws-hosted-cluster-secret-credentials/#create-a-hostedcluster-using-a-credential-secret","title":"Create a HostedCluster using a credential secret","text":"<p>Prerequisites:</p> <pre><code>REGION=us-east-1\nCLUSTER_NAME=example\nSECRET_CREDS=\"example-multi-cluster-engine-credential-secret\"\nNAMESPACE=\"example-namespace\"\n\nhypershift create cluster aws \\\n  --name $CLUSTER_NAME \\\n  --namespace $NAMESPACE \\\n  --node-pool-replicas=3 \\\n  --secret-creds $SECRET_CREDS \\\n  --region $REGION \\\n</code></pre>"},{"location":"how-to/aws/create-aws-hosted-cluster-secret-credentials/#credential-secret-defaults","title":"Credential secret defaults","text":"<p>The following values will be extracted from the credential and used when creating the hosted cluster:</p> <ul> <li>AWS key id and AWS secret key</li> <li>Base domain</li> <li>Pull secret</li> <li>SSH key</li> </ul>"},{"location":"how-to/aws/create-aws-hosted-cluster-secret-credentials/#using-credential-secret-overrides","title":"Using credential secret overrides","text":"<p>The following command line parameters can be used with <code>--secret-creds</code>. Each is optional, but when provided they will override the values found in the credential secret.</p> <ul> <li><code>--aws-creds</code></li> <li><code>--base-domain</code></li> <li><code>--pull-secret</code></li> <li><code>--ssh-key</code></li> </ul>"},{"location":"how-to/aws/create-infra-iam-separately/","title":"Create AWS infra and IAM resources separately","text":"<p>The default behavior of the <code>hypershift create cluster aws</code> command is to create cloud infrastructure along with the HostedCluster and apply it. It is possible to create the cloud infrastructure portion separately so that the <code>hypershift create cluster aws</code> command can just be used to create the cluster, or simply render it so it can be modified before applying.</p> <p>In order to do this, you need to:</p> <ol> <li>Create the AWS infrastructure</li> <li>Create AWS IAM resources</li> <li>Create the cluster</li> </ol>"},{"location":"how-to/aws/create-infra-iam-separately/#creating-the-aws-infra","title":"Creating the AWS infra","text":"<p>Use the <code>hypershift create infra aws</code> command:</p> <pre><code>hypershift create infra aws --name CLUSTER_NAME \\\n    --aws-creds AWS_CREDENTIALS_FILE \\\n    --base-domain BASEDOMAIN \\\n    --infra-id INFRA_ID \\\n    --region REGION \\\n    --output-file OUTPUT_INFRA_FILE\n</code></pre> <p>where</p> <ul> <li><code>CLUSTER_NAME</code> is the name of the hosted cluster you intend to create. This is used for creating     the Route53 private hosted zones that belong to the cluster.</li> <li><code>AWS_CREDENTIALS_FILE</code> points to an AWS credentials file that has permission to create     infrastructure resources for your cluster such as VPCs, subnets, NAT gateways, etc.     It should correspond to the AWS account for your guest cluster, where workers will live.</li> <li><code>BASEDOMAIN</code> is the base domain that will be used for your hosted cluster's ingress. It must     correspond to an existing Route53 public zone that you have access to create records in.</li> <li><code>INFRA_ID</code> is a unique name that will be used to identify your infrastructure via tags. It is used     by the cloud controller manager in Kubernetes and the CAPI manager to identify infrastructure     for your cluster. Typically this is the name of your cluster (CLUSTER_NAME) with a random     suffix appended to it.</li> <li><code>REGION</code> is the region where you want to create the infrastructure for your cluster.</li> <li><code>OUTPUT_INFRA_FILE</code> is the file where IDs of the infrastructure that has been created will be stored in JSON format.     This file can then be used as input to the <code>hypershift create cluster aws</code> command to populate     the appropriate fields in the HostedCluster and NodePool resources.</li> </ul> <p>Running this command should result in the following resources getting created:</p> <ul> <li>1 VPC</li> <li>1 DHCP Options</li> <li>1 Private Subnet</li> <li>1 Public Subnet</li> <li>1 Internet Gateway</li> <li>1 NAT Gateway</li> <li>1 Security Group for Worker Nodes</li> <li>2 Route Tables (1 Private, 1 Public)</li> <li>2 Private Hosted Zones (1 for Cluster Ingress, 1 for PrivateLink, in case you will be creating a private cluster)</li> </ul> <p>All of these resources will contain the following tag: <code>kubernetes.io/cluster/INFRA_ID=owned</code> where <code>INFRA_ID</code> is what you specified on the command invocation.</p>"},{"location":"how-to/aws/create-infra-iam-separately/#creating-the-aws-iam-resources","title":"Creating the AWS IAM resources","text":"<p>Use the <code>hypershift create iam aws</code> command:</p> <pre><code>hypershift create iam aws --infra-id INFRA_ID \\\n    --aws-creds AWS_CREDENTIALS_FILE \\\n    --oidc-storage-provider-s3-bucket-name OIDC_BUCKET_NAME \\\n    --oidc-storage-provider-s3-region OIDC_BUCKET_REGION \\\n    --region REGION \\\n    --public-zone-id PUBLIC_ZONE_ID \\\n    --private-zone-id PRIVATE_ZONE_ID \\\n    --local-zone-id LOCAL_ZONE_ID \\\n    --output-file OUTPUT_IAM_FILE\n</code></pre> <p>where</p> <ul> <li><code>INFRA_ID</code> should be the same id that was specified in the <code>create infra aws</code> command. It is     used to identify the IAM resources associated with the hosted cluster.</li> <li><code>AWS_CREDENTIALS_FILE</code> points to an AWS credentials file that has permission to create     IAM resources such as roles. It does not have to be the same credentials specified to create     the infrastructure but it does have to correspond to the same AWS account.</li> <li><code>OIDC_BUCKET_NAME</code> is the name of the bucket used to store OIDC documents. This bucket should have been     created as a prerequisite for installing Hypershift (See Prerequisites)     The name of the bucket is used to construct URLs for the OIDC provider created by this command.</li> <li><code>OIDC_BUCKET_REGION</code> is the region where the OIDC bucket lives.</li> <li><code>REGION</code> is the region where the infrastructure of the cluster will live. This is used to create a worker     instance profile for machines that belong to the hosted cluster.</li> <li><code>PUBLIC_ZONE_ID</code> is the ID of the public zone for the guest cluster. It is used in creating the policy for the ingress operator.     It can be found in the <code>OUTPUT_INFRA_FILE</code> generated by the <code>create infra aws</code> command.</li> <li><code>PRIVATE_ZONE_ID</code> is the ID of the private zone for the guest cluster. It is used in creating the policy for the ingress operator.     It can be found in the <code>OUTPUT_INFRA_FILE</code> generated by the <code>create infra aws</code> command.</li> <li><code>LOCAL_ZONE_ID</code> is the ID of the local zone for the guest cluster (when creating a private cluster). It is used in creating the policy     for the control plane operator so it can manage records for the PrivateLink endpoint.     It can be found in the <code>OUTPUT_INFRA_FILE</code> generated by the <code>create infra aws</code> command.</li> <li><code>OUTPUT_IAM_FILE</code> is the file where IDs of the IAM resources that have been created will be stored in JSON format.     This file can then be used as input to the <code>hypershift create cluster aws</code> command to populate     the appropriate fields in the HostedCluster and NodePool resource.</li> </ul> <p>Running this command should result in the following resources getting created:</p> <ul> <li>1 OIDC Provider (required for enabling STS authentication)</li> <li>7 Roles (separate roles for every component that interacts with the provider: kube controller manager, capi provider, registry, etc)</li> <li>1 Instance Profile (the profile that is assigned to all worker instances of the cluster)</li> </ul>"},{"location":"how-to/aws/create-infra-iam-separately/#creating-the-cluster","title":"Creating the Cluster","text":"<p>Use the <code>hypershift create cluster aws</code> command:</p> <pre><code>hypershift create cluster aws \\\n    --infra-id INFRA_ID \\\n    --name CLUSTER_NAME \\\n    --aws-creds AWS_CREDENTIALS \\\n    --infra-json OUTPUT_INFRA_FILE \\\n    --iam-json OUTPUT_IAM_FILE \\\n    --pull-secret PULL_SECRET_FILE \\\n    --generate-ssh \\\n    --node-pool-replicas 3\n</code></pre> <p>where</p> <ul> <li><code>INFRA_ID</code> should be the same id that was specified in the <code>create infra aws</code> command. It is     used to identify the IAM resources associated with the hosted cluster.</li> <li><code>CLUSTER_NAME</code> should be the same name that was specified in the <code>create infra aws</code> command.</li> <li><code>AWS_CREDENTIALS</code> should be the same that was specified in the <code>create infra aws</code> command.</li> <li><code>OUTPUT_INFRA_FILE</code> is the file where the output of the <code>create infra aws</code> command was saved.</li> <li><code>OUTPUT_IAM_FILE</code> is the file where the output of the <code>create iam aws</code> command was saved.</li> <li><code>PULL_SECRET_FILE</code> is a file that contains a valid OpenShift pull secret.</li> </ul> <p>Note</p> <p>The --generate-ssh flag is optional but is a good idea to have in case you need to ssh  to your workers. An ssh key will have been generated for you and stored as a secret in the  same namespace as the hosted cluster.</p> <p>Running this command should result in the following resources getting applied to your cluster:</p> <ul> <li>Namespace</li> <li>Secret with your pull secret</li> <li>HostedCluster</li> <li>NodePool</li> <li>3 AWS STS secrets for control plane components</li> <li>1 SSH key secret (if --generate-ssh was specified)</li> </ul> <p>You can also add the <code>--render</code> flag to the command and redirect output to a file where you  can do further editing of the resources before applying them to the cluster.</p>"},{"location":"how-to/aws/define-custom-kube-api-name/","title":"Define Custom KubeAPI Name","text":""},{"location":"how-to/aws/define-custom-kube-api-name/#what-is-this-for","title":"What is this for?","text":"<p><code>KubeAPIServerDNSName</code> is a spec field used to declare a custom Kubernetes API URI. To make this work, you simply need to define the URI (e.g., <code>api.example.com</code>) in the <code>HostedCluster</code> object.</p>"},{"location":"how-to/aws/define-custom-kube-api-name/#how-does-this-work","title":"How does this work?","text":"<ul> <li>This can be defined both during day-1 (initial setup) and day-2 (post-deployment updates).</li> <li>The CPO (ControlPlaneOperator) controllers will create a new kubeconfig stored in the HCP namespace. This kubeconfig will be based on certificates and named <code>custom-admin-kubeconfig</code>.</li> <li>The certificates are generated from the root CA, with their expiration and renewal managed by the <code>HostedControlPlane</code>.</li> <li>The CPO will report a new kubeconfig, called <code>CustomKubeconfig</code>, in the <code>HostedControlPlane</code>. This kubeconfig will use the new server defined in the <code>KubeAPIServerDNSName</code> field.</li> <li>This custom kubeconfig will also be referenced in the <code>HostedCluster</code> object under the status field as <code>CustomKubeconfig</code>.</li> <li>A new secret, named <code>{HOSTEDCLUSTER_NAME}-custom-admin-kubeconfig</code>, will be created in the <code>HostedCluster</code> namespace. This secret can be used to easily access the HostedCluster API server.</li> </ul> <p>Note</p> <p>This does not directly affect the dataplane, so no rollouts are expected to occur.</p> <ul> <li>If you remove this field from the spec, all newly generated secrets and the <code>CustomKubeconfig</code> reference will be removed from the cluster and from the status field.</li> </ul>"},{"location":"how-to/aws/define-custom-kube-api-name/#additional-notes","title":"Additional Notes","text":"<p>This other field called <code>CustomKubeConfig</code> is optional and can only be used if <code>KubeAPIServerDNSName</code> is not empty. When set, it triggers the generation of a secret with the specified name containing a kubeconfig within the <code>HostedCluster</code> namespace. This kubeconfig will also be referenced in the <code>HostedCluster.status</code> as <code>customkubeconfig</code>. If removed during day-2 operations, all related secrets and status references will also be deleted:</p> <ul> <li>This action will not cause a NodePool rollout, ensuring zero impact on customers.</li> <li>The <code>HostedControlPlane</code> object will receive the changes progressed by the Hypershift Operator and delete the corresponding field.</li> <li>The <code>.status.customkubeconfig</code> will be removed from both <code>HostedCluster</code> and <code>HostedControlPlane</code> objects.</li> <li>The secret in the <code>HostedControlPlane</code> namespace, named <code>custom-admin-kubeconfig</code>, will be deleted.</li> <li>The secret in the <code>HostedCluster</code> namespace, named <code>{HOSTEDCLUSTER_NAME}-custom-admin-kubeconfig</code>, will also be deleted.</li> </ul>"},{"location":"how-to/aws/deploy-aws-private-clusters/","title":"Deploying AWS private clusters","text":"<p>By default, HyperShift guest clusters are publicly accessible through public DNS and the management cluster's default router.</p> <p>For private clusters on AWS, all communication with the guest cluster occur over AWS PrivateLink. This guide will lead you through the process of configuring HyperShift for private cluster support on AWS.</p>"},{"location":"how-to/aws/deploy-aws-private-clusters/#before-you-begin","title":"Before you begin","text":"<p>To enable private hosted clusters, HyperShift must be installed with private cluster support. This guide assumes you have performed all the Getting started guide prerequisites. The following steps will reference elements of the steps you already performed.</p> <ol> <li> <p>Create the private cluster IAM policy document.</p> ShellJSON <pre><code>cat &lt;&lt; EOF &gt;&gt; policy.json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:CreateVpcEndpointServiceConfiguration\",\n        \"ec2:DescribeVpcEndpointServiceConfigurations\",\n        \"ec2:DeleteVpcEndpointServiceConfigurations\",\n        \"ec2:DescribeVpcEndpointServicePermissions\",\n        \"ec2:ModifyVpcEndpointServicePermissions\",\n        \"ec2:RejectVpcEndpointConnections\",\n        \"ec2:DescribeVpcEndpointConnections\",\n        \"ec2:DescribeInstanceTypes\",\n        \"ec2:CreateTags\",\n        \"elasticloadbalancing:DescribeLoadBalancers\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\nEOF\n</code></pre> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:CreateVpcEndpointServiceConfiguration\",\n        \"ec2:DescribeVpcEndpointServiceConfigurations\",\n        \"ec2:DeleteVpcEndpointServiceConfigurations\",\n        \"ec2:DescribeVpcEndpointServicePermissions\",\n        \"ec2:ModifyVpcEndpointServicePermissions\",\n        \"ec2:RejectVpcEndpointConnections\",\n        \"ec2:DescribeVpcEndpointConnections\",\n        \"ec2:DescribeInstanceTypes\",\n        \"ec2:CreateTags\",\n        \"elasticloadbalancing:DescribeLoadBalancers\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre> </li> <li> <p>Create the IAM policy in AWS.</p> <pre><code>aws iam create-policy --policy-name=hypershift-operator-policy --policy-document=file://policy.json\n</code></pre> </li> <li> <p>Create a <code>hypershift-operator</code> IAM user.</p> <pre><code>aws iam create-user --user-name=hypershift-operator\n</code></pre> </li> <li> <p>Attach the policy to the <code>hypershift-operator</code> user, replacing <code>$POLICY_ARN</code> with the ARN of the policy    created in step 2.</p> <pre><code>aws iam attach-user-policy --user-name=hypershift-operator --policy-arn=$POLICY_ARN\n</code></pre> </li> <li> <p>Create an IAM access key for the user.</p> <pre><code>aws iam create-access-key --user-name=hypershift-operator\n</code></pre> </li> <li> <p>Create a credentials file (<code>$AWS_PRIVATE_CREDS</code>) with the access ID and key for the user    created in step 5.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; $AWS_PRIVATE_CREDS\n[default]\naws_access_key_id = &lt;secret&gt;\naws_secret_access_key = &lt;secret&gt;\nEOF\n</code></pre> </li> <li> <p>Now you can install HyperShift with private cluster support.</p> <pre><code>REGION=us-east-1\nBUCKET_NAME=your-bucket-name\nAWS_CREDS=\"$HOME/.aws/credentials\"\n\nhypershift install \\\n--oidc-storage-provider-s3-bucket-name $BUCKET_NAME \\\n--oidc-storage-provider-s3-credentials $AWS_CREDS \\\n--oidc-storage-provider-s3-region $REGION \\\n--private-platform=AWS \\\n--aws-private-creds=$AWS_PRIVATE_CREDS \\\n--aws-private-region=$REGION\n</code></pre> <p>Note</p> <p>Even if you already installed HyperShift using the Getting started guide, you can safely run <code>hypershift install</code> again with private cluster support to update the existing installation.</p> <p>Important</p> <p>Although public clusters can be created in any region, private clusters can only be created in the same region specified by <code>--aws-private-region</code>.</p> </li> </ol>"},{"location":"how-to/aws/deploy-aws-private-clusters/#create-a-private-hostedcluster","title":"Create a private HostedCluster","text":"<p>Create a new private cluster, specifying values used in the Before you begin section.</p> <pre><code>CLUSTER_NAME=example\nBASE_DOMAIN=example.com\nAWS_CREDS=\"$HOME/.aws/credentials\"\nPULL_SECRET=\"$HOME/pull-secret\"\n\nhypershift create cluster aws \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas=3 \\\n--base-domain $BASE_DOMAIN \\\n--pull-secret $PULL_SECRET \\\n--aws-creds $AWS_CREDS \\\n--region $REGION \\\n--endpoint-access Private\n</code></pre> <p>Note</p> <p>The <code>--endpoint-access</code> flag is used to designate whether a cluster is public or private.</p> <p>The cluster's API endpoints will be accessible through a private DNS zone:</p> <ul> <li><code>api.$CLUSTER_NAME.hypershift.local</code></li> <li><code>*.apps.$CLUSTER_NAME.hypershift.local</code></li> </ul>"},{"location":"how-to/aws/deploy-aws-private-clusters/#access-a-private-hostedcluster","title":"Access a private HostedCluster","text":"<p>Use a bastion host to access a private cluster.</p> <p>Start a bastion instance, replacing <code>$SSH_KEY</code> with credentials to use for connecting to the bastion.</p> <pre><code>hypershift create bastion aws --aws-creds=$AWS_CREDS --infra-id=$INFRA_ID --region=$REGION --ssh-key-file=$SSH_KEY\n</code></pre> <p>Find the private IPs of nodes in the cluster's NodePool.</p> <pre><code>aws ec2 describe-instances --filter=\"Name=tag:kubernetes.io/cluster/$INFRA_ID,Values=owned\" | jq '.Reservations[] | .Instances[] | select(.PublicDnsName==\"\") | .PrivateIpAddress'\n</code></pre> <p>Create a kubeconfig for the cluster which can be copied to a node.</p> <pre><code>hypershift create kubeconfig &gt; $CLUSTER_KUBECONFIG\n</code></pre> <p>SSH into one of the nodes via the bastion using the IP printed from the <code>create bastion</code> command.</p> <pre><code>ssh -o ProxyCommand=\"ssh ec2-user@$BASTION_IP -W %h:%p\" core@$NODE_IP\n</code></pre> <p>From the SSH shell, copy the kubeconfig contents to a file on the node.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; kubeconfig\n&lt;paste kubeconfig contents&gt;\nEOF\nexport KUBECONFIG=$PWD/kubeconfig\n</code></pre> <p>From the SSH shell, observe the guest cluster status or run other <code>oc</code> commands.</p> <pre><code>oc get clusteroperators\noc get clusterversion\n# ...\n</code></pre>"},{"location":"how-to/aws/disaster-recovery/","title":"Disaster Recovery","text":""},{"location":"how-to/aws/disaster-recovery/#migrating-hosted-cluster-within-the-same-aws-region","title":"Migrating Hosted Cluster within the same AWS Region","text":""},{"location":"how-to/aws/disaster-recovery/#use-cases-for-disaster-recovery","title":"Use cases for Disaster Recovery","text":"<p>This procedure is helpful for:</p> <ol> <li>The control-plane is down for your hosted cluster (api-server, etcd,...)</li> <li>Hypershift operator is not working, it\u2019s down and it cannot be recovered</li> </ol> <p>This procedure is not helpful for:</p> <ol> <li> <p>My compute nodes are frozen or not working fine</p> <ul> <li>For this situation you will need to access the serial console of the node in order to see what is happening.</li> </ul> </li> <li> <p>The management cluster API-server/etcd is down?</p> <ul> <li>For this second situation it does not make sense to use this procedure, so you can use a backup/recovery tool like Velero in order to recover the Etcd.</li> </ul> </li> </ol> <p>Note</p> <p>Other situations should be carefully examined, to ensure the stability of the other deployed HostedClusters</p> <p>Warning</p> <p>These are some examples where this procedure could be useful. We don't recommend following this procedure unless it is strictly necessary.</p>"},{"location":"how-to/aws/disaster-recovery/#preface-and-considerations","title":"Preface and Considerations","text":"<p>The behaviour of this implementation is focused on the transparency for the user. Hypershift will not disrupt any customer workloads at anytime, also have in mind that the service workloads will be up and running during the migration process. Maybe at some point the Cluster API will be down but this will not affect the services running on the worker nodes.</p> <p>In the storage side, It's mandatory to have under consideration that when we move a HostedControlPlane to another Management cluster we use some external services, which allows the migration to make it happen. We reusethe storage provisioned in AWS by the initial ControlPlane (PVs/PVCs) in the destination Management cluster.</p> <p>Regarding the Workers nodes assigned to the cluster, during the migration they will still point to the same DNS entry, and we, under the hood, change the DNS Records to point to the new Management Cluster API, that way the node migration is transparent for the user.</p> <p>Important</p> <p>Keep in mind that this \"migration\" capability is only for disaster recovery purposes, please DO NOT use this to perform clusters migrations as a common task in your platform.</p> <p>These next arguments depend on how the Hypershift Operator has been deployed and how a Hosted Cluster has been created. E.G If we want to go ahead with the procedure and our cluster is private we need to make sure that our Hypershift Operator has been deployed with the arguments set in the Private tab for Hypershift Operator Deployment access endpoints arguments and our Hosted Cluster has been created using the arguments following the Private tab in the Arguments of the CLI when creating a HostedCluster section down below.</p> <p>Warning</p> <p>Since this is a disaster recovery procedure, unexpected things could happen because of all the moving components involved. To assist, see this troubleshooting section for the most common issues identified.</p> <ul> <li>Hypershift Operator Deployment endpoint access arguments</li> </ul> Public and PublicAndPrivatePrivate <pre><code>--external-dns-provider=aws \\\n--external-dns-credentials=&lt;AWS Credentials location&gt; \\\n--external-dns-domain-filter=&lt;External DNS for HostedCluster&gt;\n</code></pre> <pre><code>--private-platform aws \\\n--aws-private-creds &lt;Path to AWS Credentials&gt; \\\n--aws-private-region &lt;AWS Region&gt;\n</code></pre> <ul> <li>Arguments of the CLI when creating a HostedCluster</li> </ul> PublicPublicAndPrivatePrivate <pre><code>--external-dns-domain=&lt;External DNS Domain&gt; \\\n--endpoint-access=Public\n</code></pre> <pre><code>--external-dns-domain=&lt;External DNS Domain&gt; \\\n--endpoint-access=PublicAndPrivate\n</code></pre> <pre><code>--endpoint-access=Private\n</code></pre> <p>This way, the server URL will end in something like this: \"https://api-sample-hosted.sample-hosted.aws.openshift.com\"</p> <p>The way that a Hosted Cluster migration follows, it's basically done in 3 phases:</p> <ol> <li>Backup</li> <li>Restoration</li> <li>Teardown</li> </ol> <p>Let's setup the environment to start the migration with our first cluster.</p>"},{"location":"how-to/aws/disaster-recovery/#environment-and-context","title":"Environment and Context","text":"<p>Our scenario involves 3 Clusters, 2 Management ones and 1 HostedCluster, which will be migrated. Depending on the situation we would like to migrate just the ControlPlane or the Controlplane + nodes.</p> <p>These are the relevant data we need to know in order to migrate a cluster:</p> <ul> <li>Source MGMT Namespace: Source Management Namespace</li> <li>Source MGMT ClusterName: Source Management Cluster Name</li> <li>Source MGMT Kubeconfig: Source Management Kubeconfig</li> <li>Destination MGMT Kubeconfig: Destination Management Kubeconfig</li> <li>HC Kubeconfig: Hosted Cluster Kubeconfig</li> <li>SSH Key File: SSH Public Key</li> <li>Pull Secret: Pull Secret file to access to the Release Images</li> <li>AWS Credentials: AWS Credentials file</li> <li>AWS Region: AWS Region</li> <li>Base Domain: DNS Base Domain to use it as external DNS.</li> <li>S3 Bucket Name: This is the bucket in the same AWS Region where the ETCD backup will be uploaded</li> </ul> <p>These are the Variables we will use in the scripts:</p> Sample Environment Variables  - Ensure all the file it's correct regarding you folder tree and put this env file in your filesystem, then execute `source env_file` from a terminal  <pre><code>SSH_KEY_FILE=${HOME}/.ssh/id_rsa.pub\nBASE_PATH=${HOME}/hypershift\nBASE_DOMAIN=\"aws.sample.com\"\nPULL_SECRET_FILE=\"${HOME}/pull_secret.json\"\nAWS_CREDS=\"${HOME}/.aws/credentials\"\nAWS_ZONE_ID=\"Z02718293M33QHDEQBROL\"\n\nCONTROL_PLANE_AVAILABILITY_POLICY=SingleReplica\nHYPERSHIFT_PATH=${BASE_PATH}/src/hypershift\nHYPERSHIFT_CLI=${HYPERSHIFT_PATH}/bin/hypershift\nHYPERSHIFT_IMAGE=${HYPERSHIFT_IMAGE:-\"quay.io/${USER}/hypershift:latest\"}\nNODE_POOL_REPLICAS=${NODE_POOL_REPLICAS:-2}\n\n# MGMT Context\nMGMT_REGION=us-west-1\nMGMT_CLUSTER_NAME=\"${USER}-dev\"\nMGMT_CLUSTER_NS=${USER}\nMGMT_CLUSTER_DIR=\"${BASE_PATH}/hosted_clusters/${MGMT_CLUSTER_NS}-${MGMT_CLUSTER_NAME}\"\nMGMT_KUBECONFIG=\"${MGMT_CLUSTER_DIR}/kubeconfig\"\n\n# MGMT2 Context\nMGMT2_CLUSTER_NAME=\"${USER}-dest\"\nMGMT2_CLUSTER_NS=${USER}\nMGMT2_CLUSTER_DIR=\"${BASE_PATH}/hosted_clusters/${MGMT2_CLUSTER_NS}-${MGMT2_CLUSTER_NAME}\"\nMGMT2_KUBECONFIG=\"${MGMT2_CLUSTER_DIR}/kubeconfig\"\n\n# Hosted Cluster Context\nHC_CLUSTER_NS=clusters\nHC_REGION=us-west-1\nHC_CLUSTER_NAME=\"${USER}-hosted\"\nHC_CLUSTER_DIR=\"${BASE_PATH}/hosted_clusters/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}\"\nHC_KUBECONFIG=\"${HC_CLUSTER_DIR}/kubeconfig\"\nBACKUP_DIR=${HC_CLUSTER_DIR}/backup\n\nBUCKET_NAME=\"${USER}-hosted-${MGMT_REGION}\"\n\n# DNS\nAWS_ZONE_ID=\"Z07342811SH9AA102K1AC\"\nEXTERNAL_DNS_DOMAIN=\"hc.jpdv.aws.kerbeross.com\"\n</code></pre> <p>And this is how the Migration workflow will happen</p> <p></p>"},{"location":"how-to/aws/disaster-recovery/#backup","title":"Backup","text":"<p>This section complains interaction among multiple components. We will need to backup all the relevant things to raise up this same cluster in our target management cluster.</p> <p>To do that we will:</p> <ol> <li>Mark the Hosted Cluster with a ConfigMap which will declare the source Management Cluster it comes from (This is not mandatory but useful).</li> </ol> Config Map creation to set the Source Management Cluster <pre><code>oc create configmap mgmt-parent-cluster -n default --from-literal=from=${MGMT_CLUSTER_NAME}\n</code></pre> <ol> <li>Shutdown the reconciliation in the HostedCluster we want to migrate and also in the Nodepools.</li> </ol> ControlPlane Migration <pre><code>PAUSED_UNTIL=\"true\"\noc patch -n ${HC_CLUSTER_NS} hostedclusters/${HC_CLUSTER_NAME} -p '{\"spec\":{\"pausedUntil\":\"'${PAUSED_UNTIL}'\"}}' --type=merge\noc scale deployment -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --replicas=0 kube-apiserver openshift-apiserver openshift-oauth-apiserver control-plane-operator\n</code></pre> ControlPlane + NodePool Migration <pre><code>PAUSED_UNTIL=\"true\"\noc patch -n ${HC_CLUSTER_NS} hostedclusters/${HC_CLUSTER_NAME} -p '{\"spec\":{\"pausedUntil\":\"'${PAUSED_UNTIL}'\"}}' --type=merge\noc patch -n ${HC_CLUSTER_NS} nodepools/${NODEPOOLS} -p '{\"spec\":{\"pausedUntil\":\"'${PAUSED_UNTIL}'\"}}' --type=merge\noc scale deployment -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --replicas=0 kube-apiserver openshift-apiserver openshift-oauth-apiserver control-plane-operator\n</code></pre> <ol> <li>Backup ETCD and Upload to S3 Bucket</li> </ol> <p>The whole process of this step is documented here, even with that we will go through the process in a more programmatically way.</p> <p>To do this programmatically it's a bit more complicated, but we will try to put all the necessary steps in a bash script</p> ETCD Backup and Upload to S3 procedure  - As an advice, we recommend to wrap it up in a function and call it from the main function.  <pre><code># ETCD Backup\nETCD_PODS=\"etcd-0\"\nif [ \"${CONTROL_PLANE_AVAILABILITY_POLICY}\" = \"HighlyAvailable\" ]; then\n  ETCD_PODS=\"etcd-0 etcd-1 etcd-2\"\nfi\n\n## If you are in 4.12 or above, use this one\nETCD_CA_LOCATION=/etc/etcd/tls/etcd-ca/ca.crt\n\n## If you are in 4.11 or below, use this other one\n#ETCD_CA_LOCATION=/etc/etcd/tls/client/etcd-client-ca.crt\n\nfor POD in ${ETCD_PODS}; do\n  # Create an etcd snapshot\n  oc exec -it ${POD} -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -- env ETCDCTL_API=3 /usr/bin/etcdctl --cacert ${ETCD_CA_LOCATION} --cert /etc/etcd/tls/client/etcd-client.crt --key /etc/etcd/tls/client/etcd-client.key --endpoints=localhost:2379 snapshot save /var/lib/data/snapshot.db\n\n  oc exec -it ${POD} -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -- env ETCDCTL_API=3 /usr/bin/etcdctl -w table snapshot status /var/lib/data/snapshot.db\n\n  FILEPATH=\"/${BUCKET_NAME}/${HC_CLUSTER_NAME}-${POD}-snapshot.db\"\n  CONTENT_TYPE=\"application/x-compressed-tar\"\n  DATE_VALUE=`date -R`\n  SIGNATURE_STRING=\"PUT\\n\\n${CONTENT_TYPE}\\n${DATE_VALUE}\\n${FILEPATH}\"\n\n  set +x\n  ACCESS_KEY=$(grep aws_access_key_id ${AWS_CREDS} | head -n1 | cut -d= -f2 | sed \"s/ //g\")\n  SECRET_KEY=$(grep aws_secret_access_key ${AWS_CREDS} | head -n1 | cut -d= -f2 | sed \"s/ //g\")\n  SIGNATURE_HASH=$(echo -en ${SIGNATURE_STRING} | openssl sha1 -hmac \"${SECRET_KEY}\" -binary | base64)\n  set -x\n\n  # FIXME: this is pushing to the OIDC bucket\n  oc exec -it etcd-0 -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -- curl -X PUT -T \"/var/lib/data/snapshot.db\" \\\n    -H \"Host: ${BUCKET_NAME}.s3.amazonaws.com\" \\\n    -H \"Date: ${DATE_VALUE}\" \\\n    -H \"Content-Type: ${CONTENT_TYPE}\" \\\n    -H \"Authorization: AWS ${ACCESS_KEY}:${SIGNATURE_HASH}\" \\\n    https://${BUCKET_NAME}.s3.amazonaws.com/${HC_CLUSTER_NAME}-${POD}-snapshot.db\ndone\n</code></pre> <p>Warning</p> <p>The CA Certificate of ETCD has changed the location in 4.12, so take care about the command execution because it will fail. It's safe to reexecute this piece of code, it just will backup the ETCD in S3. In order to know which version you have installed, just execute this command <code>oc version -o json | jq -e .openshiftVersion</code></p> <ol> <li> <p>Backup Kubernetes/Openshift objects</p> <ul> <li>From HostedCluster Namespace:<ul> <li>HostedCluster and NodePool Objects</li> <li>HostedCluster Secrets</li> </ul> </li> <li>From Hosted Control Plane Namespace:<ul> <li>HostedControlPlane</li> <li>Cluster</li> <li>AWSCluster, AWSMachineTemplate, AWSMachine</li> <li>MachineDeployments, MachineSets and Machines</li> <li>ControlPlane Secrets</li> </ul> </li> </ul> </li> </ol> Openshift Objects backup <pre><code>mkdir -p ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS} ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}\nchmod 700 ${BACKUP_DIR}/namespaces/\n\n# HostedCluster\necho \"Backing Up HostedCluster Objects:\"\noc get hc ${HC_CLUSTER_NAME} -n ${HC_CLUSTER_NS} -o yaml &gt; ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/hc-${HC_CLUSTER_NAME}.yaml\necho \"--&gt; HostedCluster\"\nsed -i '' -e '/^status:$/,$d' ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/hc-${HC_CLUSTER_NAME}.yaml\n\n# NodePool\noc get np ${NODEPOOLS} -n ${HC_CLUSTER_NS} -o yaml &gt; ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/np-${NODEPOOLS}.yaml\necho \"--&gt; NodePool\"\nsed -i '' -e '/^status:$/,$ d' ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/np-${NODEPOOLS}.yaml\n\n# Secrets in the HC Namespace\necho \"--&gt; HostedCluster Secrets:\"\nfor s in $(oc get secret -n ${HC_CLUSTER_NS} | grep \"^${HC_CLUSTER_NAME}\" | awk '{print $1}'); do\n    oc get secret -n ${HC_CLUSTER_NS} $s -o yaml &gt; ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/secret-${s}.yaml\ndone\n\n# Secrets in the HC Control Plane Namespace\necho \"--&gt; HostedCluster ControlPlane Secrets:\"\nfor s in $(oc get secret -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} | egrep -v \"docker|service-account-token|oauth-openshift|NAME|token-${HC_CLUSTER_NAME}\" | awk '{print $1}'); do\n    oc get secret -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} $s -o yaml &gt; ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/secret-${s}.yaml\ndone\n\n# Hosted Control Plane\necho \"--&gt; HostedControlPlane:\"\noc get hcp ${HC_CLUSTER_NAME} -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o yaml &gt; ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/hcp-${HC_CLUSTER_NAME}.yaml\n\n# Cluster\necho \"--&gt; Cluster:\"\nCL_NAME=$(oc get hcp ${HC_CLUSTER_NAME} -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o jsonpath={.metadata.labels.\\*} | grep ${HC_CLUSTER_NAME})\noc get cluster ${CL_NAME} -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o yaml &gt; ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/cl-${HC_CLUSTER_NAME}.yaml\n\n# AWS Cluster\necho \"--&gt; AWS Cluster:\"\noc get awscluster ${HC_CLUSTER_NAME} -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o yaml &gt; ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/awscl-${HC_CLUSTER_NAME}.yaml\n\n# AWS MachineTemplate\necho \"--&gt; AWS Machine Template:\"\noc get awsmachinetemplate ${NODEPOOLS} -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o yaml &gt; ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/awsmt-${HC_CLUSTER_NAME}.yaml\n\n# AWS Machines\necho \"--&gt; AWS Machine:\"\nCL_NAME=$(oc get hcp ${HC_CLUSTER_NAME} -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o jsonpath={.metadata.labels.\\*} | grep ${HC_CLUSTER_NAME})\nfor s in $(oc get awsmachines -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --no-headers | grep ${CL_NAME} | cut -f1 -d\\ ); do\n    oc get -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} awsmachines $s -o yaml &gt; ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/awsm-${s}.yaml\ndone\n\n# MachineDeployments\necho \"--&gt; HostedCluster MachineDeployments:\"\nfor s in $(oc get machinedeployment -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name); do\n    mdp_name=$(echo ${s} | cut -f 2 -d /)\n    oc get -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} $s -o yaml &gt; ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/machinedeployment-${mdp_name}.yaml\ndone\n\n# MachineSets\necho \"--&gt; HostedCluster MachineSets:\"\nfor s in $(oc get machineset -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name); do\n    ms_name=$(echo ${s} | cut -f 2 -d /)\n    oc get -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} $s -o yaml &gt; ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/machineset-${ms_name}.yaml\ndone\n\n# Machines\necho \"--&gt; HostedCluster Machine:\"\nfor s in $(oc get machine -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name); do\n    m_name=$(echo ${s} | cut -f 2 -d /)\n    oc get -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} $s -o yaml &gt; ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/machine-${m_name}.yaml\ndone\n</code></pre> <ol> <li> <p>Cleanup the ControlPlane Routes (only in <code>PublicAndPrivate</code> and <code>Public</code> clusters)</p> <ul> <li>This will allow the ExternalDNS Operator to delete the Route53 entries in AWS and they will not be recreated because of this HostedCluster it's paused.</li> </ul> </li> </ol> HostedCluster ControlPlane Routes Cleanup  - Just to clean the routes you could execute this command, but you will need to wait until the Route53 are clean (this is why I will add an alternative to validate this step).  <pre><code>oc delete routes -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --all\n</code></pre>  - (Alternative bash script) Cleanup OCP HC ControlPlane Routes and wait until Route53 it's clean (only in `PublicAndPrivate` and `Public` clusters) <pre><code>function clean_routes() {\n\n    if [[ -z \"${1}\" ]];then\n        echo \"Give me the NS where to clean the routes\"\n        exit 1\n    fi\n\n    # Constants\n    if [[ -z \"${2}\" ]];then\n        echo \"Give me the Route53 zone ID\"\n        exit 1\n    fi\n\n    ZONE_ID=${2}\n    ROUTES=10\n    timeout=40\n    count=0\n\n    # This allows us to remove the ownership in the AWS for the API route\n    oc delete route -n ${1} --all\n\n    while [ ${ROUTES} -gt 2 ]\n    do\n        echo \"Waiting for ExternalDNS Operator to clean the DNS Records in AWS Route53 where the zone id is: ${ZONE_ID}...\"\n        echo \"Try: (${count}/${timeout})\"\n        sleep 10\n        if [[ $count -eq timeout ]];then\n            echo \"Timeout waiting for cleaning the Route53 DNS records\"\n            exit 1\n        fi\n        count=$((count+1))\n        ROUTES=$(aws route53 list-resource-record-sets --hosted-zone-id ${ZONE_ID} --max-items 10000 --output json | grep -c ${EXTERNAL_DNS_DOMAIN})\n    done\n}\n\n# SAMPLE: clean_routes \"&lt;HC ControlPlane Namespace&gt;\" \"&lt;AWS_ZONE_ID&gt;\"\nclean_routes \"${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}\" \"${AWS_ZONE_ID}\"\n</code></pre> <p>Warning</p> <p>This step is only relevant if you have a HostedCluster with a <code>--endpoint-access</code> argument as <code>PublicAndPrivate</code> or <code>Public</code>. If that's not the case, you will not have the need to execute this part.</p> <p>This was the last step on Backup stage, now we encourage you to validate all the OCP Objects and the S3 Bucket in order to ensure all is fine.</p>"},{"location":"how-to/aws/disaster-recovery/#restoration","title":"Restoration","text":"<p>This step it's basically catch all the objects which has been backuped up and restore them in the Destination Management Cluster.</p> <p>Note</p> <p>Ensure you have the destination's cluster's Kubeconfig placed as is set in <code>MGMT2_KUBECONFIG</code> (if you follow the final script) or <code>KUBECONFIG</code> variable if you are going step by step. <code>export KUBECONFIG=${MGMT2_KUBECONFIG}</code> or <code>export KUBECONFIG=&lt;Kubeconfig FilePath&gt;</code></p> <ol> <li>Ensure you don't have an old Namespace in the new MGMT Cluster with the same name as the cluster that are you migrating.</li> </ol> Delete the Namespace that will be used by the Migrated Cluster and the Control plane <pre><code># Just in case\nexport KUBECONFIG=${MGMT2_KUBECONFIG}\nBACKUP_DIR=${HC_CLUSTER_DIR}/backup\n\n# Namespace deletion in the destination Management cluster\noc delete ns ${HC_CLUSTER_NS} || true\noc delete ns ${HC_CLUSTER_NS}-{HC_CLUSTER_NAME} || true\n</code></pre> <ol> <li>ReCreate the deleted namespaces from fresh start</li> </ol> Namespace creation <pre><code># Namespace creation\noc new-project ${HC_CLUSTER_NS}\noc new-project ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}\n</code></pre> <ol> <li>Restore Secrets in the HC Namespace</li> </ol> Secrets Restoration in HostedCluster Namespace <pre><code>oc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/secret-*\n</code></pre> <ol> <li>Restore Objects in the HC ControlPlane Namespace</li> </ol> Restore OCP Objects related with the HC ControlPlane <pre><code># Secrets\noc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/secret-*\n\n# Cluster\noc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/hcp-*\noc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/cl-*\n</code></pre> <ol> <li> <p>(Optional) Restore objects in the HC ControlPlane Namespace</p> <p>Note</p> <p>This step it's only relevant if you are migrating the Nodes and the NodePool to reuse the AWS Instances.</p> </li> </ol> Restore OCP Nodes related objects within the HC ControlPlane Namespace <pre><code># AWS\noc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/awscl-*\noc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/awsmt-*\noc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/awsm-*\n\n# Machines\noc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/machinedeployment-*\noc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/machineset-*\noc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}/machine-*\n</code></pre> <ol> <li>Restore the ETCD Backup and HostedCluster</li> </ol> Bash script to restore ETCD and HostedCluster object <pre><code>ETCD_PODS=\"etcd-0\"\nif [ \"${CONTROL_PLANE_AVAILABILITY_POLICY}\" = \"HighlyAvailable\" ]; then\n  ETCD_PODS=\"etcd-0 etcd-1 etcd-2\"\nfi\n\nHC_RESTORE_FILE=${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/hc-${HC_CLUSTER_NAME}-restore.yaml\nHC_BACKUP_FILE=${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/hc-${HC_CLUSTER_NAME}.yaml\nHC_NEW_FILE=${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/hc-${HC_CLUSTER_NAME}-new.yaml\ncat ${HC_BACKUP_FILE} &gt; ${HC_NEW_FILE}\ncat &gt; ${HC_RESTORE_FILE} &lt;&lt;EOF\n    restoreSnapshotURL:\nEOF\n\nfor POD in ${ETCD_PODS}; do\n  # Create a pre-signed URL for the etcd snapshot\n  ETCD_SNAPSHOT=\"s3://${BUCKET_NAME}/${HC_CLUSTER_NAME}-${POD}-snapshot.db\"\n  ETCD_SNAPSHOT_URL=$(AWS_DEFAULT_REGION=${MGMT2_REGION} aws s3 presign ${ETCD_SNAPSHOT})\n\n  # FIXME no CLI support for restoreSnapshotURL yet\n  cat &gt;&gt; ${HC_RESTORE_FILE} &lt;&lt;EOF\n    - \"${ETCD_SNAPSHOT_URL}\"\nEOF\ndone\n\ncat ${HC_RESTORE_FILE}\n\nif ! grep ${HC_CLUSTER_NAME}-snapshot.db ${HC_NEW_FILE}; then\n  sed -i '' -e \"/type: PersistentVolume/r ${HC_RESTORE_FILE}\" ${HC_NEW_FILE}\n  sed -i '' -e '/pausedUntil:/d' ${HC_NEW_FILE}\nfi\n\nHC=$(oc get hc -n ${HC_CLUSTER_NS} ${HC_CLUSTER_NAME} -o name || true)\nif [[ ${HC} == \"\" ]];then\n    echo \"Deploying HC Cluster: ${HC_CLUSTER_NAME} in ${HC_CLUSTER_NS} namespace\"\n    oc apply -f ${HC_NEW_FILE}\nelse\n    echo \"HC Cluster ${HC_CLUSTER_NAME} already exists, avoiding step\"\nfi\n</code></pre> <ol> <li> <p>(Optional) Restore the NodePool</p> <p>Note</p> <p>This step it's only relevant if you are migrating the Nodes and the NodePool to reuse the AWS Instances.</p> </li> </ol> Restore the NodePool object <pre><code>oc apply -f ${BACKUP_DIR}/namespaces/${HC_CLUSTER_NS}/np-*\n</code></pre> <p>This was our last step in the Restoration phase. If you are not migrating nodes, congratulations, you can pass to the next section Teardown.</p> <p>(Optional) Now we will need to wait for some time until the Nodes gets fully migrated. We recommend to use this function</p> Ensure Nodes Migrated <pre><code>timeout=40\ncount=0\nNODE_STATUS=$(oc get nodes --kubeconfig=${HC_KUBECONFIG} | grep -v NotReady | grep -c \"worker\") || NODE_STATUS=0\n\nwhile [ ${NODE_POOL_REPLICAS} != ${NODE_STATUS} ]\ndo\n    echo \"Waiting for Nodes to be Ready in the destination MGMT Cluster: ${MGMT2_CLUSTER_NAME}\"\n    echo \"Try: (${count}/${timeout})\"\n    sleep 30\n    if [[ $count -eq timeout ]];then\n        echo \"Timeout waiting for Nodes in the destination MGMT Cluster\"\n        exit 1\n    fi\n    count=$((count+1))\n    NODE_STATUS=$(oc get nodes --kubeconfig=${HC_KUBECONFIG} | grep -v NotReady | grep -c \"worker\") || NODE_STATUS=0\ndone\n</code></pre>"},{"location":"how-to/aws/disaster-recovery/#teardown","title":"Teardown","text":"<p>In this section we will shutdown and delete the HostedCluster in the source Management Cluster.</p> <p>Note</p> <p>Ensure you have the source's cluster's Kubeconfig placed as is set in <code>MGMT_KUBECONFIG</code> (if you follow the final script) or <code>KUBECONFIG</code> variable if you are going step by step. <code>export KUBECONFIG=${MGMT_KUBECONFIG}</code> or <code>export KUBECONFIG=&lt;Kubeconfig FilePath&gt;</code></p> <ol> <li>Scale The Deployments and StatefulSets</li> </ol> ScaleDown Pod relevant objects in the HC ControlPlane Namespace <pre><code># Just in case\nexport KUBECONFIG=${MGMT_KUBECONFIG}\n\n# Scale down deployments\noc scale deployment -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --replicas=0 --all\noc scale statefulset.apps -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --replicas=0 --all\nsleep 15\n</code></pre> <ol> <li>Delete the NodePool objects</li> </ol> Delete NodePools <pre><code>NODEPOOLS=$(oc get nodepools -n ${HC_CLUSTER_NS} -o=jsonpath='{.items[?(@.spec.clusterName==\"'${HC_CLUSTER_NAME}'\")].metadata.name}')\nif [[ ! -z \"${NODEPOOLS}\" ]];then\n    oc patch -n \"${HC_CLUSTER_NS}\" nodepool ${NODEPOOLS} --type=json --patch='[ { \"op\":\"remove\", \"path\": \"/metadata/finalizers\" }]'\n    oc delete np -n ${HC_CLUSTER_NS} ${NODEPOOLS}\nfi\n</code></pre> <ol> <li>Delete the Machines and MachineSets</li> </ol> Delete Machines and MachineSets <pre><code># Machines\nfor m in $(oc get machines -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name); do\n    oc patch -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} ${m} --type=json --patch='[ { \"op\":\"remove\", \"path\": \"/metadata/finalizers\" }]' || true\n    oc delete -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} ${m} || true\ndone\n\noc delete machineset -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --all || true\n</code></pre> <ol> <li>Delete Cluster object</li> </ol> Delete the Cluster <pre><code># Cluster\nC_NAME=$(oc get cluster -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name)\noc patch -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} ${C_NAME} --type=json --patch='[ { \"op\":\"remove\", \"path\": \"/metadata/finalizers\" }]'\noc delete cluster.cluster.x-k8s.io -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --all\n</code></pre> <ol> <li>Delete the AWS Machines (Kubernetes Objects)</li> </ol> <p>Note</p> <p>Don't worry about the real AWS Machines, even if you delete this object, the CAPI controllers are down and will not affect the cloud instances</p> Delete AWS Machines OCP Objects <pre><code># AWS Machines\nfor m in $(oc get awsmachine.infrastructure.cluster.x-k8s.io -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} -o name)\ndo\n    oc patch -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} ${m} --type=json --patch='[ { \"op\":\"remove\", \"path\": \"/metadata/finalizers\" }]' || true\n    oc delete -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} ${m} || true\ndone\n</code></pre> <ol> <li>Delete HostedControlPlane and Controlplane HC Namespace</li> </ol> Delete HostedControlPlane and ControlPlane HC Namespace objects <pre><code># Delete HCP and ControlPlane HC NS\noc patch -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} hostedcontrolplane.hypershift.openshift.io ${HC_CLUSTER_NAME} --type=json --patch='[ { \"op\":\"remove\", \"path\": \"/metadata/finalizers\" }]'\noc delete hostedcontrolplane.hypershift.openshift.io -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} --all\noc delete ns ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME} || true\n</code></pre> <ol> <li>Delete the HostedCluster and HC Namespace</li> </ol> Delete the HostedCluster object <pre><code># Delete HC and HC Namespace\noc -n ${HC_CLUSTER_NS} patch hostedclusters ${HC_CLUSTER_NAME} -p '{\"metadata\":{\"finalizers\":null}}' --type merge || true\noc delete hc -n ${HC_CLUSTER_NS} ${HC_CLUSTER_NAME}  || true\noc delete ns ${HC_CLUSTER_NS} || true\n</code></pre> <p>And that was it, following this whole process you could migrate an HostedCluster from one Management Cluster to other one in the same AWS Region.</p> <p>To ensure all is working fine, you just need to validate that all the objects are in the right place:</p> <pre><code># Validations\nexport KUBECONFIG=${MGMT2_KUBECONFIG}\n\noc get hc -n ${HC_CLUSTER_NS}\noc get np -n ${HC_CLUSTER_NS}\noc get pod -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}\noc get machines -n ${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}\n\n# Inside the HostedCluster\nexport KUBECONFIG=${HC_KUBECONFIG}\noc get clusterversion\noc get nodes\n</code></pre> <ol> <li>(Optional) Restart OVN Pods in compute nodes (only in <code>PublicAndPrivate</code> and <code>Public</code> clusters) After the Teardown of the HostedCluster in the source Management Cluster you will need to delete the OVN pods in the HostedCluster in order to perform the connection with the new OVN Master running in the new Management Cluster.</li> </ol> <p>To do that you just need to load the proper KUBECONFIG env var with the Hosted Cluster Kubeconfig path and execute this command:</p> <pre><code>oc delete pod -n openshift-ovn-kubernetes --all\n</code></pre> <p>with that, all the ClusterOperators that were failing and all the new pods generated, will get executed without issues.</p> <p>Warning</p> <p>This step is only relevant if you have a HostedCluster with a <code>--endpoint-access</code> argument as <code>PublicAndPrivate</code> or <code>Public</code>. If that's not the case, you will not have the need to execute this part.</p>"},{"location":"how-to/aws/disaster-recovery/#migration-helper-script","title":"Migration Helper script","text":"<p>In order to ensure the that whole migration works fine, you could use this helper script that should work out of the box.</p> HC Migration Script  In order to execute the script, just: - Fill the common variables and save the file as `../common/common.sh` - Execute the migration script without params.  Now let's take a look to that script  - Common Variables  <pre><code># Fill the Common variables to fit your environment, this is just a sample\nSSH_KEY_FILE=${HOME}/.ssh/id_rsa.pub\nBASE_PATH=${HOME}/hypershift\nBASE_DOMAIN=\"aws.sample.com\"\nPULL_SECRET_FILE=\"${HOME}/pull_secret.json\"\nAWS_CREDS=\"${HOME}/.aws/credentials\"\nCONTROL_PLANE_AVAILABILITY_POLICY=SingleReplica\nHYPERSHIFT_PATH=${BASE_PATH}/src/hypershift\nHYPERSHIFT_CLI=${HYPERSHIFT_PATH}/bin/hypershift\nHYPERSHIFT_IMAGE=${HYPERSHIFT_IMAGE:-\"quay.io/${USER}/hypershift:latest\"}\nNODE_POOL_REPLICAS=${NODE_POOL_REPLICAS:-2}\n\n# MGMT Context\nMGMT_REGION=us-west-1\nMGMT_CLUSTER_NAME=\"${USER}-dev\"\nMGMT_CLUSTER_NS=${USER}\nMGMT_CLUSTER_DIR=\"${BASE_PATH}/hosted_clusters/${MGMT_CLUSTER_NS}-${MGMT_CLUSTER_NAME}\"\nMGMT_KUBECONFIG=\"${MGMT_CLUSTER_DIR}/kubeconfig\"\n\n# MGMT2 Context\nMGMT2_REGION=us-west-1\nMGMT2_CLUSTER_NAME=\"${USER}-dest\"\nMGMT2_CLUSTER_NS=${USER}\nMGMT2_CLUSTER_DIR=\"${BASE_PATH}/hosted_clusters/${MGMT2_CLUSTER_NS}-${MGMT2_CLUSTER_NAME}\"\nMGMT2_KUBECONFIG=\"${MGMT2_CLUSTER_DIR}/kubeconfig\"\n\n# Hosted Cluster Context\nHC_CLUSTER_NS=clusters\nHC_REGION=us-west-1\nHC_CLUSTER_NAME=\"${USER}-hosted\"\nHC_CLUSTER_DIR=\"${BASE_PATH}/hosted_clusters/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}\"\nHC_KUBECONFIG=\"${HC_CLUSTER_DIR}/kubeconfig\"\nBACKUP_DIR=${HC_CLUSTER_DIR}/backup\n\nBUCKET_NAME=\"${USER}-hosted-${MGMT_REGION}\"\n\n# DNS\nAWS_ZONE_ID=\"Z026552815SS3YPH9H6MG\"\nEXTERNAL_DNS_DOMAIN=\"guest.jpdv.aws.kerbeross.com\"\n</code></pre>  - Migration Script The migration script is maintained at https://github.com/openshift/hypershift/blob/main/contrib/migration/migrate-hcp.sh"},{"location":"how-to/aws/etc-backup-restore/","title":"Etcd backup/restore (manual process)","text":"<p>It is possible to manually snapshot the etcd state for a HostedCluster, currently this is a fully manual process and requires API downtime.</p>"},{"location":"how-to/aws/etc-backup-restore/#creating-the-etcd-snapshot","title":"Creating the Etcd snapshot","text":"<p>First we must Pause reconciliation of the HostedCluster:</p> <pre><code>oc patch -n clusters hostedclusters/${CLUSTER_NAME} -p '{\"spec\":{\"pausedUntil\":\"'${PAUSED_UNTIL}'\"}}' --type=merge\n</code></pre> <p>Now stop all etcd-writer deployments:</p> <pre><code>oc scale deployment -n ${HOSTED_CLUSTER_NAMESPACE} --replicas=0 kube-apiserver openshift-apiserver openshift-oauth-apiserver\n</code></pre> <p>You can now perform an etcd snapshot via <code>exec</code> in any etcd container:</p> <pre><code>oc exec -it etcd-0 -n ${HOSTED_CLUSTER_NAMESPACE} -- env ETCDCTL_API=3 /usr/bin/etcdctl --cacert /etc/etcd/tls/client/etcd-client-ca.crt --cert /etc/etcd/tls/client/etcd-client.crt --key /etc/etcd/tls/client/etcd-client.key --endpoints=localhost:2379 snapshot save /var/lib/data/snapshot.db\noc exec -it etcd-0 -n ${HOSTED_CLUSTER_NAMESPACE} -- env ETCDCTL_API=3 /usr/bin/etcdctl -w table snapshot status /var/lib/data/snapshot.db\n</code></pre> <p>The snapshot data can then be copied to some location where it can be later retrieved, for example S3:</p> <pre><code>BUCKET_NAME=somebucket\nFILEPATH=\"/${BUCKET_NAME}/${CLUSTER_NAME}-snapshot.db\"\nCONTENT_TYPE=\"application/x-compressed-tar\"\nDATE_VALUE=`date -R`\nSIGNATURE_STRING=\"PUT\\n\\n${CONTENT_TYPE}\\n${DATE_VALUE}\\n${FILEPATH}\"\nACCESS_KEY=accesskey\nSECRET_KEY=secret\nSIGNATURE_HASH=`echo -en ${SIGNATURE_STRING} | openssl sha1 -hmac ${SECRET_KEY} -binary | base64`\n\noc exec -it etcd-0 -n ${HOSTED_CLUSTER_NAMESPACE} -- curl -X PUT -T \"/var/lib/data/snapshot.db\" \\\n  -H \"Host: ${BUCKET_NAME}.s3.amazonaws.com\" \\\n  -H \"Date: ${DATE_VALUE}\" \\\n  -H \"Content-Type: ${CONTENT_TYPE}\" \\\n  -H \"Authorization: AWS ${ACCESS_KEY}:${SIGNATURE_HASH}\" \\\n  https://${BUCKET_NAME}.s3.amazonaws.com/${CLUSTER_NAME}-snapshot.db\n</code></pre> <p>If you wish to restore the snapshot on some new cluster later, ensure you have saved the encryption secret referenced from the HostedCluster resource, e.g:</p> <pre><code>oc get hostedcluster $CLUSTER_NAME -o=jsonpath='{.spec.secretEncryption.aescbc}'\n{\"activeKey\":{\"name\":\"CLUSTER_NAME-etcd-encryption-key\"}}\n\n# Save this secret, or the key it contains so the etcd data can later be decrypted\noc get secret ${CLUSTER_NAME}-etcd-encryption-key -o=jsonpath='{.data.key}'\n</code></pre>"},{"location":"how-to/aws/etc-backup-restore/#restoring-an-etcd-snapshot","title":"Restoring an etcd snapshot","text":"<p>An etcd snapshot can currently only be restored on cluster creation, this can be achieved by modifying the output from <code>create cluster --render</code>, and defining a <code>restoreSnapshotURL</code> in the etcd section of the HostedCluster spec.</p> <p>First we must create a pre-signed URL so the previously saved etcd snapshot can be downloaded from S3 without passing credentials to the etcd deployment, with the <code>aws</code> CLI this can be achieved like:</p> <pre><code>ETCD_SNAPSHOT=${ETCD_SNAPSHOT:-\"s3://${BUCKET_NAME}/${CLUSTER_NAME}-snapshot.db\"}\nETCD_SNAPSHOT_URL=$(aws s3 presign ${ETCD_SNAPSHOT})\n</code></pre> <p>Now we modify the HostedCluster spec to refer to this URL, e.g:</p> <pre><code>spec:\n  etcd:\n    managed:\n      storage:\n        persistentVolume:\n          size: 4Gi\n        type: PersistentVolume\n        restoreSnapshotURL:\n        - \"${ETCD_SNAPSHOT_URL}\"\n    managementType: Managed\n</code></pre> <p>Finally you must ensure the secret referenced from the <code>spec.secretEncryption.aescbc</code> contains the same AES key saved in the previous steps.</p>"},{"location":"how-to/aws/external-dns/","title":"External DNS","text":"<p>Hypershift separation between Control Plane and Data Plane enables two independent areas for DNS configuration: </p> <ul> <li>Ingress for workloads within the hosted cluster (traditionally *.apps.service-consumer-domain.com).</li> <li>Ingress for service endpoints within the management cluster (e.g. api / oauth endpoints via *.service-provider-domain.com).</li> </ul> <p>The input for the <code>hostedCluster.spec.dns</code> dictates the first one. The input for <code>hostedCluster.spec.services.routePublishingStrategy.hostname</code> dictates the second one, which we'll elaborate in this doc.</p> <p>Note: External DNS will only make a difference for setups with Public endpoints i.e. \"Public\" or \"PublicAndPrivate\". For a \"Private\" setup all endpoints will be accessible via <code>.hypershift.local</code>, which will contain CNAME records to the appropriate Private Link Endpoint Services.</p>"},{"location":"how-to/aws/external-dns/#use-service-level-dns-for-control-plane-services","title":"Use Service-level DNS for Control Plane Services","text":"<p>There are four service that are exposed by a Hosted Control Plane (HCP)</p> <ul> <li><code>APIServer</code></li> <li><code>OAuthService</code></li> <li><code>Konnectivity</code></li> <li><code>Ignition</code></li> </ul> <p>Each of these services is exposed using a <code>servicePublishingStrategy</code> in the HostedCluster spec.</p> <p>By default, for <code>servicePublishingStrategy</code> types <code>LoadBalancer</code> and <code>Route</code>, the service will be published using the hostname of the LoadBalancer found in the status of the <code>Service</code> with type <code>LoadBalancer</code>, or in the <code>status.host</code> field of the <code>Route</code>.</p> <p>This is acceptable for Hypershift development environments.  However, when deploying Hypershift in a managed service context, this method leaks the ingress subdomain of the underlying management cluster and can limit options for management cluster lifecycling and disaster recovery.  For example, if the AWS load balancer for a service is lost for whatever reason, the DNS name of that load balancer is in the kubelet kubeconfig of each node in the guest cluster.  Restoring the cluster would involve an out-of-band update of all kubelet kubeconfigs on existing nodes.</p> <p>Having a DNS indirection layer on top of the <code>LoadBalancer</code> and <code>Route</code> publishing types allows a managed service operator to publish all public HostedCluster <code>services</code> using a service-level domain.  This allows remampping on the DNS name to a new <code>LoadBalancer</code> or <code>Route</code> and does not expose the ingress domain of the management cluster.</p>"},{"location":"how-to/aws/external-dns/#external-dns-setup","title":"External-dns setup","text":"<p>Hypershift uses external-dns to achieve this indirection.</p> <p><code>external-dns</code> is optionally deployed alongside the <code>hypershift-operator</code> in the <code>hypershift</code> namespace of the management cluster. It watches the cluster for <code>Services</code> or <code>Routes</code> with the <code>external-dns.alpha.kubernetes.io/hostname</code> annotation.  This value of this annotation is used to create a DNS record pointing to the <code>Service</code> (A record) or <code>Route</code> (CNAME record).</p> <p><code>hypershift install</code> will create the <code>external-dns</code> deployment if the proper flags are set:</p> <pre><code>hypershift install --external-dns-provider=aws --external-dns-credentials=route53-aws-creds --external-dns-domain-filter=service-provider-domain.com ...\n</code></pre> <p>where <code>external-dns-provider</code> is the DNS provider that manages the service-level DNS zone, <code>external-dns-credentials</code> is the credentials file appropriate for the specified provider, and <code>external-dns-domain-filter</code> is the service-level domain.</p>"},{"location":"how-to/aws/external-dns/#creating-the-public-dns-hosted-zone","title":"Creating the public DNS Hosted zone","text":"<p>external-dns-domain: Should point to an external public domain (pre-created).</p> <p>To create this Hosted Zone, you can do it in the AWS Route53 Management console:</p> <ul> <li>Create a Hosted zone in your Management console</li> </ul> <p></p> <ul> <li>Grab the values from the zone created to configure them (in our case) in the subzone</li> </ul> <p></p> <ul> <li>In the main domain, create the NS record to redirect the DNS requests to that delegated zone (use the values from the previous step)</li> </ul> <p></p> <ul> <li>In order to verify that all works fine, create a test entry in the new subzone and test it with a dig command</li> </ul> <p></p> <p></p>"},{"location":"how-to/aws/external-dns/#hostedcluster-with-service-hostnames","title":"HostedCluster with Service Hostnames","text":"<p>Create a HostedCluster that sets <code>hostname</code> for <code>LoadBalancer</code> and <code>Route</code> services:</p> <pre><code>hypershift create cluster aws --name=example --endpoint-access=PublicAndPrivate --external-dns-domain=service-provider-domain.com ...\n</code></pre> <p>NOTE: The external-dns-domain should match the Public Hosted Zone created in the previous step</p> <p>The resulting HostedCluster <code>services</code> block looks like this:</p> <pre><code>  platform:\n    aws:\n      endpointAccess: PublicAndPrivate\n...\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      route:\n        hostname: api-example.service-provider-domain.com\n      type: Route\n  - service: OAuthServer\n    servicePublishingStrategy:\n      route:\n        hostname: oauth-example.service-provider-domain.com\n      type: Route\n  - service: Konnectivity\n    servicePublishingStrategy:\n      type: Route\n  - service: Ignition\n    servicePublishingStrategy:\n      type: Route\n</code></pre> <p>When the <code>Services</code> and <code>Routes</code> are created by the Control Plane Operator (CPO), it will annotate them with the <code>external-dns.alpha.kubernetes.io/hostname</code> annotation. The value will be the <code>hostname</code> field in the <code>servicePublishingStrategy</code> for that type.  The CPO uses this name blindly for the service endpoints and assumes that if <code>hostname</code> is set, there is some mechanism external-dns or otherwise, that will create the DNS records.</p> <p>There is an interaction between the <code>spec.platform.aws.endpointAccess</code> and which services are permitted to set <code>hostname</code> when using AWS Private clustering.  Only public services can have service-level DNS indirection.  Private services use the <code>hypershift.local</code> private zone and it is not valid to set <code>hostname</code> for <code>services</code> that are private for a given <code>endpointAccess</code> type.</p> <p>The following table notes when it is valid to set hostname for a particular <code>service</code> and <code>endpointAccess</code> combination:</p> Public PublicAndPrivate Private APIServer Y Y N OAuthServer Y Y N Konnectivity Y N N Ingition Y N N"},{"location":"how-to/aws/external-dns/#examples-of-how-to-deploy-a-cluster-using-the-cli-and-externaldns","title":"Examples of how to deploy a cluster using the CLI and externalDNS","text":"<p>Having a vanilla Openshift cluster, follow this steps</p> Deploy Hypershift and ExternalDNS operators with the external Public HostedZone already created  - Ensure the public hosted zone already exists, in our case is `service-provider-domain.com`  - Hypershift Deployment command <pre><code>export KUBECONFIG=&lt;PATH TO MANAGEMENT's CLUSTER's KUBECONFIG's&gt;\nexport REGION=us-west-1\nexport BUCKET_NAME=jparrill-hosted-us-west-1\nexport AWS_CREDS=~/.aws/credentials\n\nhypershift install \\\n    --oidc-storage-provider-s3-bucket-name ${BUCKET_NAME} \\\n    --oidc-storage-provider-s3-credentials ${AWS_CREDS} \\\n    --oidc-storage-provider-s3-region ${REGION} \\\n    --external-dns-provider=aws \\\n    --external-dns-credentials=${AWS_CREDS} \\\n    --external-dns-domain-filter=service-provider-domain.com \\\n    --private-platform AWS \\\n    --aws-private-creds ${AWS_CREDS} \\\n    --aws-private-region ${REGION}\n</code></pre> Deploy HostedCluster using ExternalDNS feature  - Ensure the `externaldns` operator is up and the internal flags points to the desired public hosted zone - HostedCluster Deployment command  <pre><code>export KUBECONFIG=&lt;MGMT Cluster Kubeconfig&gt;\nexport AWS_CREDS=~/.aws/credentials\nexport REGION=us-west-1\n\nhypershift create cluster aws \\\n    --aws-creds ${AWS_CREDS} \\\n    --instance-type m6i.xlarge \\\n    --region ${REGION} \\\n    --auto-repair \\\n    --generate-ssh \\\n    --name jparrill-hosted \\\n    --namespace clusters \\\n    --base-domain service-consumer-domain.com \\\n    --node-pool-replicas 2 \\\n    --pull-secret ${HOME}/pull_secret.json \\\n    --release-image quay.io/openshift-release-dev/ocp-release:4.12.0-ec.3-x86_64 \\\n    --external-dns-domain=service-provider-domain.com \\\n    --endpoint-access=PublicAndPrivate\n</code></pre>  Let's remark some things from this command: <pre><code>- external-dns-domain: Points to our public externalDNS hosted zone service-provider-domain.com, typically in an AWS account owned by the service provider.\n- base-domain: Points to the public hosted zone service-consumer-domain.com, typically in an AWS account owned by the service consumer. \n- endpoint-access: Is set as PublicAndPrivate. ExternalDNS feature only could be used with Public and PublicAndPrivate configurations.\n</code></pre>"},{"location":"how-to/aws/global-pull-secret/","title":"Global Pull Secret for Hosted Control Planes","text":""},{"location":"how-to/aws/global-pull-secret/#overview","title":"Overview","text":"<p>The Global Pull Secret functionality enables Hosted Cluster administrators to include additional pull secrets for accessing container images from private registries without requiring assistance from the Management Cluster administrator. This feature allows you to merge your custom pull secret with the original HostedCluster pull secret, making it available to all nodes in the cluster.</p> <p>The implementation uses a DaemonSet approach that automatically detects when you create an <code>additional-pull-secret</code> in the <code>kube-system</code> namespace of your DataPlane (Hosted Cluster). The system then merges this secret with the original pull secret and deploys the merged result to all nodes via a DaemonSet that updates the kubelet configuration.</p> <p>Note</p> <p>This feature is designed to work autonomously - once you create the additional pull secret, the system automatically handles the rest without requiring Management Cluster administrator intervention.</p>"},{"location":"how-to/aws/global-pull-secret/#adding-your-pull-secret","title":"Adding your Pull Secret","text":"<p>Important</p> <p>All actions described in this section must be performed on the HostedCluster's workers (DataPlane), not on the Management Cluster.</p> <p>To use this functionality, follow these steps:</p>"},{"location":"how-to/aws/global-pull-secret/#1-create-your-additional-pull-secret","title":"1. Create your additional pull secret","text":"<p>Create a secret named <code>additional-pull-secret</code> in the <code>kube-system</code> namespace of your Hosted Cluster (DataPlane). The secret must contain a valid DockerConfigJSON format:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: additional-pull-secret\n  namespace: kube-system\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: &lt;base64-encoded-docker-config-json&gt;\n</code></pre>"},{"location":"how-to/aws/global-pull-secret/#2-example-dockerconfigjson-format","title":"2. Example DockerConfigJSON format","text":"<p>Your <code>.dockerconfigjson</code> should follow this structure:</p> <pre><code>{\n  \"auths\": {\n    \"registry.example.com\": {\n      \"auth\": \"base64-encoded-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"base64-encoded-credentials\"\n    }\n  }\n}\n</code></pre> <p>Using Namespace-Specific Registry Entries</p> <p>For registries like Quay.io that support organization/namespace-specific authentication, you can specify the full path in your registry entry (e.g., <code>quay.io/mycompany</code> instead of just <code>quay.io</code>). This allows you to provide different credentials for different namespaces within the same registry, and helps avoid conflicts with existing registry entries in the original pull secret.</p>"},{"location":"how-to/aws/global-pull-secret/#3-apply-the-secret","title":"3. Apply the secret","text":"<pre><code>kubectl apply -f additional-pull-secret.yaml\n</code></pre>"},{"location":"how-to/aws/global-pull-secret/#4-verification","title":"4. Verification","text":"<p>After creating the secret, the system will automatically:</p> <ol> <li>Validate the secret format</li> <li>Merge it with the original pull secret</li> <li>Deploy a DaemonSet to all nodes</li> <li>Update the kubelet configuration on each node</li> </ol> <p>You can verify the deployment by checking:</p> <pre><code># Check if the DaemonSet is running\nkubectl get daemonset global-pull-secret-syncer -n kube-system\n\n# Check the merged pull secret\nkubectl get secret global-pull-secret -n kube-system\n\n# Check DaemonSet pods\nkubectl get pods -n kube-system -l name=global-pull-secret-syncer\n</code></pre>"},{"location":"how-to/aws/global-pull-secret/#how-it-works","title":"How it works","text":"<p>The Global Pull Secret functionality operates through a multi-component system:</p>"},{"location":"how-to/aws/global-pull-secret/#automatic-detection","title":"Automatic Detection","text":"<ul> <li>The Hosted Cluster Config Operator (HCCO) continuously monitors the <code>kube-system</code> namespace</li> <li>When it detects the creation of <code>additional-pull-secret</code>, it triggers the reconciliation process</li> </ul>"},{"location":"how-to/aws/global-pull-secret/#validation-and-merging","title":"Validation and Merging","text":"<ul> <li>The system validates that your secret contains a proper DockerConfigJSON format</li> <li>It retrieves the original pull secret from the HostedControlPlane</li> <li>Your additional pull secret is merged with the original one</li> <li>If there are conflicting registry entries, the original pull secret takes precedence (the additional pull secret entry is ignored for conflicting registries)</li> <li>The system supports namespace-specific registry entries (e.g., <code>quay.io/namespace</code>) for better credential specificity</li> </ul>"},{"location":"how-to/aws/global-pull-secret/#deployment-process","title":"Deployment Process","text":"<ul> <li>A <code>global-pull-secret</code> is created in the <code>kube-system</code> namespace containing the merged result</li> <li>RBAC resources (ServiceAccount, Role, RoleBinding) are created for the DaemonSet in both <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>We use Role and RoleBinding in both namespaces to access secrets in <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>A DaemonSet named <code>global-pull-secret-syncer</code> is deployed to eligible nodes</li> </ul> <p>NodePool InPlace Strategy Restriction</p> <p>The Global Pull Secret DaemonSet is not deployed to nodes that belong to NodePools using the InPlace upgrade strategy. This restriction prevents conflicts between the DaemonSet's modifications to <code>/var/lib/kubelet/config.json</code> and the Machine Config Daemon (MCD) during InPlace upgrades.</p> <ul> <li>Nodes with Replace strategy: \u2705 Receive Global Pull Secret DaemonSet</li> <li>Nodes with InPlace strategy: \u274c Do not receive Global Pull Secret DaemonSet</li> </ul> <p>This ensures that MCD operations during InPlace upgrades do not fail due to unexpected changes in kubelet configuration files.</p>"},{"location":"how-to/aws/global-pull-secret/#node-level-synchronization","title":"Node-Level Synchronization","text":"<ul> <li>Each DaemonSet pod runs a controller that watches the secrets under kube-system namespace</li> <li>When changes are detected, it updates <code>/var/lib/kubelet/config.json</code> on the node</li> <li>The kubelet service is restarted via DBus to apply the new configuration</li> <li>If the restart fails after 3 attempts, the system rolls back the file changes</li> </ul>"},{"location":"how-to/aws/global-pull-secret/#automatic-cleanup","title":"Automatic Cleanup","text":"<ul> <li>If you delete the <code>additional-pull-secret</code>, the HCCO automatically removes the <code>global-pull-secret</code> secret</li> <li>The system reverts to using only the original pull secret from the HostedControlPlane</li> <li>The DaemonSet continues running but now syncs only the original pull secret to nodes</li> </ul>"},{"location":"how-to/aws/global-pull-secret/#registry-precedence-and-conflict-resolution","title":"Registry Precedence and Conflict Resolution","text":"<p>The Global Pull Secret system uses a specific precedence model when merging your additional pull secret with the original one:</p>"},{"location":"how-to/aws/global-pull-secret/#merge-behavior","title":"Merge Behavior","text":"<ul> <li>Original pull secret entries always take precedence over additional pull secret entries for the same registry</li> <li>If both secrets contain an entry for <code>quay.io</code>, the original pull secret's credentials will be used</li> <li>Your additional pull secret entries are only added if they don't conflict with existing entries</li> <li>Warnings are logged when conflicts are detected</li> </ul>"},{"location":"how-to/aws/global-pull-secret/#recommended-approach","title":"Recommended Approach","text":"<p>To avoid conflicts and ensure your credentials are used, consider these strategies:</p> <ol> <li>Use namespace-specific entries: Instead of <code>quay.io</code>, use <code>quay.io/your-namespace</code></li> <li>Target specific registries: Add entries only for registries not already in the original pull secret</li> <li>Check existing entries: Review what registries are already configured in the HostedControlPlane</li> </ol>"},{"location":"how-to/aws/global-pull-secret/#example-merge-scenario","title":"Example Merge Scenario","text":"<p>Original Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"original-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Your Additional Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"your-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"your-namespace-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Resulting Merged Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"original-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"your-namespace-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Note how the <code>quay.io</code> entry keeps the original credentials, but <code>quay.io/mycompany</code> is added from your additional secret.</p>"},{"location":"how-to/aws/global-pull-secret/#implementation-details","title":"Implementation details","text":"<p>The implementation consists of several key components working together:</p>"},{"location":"how-to/aws/global-pull-secret/#core-components","title":"Core Components","text":"<ol> <li>Global Pull Secret Controller (<code>globalps</code> package)</li> <li>Handles validation of user-provided pull secrets</li> <li>Manages the merging logic between original and additional pull secrets</li> <li>Creates and manages RBAC resources</li> <li>Deploys and manages the DaemonSet</li> <li> <p>Node eligibility assessment: Labels nodes from InPlace NodePools and configures DaemonSet scheduling restrictions</p> </li> <li> <p>Sync Global Pull Secret Command (<code>sync-global-pullsecret</code> package)</p> </li> <li>Runs as a DaemonSet on each node</li> <li>Watches for changes to the <code>global-pull-secret</code> in <code>kube-system</code> namespace</li> <li>Accesses the original <code>pull-secret</code> in <code>openshift-config</code> namespace</li> <li>Updates the kubelet configuration file</li> <li> <p>Manages kubelet service restarts via DBus</p> </li> <li> <p>Hosted Cluster Config Operator Integration</p> </li> <li>Monitors for the presence of <code>additional-pull-secret</code></li> <li>Orchestrates the entire process</li> <li>Handles cleanup when the secret is removed</li> </ol>"},{"location":"how-to/aws/global-pull-secret/#architecture-diagram","title":"Architecture Diagram","text":"graph TB     %% User Input     User[User creates additional-pull-secret] --&gt; |kube-system namespace| AdditionalPS[additional-pull-secret Secret]      %% HCCO Controller     HCCO[Hosted Cluster Config Operator] --&gt; |Watches kube-system secrets| GlobalPSController[Global Pull Secret Controller]     GlobalPSController --&gt; |Validates| AdditionalPS     GlobalPSController --&gt; |Gets original| OriginalPS[Original pull-secret from HCP]      %% Secret Processing     AdditionalPS --&gt; |Validates format| ValidatePS[Validate Additional Pull Secret]     OriginalPS --&gt; |Extracts data| OriginalPSData[Original Pull Secret Data]     ValidatePS --&gt; |Extracts data| AdditionalPSData[Additional Pull Secret Data]      %% Merge Process     OriginalPSData --&gt; MergeSecrets[Merge Pull Secrets]     AdditionalPSData --&gt; MergeSecrets     MergeSecrets --&gt; |Creates merged JSON| GlobalPSData[Global Pull Secret Data]      %% Secret Creation     GlobalPSData --&gt; |Creates in kube-system| GlobalPSSecret[global-pull-secret Secret]      %% RBAC Setup     GlobalPSController --&gt; |Creates RBAC| RBACSetup[Setup RBAC Resources]     RBACSetup --&gt; ServiceAccount[global-pull-secret-syncer ServiceAccount]     RBACSetup --&gt; KubeSystemRole[global-pull-secret-syncer Role in kube-system]     RBACSetup --&gt; KubeSystemRoleBinding[global-pull-secret-syncer RoleBinding in kube-system]     RBACSetup --&gt; OpenshiftConfigRole[global-pull-secret-syncer Role in openshift-config]     RBACSetup --&gt; OpenshiftConfigRoleBinding[global-pull-secret-syncer RoleBinding in openshift-config]      %% DaemonSet Deployment     GlobalPSController --&gt; |Deploys DaemonSet| DaemonSet[global-pull-secret-syncer DaemonSet]     DaemonSet --&gt; |Runs on each node| DaemonSetPod[DaemonSet Pod]      %% DaemonSet Pod Details     DaemonSetPod --&gt; |Mounts host paths| HostMounts[Host Path Mounts]     HostMounts --&gt; KubeletPath[\"/var/lib/kubelet\"]     HostMounts --&gt; DbusPath[\"/var/run/dbus\"]      %% Container Execution     DaemonSetPod --&gt; |Runs command| Container[control-plane-operator Container]     Container --&gt; |Executes| SyncCommand[sync-global-pullsecret command]      %% Sync Process     SyncCommand --&gt; |Watches global-pull-secret| SyncController[Global Pull Secret Reconciler]     SyncController --&gt; |Reads secret| ReadGlobalPS[Read global-pull-secret]     SyncController --&gt; |Reads original| ReadOriginalPS[Read original pull-secret]      %% File Update Process     ReadGlobalPS --&gt; |Gets data| GlobalPSBytes[Global Pull Secret Bytes]     ReadOriginalPS --&gt; |Gets data| OriginalPSBytes[Original Pull Secret Bytes]      %% Decision Logic     GlobalPSBytes --&gt; |If exists| UseGlobalPS[Use Global Pull Secret]     OriginalPSBytes --&gt; |If not exists| UseOriginalPS[Use Original Pull Secret]      %% File Update     UseGlobalPS --&gt; |Updates file| UpdateKubeletConfig[\"Update /var/lib/kubelet/config.json\"]     UseOriginalPS --&gt; |Updates file| UpdateKubeletConfig      %% Kubelet Restart     UpdateKubeletConfig --&gt; |Restarts kubelet| RestartKubelet[Restart kubelet.service via systemd]     RestartKubelet --&gt; |Via dbus| DbusConnection[DBus Connection]      %% Error Handling     UpdateKubeletConfig --&gt; |If restart fails| RollbackProcess[Rollback Process]     RollbackProcess --&gt; |Restore original| RestoreOriginal[Restore Original File Content]      %% Cleanup Process     GlobalPSController --&gt; |If additional PS deleted| CleanupProcess[Cleanup Process]     CleanupProcess --&gt; |Deletes global PS| DeleteGlobalPS[Delete global-pull-secret]     CleanupProcess --&gt; |Removes DaemonSet| RemoveDaemonSet[Remove DaemonSet]      %% Styling     classDef userInput fill:#e1f5fe     classDef controller fill:#f3e5f5     classDef secret fill:#e8f5e8     classDef process fill:#fff3e0     classDef daemonSet fill:#fce4ec     classDef fileSystem fill:#f1f8e9      class User,AdditionalPS userInput     class HCCO,GlobalPSController,SyncController controller     class OriginalPS,GlobalPSSecret,ServiceAccount,KubeSystemRole,KubeSystemRoleBinding,OpenshiftConfigRole,OpenshiftConfigRoleBinding secret     class ValidatePS,MergeSecrets,RBACSetup,UpdateKubeletConfig,RestartKubelet process     class DaemonSet,DaemonSetPod,Container daemonSet     class KubeletPath,DbusPath fileSystem"},{"location":"how-to/aws/global-pull-secret/#key-features","title":"Key Features","text":"<ul> <li>Security: Only watches specific secrets in <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>Robustness: Includes automatic rollback in case of failures</li> <li>Efficiency</li> <li>Only updates when there are actual changes</li> <li>The globalPullSecret implementation has their own controller so it cannot interfere with the HCCO reonciliation</li> <li>Security considerations: Uses specific RBAC for only the required resources in each namespace. The DaemonSet containers run in privileged mode due to the need to:</li> <li>Write to <code>/var/lib/kubelet/config.json</code> (kubelet configuration file)</li> <li>Connect to systemd via DBus for service management</li> <li>Restart kubelet.service, which requires root privileges</li> <li>Smart node targeting: Automatically excludes nodes from InPlace NodePools to prevent MCD conflicts</li> </ul>"},{"location":"how-to/aws/global-pull-secret/#inplace-nodepool-handling","title":"InPlace NodePool Handling","text":"<p>To prevent conflicts with Machine Config Daemon operations, the implementation includes intelligent node targeting:</p>"},{"location":"how-to/aws/global-pull-secret/#node-labeling-process","title":"Node Labeling Process","text":"<ol> <li>MachineSets Discovery: The controller queries the management cluster for MachineSets with InPlace-specific annotations (<code>hypershift.openshift.io/nodePoolTargetConfigVersion</code>)</li> <li>Machine Enumeration: For each InPlace MachineSets, it lists all associated Machines</li> <li>Node Identification: Maps Machine objects to their corresponding nodes via <code>machine.Status.NodeRef.Name</code></li> <li>Labeling: Applies <code>hypershift.openshift.io/nodepool-inplace-strategy=true</code> label to identified nodes</li> </ol>"},{"location":"how-to/aws/global-pull-secret/#daemonset-scheduling-configuration","title":"DaemonSet Scheduling Configuration","text":"<p>The DaemonSet uses NodeAffinity to exclude InPlace nodes:</p> <pre><code>spec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: hypershift.openshift.io/nodepool-inplace-strategy\n                operator: DoesNotExist\n</code></pre> <p>This ensures that: - Nodes without the label: \u2705 Are eligible for DaemonSet scheduling - Nodes with the label (any value): \u274c Are excluded from DaemonSet scheduling</p>"},{"location":"how-to/aws/global-pull-secret/#conflict-prevention-benefits","title":"Conflict Prevention Benefits","text":"<ul> <li>Prevents MCD failures: Avoids conflicts when MCD expects specific kubelet configuration during InPlace upgrades</li> <li>Maintains upgrade reliability: InPlace upgrade processes are not interrupted by Global Pull Secret modifications</li> <li>Automatic detection: No manual intervention required - the system automatically identifies and handles InPlace nodes</li> </ul>"},{"location":"how-to/aws/global-pull-secret/#error-handling","title":"Error Handling","text":"<p>The system includes comprehensive error handling:</p> <ul> <li>Validation errors: Invalid DockerConfigJSON format is caught early</li> <li>Restart failures: If kubelet restart fails after 3 attempts, the file is rolled back</li> <li>Resource cleanup: If the additional pull secret is deleted, the HCCO automatically removes the globalPullSecret</li> </ul> <p>This implementation provides a secure, autonomous solution that allows HostedCluster administrators to add private registry credentials without requiring Management Cluster administrator intervention.</p>"},{"location":"how-to/aws/other-sdn-providers/","title":"Other sdn providers","text":"<p>This document explains how to create a HostedCluster that runs an SDN provider different from OVNKubernetes. The document assumes that you already have the required infrastructure in place to create HostedClusters.</p> <p>Important</p> <p>The work described here is not supported. SDN providers must certify their software on HyperShift before it becomes a supported solution. The steps described here are just a technical reference for people who wants to try different SDN providers in HyperShift.</p> <p>Versions used while writing this doc:</p> <ul> <li>Management cluster running OpenShift <code>v4.14.5</code> and HyperShift Operator version <code>e87182ca75da37c74b371aa0f17aeaa41437561a</code>.</li> <li>HostedCluster release set to OpenShift <code>v4.14.10</code>.</li> </ul> <p>Important</p> <p>To configure a different CNI provider for the Hosted Cluster, you must adjust the <code>hostedcluster.spec.networking.networkType</code> to <code>Other</code>. By doing so, the Control Plane Operator will skip the deployment of the default CNI provider.</p>"},{"location":"how-to/aws/other-sdn-providers/#calico","title":"Calico","text":""},{"location":"how-to/aws/other-sdn-providers/#deployment","title":"Deployment","text":"<p>In this scenario we are using the Calico version v3.27.0 which is the last one at the time of this writing. The steps followed rely on the docs by Tigera to deploy Calico on OpenShift.</p> <ol> <li> <p>Create a <code>HostedCluster</code> and set its <code>HostedCluster.spec.networking.networkType</code> to <code>Other</code>.</p> </li> <li> <p>Wait for the HostedCluster's API to be ready. Once it's ready, get the admin kubeconfig.</p> </li> <li> <p>Eventually the compute nodes will show up in the cluster. Keep in mind since the SDN is not deployed yet, they will remain in <code>NotReady</code> state.</p> <pre><code>export KUBECONFIG=/path/to/hostedcluster/admin/kubeconfig\noc get nodes\n</code></pre> <pre><code>NAME             STATUS     ROLES    AGE     VERSION\nhosted-worker1   NotReady   worker   2m51s   v1.27.8+4fab27b\nhosted-worker2   NotReady   worker   2m52s   v1.27.8+4fab27b\n</code></pre> </li> <li> <p>Apply the yaml manifests provided by <code>Tigera</code> in the HostedCluster:</p> <pre><code>mkdir calico\nwget -qO- https://github.com/projectcalico/calico/releases/download/v3.27.0/ocp.tgz | tar xvz --strip-components=1 -C calico\ncd calico/\nls *crd*.yaml | xargs -n1 oc apply -f\nls 00* | xargs -n1 oc apply -f\nls 01* | xargs -n1 oc apply -f\nls 02* | xargs -n1 oc apply -f\n</code></pre> <pre><code>customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/apiservers.operator.tigera.io created\ncustomresourcedefinition.apiextensions.k8s.io/imagesets.operator.tigera.io created\ncustomresourcedefinition.apiextensions.k8s.io/installations.operator.tigera.io created\ncustomresourcedefinition.apiextensions.k8s.io/tigerastatuses.operator.tigera.io created\nnamespace/calico-apiserver created\nnamespace/calico-system created\nnamespace/tigera-operator created\napiserver.operator.tigera.io/default created\ninstallation.operator.tigera.io/default created\nconfigmap/calico-resources created\nclusterrolebinding.rbac.authorization.k8s.io/tigera-operator created\nclusterrole.rbac.authorization.k8s.io/tigera-operator created\nserviceaccount/tigera-operator created\ndeployment.apps/tigera-operator created\n</code></pre> </li> </ol>"},{"location":"how-to/aws/other-sdn-providers/#checks","title":"Checks","text":"<p>We should see the following pods running in the <code>tigera-operator</code> namespace:</p> <pre><code>oc -n tigera-operator get pods\n</code></pre> <pre><code>NAME                              READY   STATUS    RESTARTS   AGE\ntigera-operator-dc7c9647f-fvcvd   1/1     Running   0          2m1s\n</code></pre> <p>We should see the following pods running in the <code>calico-system</code> namespace:</p> <pre><code>oc -n calico-system get pods\n</code></pre> <pre><code>NAME                                       READY   STATUS    RESTARTS   AGE\ncalico-kube-controllers-69d6d5ff89-5ftcn   1/1     Running   0          2m1s\ncalico-node-6bzth                          1/1     Running   0          2m2s\ncalico-node-bl4b6                          1/1     Running   0          2m2s\ncalico-typha-6558c4c89d-mq2hw              1/1     Running   0          2m2s\ncsi-node-driver-l948w                      2/2     Running   0          2m1s\ncsi-node-driver-r6rgw                      2/2     Running   0          2m2s\n</code></pre> <p>We should see the following pods running in the <code>calico-apiserver</code> namespace:</p> <pre><code>oc -n calico-apiserver get pods\n</code></pre> <pre><code>NAME                                READY   STATUS    RESTARTS   AGE\ncalico-apiserver-7bfbf8fd7c-d75fw   1/1     Running   0          84s\ncalico-apiserver-7bfbf8fd7c-fqxjm   1/1     Running   0          84s\n</code></pre> <p>The nodes should've moved to <code>Ready</code> state:</p> <pre><code>oc get nodes\n</code></pre> <pre><code>NAME             STATUS   ROLES    AGE   VERSION\nhosted-worker1   Ready    worker   10m   v1.27.8+4fab27b\nhosted-worker2   Ready    worker   10m   v1.27.8+4fab27b\n</code></pre> <p>The HostedCluster deployment will continue, at this point the SDN is running.</p>"},{"location":"how-to/aws/other-sdn-providers/#cilium","title":"Cilium","text":""},{"location":"how-to/aws/other-sdn-providers/#deployment_1","title":"Deployment","text":"<p>In this scenario we are using the Cilium version v1.14.5 which is the last one at the time of this writing. The steps followed rely on the docs by Cilium project to deploy Cilium on OpenShift.</p> <ol> <li> <p>Create a <code>HostedCluster</code> and set its <code>HostedCluster.spec.networking.networkType</code> to <code>Other</code>.</p> </li> <li> <p>Wait for the HostedCluster's API to be ready. Once it's ready, get the admin kubeconfig.</p> </li> <li> <p>Eventually the compute nodes will show up in the cluster. Keep in mind since the SDN is not deployed yet, they will remain in <code>NotReady</code> state.</p> <pre><code>export KUBECONFIG=/path/to/hostedcluster/admin/kubeconfig\noc get nodes\n</code></pre> <pre><code>NAME             STATUS     ROLES    AGE     VERSION\nhosted-worker1   NotReady   worker   2m30s   v1.27.8+4fab27b\nhosted-worker2   NotReady   worker   2m33s   v1.27.8+4fab27b\n</code></pre> </li> <li> <p>Apply the yaml manifests provided by <code>Isovalent</code> in the HostedCluster:</p> <pre><code>#!/bin/bash\n\nversion=\"1.14.5\"\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-03-cilium-ciliumconfigs-crd.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00000-cilium-namespace.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00001-cilium-olm-serviceaccount.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00002-cilium-olm-deployment.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00003-cilium-olm-service.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00004-cilium-olm-leader-election-role.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00005-cilium-olm-role.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00006-leader-election-rolebinding.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00007-cilium-olm-rolebinding.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00008-cilium-cilium-olm-clusterrole.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00009-cilium-cilium-clusterrole.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00010-cilium-cilium-olm-clusterrolebinding.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00011-cilium-cilium-clusterrolebinding.yaml\n</code></pre> </li> <li> <p>Use the right configuration for each network stack</p> </li> </ol> IPv4IPv6Dual stack <pre><code>apiVersion: cilium.io/v1alpha1\nkind: CiliumConfig\nmetadata:\n  name: cilium\n  namespace: cilium\nspec:\n  debug:\n    enabled: true\n  k8s:\n    requireIPv4PodCIDR: true\n  logSystemLoad: true\n  bpf:\n    preallocateMaps: true\n  etcd:\n    leaseTTL: 30s\n  ipv4:\n    enabled: true\n  ipv6:\n    enabled: false\n  identityChangeGracePeriod: 0s\n  ipam:\n    mode: \"cluster-pool\"\n    operator:\n      clusterPoolIPv4PodCIDRList:\n        - \"10.128.0.0/14\"\n      clusterPoolIPv4MaskSize: \"23\"\n  nativeRoutingCIDR: \"10.128.0.0/14\"\n  endpointRoutes: {enabled: true}\n  clusterHealthPort: 9940\n  tunnelPort: 4789\n  cni:\n    binPath: \"/var/lib/cni/bin\"\n    confPath: \"/var/run/multus/cni/net.d\"\n    chainingMode: portmap\n  prometheus:\n    serviceMonitor: {enabled: false}\n  hubble:\n    tls: {enabled: false}\n  sessionAffinity: true\n</code></pre> <pre><code>oc apply -f ciliumconfig.yaml\n</code></pre> <pre><code>apiVersion: cilium.io/v1alpha1\nkind: CiliumConfig\nmetadata:\n  name: cilium\n  namespace: cilium\nspec:\n  debug:\n    enabled: true\n  k8s:\n    requireIPv6PodCIDR: true\n  logSystemLoad: true\n  bpf:\n    preallocateMaps: true\n  etcd:\n    leaseTTL: 30s\n  ipv4:\n    enabled: false\n  ipv6:\n    enabled: true\n  identityChangeGracePeriod: 0s\n  ipam:\n    mode: \"cluster-pool\"\n    operator:\n      clusterPoolIPv6PodCIDRList:\n        - \"fd01::/48\"\n      clusterPoolIPv6MaskSize: \"48\"\n  nativeRoutingCIDR: \"fd01::/48\"\n  endpointRoutes: {enabled: true}\n  clusterHealthPort: 9940\n  tunnelPort: 4789\n  cni:\n    binPath: \"/var/lib/cni/bin\"\n    confPath: \"/var/run/multus/cni/net.d\"\n    chainingMode: portmap\n  prometheus:\n    serviceMonitor: {enabled: false}\n  hubble:\n    tls: {enabled: false}\n  sessionAffinity: true\n</code></pre> <pre><code>oc apply -f ciliumconfig.yaml\n</code></pre> <pre><code>apiVersion: cilium.io/v1alpha1\nkind: CiliumConfig\nmetadata:\n  name: cilium\n  namespace: cilium\nspec:\n  debug:\n    enabled: true\n  k8s:\n    requireIPv4PodCIDR: true\n  logSystemLoad: true\n  bpf:\n    preallocateMaps: true\n  etcd:\n    leaseTTL: 30s\n  ipv4:\n    enabled: true\n  ipv6:\n    enabled: true\n  identityChangeGracePeriod: 0s\n  ipam:\n    mode: \"cluster-pool\"\n    operator:\n      clusterPoolIPv4PodCIDRList:\n        - \"10.128.0.0/14\"\n      clusterPoolIPv4MaskSize: \"23\"\n  nativeRoutingCIDR: \"10.128.0.0/14\"\n  endpointRoutes: {enabled: true}\n  clusterHealthPort: 9940\n  tunnelPort: 4789\n  cni:\n    binPath: \"/var/lib/cni/bin\"\n    confPath: \"/var/run/multus/cni/net.d\"\n    chainingMode: portmap\n  prometheus:\n    serviceMonitor: {enabled: false}\n  hubble:\n    tls: {enabled: false}\n  sessionAffinity: true\n</code></pre> <pre><code>oc apply -f ciliumconfig.yaml\n</code></pre> <p>Important</p> <p>Make sure you've changed the networking values according to your platform details <code>spec.ipam.operator.clusterPoolIPv4PodCIDRList</code>, <code>spec.ipam.operator.clusterPoolIPv4MaskSize</code> and <code>nativeRoutingCIDR</code> in IPv4 and <code>spec.ipam.operator.clusterPoolIPv6PodCIDRList</code>, <code>spec.ipam.operator.clusterPoolIPv6MaskSize</code> and <code>nativeRoutingCIDR</code> in IPv6 case.</p>"},{"location":"how-to/aws/other-sdn-providers/#checks_1","title":"Checks","text":"<p>This will be the output:</p> <pre><code>customresourcedefinition.apiextensions.k8s.io/ciliumconfigs.cilium.io created\nnamespace/cilium created\nserviceaccount/cilium-olm created\nWarning: would violate PodSecurity \"restricted:v1.24\": host namespaces (hostNetwork=true), hostPort (container \"operator\" uses hostPort 9443), allowPrivilegeEscalation != false (container \"operator\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"operator\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"operator\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"operator\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\ndeployment.apps/cilium-olm created\nservice/cilium-olm created\nrole.rbac.authorization.k8s.io/cilium-olm-leader-election created\nrole.rbac.authorization.k8s.io/cilium-olm created\nrolebinding.rbac.authorization.k8s.io/leader-election created\nrolebinding.rbac.authorization.k8s.io/cilium-olm created\nclusterrole.rbac.authorization.k8s.io/cilium-cilium-olm created\nclusterrole.rbac.authorization.k8s.io/cilium-cilium created\nclusterrolebinding.rbac.authorization.k8s.io/cilium-cilium-olm created\nclusterrolebinding.rbac.authorization.k8s.io/cilium-cilium created\nciliumconfig.cilium.io/cilium created\n</code></pre> <p>We should see the following pods running in the <code>cilium</code> namespace:</p> <pre><code>oc -n cilium get pods\n</code></pre> <pre><code>NAME                               READY   STATUS    RESTARTS   AGE\ncilium-ds5tr                       1/1     Running   0          106s\ncilium-olm-7c9cf7c948-txkvt        1/1     Running   0          2m36s\ncilium-operator-595594bf7d-gbnns   1/1     Running   0          106s\ncilium-operator-595594bf7d-mn5wc   1/1     Running   0          106s\ncilium-wzhdk                       1/1     Running   0          106s\n</code></pre> <p>The nodes should've moved to <code>Ready</code> state:</p> <pre><code>oc get nodes\n</code></pre> <pre><code>NAME             STATUS   ROLES    AGE   VERSION\nhosted-worker1   Ready    worker   8m    v1.27.8+4fab27b\nhosted-worker2   Ready    worker   8m    v1.27.8+4fab27b\n</code></pre> <p>The HostedCluster deployment will continue, at this point the SDN is running.</p>"},{"location":"how-to/aws/other-sdn-providers/#validation","title":"Validation","text":"<p>Additionally you have some conformance tests that could be deployed into the HostedCluster in order to validate if the Cilium SDN was deployed and working properly.</p> <p>In order for Cilium connectivity test pods to run on OpenShift, a simple custom SecurityContextConstraints object is required to allow hostPort/hostNetwork which the connectivity test pods relies on. it should only set allowHostPorts and allowHostNetwork without any other privileges.</p> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: security.openshift.io/v1\nkind: SecurityContextConstraints\nmetadata:\n  name: cilium-test\nallowHostPorts: true\nallowHostNetwork: true\nusers:\n  - system:serviceaccount:cilium-test:default\npriority: null\nreadOnlyRootFilesystem: false\nrunAsUser:\n  type: MustRunAsRange\nseLinuxContext:\n  type: MustRunAs\nvolumes: null\nallowHostDirVolumePlugin: false\nallowHostIPC: false\nallowHostPID: false\nallowPrivilegeEscalation: false\nallowPrivilegedContainer: false\nallowedCapabilities: null\ndefaultAddCapabilities: null\nrequiredDropCapabilities: null\ngroups: null\nEOF\n</code></pre> <pre><code>version=\"1.14.5\"\noc apply -n cilium-test -f https://raw.githubusercontent.com/cilium/cilium/${version}/examples/kubernetes/connectivity-check/connectivity-check.yaml\n</code></pre> <pre><code>oc get pod -n cilium-test\n</code></pre> <pre><code>NAME                                                     READY   STATUS    RESTARTS   AGE\necho-a-846dcb4-kq7zh                                     1/1     Running   0          23h\necho-b-58f67d5b86-5mrtx                                  1/1     Running   0          23h\necho-b-host-84d7468c8d-nf4vk                             1/1     Running   0          23h\nhost-to-b-multi-node-clusterip-b98ff785c-b9vgf           1/1     Running   0          23h\nhost-to-b-multi-node-headless-5c55d85dfc-5xjbc           1/1     Running   0          23h\npod-to-a-6b996b7675-46kkf                                1/1     Running   0          23h\npod-to-a-allowed-cnp-c958b55bf-6vskb                     1/1     Running   0          23h\npod-to-a-denied-cnp-6d9b8cbff5-lbrgp                     1/1     Running   0          23h\npod-to-b-intra-node-nodeport-5f9c4c866f-mhfs4            1/1     Running   0          23h\npod-to-b-multi-node-clusterip-7cb4bf5495-hmmtg           1/1     Running   0          23h\npod-to-b-multi-node-headless-68975fc557-sqbgq            1/1     Running   0          23h\npod-to-b-multi-node-nodeport-559c54c6fc-2rhvv            1/1     Running   0          23h\npod-to-external-1111-5c4cfd9497-6slss                    1/1     Running   0          23h\npod-to-external-fqdn-allow-google-cnp-7d65d9b747-w4cx5   1/1     Running   0          23h\n</code></pre>"},{"location":"how-to/aws/shared-vpc/","title":"Creating a HostedCluster in a Shared VPC","text":""},{"location":"how-to/aws/shared-vpc/#background","title":"Background","text":"<p>AWS allows sharing a VPC's subnets from one account to another. This enables a use case where network management can be done in a single AWS account, while workloads such as OpenShift clusters exist in a separate, satellite account.</p> <p>Creating a standalone OCP cluster is already supported in ROSA classic.</p> <p>In hosted clusters, the shared VPC architecture is very similar to standalone OCP. However, hosted clusters require some new resources to be created.</p> <p>The following resources must be created in the VPC owner account:</p> <ul> <li>The VPC (along with related resources such as subnets, nat gateway, internet gateway, route tables, etc.)</li> <li>Private Hosted Zone - this will be the same domain as the public hosted zone of the HostedCluster where the cluster ingress operator will place router records.</li> <li>Local Hosted Zone - the <code>[cluster-name].hypershift.local</code> hosted zone where the control plane operator places records that point to the VPC endpoint that sends traffic to the control plane.</li> <li>Ingress Shared VPC Role - a role that allows managing records in the private hosted zone. The ingress operator of the hosted cluster will assume this role in order to create records in the private hosted zone.</li> <li>Control Plane Shared VPC Role - a role that allows:</li> <li>Managing a VPC endpoint so that workers can communicate with the control plane</li> <li>Managing a Security Group for the VPC endpoint</li> <li>Managing records in the local hosted zone so that workers can use known names to communicate with the VPC endpoint.</li> </ul> <p>Once created, both public and private subnets of the VPC must be shared with the account where the cluster will be created. Once shared, the subnets in the cluster creator account must be tagged with either <code>kubernetes.io/role/elb=1</code> (for public subnets) or <code>kubernetes.io/role/internal-elb=1</code> (for private subnets). It is important to note that any tags that were set on the subnets in the VPC owner account do not transfer to the cluster creator account when shared.</p> --- config:   layout: dagre   theme: mc   look: handDrawn --- flowchart LR  subgraph vpc1[\"VPC\"]         vpc1_subnets[\"vpc1_subnets\"]         igw[\"Internet Gateway\"]         nat[\"NAT Gateway\"]         ep[\"VPC Endpoint\"]   end  subgraph vpc1_subnets[\"Subnets\"]         subnet1_1[\"Subnet A\"]         subnet1_2[\"Subnet B\"]         subnet1_3[\"Subnet C\"]   end  subgraph account1[\"VPC Owner AWS Account\"]         vpc1         account1_hz[\"account1_hz\"]         account1_iam[\"account1_iam\"]   end  subgraph account1_hz[\"Route53\"]         hz1[\"Private Hosted Zone\"]         hz2[\"HyperShift Local Hosted Zone\"]   end  subgraph account1_iam[\"IAM\"]         role1[\"Ingress Shared VPC Role\"]         role2[\"Control Plane Shared VPC Role\"]   end  subgraph vpc2[\"VPC\"]         vpc2_subnets[\"vpc2_subnets\"]   end  subgraph vpc2_subnets[\"Subnets\"]         subnet2_1[\"subnet2_1\"]         subnet2_2[\"subnet2_2\"]         subnet2_3[\"subnet2_3\"]   end  subgraph subnet2_1[\"Subnet A\"]         instance1[\"Instance 1\"]         instance2[\"Instance 2\"]   end  subgraph subnet2_2[\"Subnet B\"]         instance3[\"Instance 3\"]         instance4[\"Instance 4\"]   end  subgraph subnet2_3[\"Subnet C\"]         instance5[\"Instance 5\"]   end  subgraph account2[\"Cluster Creator AWS Account\"]         vpc2   end     hz1 --- vpc1     hz2 --- vpc1     role1 -.-&gt; hz1     role2 -.-&gt; hz2 &amp; ep     vpc1_subnets -.-&gt; vpc2_subnets     style vpc2_subnets stroke:#777     style subnet2_1 stroke:#777,fill:#eee,stroke-dasharray: 5 5     style subnet2_2 stroke:#777,fill:#eee,stroke-dasharray: 5 5     style subnet2_3 stroke:#777,fill:#eee,stroke-dasharray: 5 5     style vpc2 stroke:#777"},{"location":"how-to/aws/shared-vpc/#api-fields-to-support-a-shared-vpc-cluster","title":"API fields to support a Shared VPC cluster","text":"<p>The HostedCluster API exposes a <code>sharedVPC</code> field under <code>.spec.platform.aws</code> that allows specifying the following fields: - IngressRoleARN - the ARN of a role in the VPC owner account that allows managing DNS records in the private hosted zone of the HostedCluster. The ingress operator will assume this role to manage DNS records. - ControlPlaneRoleARN - the ARN of a role in the VPC owner account that allows creating a VPC endpoint, a corresponding security group and DNS records in the <code>[cluster-name].hypershift.local</code> hosted zone for this HostedCluster. - LocalZoneID - the ID of the route53 hosted zone for <code>[cluster-name].hypershift.local</code> that is associated with the HostedCluster's VPC and exists in the VPC owner account.</p> <p>The <code>sharedVPC</code> field is optional. However, if it is set, all of the above fields are required and the HostedCluster is assumed to be using a shared VPC for its infrastructure.</p>"},{"location":"how-to/aws/shared-vpc/#using-the-cli-to-create-a-hosted-cluster-in-a-shared-vpc","title":"Using the CLI to create a Hosted Cluster in a Shared VPC","text":"<p>The <code>hypershift create cluster aws</code> command exposes a flag <code>--vpc-owner-aws-creds</code> that allows specifying a file that contains credentials to the VPC owner account. With this credential file, the <code>hypershift create cluster aws</code> command will create the VPC and all required artifacts in the VPC owner account, share the subnets, and create a hosted cluster in the cluster creator account (the one for which AWS creds are normally specified).</p> <p>Example command:</p> <pre><code>hypershift create cluster aws \\\n  --aws-creds ${AWS_CREDS_FILE} \\\n  --vpc-owner-aws-creds ${VPC_OWNER_AWS_CREDS_FILE} \\\n  --region us-east-2 \\\n  --auto-repair \\\n  --generate-ssh \\\n  --name ${NAME} \\\n  --endpoint-access PublicAndPrivate \\\n  --node-pool-replicas 2 \\\n  --pull-secret ${PULL_SECRET_FILE} \\\n  --base-domain ${BASE_DOMAIN} \\\n  --external-dns-domain ${CUSTOM_DOMAIN} \\\n  --release-image ${RELEASE_IMAGE}\n</code></pre> <p>To destroy the HostedCluster, the <code>--vpc-owner-aws-creds</code> must also be specified so that resources in the VPC owner account can also be destroyed.</p> <p>The <code>--vpc-owner-aws-creds</code> flag has been added to the following commands:</p> <ul> <li><code>hypershift create cluster aws</code></li> <li><code>hypershift create infra aws</code></li> <li><code>hypershift create iam aws</code></li> <li><code>hypershift destroy cluster aws</code></li> <li><code>hypershift destroy infra aws</code></li> <li><code>hypershift destroy iam aws</code></li> </ul>"},{"location":"how-to/aws/troubleshooting/","title":"Troubleshooting HyperShift on AWS","text":"<p>This section of the HyperShift documentation contains pages related to troubleshooting specific issues when using the AWS cloud provider.</p> <ul> <li>Debug Missing Nodes</li> <li>Debug Disaster Recovery Issues</li> </ul>"},{"location":"how-to/aws/troubleshooting/debug-nodes/","title":"Debug why nodes have not joined the cluster","text":"<p>If your control plane API endpoint has become available, but nodes are not joining the hosted cluster, you can check the following:</p> <ol> <li> <p>Check that your machines have been created in the control plane namespace:    <pre><code>HC_NAMESPACE=\"clusters\"\nHC_NAME=\"cluster-name\"\nCONTROL_PLANE_NAMESPACE=\"${HC_NAMESPACE}-${HC_NAME}\"\noc get machines.cluster.x-k8s.io -n $CONTROL_PLANE_NAMESPACE\noc get awsmachines -n $CONTROL_PLANE_NAMESPACE\n</code></pre></p> <p>If machines don't exist, check that a machinedeployment and machineset have been created: <pre><code>oc get machinedeployment -n $CONTROL_PLANE_NAMESPACE\noc get machineset -n $CONTROL_PLANE_NAMESPACE\n</code></pre> In the case that no machinedeployment has been created look at the logs of the hypershift  operator: <pre><code>oc logs deployment/operator -n hypershift\n</code></pre></p> <p>If the machines exist but have not been provisioned, check the log of the cluster API provider: <pre><code>oc logs deployment/capi-provider -c manager -n $CONTROL_PLANE_NAMESPACE\n</code></pre></p> </li> <li> <p>If machines exist and have been provisioned, check that they have been initialized via ignition.    If you are using AWS, you can look at the system console logs of the machines by using the hypershift    console-logs utility:</p> <pre><code>./bin/hypershift console-logs aws --name $HC_NAME --aws-creds ~/.aws/credentials --output-dir /tmp/console-logs\n</code></pre> <p>The console logs will be placed in the destination directory. When looking at the console logs look for any errors accessing the ignition endpoint via https. If there are, then issue is somehow the ignition endpoint exposed by the control plane is not accessible from the worker instances.</p> </li> <li> <p>If the machines look like they have been provisioned correctly, you can access the systemd journal of    each machine. You can do this by first creating a bastion machine that will allow you to ssh to worker nodes    and running a utility script that will download logs from the machines.</p> <p>Extract the public/private key for the cluster. If you created the cluster with the --generate-ssh flag, a ssh key for your cluster was placed in the same namespace as the hosted cluster (default <code>clusters</code>). If you  specified your own key and know how to access it, you can skip this step. <pre><code>mkdir /tmp/ssh\noc get secret -n clusters ${HC_NAME}-ssh-key -o jsonpath='{ .data.id_rsa }' | base64 -d &gt; /tmp/ssh/id_rsa\noc get secret -n clusters ${HC_NAME}-ssh-key -o jsonpath='{ .data.id_rsa\\.pub }' | base64 -d &gt; /tmp/ssh/id_rsa.pub\n</code></pre></p> <p>Create a bastion machine <pre><code>./bin/hypershift create bastion aws --aws-creds ~/.aws/credentials --name $CLUSTER_NAME --ssh-key-file /tmp/ssh/id_rsa.pub\n</code></pre></p> <p>Run the following script to extract journals from each of your workers: <pre><code>mkdir /tmp/journals\nINFRAID=\"$(oc get hc -n clusters $CLUSTER_NAME -o jsonpath='{ .spec.infraID }')\"\nSSH_PRIVATE_KEY=/tmp/ssh/id_rsa\n./test/e2e/util/dump/copy-machine-journals.sh /tmp/journals\n</code></pre></p> <p>Machine journals should be placed in the <code>/tmp/journals</code> directory in compressed format. Extract them and look for a repeating error near the bottom that should indicate why the kubelet has not been able to join the cluster.</p> </li> </ol>"},{"location":"how-to/aws/troubleshooting/troubleshooting-disaster-recovery/","title":"Debug Disaster Recovery - Hosted Cluster Migration","text":"<p>These are issues related to disaster recovery that we've identified, and you could face during a Hosted Cluster migration.</p>"},{"location":"how-to/aws/troubleshooting/troubleshooting-disaster-recovery/#new-workloads-do-not-get-scheduled-in-the-new-migrated-cluster","title":"New workloads do not get scheduled in the new migrated cluster","text":"<p>Everything looks normal, in the destination Management or Hosted Cluster and in the old Management and Hosted Cluster, but your new workloads do not schedule in your migrated Hosted Cluster (your old ones should work properly).</p> <p>Eventually your pods begin to fall down and the cluster status becomes degraded.</p> <ol> <li> <p>First thing you need to check is the cluster operators and validate all of them work properly: <pre><code>oc get co\n</code></pre></p> </li> <li> <p>If there are some of them degraded and with errors, please check the logs and validate things point to an OVN issue. <pre><code>oc get co &lt;Operator's Name&gt; -o yaml\n</code></pre></p> </li> <li> <p>To solve the issue, we need to ensure the old Hosted Cluster is in pause (we also need this) or deleted.</p> </li> <li> <p>Now we need to delete the OVN pods <pre><code>oc --kubeconfig=${HC_KUBECONFIG} delete pod -n openshift-ovn-kubernetes --all\n</code></pre></p> </li> </ol> <p>Eventually the Hosted Cluster will start self healing and the ClusterOperator will come back.</p> <p>Cause: The cause of this issue is after the Hosted Cluster Migration the KAS (Kube API Server) uses the same DNS name, but it points to different load balancer in AWS platform. Sometimes OVN does not behave correctly facing this situation.</p>"},{"location":"how-to/aws/troubleshooting/troubleshooting-disaster-recovery/#the-migration-gets-blocked-in-etcd-recovery","title":"The migration gets blocked in ETCD recovery","text":"<p>The context around it's basically \"I've edited the Hosted Cluster adding the <code>ETCDSnapshotURL</code> but the modification disappears and does not continue\".</p> <p>The first symptom is the status of the Hypershift Operator pod, in this case the pod is usually in <code>CrashLoopBackOff</code> status.</p> <p>To solve this issue we need to:</p> <ol> <li> <p>Kill the Hypershift Operator pod <pre><code>oc delete pod -n hypershift -lapp=operator\n</code></pre></p> </li> <li> <p>Continue editing the HostedCluster, in order to add the <code>ETCDSnapshotURL</code> to the Hosted Cluster spec.</p> </li> <li> <p>Now the ETCD pod will raise up using the snapshot from S3 bucket.</p> </li> </ol> <p>Cause: This issue happens when the Hypershift operator is down and the Hosted Cluster controller cannot handle the modifications in the objects which belong to it.</p>"},{"location":"how-to/aws/troubleshooting/troubleshooting-disaster-recovery/#the-nodes-cannot-join-the-new-hosted-cluster-and-stay-in-the-older-one","title":"The nodes cannot join the new Hosted Cluster and stay in the older one","text":"<p>We have 2 paths to follow, and it depends on if this code is in your Hypershift Operator.</p>"},{"location":"how-to/aws/troubleshooting/troubleshooting-disaster-recovery/#the-pr-is-merged-and-my-hypershift-operator-has-that-code-running","title":"The PR is merged and my Hypershift Operator has that code running","text":"<p>If that's the case, you need to make sure your Hosted Cluster is paused: <pre><code>oc get hostedcluster -n &lt;HC Namespace&gt; &lt;HC Name&gt; -ojsonpath={.Spec.pausedUntil}\n</code></pre></p> <p>If this command does not give you any output, make sure you've followed properly the \"Disaster Recovery\" procedure, more concretelly pausing the Hosted Cluster and NodePool.</p> <p>Even if it's paused and is still in that situation, please continue to the next section because it's highly probable that you don't have the code which manages this situation properly.</p>"},{"location":"how-to/aws/troubleshooting/troubleshooting-disaster-recovery/#the-pr-is-not-merged-or-my-hypershift-operator-does-not-have-that-code-running","title":"The PR is not merged or my Hypershift Operator does not have that code running","text":"<p>If that's not the case, the only way to solve it is executing the teardown of the old Hosted Cluster prior the full restoration in the new Management cluster. Make sure you already have all the Manifests and the ETCD backed up.</p> <p>Once you followed the Teardown procedure of the old Hosted Cluster, you will see how the migrated Hosted Cluster begins to self-recover.</p> <p>Cause: This issue occurs when the old Hosted Cluster has a conflict with the AWSPrivateLink object. The old one is still running and the new one cannot handle it because the <code>hypershift.local</code> AWS internal DNS entry still points to the old LoadBalancer.</p>"},{"location":"how-to/aws/troubleshooting/troubleshooting-disaster-recovery/#dependent-resources-block-the-old-hosted-cluster-teardown","title":"Dependent resources block the old Hosted Cluster teardown","text":"<p>To solve this issue you need to check all the objects in the HostedControlPlane Namespace and make sure all of them are being terminated. To do that we recommend to use an external tool called ketall which gives you a complete overview of all resources in a kubernetes cluster.</p> <p>You need to know what object is preventing the Hosted Cluster from being deleted and ensure that the finalizer finishes successfully.</p> <p>If you don't care about the stability of the old Management cluster, this script could help you to delete all the components in the HostedControlPlane Namespace (you need the ketall tool):</p> <pre><code>#!/bin/bash\n\n####\n# Execution sample:\n# ./delete_ns.sh $NAMESPACE\n####\n\nNAMESPACE=$1\n\nif [[ -z $1 ]];then\n        echo \"Specify the Namespace!\"\n        exit 1\nfi\n\nfor object in $(ketall -n $NAMESPACE -o name | grep -v packa)\ndo\n    oc -n $NAMESPACE patch $object -p '{\"metadata\":{\"finalizers\":null}}' --type merge\ndone\n</code></pre> <p>Eventually, the namespace will be successfully terminated and also the Hosted Cluster.</p> <p>Cause: This is pretty common issue in the Kubernetes/Openshift world. You are trying to delete a resource that has other dependedent objects. The finalizer is still trying to delete them but it cannot progress.</p>"},{"location":"how-to/aws/troubleshooting/troubleshooting-disaster-recovery/#the-storage-clusteroperator-keeps-reporting-waiting-for-deployment","title":"The Storage ClusterOperator keeps reporting \"Waiting for Deployment\"","text":"<p>To solve this issue you need to check that all the pods from the HostedCluster and the HostedControlPlane are running, not blocked and there are no issues in the <code>cluster-storage-operator</code> pod. After that you need to delete the AWS EBS CSI Drivers from the HCP namespace in the destination management cluster:</p> <ul> <li>Delete the AWS EBS CSI Drivers deployments <pre><code>oc delete aws-ebs-csi-driver-controller aws-ebs-csi-driver-operator\n</code></pre></li> </ul> <p>The operator will take a while to raise up again and eventually the driver controller will be deployed by the <code>aws-ebs-csi-driver-operator</code>.</p> <p>Cause: This issue probably comes from objects that are deployed by the Operator. In this case, <code>cluster-storage-operator</code>, but the controller or the operator does not reconcile over them. If you delete the deployments, you ensure the operator is recreated from scratch.</p>"},{"location":"how-to/aws/troubleshooting/troubleshooting-disaster-recovery/#the-image-registry-clusteroperator-keeps-reporting-a-degraded-status","title":"The image-registry ClusterOperator keeps reporting a degraded status","text":"<p>When a migration is done and the image-registry clusteroperator is marked as degraded, you will need to figure out how it reaches that status. The message will look like <code>ImagePrunerDegraded: Job has reached the specified backoff limit</code>.</p> <p>Things we need to review:</p> <ul> <li>Look for failure pods in the HostedControlPlane namespace at the destination management cluster.</li> <li>Check the other Cluster operators in the HostedCluster.</li> <li>Check if the nodes are ready and working fine in the HostedCluster.</li> </ul> <p>If all three components are working fine, the issue is in the backoff times of the executed job <code>image-pruner-XXXX</code>; this job has most likely failed. Once the migrated cluster has already converged and looks fine, you will need to make sure to fix this cluster operator manually; you will need to determine if you want immediate resolution, or you can wait 24h and the cronjob will raise up another job by itself.</p> <p>To solve it manually, you need to:</p> <ul> <li>Reexecute the job <code>image-pruner-xxxx</code> from the <code>openshift-image-registry</code> namespace, using a cronjob called <code>image-pruner</code> <pre><code>oc create job -n openshift-image-registry --from=cronjob/image-pruner image-pruner-recover\n</code></pre></li> </ul> <p>This command creates a new job in that namespace and eventually will report the new status to the cluster operator.</p>"},{"location":"how-to/azure/azure-workload-identity-setup/","title":"Azure Workload Identity Setup for Self-Managed Clusters","text":"<p>Developer Preview in OCP 4.21</p> <p>Self-managed Azure HostedClusters are available as a Developer Preview feature in OpenShift Container Platform 4.21.</p> <p>This document describes how to set up Azure Workload Identities and OIDC issuer for self-managed Azure HostedClusters.</p> <p>Persistent Resource Groups</p> <p>When setting up workload identities and OIDC issuer for the first time, create them in a persistent resource group that will not be deleted when individual clusters are destroyed. This allows you to reuse the same workload identities and OIDC issuer across multiple HostedClusters, reducing setup time and avoiding unnecessary resource recreation.</p> <ul> <li>Use a persistent resource group like <code>os4-common</code>.</li> <li>This resource group should be separate from the cluster-specific resource groups that get created and deleted with each HostedCluster.</li> <li>The OIDC issuer storage account should also be created in this persistent resource group.</li> </ul>"},{"location":"how-to/azure/azure-workload-identity-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure CLI (<code>az</code>) installed and configured</li> <li><code>jq</code> command-line JSON processor</li> <li>Cloud Credential Operator (CCO) tool installed</li> <li>Appropriate Azure permissions</li> </ul>"},{"location":"how-to/azure/azure-workload-identity-setup/#create-azure-workload-identities","title":"Create Azure Workload Identities","text":"<p>Create managed identities for each OpenShift component that needs Azure access:</p> <pre><code># Set environment variables\nPERSISTENT_RG_NAME=\"os4-common\"  # Use persistent resource group\nLOCATION=\"eastus\"\nCLUSTER_NAME=\"my-self-managed-cluster\"\nSUBSCRIPTION_ID=$(az account show --query id -o tsv)\n\n# Create persistent resource group (if it doesn't exist)\naz group create --name $PERSISTENT_RG_NAME --location $LOCATION\n\n# Create managed identities for each component\ndeclare -A COMPONENTS=(\n    [\"image-registry\"]=\"cluster-image-registry-operator\"\n    [\"ingress\"]=\"cluster-ingress-operator\"  \n    [\"file-csi\"]=\"cluster-storage-operator-file\"\n    [\"disk-csi\"]=\"cluster-storage-operator-disk\"\n    [\"nodepool-mgmt\"]=\"cluster-api-provider-azure\"\n    [\"cloud-provider\"]=\"azure-cloud-provider\"\n    [\"network\"]=\"cluster-network-operator\"\n)\n\n# Create managed identities and capture client IDs\ndeclare -A CLIENT_IDS\nfor component in \"${!COMPONENTS[@]}\"; do\n    echo \"Creating managed identity for $component...\"\n    CLIENT_ID=$(az identity create \\\n        --name \"${CLUSTER_NAME}-${component}\" \\\n        --resource-group $PERSISTENT_RG_NAME \\\n        --query clientId -o tsv)\n    CLIENT_IDS[$component]=$CLIENT_ID\n    echo \"Created identity ${CLUSTER_NAME}-${component} with client ID: $CLIENT_ID\"\ndone\n</code></pre>"},{"location":"how-to/azure/azure-workload-identity-setup/#create-workload-identities-configuration-file","title":"Create Workload Identities Configuration File","text":"<p>Create a JSON file with all the workload identity client IDs:</p> <pre><code>cat &lt;&lt;EOF &gt; workload-identities.json\n{\n  \"imageRegistry\": {\n    \"clientID\": \"${CLIENT_IDS[image-registry]}\"\n  },\n  \"ingress\": {\n    \"clientID\": \"${CLIENT_IDS[ingress]}\"\n  },\n  \"file\": {\n    \"clientID\": \"${CLIENT_IDS[file-csi]}\"\n  },\n  \"disk\": {\n    \"clientID\": \"${CLIENT_IDS[disk-csi]}\"\n  },\n  \"nodePoolManagement\": {\n    \"clientID\": \"${CLIENT_IDS[nodepool-mgmt]}\"\n  },\n  \"cloudProvider\": {\n    \"clientID\": \"${CLIENT_IDS[cloud-provider]}\"\n  },\n  \"network\": {\n    \"clientID\": \"${CLIENT_IDS[network]}\"\n  }\n}\nEOF\n</code></pre>"},{"location":"how-to/azure/azure-workload-identity-setup/#configure-oidc-issuer","title":"Configure OIDC Issuer","text":"<p>Use the Cloud Credential Operator (CCO) tool to create the OIDC issuer:</p> <pre><code># Set OIDC issuer variables (reusing variables from previous steps)\nOIDC_STORAGE_ACCOUNT_NAME=\"yourstorageaccount\"\nTENANT_ID=\"your-tenant-id\"\n# SUBSCRIPTION_ID and PERSISTENT_RG_NAME already set from previous section\n# Create an RSA key pair and save the private and public key\nccoctl azure create-key-pair  \n\nSA_TOKEN_ISSUER_PRIVATE_KEY_PATH=\"/path/to/serviceaccount-signer.private\"\nSA_TOKEN_ISSUER_PUBLIC_KEY_PATH=\"/path/to/serviceaccount-signer.public\"\n\n# Create OIDC issuer using CCO tool in os4-common resource group\nccoctl azure create-oidc-issuer \\\n    --oidc-resource-group-name ${PERSISTENT_RG_NAME} \\\n    --tenant-id ${TENANT_ID} \\\n    --region ${LOCATION} \\\n    --name ${OIDC_STORAGE_ACCOUNT_NAME} \\\n    --subscription-id ${SUBSCRIPTION_ID} \\\n    --public-key-file ${SA_TOKEN_ISSUER_PUBLIC_KEY_PATH}\n\n# Set OIDC issuer URL\nOIDC_ISSUER_URL=\"https://${OIDC_STORAGE_ACCOUNT_NAME}.blob.core.windows.net/${OIDC_STORAGE_ACCOUNT_NAME}\"\n</code></pre>"},{"location":"how-to/azure/azure-workload-identity-setup/#set-up-federated-identity-credentials","title":"Set Up Federated Identity Credentials","text":"<p>Configure federated identity credentials for each workload identity. These establish trust between Azure Entra ID and the specific service accounts in your HostedCluster:</p> <pre><code># Define workload identity names (matching those created earlier)\n# These should match the managed identities created in the previous section\nAZURE_DISK_MI_NAME=\"${CLUSTER_NAME}-disk-csi\"\nAZURE_FILE_MI_NAME=\"${CLUSTER_NAME}-file-csi\"\nIMAGE_REGISTRY_MI_NAME=\"${CLUSTER_NAME}-image-registry\"\nINGRESS_MI_NAME=\"${CLUSTER_NAME}-ingress\"\nCLOUD_PROVIDER_MI_NAME=\"${CLUSTER_NAME}-cloud-provider\"\nNODE_POOL_MANAGEMENT_MI_NAME=\"${CLUSTER_NAME}-nodepool-mgmt\"\nNETWORK_MI_NAME=\"${CLUSTER_NAME}-network\"\n\n# Azure Disk CSI Driver federated credentials\naz identity federated-credential create \\\n    --name \"${AZURE_DISK_MI_NAME}-fed-id-node\" \\\n    --identity-name \"${AZURE_DISK_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-cluster-csi-drivers:azure-disk-csi-driver-node-sa \\\n    --audience openshift\n\naz identity federated-credential create \\\n    --name \"${AZURE_DISK_MI_NAME}-fed-id-operator\" \\\n    --identity-name \"${AZURE_DISK_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-cluster-csi-drivers:azure-disk-csi-driver-operator \\\n    --audience openshift\n\naz identity federated-credential create \\\n    --name \"${AZURE_DISK_MI_NAME}-fed-id-controller\" \\\n    --identity-name \"${AZURE_DISK_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-cluster-csi-drivers:azure-disk-csi-driver-controller-sa \\\n    --audience openshift\n\n# Azure File CSI Driver federated credentials\naz identity federated-credential create \\\n    --name \"${AZURE_FILE_MI_NAME}-fed-id-node\" \\\n    --identity-name \"${AZURE_FILE_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-cluster-csi-drivers:azure-file-csi-driver-node-sa \\\n    --audience openshift\n\naz identity federated-credential create \\\n    --name \"${AZURE_FILE_MI_NAME}-fed-id-operator\" \\\n    --identity-name \"${AZURE_FILE_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-cluster-csi-drivers:azure-file-csi-driver-operator \\\n    --audience openshift\n\naz identity federated-credential create \\\n    --name \"${AZURE_FILE_MI_NAME}-fed-id-controller\" \\\n    --identity-name \"${AZURE_FILE_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-cluster-csi-drivers:azure-file-csi-driver-controller-sa \\\n    --audience openshift\n\n# Image Registry federated credentials\naz identity federated-credential create \\\n    --name \"${IMAGE_REGISTRY_MI_NAME}-fed-id-registry\" \\\n    --identity-name \"${IMAGE_REGISTRY_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-image-registry:registry \\\n    --audience openshift\n\naz identity federated-credential create \\\n    --name \"${IMAGE_REGISTRY_MI_NAME}-fed-id-operator\" \\\n    --identity-name \"${IMAGE_REGISTRY_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-image-registry:cluster-image-registry-operator \\\n    --audience openshift\n\n# Ingress Operator federated credential\naz identity federated-credential create \\\n    --name \"${INGRESS_MI_NAME}-fed-id\" \\\n    --identity-name \"${INGRESS_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-ingress-operator:ingress-operator \\\n    --audience openshift\n\n# Cloud Provider federated credential\naz identity federated-credential create \\\n    --name \"${CLOUD_PROVIDER_MI_NAME}-fed-id\" \\\n    --identity-name \"${CLOUD_PROVIDER_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:kube-system:azure-cloud-provider \\\n    --audience openshift\n\n# Node Pool Management federated credential\naz identity federated-credential create \\\n    --name \"${NODE_POOL_MANAGEMENT_MI_NAME}-fed-id\" \\\n    --identity-name \"${NODE_POOL_MANAGEMENT_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:kube-system:capi-provider \\\n    --audience openshift\n\n# Network Operator federated credential\naz identity federated-credential create \\\n    --name \"${NETWORK_MI_NAME}-fed-id\" \\\n    --identity-name \"${NETWORK_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-cloud-network-config-controller:cloud-network-config-controller \\\n    --audience openshift\n</code></pre> <p>Service Account Mapping</p> <p>Each federated identity credential maps a specific Azure managed identity to an OpenShift service account. The service accounts listed above are the default service accounts used by various OpenShift components for Azure integration.</p>"},{"location":"how-to/azure/azure-workload-identity-setup/#verification","title":"Verification","text":"<p>Verify the setup:</p> <pre><code># List created managed identities\naz identity list --resource-group $PERSISTENT_RG_NAME --output table\n\n# Verify federated credentials for one identity\naz identity federated-credential list \\\n    --identity-name \"${AZURE_DISK_MI_NAME}\" \\\n    --resource-group $PERSISTENT_RG_NAME\n\n# Test OIDC issuer accessibility\ncurl -s \"${OIDC_ISSUER_URL}/.well-known/openid-configuration\" | jq .\n</code></pre>"},{"location":"how-to/azure/azure-workload-identity-setup/#next-steps","title":"Next Steps","text":"<p>After setting up workload identities, you can proceed to:</p> <ul> <li>Setup Azure Management Cluster for HyperShift</li> </ul>"},{"location":"how-to/azure/create-azure-cluster-on-aks/","title":"Create an Azure Hosted Cluster on AKS","text":""},{"location":"how-to/azure/create-azure-cluster-on-aks/#general","title":"General","text":"<p>This document describes how to set up an Azure Hosted Cluster on an AKS management cluster with an ExternalDNS setup.  Azure HostedClusters on AKS are supported from OCP 4.19.0+.</p> <p>This guide provides both automated script-based setup and manual step-by-step instructions. The automated scripts are located in the /contrib/managed-azure folder in the HyperShift repo and can significantly simplify the setup process.</p>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure CLI</li> <li>jq</li> <li>oc (OpenShift CLI)</li> <li>kubectl</li> <li>ccoctl tool</li> <li>The ccoctl tool provides various commands to assist with the creating and maintenance of cloud credentials from outside     a cluster. More information on the tool can be found here.</li> </ul>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#automated-setup-recommended","title":"Automated Setup (Recommended)","text":"<p>For the quickest setup, you can use the automated scripts. First, create your configuration:</p>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#quick-start-with-automation-scripts","title":"Quick Start with Automation Scripts","text":"<ol> <li> <p>Create your user configuration file:    <pre><code>cat &lt;&lt;EOF &gt; user-vars.sh\n# User variables.\nexport PREFIX=\"YOUR-management\"\nexport PULL_SECRET=\"/path/to/pull-secret.txt\"\nexport HYPERSHIFT_BINARY_PATH=\"/path/to/hypershift/bin/\"\nexport HYPERSHIFT_IMAGE=\"quay.io/hypershift/hypershift-operator:latest\"\nexport RELEASE_IMAGE=\"quay.io/openshift-release-dev/ocp-release:4.20.0-ec.3-multi\"\nexport LOCATION=\"eastus\"\nexport AZURE_CREDS=\"/path/to/azure-creds.json\"\n# Azure storage account names must be between 3 and 24 characters in length and may contain numbers and lowercase letters only.\nexport OIDC_ISSUER_NAME=\"YOURmanagement\"\nEOF\n</code></pre></p> </li> <li> <p>Create Azure credentials file (see Manual Setup Step 2 below for details)</p> </li> <li> <p>Run the complete automated setup (authentication is automatic):</p> </li> </ol> <p>For your first cluster (includes one-time resource setup):    <pre><code>../contrib/managed-azure/setup_all.sh --first-time\n</code></pre></p> <p>For additional clusters (reuses existing resources):    <pre><code>../contrib/managed-azure/setup_all.sh\n</code></pre></p> <p>View the script: setup_all.sh</p> <p>Automatic Authentication</p> <p>The setup script automatically logs you into Azure if you're not already authenticated. No separate login step is required!</p> <p>Important: One-Time Setup Components</p> <p>Three scripts create resources that should be reused across multiple clusters to avoid quota issues: setup_MIv3_kv.sh (service principals and Key Vault), setup_oidc_provider.sh (OIDC issuer), and setup_dataplane_identities.sh (data plane identities). Use the <code>--first-time</code> flag only for your first cluster setup. For subsequent clusters, run the script without this flag to skip the one-time setup and reuse existing resources.</p> <p>Tip</p> <p>The <code>--first-time</code> flag automatically handles the one-time setup resources. If you need to run individual scripts for troubleshooting, you can execute them directly from the contrib/managed-azure folder. For manual Azure authentication, a <code>login.sh</code> script is also available.</p> <p>The automated setup runs these scripts in sequence:</p> <ul> <li>setup_MIv3_kv.sh - Sets up control plane identities and Key Vault</li> <li>setup_aks_cluster.sh - Creates the AKS management cluster  </li> <li>setup_external_dns.sh - Configures DNS zones and external DNS</li> <li>setup_install_ho_on_aks.sh - Installs the HyperShift operator</li> <li>setup_oidc_provider.sh - Sets up OIDC issuer for workload identity</li> <li>setup_dataplane_identities.sh - Creates data plane managed identities</li> <li>create_basic_hosted_cluster.sh - Creates the hosted cluster</li> </ul>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#manual-setup-step-by-step","title":"Manual Setup (Step-by-Step)","text":"<p>If you prefer to understand each step or need to customize the process, follow these manual steps:</p>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#configuration-notes","title":"Configuration Notes","text":"<p>Tip</p> <p>For most users, the automated scripts handle all variable configuration automatically. The variables you need to set are defined in your <code>user-vars.sh</code> file as shown in the Quick Start section above.</p> <p>The <code>PARENT_DNS_ZONE</code> value may be different for different teams. Check the <code>os4-common</code> resource group associated with your subscription for pre-existing DNS zones. If there are multiple DNS zones, the parent DNS zone is typically the common suffix among them (e.g., <code>hypershift.azure.devcluster.openshift.com</code>).</p>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#manual-steps","title":"Manual Steps","text":""},{"location":"how-to/azure/create-azure-cluster-on-aks/#1-retrieve-user-account-details","title":"1. Retrieve User Account Details","text":"<p>Goal: Get your Azure subscription and tenant information for authentication setup.</p> <pre><code>ACCOUNT_DETAILS=$(az account show --query '{subscriptionId: id, tenantId: tenantId}' -o json)\nSUBSCRIPTION_ID=$(echo \"$ACCOUNT_DETAILS\" | jq -r '.subscriptionId')\nTENANT_ID=$(echo \"$ACCOUNT_DETAILS\" | jq -r '.tenantId')\n</code></pre>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#2-create-service-principal-for-authentication","title":"2. Create Service Principal for Authentication","text":"<p>Goal: Create a service principal that will be used by the HyperShift CLI to create cluster infrastructure.</p> <pre><code>SP_DETAILS=$(az ad sp create-for-rbac --name \"$PERSONAL_SP_NAME\" --role Contributor --scopes \"/subscriptions/$SUBSCRIPTION_ID\" -o json)\nCLIENT_ID=$(echo \"$SP_DETAILS\" | jq -r '.appId')\nCLIENT_SECRET=$(echo \"$SP_DETAILS\" | jq -r '.password')\n\ncat &lt;&lt;EOF &gt; $SP_AKS_CREDS\n{\n  \"subscriptionId\": \"$SUBSCRIPTION_ID\",\n  \"tenantId\": \"$TENANT_ID\",\n  \"clientId\": \"$CLIENT_ID\",\n  \"clientSecret\": \"$CLIENT_SECRET\"\n}\nEOF\n</code></pre> <p>Warning</p> <p>In order for your Hypershift cluster to create properly, the Microsoft Graph <code>Application.ReadWrite.OwnedBy</code>  permission must be added to your Service Principal and it also must be assigned to User Access Administrator at the  subscription level. </p> <p>In most cases, you'll need to submit a DPTP request to have this done.</p>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#3-control-plane-identity-and-key-vault-setup","title":"3. Control Plane Identity and Key Vault Setup","text":"<p>Goal: Set up managed identities, key vault, and service principals required for the control plane components. This includes creating certificate-based authentication for various OpenShift services and storing credentials securely.</p> <p>One-Time Setup Only (Steps 3-5)</p> <p>Steps 3-5 create resources that should be reused across multiple clusters to avoid Azure quota limits: service principals and Key Vault (setup_MIv3_kv.sh), OIDC issuer (setup_oidc_provider.sh), and data plane identities (setup_dataplane_identities.sh). Only run these steps once per environment. For subsequent clusters, use <code>setup_all.sh</code> without the <code>--first-time</code> flag to skip these one-time setup steps.</p> <p>Automated Script: setup_MIv3_kv.sh</p> <p>This script handles:</p> <ul> <li>Creating managed identities for AKS cluster components</li> <li>Setting up Azure Key Vault with proper RBAC authorization</li> <li>Creating service principals for control plane services (cloud provider, disk, file, image registry, etc.)</li> <li>Generating and storing certificate JSON in Key Vault</li> <li>Creating a JSON credential file for the service principals</li> </ul>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#4-oidc-provider-setup","title":"4. OIDC Provider Setup","text":"<p>Goal: Create RSA keys and OIDC issuer for workload identity authentication.</p> <p>Automated Script: setup_oidc_provider.sh</p> <p>This script handles:</p> <ul> <li>Creating RSA key pairs for service account token signing</li> <li>Setting up the OIDC issuer URL in Azure storage</li> <li>Configuring the issuer for workload identity federation</li> </ul>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#5-data-plane-identity-configuration","title":"5. Data Plane Identity Configuration","text":"<p>Goal: Create managed identities for data plane components and configure federated identity credentials for workload identity.</p> <p>Automated Script: setup_dataplane_identities.sh</p> <p>This script handles:</p> <ul> <li>Creating managed identities for Azure Disk CSI, Azure File CSI, and Image Registry</li> <li>Setting up federated identity credentials linking these identities to specific service accounts</li> <li>Generating the data plane identities configuration file</li> </ul>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#6-aks-management-cluster-creation","title":"6. AKS Management Cluster Creation","text":"<p>Goal: Create and configure the AKS cluster that will host the HyperShift operator and manage hosted clusters.</p> <p>Automated Script: setup_aks_cluster.sh</p> <p>This script handles:</p> <ul> <li>Creating a resource group for the AKS cluster</li> <li>Creating managed identities for AKS cluster and kubelet</li> <li>Creating the AKS cluster with required features (Key Vault secrets provider, FIPS, autoscaling, etc.)</li> <li>Configuring kubeconfig access</li> <li>Setting up role assignments for the Key Vault secrets provider</li> </ul>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#7-dns-configuration-setup","title":"7. DNS Configuration Setup","text":"<p>Goal: Create DNS zones and configure external DNS for cluster ingress and API access.</p> <p>Automated Script: setup_external_dns.sh</p> <p>This script handles:</p> <ul> <li>Creating a DNS zone for your cluster</li> <li>Configuring name server delegation to the parent DNS zone</li> <li>Creating a service principal for external DNS management</li> <li>Setting up proper DNS permissions and role assignments</li> </ul>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#8-hypershift-operator-installation","title":"8. HyperShift Operator Installation","text":"<p>Goal: Install the HyperShift operator on the AKS management cluster with proper external DNS and Azure integration.</p> <p>Automated Script: setup_install_ho_on_aks.sh</p> <p>This script handles:</p> <ul> <li>Installing required CRDs (ServiceMonitors, PrometheusRules, PodMonitors, Routes)</li> <li>Installing the HyperShift operator with Azure-specific configuration</li> <li>Configuring external DNS integration</li> <li>Setting up managed service configuration for ARO-HCP</li> </ul>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#9-hosted-cluster-creation","title":"9. Hosted Cluster Creation","text":"<p>Goal: Create the actual hosted OpenShift cluster using all the previously configured infrastructure.</p> <p>Automated Script: create_basic_hosted_cluster.sh</p> <p>This script handles:</p> <ul> <li>Creating the hosted cluster with all required Azure integrations</li> <li>Configuring networking (VNet, subnets, NSGs)</li> <li>Setting up marketplace image references</li> <li>Applying FIPS configuration and security settings</li> <li>Enabling workload identity and managed identity integration</li> </ul>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#10-cleanup-and-deletion","title":"10. Cleanup and Deletion","text":""},{"location":"how-to/azure/create-azure-cluster-on-aks/#automated-cleanup-recommended","title":"Automated Cleanup (Recommended)","text":"<p>If you used the automated setup scripts, you can use the corresponding deletion scripts for easy cleanup:</p> <p>Complete Cleanup - Delete both hosted cluster and AKS management cluster: <pre><code>../contrib/managed-azure/delete_all.sh\n</code></pre></p> <p>This is the recommended approach as it will clean up all cluster-specific resources in the correct order: 1. Hosted cluster and its managed resources 2. AKS management cluster and resource group 3. Customer VNet and NSG resource groups 4. AKS-specific Key Vault role assignments</p> <p>Note: Managed identities, service principals, and Key Vault itself are preserved for reuse across multiple clusters</p> <p>Individual Cleanup - For more granular control:</p> <p>Delete only the hosted cluster: <pre><code>../contrib/managed-azure/delete_hosted_cluster.sh\n</code></pre></p> <p>Delete only the AKS management cluster: <pre><code>../contrib/managed-azure/delete_aks_cluster.sh\n</code></pre></p>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#manual-deletion","title":"Manual Deletion","text":"<p>You can also delete the hosted cluster manually using the hypershift CLI: <pre><code>${HYPERSHIFT_BINARY_PATH}/hypershift destroy cluster azure \\\n--name $CLUSTER_NAME \\\n--azure-creds $AZURE_CREDS \\\n--resource-group-name ${MANAGED_RG_NAME}\n</code></pre></p> <p>Tip</p> <p>If you used the automated scripts, <code>CLUSTER_NAME</code> is set to <code>\"${PREFIX}-hc\"</code>, <code>AZURE_CREDS</code> matches your <code>user-vars.sh</code> configuration, and <code>MANAGED_RG_NAME</code> is <code>\"${PREFIX}-managed-rg\"</code>.</p> <p>Warning</p> <p>The manual deletion command only removes the hosted cluster. You'll need to manually clean up the AKS management cluster and other Azure resources created during setup.</p> <p>Note</p> <p>The automated deletion scripts include safety confirmations and handle resource dependencies correctly. Some deletions may take several minutes to complete in the background.</p> <p>Important</p> <p>Resource Reuse: The deletion scripts preserve managed identities, service principals, and the Key Vault itself for reuse across multiple clusters. AKS-specific role assignments are removed, but the underlying identities and vault remain for reuse.</p>"},{"location":"how-to/azure/create-azure-cluster-on-aks/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with the automated scripts:</p> <ol> <li>Check prerequisites: Ensure all required tools are installed and configured</li> <li>Verify permissions: Confirm your service principal has the required permissions</li> <li>Review logs: The scripts use <code>set -x</code> for detailed logging</li> <li>Partial execution: Run individual scripts directly from the contrib/managed-azure folder if you need to resume from a specific point</li> <li>Manual verification: Use the Azure portal to verify resources were created correctly</li> </ol> <p>For additional help, reach out to #project-hypershift on Red Hat Slack.</p>"},{"location":"how-to/azure/create-azure-cluster-with-options/","title":"Create an Azure cluster with Additional Options","text":"<p>This document describes how to set up an Azure cluster with Hypershift with additional options.</p> <p>Creating an Azure cluster with Hypershift without any additional flag options can be found here.</p>"},{"location":"how-to/azure/create-azure-cluster-with-options/#prerequisites","title":"Prerequisites","text":"<p>All sections assume you are: 1. Using an AKS management cluster 2. Set up external DNS 3. Installed the HyperShift Operator</p>"},{"location":"how-to/azure/create-azure-cluster-with-options/#encrypting-the-os-disks-on-azure-vms","title":"Encrypting the OS Disks on Azure VMs","text":"<p>There are a few prerequisites for encrypting the OS disks on the Azure VMs:</p> <ol> <li>Create your own resource group</li> <li>Create an Azure Key Vault, with purge protection required, within the resource group</li> <li>Create a key in the vault to use to create a DiskEncryptionSet</li> <li>Create a DiskEncryptionSet with key in the vault and grant it permissions to assess the key vault</li> </ol> <p>Note</p> <p>You will need to use the <code>resource-group-name</code> flag when using the <code>DiskEncryptionSetID</code> flag.</p> <p>After performing these steps, you just need to provide the DiskEncryptionSet ID when creating a hosted cluster.</p>"},{"location":"how-to/azure/create-azure-cluster-with-options/#cli-example","title":"CLI Example","text":"<pre><code>${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \\\n--name \"$CLUSTER_NAME\" \\\n--azure-creds $AZURE_CREDS \\\n--location ${LOCATION} \\\n--node-pool-replicas 2 \\\n--base-domain $AZURE_BASE_DOMAIN \\\n--pull-secret $PULL_SECRET \\\n--generate-ssh \\\n--release-image ${RELEASE_IMAGE} \\\n--external-dns-domain ${MGMT_DNS_ZONE_NAME} \\\n--resource-group-name \"${MANAGED_RG_NAME}\" \\\n--vnet-id \"${GetVnetID}\" \\\n--subnet-id \"${GetSubnetID}\" \\\n--network-security-group-id \"${GetNsgID}\" \\\n--annotations hypershift.openshift.io/pod-security-admission-label-override=baseline \\\n--managed-identities-file ${SP_FILE} \\\n--assign-service-principal-roles \\\n--dns-zone-rg-name ${DNS_ZONE_RG_NAME} \\\n--fips=true \\\n--marketplace-publisher azureopenshift \\\n--marketplace-offer aro4 \\\n--marketplace-sku aro_417 \\\n--marketplace-version 417.94.20240701 \\\n--disk-encryption-set-id &lt;disk_encryption_set_id&gt;\n</code></pre> <p>You can also pass in the DiskEncryptionSet ID when creating a NodePool.</p> <pre><code>hypershift create nodepool azure \\\n--name &lt;name_of_nodepool&gt; \\\n--cluster-name &lt;cluster_name&gt; \\\n--resource-group-name &lt;resource_group_name&gt; \\\n--disk-encryption-set-id &lt;disk_encryption_set_id&gt;\n</code></pre>"},{"location":"how-to/azure/create-azure-cluster-with-options/#nodepool-cr-example","title":"NodePool CR Example","text":"<p>The DiskEncryptionSet ID can also be set directly through the NodePool CR.</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: NodePool\nmetadata:\n  creationTimestamp: null\n  name: &lt;nodepool_name&gt;\n  namespace: clusters\nspec:\n  arch: amd64\n  clusterName: &lt;cluster_name&gt;\n  management:\n    autoRepair: false\n    upgradeType: Replace\n  platform:\n    azure:\n      diskEncryptionSetID: &lt;disk_encryption_set_id&gt;\n      diskSizeGB: 120\n      vmsize: Standard_D4s_v4\n    type: Azure\n  release:\n    image: &lt;release_image&gt;\n  replicas: &lt;number_of_replicas&gt;\nstatus:\n  replicas: 0\n</code></pre>"},{"location":"how-to/azure/create-azure-cluster-with-options/#enabling-ephemeral-os-disks-on-azure-vms","title":"Enabling Ephemeral OS Disks on Azure VMs","text":"<p>To enable the ephemeral OS disk option on the Azure VMs in your HostedCluster, set the <code>enable-ephemeral-disk</code> flag to true.</p> <p>Important</p> <p>Ephermeral OS disks are not available in every region or for every instance type.</p> <p>You may need to adjust the disk storage account type; to adjust the disk storage account type, use the <code>disk-storage-account-type</code> flag as shown in the example below.</p> <p>You may need to adjust the disk size depending on the instance type used; to adjust the disk size, use the <code>root-disk-size</code> flag.</p> <p>See Ephemeral OS disks for Azure VMs for more details.</p> <pre><code>${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \\\n--name \"$CLUSTER_NAME\" \\\n--azure-creds $AZURE_CREDS \\\n--location ${LOCATION} \\\n--node-pool-replicas 2 \\\n--base-domain $AZURE_BASE_DOMAIN \\\n--pull-secret $PULL_SECRET \\\n--generate-ssh \\\n--release-image ${RELEASE_IMAGE} \\\n--external-dns-domain ${MGMT_DNS_ZONE_NAME} \\\n--resource-group-name \"${MANAGED_RG_NAME}\" \\\n--vnet-id \"${GetVnetID}\" \\\n--subnet-id \"${GetSubnetID}\" \\\n--network-security-group-id \"${GetNsgID}\" \\\n--annotations hypershift.openshift.io/pod-security-admission-label-override=baseline \\\n--managed-identities-file ${MANAGED_IDENTITIES_FILE} \\\n--assign-service-principal-roles \\\n--dns-zone-rg-name ${DNS_ZONE_RG_NAME} \\\n--fips=true \\\n--marketplace-publisher azureopenshift \\\n--marketplace-offer aro4 \\\n--marketplace-sku aro_417 \\\n--marketplace-version 417.94.20240701 \\\n--enable-ephemeral-disk true \\\n--instance-type Standard_DS4_v2 \\\n--disk-storage-account-type Standard_LRS\n</code></pre> <p>You can also set the <code>enable-ephemeral-disk</code> flag when creating a NodePool. <pre><code>hypershift create nodepool azure \\\n--name &lt;name_of_nodepool&gt; \\\n--cluster-name &lt;cluster_name&gt; \\\n--replicas &lt;number_of_replicas&gt; \\\n--release-image &lt;release_image&gt; \\\n--enable-ephemeral-disk true \\\n--instance-type Standard_DS4_v2 \\\n--disk-storage-account-type Standard_LRS\n</code></pre></p>"},{"location":"how-to/azure/create-azure-cluster-with-options/#enabling-kms-encryption","title":"Enabling KMS encryption","text":"<p>This section walks through how to:</p> <ol> <li>Set up a new resource group, key vault, and key for etcd encryption using KMSv2</li> <li>Set up the role assignment between the KMS managed identity (MI) and the key vault</li> <li>Set up the flags needed when creating the Azure HostedCluster</li> <li>Verify the etcd encryption is setup and working properly </li> </ol> <p>There is a <code>setup_etcd_kv.sh</code> script in the contrib folder in the HyperShift repo to help automate the first couple of  steps mentioned above. However, this guide will manually walk through those steps.</p> <p>1a) Create a resource group for the key vault that will house the key used for etcd encryption. </p> <p>Note</p> <p>It is assumed this key vault is a different key vault, let's call it MI KV, than the one containing all of the  managed identities for the control plane. However, the managed identity for KMS is assumed to be in the MI KV.</p> <pre><code>az group create --name example-kms --location eastus\n</code></pre> <p>1b) Create the etcd encryption key vault <pre><code>az keyvault create --name example-kms --resource-group example-kms --location eastus --enable-rbac-authorization\n</code></pre></p> <p>1c) Create a key in the etcd encryption key vault and capture the ID in a variable, KEY_ID. This will be passed when  creating the Azure HostedCluster in a later step below. <pre><code>KEY_ID=$(az keyvault key create \\\n  --vault-name example-kms \\\n  --name example-key \\\n  --protection software \\\n  --kty RSA \\\n  --query key.kid \\\n  -o tsv)\n</code></pre></p> <p>2) Create a role assignment between the KMS MI and the resource group where the etcd encryption key vault is located so  that it can encrypt &amp; decrypt objects.</p> <pre><code>OBJECT_ID=\"the object ID of the KMS Managed Identity. This object ID can be found under the enterprise application for your KMS Managed Identity\"\n\naz role assignment create --assignee $OBJECT_ID --role \"Key Vault Crypto User\" \\\n--scope $(az keyvault show --name example-kms --query \"resourceGroup\" -o tsv | xargs -I{} az group show --name {} --query \"id\" -o tsv)\n</code></pre> <p>3) Add these flags to your HyperShift CLI command when creating the Azure HostedCluster. KEY_ID is from step 1d. <pre><code>`--encryption-key-id $KEY_ID` \\\n`--kms-credentials-secret-name &lt;your KMS credentials secret name&gt;`\n</code></pre></p> <p>4) Here are some different things you can do to confirm etcd encryption using KMSv2 is set up properly on the  HCP/HostedCluster:</p> <p>First, confirm the kube-apiserver pod is using the <code>encryption-provider-config</code> flag such as: <pre><code>--encryption-provider-config=/etc/kubernetes/secret-encryption/config.yaml \n</code></pre></p> <p>If you look at this data, it should contain something like this: <pre><code>apiVersion: apiserver.config.k8s.io/v1\nkind: EncryptionConfiguration\nresources:\n- providers:\n  - kms:\n      apiVersion: v2\n      endpoint: unix:///opt/azurekmsactive.socket\n      name: azure-20514bc7\n      timeout: 35s\n  - identity: {}\n  resources:\n  - secrets\n  - configmaps\n  - routes.route.openshift.io\n  - oauthaccesstokens.oauth.openshift.io\n  - oauthauthorizetokens.oauth.openshift.io\n</code></pre></p> <p>Next, confirm the <code>azure-kms-provider-active</code> container in the kube-apiserver pod is running properly, there are no  errors in the log, and the config file is using the KMS MI. The config file path can be found in the flag on the  container spec: <pre><code>--config-file-path=/etc/kubernetes/azure.json \n</code></pre></p> <p>If you review this data, you should see the KMS MI credentials secret used within it.</p> <p>Finally, you can create a secret on the HostedCluster and then check the secret on etcd in the etcd pod on the HCP  directly:</p> <p>1) Create a secret on the HostedCluster. Example <code>kubectl create secret generic kms-test --from-literal=foo=bar</code>.</p> <p>2) Verify you can see the secret contents on the HostedCluster unencrypted.</p> <p>3) Switch back to the AKS management cluster and exec into the etcd pod, <code>kubectl exec -it pod/etcd-0 -- /bin/bash</code>.</p> <p>4) Run these commands in the etcd pod <pre><code>export ETCDCTL_API=3\nexport ETCDCTL_CACERT=/etc/etcd/tls/etcd-ca/ca.crt\nexport ETCDCTL_CERT=/etc/etcd/tls/client/etcd-client.crt\nexport ETCDCTL_KEY=/etc/etcd/tls/client/etcd-client.key\nexport ETCDCTL_ENDPOINTS=https://etcd-client:2379\n</code></pre> 5) Get the secret created on the HostedCluster <code>etcdctl get /kubernetes.io/secrets/default/kms-test</code>. You should see it  is encrypted with KMSv2 by the azure provider: <pre><code>k8s:enc:kms:v2:azure-8298bce7:\n\ufffdd2%&amp;G\n        E\ufffd\ufffdk(\ufffdB\ufffd    \ufffdH\ufffd\ufffd\ufffd\ufffd\ufffd6#\ufffd]\ufffd[\ufffd\ufffd\ufffdI\n...\n\ufffd=\ufffds\ufffd\ufffdh\ufffd\ufffd\ufffdFq\ufffd\ufffda^(\ufffd\ufffdz \ufffd\ufffdwI\u022b\ufffd\u0779\ufffd\ufffd,\ufffd\u0562Ra\ufffdA\ud65f\ufffd\ufffd5u\ufffd\u0380\ufffd\ufffd*\ufffd\ufffd\u1943\ufffd\ufffd\u019aL\ufffd$L1Y'\ufffdV\ufffd\u04e6i\ufffd\ufffd.\ufffd   R\ufffd\"                                            \ufffd\nversion.azure.akv.io1\"&amp;\nalgorithm.azure.akv.io\n                      RSA-OAEP-256(\n</code></pre></p>"},{"location":"how-to/azure/create-self-managed-azure-cluster/","title":"Create a Self-Managed Azure HostedCluster","text":"<p>Developer Preview in OCP 4.21</p> <p>Self-managed Azure HostedClusters are available as a Developer Preview feature in OpenShift Container Platform 4.21.</p> <p>This document describes how to create a self-managed Azure HostedCluster using workload identities for authentication.</p>"},{"location":"how-to/azure/create-self-managed-azure-cluster/#overview","title":"Overview","text":"<p>Self-managed Azure HostedClusters use Azure Workload Identity for authentication.</p>"},{"location":"how-to/azure/create-self-managed-azure-cluster/#prerequisites","title":"Prerequisites","text":"<p>Before creating a self-managed Azure HostedCluster, ensure you have:</p> <ul> <li>Azure CLI (<code>az</code>) installed and configured</li> <li>HyperShift CLI binary</li> <li>OpenShift CLI (<code>oc</code>) or Kubernetes CLI (<code>kubectl</code>)</li> <li><code>jq</code> command-line JSON processor</li> <li>An Azure OpenShift management cluster with HyperShift operator installed</li> <li>Azure workload identities and OIDC issuer configured</li> <li>Appropriate Azure permissions (see Permission Requirements)</li> <li>Optional: External DNS configured (only if you want automatic DNS management)</li> </ul> <p>Setup Requirements</p> <p>This guide assumes you have already completed the workload identity configuration and management cluster setup. Follow these guides in order:</p> <ol> <li>Azure Workload Identity Setup - Workload identities and OIDC issuer configuration</li> <li>Setup Azure Management Cluster for HyperShift - HyperShift operator installation (with or without External DNS)</li> </ol>"},{"location":"how-to/azure/create-self-managed-azure-cluster/#permission-requirements","title":"Permission Requirements","text":"<p>Your Azure service principal must have the following permissions:</p> <ul> <li>Subscription Level:<ul> <li><code>Contributor</code> role</li> <li><code>User Access Administrator</code> role</li> </ul> </li> <li>Microsoft Graph API:<ul> <li><code>Application.ReadWrite.OwnedBy</code> permission (requires DPTP request in most cases)</li> </ul> </li> </ul>"},{"location":"how-to/azure/create-self-managed-azure-cluster/#creating-the-self-managed-azure-hostedcluster","title":"Creating the Self-Managed Azure HostedCluster","text":""},{"location":"how-to/azure/create-self-managed-azure-cluster/#infrastructure-setup","title":"Infrastructure Setup","text":"<p>Before creating the HostedCluster, set up the necessary Azure infrastructure:</p> <p>About PERSISTENT_RG_NAME</p> <p>In Red Hat environments, a periodic Azure resource \"reaper\" deletes resources that are not properly tagged or not located in an approved resource group. We frequently use the <code>os4-common</code> resource group for shared, long-lived assets (for example, public DNS zones) to avoid accidental cleanup. If you are not in Red Hat infrastructure, set <code>PERSISTENT_RG_NAME</code> to any long-lived resource group in your subscription that will not be automatically reaped, or ensure your organization's required tags/policies are applied. The name does not have to be <code>os4-common</code>\u2014use whatever persistent resource group fits your environment.</p> <pre><code># Set cluster configuration variables\nPREFIX=\"your-prefix-sm\"\nRELEASE_IMAGE=\"quay.io/openshift-release-dev/ocp-release:XYZ\"\nTAG=\"latest\"\n\nLOCATION=\"eastus\"\nMANAGED_RG_NAME=\"${PREFIX}-managed-rg\"\nVNET_RG_NAME=\"${PREFIX}-customer-vnet-rg\"\nNSG_RG_NAME=\"${PREFIX}-customer-nsg-rg\"\nVNET_NAME=\"${PREFIX}-customer-vnet\"\nVNET_SUBNET1=\"${PREFIX}-customer-subnet-1\"\nNSG=\"${PREFIX}-customer-nsg\"\nDNS_ZONE_NAME=\"your-subdomain.your-parent.dns.zone.com\"\nCLUSTER_NAMESPACE=\"clusters\"\nCLUSTER_NAME=\"${PREFIX}-hc\"\nAZURE_CREDS=\"/path/to/azure/credentials\"\nPULL_SECRET=\"/path/to/pull-secret.json\"\nHYPERSHIFT_BINARY_PATH=\"/path/to/hypershift/bin\"\nOIDC_ISSUER_URL=\"https://yourstorageaccount.blob.core.windows.net/yourstorageaccount\"\nSA_TOKEN_ISSUER_PRIVATE_KEY_PATH=\"/path/to/serviceaccount-signer.private\"\nPERSISTENT_RG_NAME=\"os4-common\"\nPARENT_DNS_ZONE=\"your-parent.dns.zone.com\"\n\n# Clean up any previous instances (optional)\naz group delete -n \"${VNET_RG_NAME}\" --yes --no-wait || true\naz group delete -n \"${NSG_RG_NAME}\" --yes --no-wait || true\n\n# Create managed resource group\naz group create --name \"${MANAGED_RG_NAME}\" --location ${LOCATION}\n\n# Create VNET &amp; NSG resource groups\naz group create --name \"${VNET_RG_NAME}\" --location ${LOCATION}\naz group create --name \"${NSG_RG_NAME}\" --location ${LOCATION}\n\n# Create network security group\naz network nsg create \\\n    --resource-group \"${NSG_RG_NAME}\" \\\n    --name \"${NSG}\"\n\n# Get NSG ID\nGetNsgID=$(az network nsg list --query \"[?name=='${NSG}'].id\" -o tsv)\n\n# Create VNet with subnet\naz network vnet create \\\n    --name \"${VNET_NAME}\" \\\n    --resource-group \"${VNET_RG_NAME}\" \\\n    --address-prefix 10.0.0.0/16 \\\n    --subnet-name \"${VNET_SUBNET1}\" \\\n    --subnet-prefixes 10.0.0.0/24 \\\n    --nsg \"${GetNsgID}\"\n\n# Get VNet and Subnet IDs\nGetVnetID=$(az network vnet list --query \"[?name=='${VNET_NAME}'].id\" -o tsv)\nGetSubnetID=$(az network vnet subnet show \\\n    --vnet-name \"${VNET_NAME}\" \\\n    --name \"${VNET_SUBNET1}\" \\\n    --resource-group \"${VNET_RG_NAME}\" \\\n    --query id --output tsv)\n</code></pre>"},{"location":"how-to/azure/create-self-managed-azure-cluster/#create-the-hostedcluster","title":"Create the HostedCluster","text":"<p>Federated Identity Prerequisites</p> <p>Before creating the cluster, ensure that all federated identity credentials have been set up for your workload identities as described in the Azure Workload Identity Setup guide. The cluster creation will fail if these are not properly configured.</p> <p>Azure Marketplace Images</p> <p>For OpenShift 4.20 and later, HyperShift automatically selects the appropriate Azure Marketplace image from the release payload. You no longer need to specify <code>--marketplace-*</code> flags unless you want to use a specific custom image. See Configuring Azure Marketplace Images for more details.</p> <p>Create the HostedCluster:</p> <pre><code># Create the HostedCluster\n${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \\\n    --name \"$CLUSTER_NAME\" \\\n    --namespace \"$CLUSTER_NAMESPACE\" \\\n    --azure-creds $AZURE_CREDS \\\n    --location ${LOCATION} \\\n    --node-pool-replicas 2 \\\n    --base-domain $PARENT_DNS_ZONE \\\n    --pull-secret $PULL_SECRET \\\n    --generate-ssh \\\n    --release-image ${RELEASE_IMAGE} \\\n    --external-dns-domain ${DNS_ZONE_NAME} \\\n    --resource-group-name \"${MANAGED_RG_NAME}\" \\\n    --vnet-id \"${GetVnetID}\" \\\n    --subnet-id \"${GetSubnetID}\" \\\n    --network-security-group-id \"${GetNsgID}\" \\\n    --sa-token-issuer-private-key-path \"${SA_TOKEN_ISSUER_PRIVATE_KEY_PATH}\" \\\n    --oidc-issuer-url \"${OIDC_ISSUER_URL}\" \\\n    --control-plane-operator-image=\"quay.io/hypershift/hypershift:${TAG}\" \\\n    --dns-zone-rg-name ${PERSISTENT_RG_NAME} \\\n    --assign-service-principal-roles \\\n    --workload-identities-file ./workload-identities.json \\\n    --diagnostics-storage-account-type Managed\n</code></pre>"},{"location":"how-to/azure/create-self-managed-azure-cluster/#configuring-azure-marketplace-images","title":"Configuring Azure Marketplace Images","text":"<p>HyperShift supports multiple approaches for configuring Azure Marketplace images for your cluster nodes. The recommended approach varies based on your OpenShift version and requirements.</p>"},{"location":"how-to/azure/create-self-managed-azure-cluster/#for-openshift-420-and-later-recommended","title":"For OpenShift 4.20 and Later (Recommended)","text":"<p>Pattern 1: Use Release Payload Defaults (Simplest)</p> <p>For OpenShift 4.20+, HyperShift automatically selects the appropriate Azure Marketplace image from the release payload. Simply omit all marketplace-related flags:</p> <pre><code># No marketplace flags needed - HyperShift will auto-select the image\n# Gen2 VM generation is used by default\n${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \\\n    --name \"$CLUSTER_NAME\" \\\n    # ... other flags ...\n</code></pre> <p>This is the recommended approach as it ensures your nodes use the officially tested and supported image for your OpenShift version.</p> <p>Pattern 2: Specify VM Generation Only</p> <p>If you need to use a specific VM generation (Gen1 or Gen2), you can specify only the <code>--image-generation</code> flag:</p> <pre><code>${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \\\n    --name \"$CLUSTER_NAME\" \\\n    --image-generation Gen2 \\  # Or Gen1 (case-sensitive)\n    # ... other flags ...\n</code></pre> <p>VM Generation</p> <ul> <li>Valid values: <code>Gen1</code> or <code>Gen2</code> (case-sensitive)</li> <li>Default: <code>Gen2</code> (recommended for new clusters)</li> <li>Gen2 VMs offer better performance and support for newer Azure features</li> </ul> <p>Pattern 3: Use Custom Marketplace Image</p> <p>If you need to use a specific custom marketplace image, provide all marketplace details:</p> <pre><code>${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \\\n    --name \"$CLUSTER_NAME\" \\\n    --marketplace-publisher azureopenshift \\\n    --marketplace-offer aro4 \\\n    --marketplace-sku aro_419 \\\n    --marketplace-version 419.6.20250523 \\\n    --image-generation Gen2 \\  # Optional, defaults to Gen2\n    # ... other flags ...\n</code></pre> <p>Marketplace Flag Requirements</p> <p>When specifying marketplace details, you must provide all four flags (<code>--marketplace-publisher</code>, <code>--marketplace-offer</code>, <code>--marketplace-sku</code>, <code>--marketplace-version</code>) together. Partial specification is not allowed.</p>"},{"location":"how-to/azure/create-self-managed-azure-cluster/#for-openshift-versions-before-420","title":"For OpenShift Versions Before 4.20","text":"<p>For OpenShift versions prior to 4.20, you must explicitly specify marketplace image details (Pattern 3 above) or provide a custom image ID. The automatic image selection from release payload is not available.</p>"},{"location":"how-to/azure/create-self-managed-azure-cluster/#adding-nodepools-with-custom-images","title":"Adding NodePools with Custom Images","text":"<p>When creating additional NodePools, you can specify image configuration in the same way:</p> <pre><code># Use default from release payload (OCP 4.20+)\n${HYPERSHIFT_BINARY_PATH}/hypershift create nodepool azure \\\n    --cluster-name \"$CLUSTER_NAME\" \\\n    # ... other flags ...\n\n# Or specify generation\n${HYPERSHIFT_BINARY_PATH}/hypershift create nodepool azure \\\n    --cluster-name \"$CLUSTER_NAME\" \\\n    --image-generation Gen1 \\\n    # ... other flags ...\n\n# Or use custom marketplace image\n${HYPERSHIFT_BINARY_PATH}/hypershift create nodepool azure \\\n    --cluster-name \"$CLUSTER_NAME\" \\\n    --marketplace-publisher azureopenshift \\\n    --marketplace-offer aro4 \\\n    --marketplace-sku aro_419 \\\n    --marketplace-version 419.6.20250523 \\\n    # ... other flags ...\n</code></pre> <p>Key Configuration Options</p> <ul> <li><code>--workload-identities-file</code>: References the workload identities configuration created in the setup guide</li> <li><code>--assign-service-principal-roles</code>: Automatically assigns required Azure roles to workload identities</li> <li><code>--sa-token-issuer-private-key-path</code>: Path to the private key for service account token signing</li> <li><code>--oidc-issuer-url</code>: URL of the OIDC issuer created in the workload identity setup</li> <li><code>--vnet-id</code>, <code>--subnet-id</code>, <code>--network-security-group-id</code>: Custom networking infrastructure</li> <li><code>--image-generation</code>: (Optional) VM generation (<code>Gen1</code> or <code>Gen2</code>, defaults to <code>Gen2</code>). For OCP 4.20+, omit to use release payload defaults. See Configuring Azure Marketplace Images</li> <li><code>--marketplace-publisher/offer/sku/version</code>: (Optional) Explicit Azure Marketplace image. Must specify all four flags together, or omit all to use defaults (OCP 4.20+)</li> <li><code>--dns-zone-rg-name</code>: Resource group containing the DNS zone (os4-common)</li> <li><code>--diagnostics-storage-account-type Managed</code>: Use Azure managed storage for diagnostics</li> <li><code>--control-plane-operator-image</code>: Custom HyperShift operator image (optional)</li> </ul>"},{"location":"how-to/azure/create-self-managed-azure-cluster/#verification","title":"Verification","text":"<p>Check the cluster status and access:</p> <pre><code># Check HostedCluster status\noc get hostedcluster $CLUSTER_NAME -n clusters\n\n# Wait for cluster to be available\noc wait --for=condition=Available hostedcluster/$CLUSTER_NAME -n clusters --timeout=30m\n\n# Get kubeconfig and access the cluster\nhypershift create kubeconfig --name $CLUSTER_NAME &gt; $CLUSTER_NAME-kubeconfig\nexport KUBECONFIG=$CLUSTER_NAME-kubeconfig\noc get nodes\noc get clusterversion\n</code></pre>"},{"location":"how-to/azure/create-self-managed-azure-cluster/#cleanup","title":"Cleanup","text":"<p>To delete the HostedCluster:</p> <pre><code># Delete the HostedCluster\nhypershift destroy cluster azure \\\n    --name $CLUSTER_NAME \\\n    --azure-creds $AZURE_CREDS \\\n    --resource-group-name $MANAGED_RG_NAME\n</code></pre> <p>Resource Cleanup</p> <p>The HyperShift destroy command will clean up the cluster resources. Workload identities and OIDC issuer created during setup can be reused for other clusters or cleaned up separately if no longer needed.</p>"},{"location":"how-to/azure/create-self-managed-azure-cluster/#related-documentation","title":"Related Documentation","text":"<ol> <li>Azure Workload Identity Setup - Workload identities and OIDC issuer setup</li> <li>Setup Azure Management Cluster for HyperShift - DNS and HyperShift operator setup</li> </ol>"},{"location":"how-to/azure/global-pull-secret/","title":"Global Pull Secret for Hosted Control Planes","text":""},{"location":"how-to/azure/global-pull-secret/#overview","title":"Overview","text":"<p>The Global Pull Secret functionality enables Hosted Cluster administrators to include additional pull secrets for accessing container images from private registries without requiring assistance from the Management Cluster administrator. This feature allows you to merge your custom pull secret with the original HostedCluster pull secret, making it available to all nodes in the cluster.</p> <p>The implementation uses a DaemonSet approach that automatically detects when you create an <code>additional-pull-secret</code> in the <code>kube-system</code> namespace of your DataPlane (Hosted Cluster). The system then merges this secret with the original pull secret and deploys the merged result to all nodes via a DaemonSet that updates the kubelet configuration.</p> <p>Note</p> <p>This feature is designed to work autonomously - once you create the additional pull secret, the system automatically handles the rest without requiring Management Cluster administrator intervention.</p>"},{"location":"how-to/azure/global-pull-secret/#adding-your-pull-secret","title":"Adding your Pull Secret","text":"<p>Important</p> <p>All actions described in this section must be performed on the HostedCluster's workers (DataPlane), not on the Management Cluster.</p> <p>To use this functionality, follow these steps:</p>"},{"location":"how-to/azure/global-pull-secret/#1-create-your-additional-pull-secret","title":"1. Create your additional pull secret","text":"<p>Create a secret named <code>additional-pull-secret</code> in the <code>kube-system</code> namespace of your Hosted Cluster (DataPlane). The secret must contain a valid DockerConfigJSON format:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: additional-pull-secret\n  namespace: kube-system\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: &lt;base64-encoded-docker-config-json&gt;\n</code></pre>"},{"location":"how-to/azure/global-pull-secret/#2-example-dockerconfigjson-format","title":"2. Example DockerConfigJSON format","text":"<p>Your <code>.dockerconfigjson</code> should follow this structure:</p> <pre><code>{\n  \"auths\": {\n    \"registry.example.com\": {\n      \"auth\": \"base64-encoded-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"base64-encoded-credentials\"\n    }\n  }\n}\n</code></pre> <p>Using Namespace-Specific Registry Entries</p> <p>For registries like Quay.io that support organization/namespace-specific authentication, you can specify the full path in your registry entry (e.g., <code>quay.io/mycompany</code> instead of just <code>quay.io</code>). This allows you to provide different credentials for different namespaces within the same registry, and helps avoid conflicts with existing registry entries in the original pull secret.</p>"},{"location":"how-to/azure/global-pull-secret/#3-apply-the-secret","title":"3. Apply the secret","text":"<pre><code>kubectl apply -f additional-pull-secret.yaml\n</code></pre>"},{"location":"how-to/azure/global-pull-secret/#4-verification","title":"4. Verification","text":"<p>After creating the secret, the system will automatically:</p> <ol> <li>Validate the secret format</li> <li>Merge it with the original pull secret</li> <li>Deploy a DaemonSet to all nodes</li> <li>Update the kubelet configuration on each node</li> </ol> <p>You can verify the deployment by checking:</p> <pre><code># Check if the DaemonSet is running\nkubectl get daemonset global-pull-secret-syncer -n kube-system\n\n# Check the merged pull secret\nkubectl get secret global-pull-secret -n kube-system\n\n# Check DaemonSet pods\nkubectl get pods -n kube-system -l name=global-pull-secret-syncer\n</code></pre>"},{"location":"how-to/azure/global-pull-secret/#how-it-works","title":"How it works","text":"<p>The Global Pull Secret functionality operates through a multi-component system:</p>"},{"location":"how-to/azure/global-pull-secret/#automatic-detection","title":"Automatic Detection","text":"<ul> <li>The Hosted Cluster Config Operator (HCCO) continuously monitors the <code>kube-system</code> namespace</li> <li>When it detects the creation of <code>additional-pull-secret</code>, it triggers the reconciliation process</li> </ul>"},{"location":"how-to/azure/global-pull-secret/#validation-and-merging","title":"Validation and Merging","text":"<ul> <li>The system validates that your secret contains a proper DockerConfigJSON format</li> <li>It retrieves the original pull secret from the HostedControlPlane</li> <li>Your additional pull secret is merged with the original one</li> <li>If there are conflicting registry entries, the original pull secret takes precedence (the additional pull secret entry is ignored for conflicting registries)</li> <li>The system supports namespace-specific registry entries (e.g., <code>quay.io/namespace</code>) for better credential specificity</li> </ul>"},{"location":"how-to/azure/global-pull-secret/#deployment-process","title":"Deployment Process","text":"<ul> <li>A <code>global-pull-secret</code> is created in the <code>kube-system</code> namespace containing the merged result</li> <li>RBAC resources (ServiceAccount, Role, RoleBinding) are created for the DaemonSet in both <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>We use Role and RoleBinding in both namespaces to access secrets in <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>A DaemonSet named <code>global-pull-secret-syncer</code> is deployed to eligible nodes</li> </ul> <p>NodePool InPlace Strategy Restriction</p> <p>The Global Pull Secret DaemonSet is not deployed to nodes that belong to NodePools using the InPlace upgrade strategy. This restriction prevents conflicts between the DaemonSet's modifications to <code>/var/lib/kubelet/config.json</code> and the Machine Config Daemon (MCD) during InPlace upgrades.</p> <ul> <li>Nodes with Replace strategy: \u2705 Receive Global Pull Secret DaemonSet</li> <li>Nodes with InPlace strategy: \u274c Do not receive Global Pull Secret DaemonSet</li> </ul> <p>This ensures that MCD operations during InPlace upgrades do not fail due to unexpected changes in kubelet configuration files.</p>"},{"location":"how-to/azure/global-pull-secret/#node-level-synchronization","title":"Node-Level Synchronization","text":"<ul> <li>Each DaemonSet pod runs a controller that watches the secrets under kube-system namespace</li> <li>When changes are detected, it updates <code>/var/lib/kubelet/config.json</code> on the node</li> <li>The kubelet service is restarted via DBus to apply the new configuration</li> <li>If the restart fails after 3 attempts, the system rolls back the file changes</li> </ul>"},{"location":"how-to/azure/global-pull-secret/#automatic-cleanup","title":"Automatic Cleanup","text":"<ul> <li>If you delete the <code>additional-pull-secret</code>, the HCCO automatically removes the <code>global-pull-secret</code> secret</li> <li>The system reverts to using only the original pull secret from the HostedControlPlane</li> <li>The DaemonSet continues running but now syncs only the original pull secret to nodes</li> </ul>"},{"location":"how-to/azure/global-pull-secret/#registry-precedence-and-conflict-resolution","title":"Registry Precedence and Conflict Resolution","text":"<p>The Global Pull Secret system uses a specific precedence model when merging your additional pull secret with the original one:</p>"},{"location":"how-to/azure/global-pull-secret/#merge-behavior","title":"Merge Behavior","text":"<ul> <li>Original pull secret entries always take precedence over additional pull secret entries for the same registry</li> <li>If both secrets contain an entry for <code>quay.io</code>, the original pull secret's credentials will be used</li> <li>Your additional pull secret entries are only added if they don't conflict with existing entries</li> <li>Warnings are logged when conflicts are detected</li> </ul>"},{"location":"how-to/azure/global-pull-secret/#recommended-approach","title":"Recommended Approach","text":"<p>To avoid conflicts and ensure your credentials are used, consider these strategies:</p> <ol> <li>Use namespace-specific entries: Instead of <code>quay.io</code>, use <code>quay.io/your-namespace</code></li> <li>Target specific registries: Add entries only for registries not already in the original pull secret</li> <li>Check existing entries: Review what registries are already configured in the HostedControlPlane</li> </ol>"},{"location":"how-to/azure/global-pull-secret/#example-merge-scenario","title":"Example Merge Scenario","text":"<p>Original Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"original-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Your Additional Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"your-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"your-namespace-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Resulting Merged Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"original-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"your-namespace-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Note how the <code>quay.io</code> entry keeps the original credentials, but <code>quay.io/mycompany</code> is added from your additional secret.</p>"},{"location":"how-to/azure/global-pull-secret/#implementation-details","title":"Implementation details","text":"<p>The implementation consists of several key components working together:</p>"},{"location":"how-to/azure/global-pull-secret/#core-components","title":"Core Components","text":"<ol> <li>Global Pull Secret Controller (<code>globalps</code> package)</li> <li>Handles validation of user-provided pull secrets</li> <li>Manages the merging logic between original and additional pull secrets</li> <li>Creates and manages RBAC resources</li> <li>Deploys and manages the DaemonSet</li> <li> <p>Node eligibility assessment: Labels nodes from InPlace NodePools and configures DaemonSet scheduling restrictions</p> </li> <li> <p>Sync Global Pull Secret Command (<code>sync-global-pullsecret</code> package)</p> </li> <li>Runs as a DaemonSet on each node</li> <li>Watches for changes to the <code>global-pull-secret</code> in <code>kube-system</code> namespace</li> <li>Accesses the original <code>pull-secret</code> in <code>openshift-config</code> namespace</li> <li>Updates the kubelet configuration file</li> <li> <p>Manages kubelet service restarts via DBus</p> </li> <li> <p>Hosted Cluster Config Operator Integration</p> </li> <li>Monitors for the presence of <code>additional-pull-secret</code></li> <li>Orchestrates the entire process</li> <li>Handles cleanup when the secret is removed</li> </ol>"},{"location":"how-to/azure/global-pull-secret/#architecture-diagram","title":"Architecture Diagram","text":"graph TB     %% User Input     User[User creates additional-pull-secret] --&gt; |kube-system namespace| AdditionalPS[additional-pull-secret Secret]      %% HCCO Controller     HCCO[Hosted Cluster Config Operator] --&gt; |Watches kube-system secrets| GlobalPSController[Global Pull Secret Controller]     GlobalPSController --&gt; |Validates| AdditionalPS     GlobalPSController --&gt; |Gets original| OriginalPS[Original pull-secret from HCP]      %% Secret Processing     AdditionalPS --&gt; |Validates format| ValidatePS[Validate Additional Pull Secret]     OriginalPS --&gt; |Extracts data| OriginalPSData[Original Pull Secret Data]     ValidatePS --&gt; |Extracts data| AdditionalPSData[Additional Pull Secret Data]      %% Merge Process     OriginalPSData --&gt; MergeSecrets[Merge Pull Secrets]     AdditionalPSData --&gt; MergeSecrets     MergeSecrets --&gt; |Creates merged JSON| GlobalPSData[Global Pull Secret Data]      %% Secret Creation     GlobalPSData --&gt; |Creates in kube-system| GlobalPSSecret[global-pull-secret Secret]      %% RBAC Setup     GlobalPSController --&gt; |Creates RBAC| RBACSetup[Setup RBAC Resources]     RBACSetup --&gt; ServiceAccount[global-pull-secret-syncer ServiceAccount]     RBACSetup --&gt; KubeSystemRole[global-pull-secret-syncer Role in kube-system]     RBACSetup --&gt; KubeSystemRoleBinding[global-pull-secret-syncer RoleBinding in kube-system]     RBACSetup --&gt; OpenshiftConfigRole[global-pull-secret-syncer Role in openshift-config]     RBACSetup --&gt; OpenshiftConfigRoleBinding[global-pull-secret-syncer RoleBinding in openshift-config]      %% DaemonSet Deployment     GlobalPSController --&gt; |Deploys DaemonSet| DaemonSet[global-pull-secret-syncer DaemonSet]     DaemonSet --&gt; |Runs on each node| DaemonSetPod[DaemonSet Pod]      %% DaemonSet Pod Details     DaemonSetPod --&gt; |Mounts host paths| HostMounts[Host Path Mounts]     HostMounts --&gt; KubeletPath[\"/var/lib/kubelet\"]     HostMounts --&gt; DbusPath[\"/var/run/dbus\"]      %% Container Execution     DaemonSetPod --&gt; |Runs command| Container[control-plane-operator Container]     Container --&gt; |Executes| SyncCommand[sync-global-pullsecret command]      %% Sync Process     SyncCommand --&gt; |Watches global-pull-secret| SyncController[Global Pull Secret Reconciler]     SyncController --&gt; |Reads secret| ReadGlobalPS[Read global-pull-secret]     SyncController --&gt; |Reads original| ReadOriginalPS[Read original pull-secret]      %% File Update Process     ReadGlobalPS --&gt; |Gets data| GlobalPSBytes[Global Pull Secret Bytes]     ReadOriginalPS --&gt; |Gets data| OriginalPSBytes[Original Pull Secret Bytes]      %% Decision Logic     GlobalPSBytes --&gt; |If exists| UseGlobalPS[Use Global Pull Secret]     OriginalPSBytes --&gt; |If not exists| UseOriginalPS[Use Original Pull Secret]      %% File Update     UseGlobalPS --&gt; |Updates file| UpdateKubeletConfig[\"Update /var/lib/kubelet/config.json\"]     UseOriginalPS --&gt; |Updates file| UpdateKubeletConfig      %% Kubelet Restart     UpdateKubeletConfig --&gt; |Restarts kubelet| RestartKubelet[Restart kubelet.service via systemd]     RestartKubelet --&gt; |Via dbus| DbusConnection[DBus Connection]      %% Error Handling     UpdateKubeletConfig --&gt; |If restart fails| RollbackProcess[Rollback Process]     RollbackProcess --&gt; |Restore original| RestoreOriginal[Restore Original File Content]      %% Cleanup Process     GlobalPSController --&gt; |If additional PS deleted| CleanupProcess[Cleanup Process]     CleanupProcess --&gt; |Deletes global PS| DeleteGlobalPS[Delete global-pull-secret]     CleanupProcess --&gt; |Removes DaemonSet| RemoveDaemonSet[Remove DaemonSet]      %% Styling     classDef userInput fill:#e1f5fe     classDef controller fill:#f3e5f5     classDef secret fill:#e8f5e8     classDef process fill:#fff3e0     classDef daemonSet fill:#fce4ec     classDef fileSystem fill:#f1f8e9      class User,AdditionalPS userInput     class HCCO,GlobalPSController,SyncController controller     class OriginalPS,GlobalPSSecret,ServiceAccount,KubeSystemRole,KubeSystemRoleBinding,OpenshiftConfigRole,OpenshiftConfigRoleBinding secret     class ValidatePS,MergeSecrets,RBACSetup,UpdateKubeletConfig,RestartKubelet process     class DaemonSet,DaemonSetPod,Container daemonSet     class KubeletPath,DbusPath fileSystem"},{"location":"how-to/azure/global-pull-secret/#key-features","title":"Key Features","text":"<ul> <li>Security: Only watches specific secrets in <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>Robustness: Includes automatic rollback in case of failures</li> <li>Efficiency</li> <li>Only updates when there are actual changes</li> <li>The globalPullSecret implementation has their own controller so it cannot interfere with the HCCO reonciliation</li> <li>Security considerations: Uses specific RBAC for only the required resources in each namespace. The DaemonSet containers run in privileged mode due to the need to:</li> <li>Write to <code>/var/lib/kubelet/config.json</code> (kubelet configuration file)</li> <li>Connect to systemd via DBus for service management</li> <li>Restart kubelet.service, which requires root privileges</li> <li>Smart node targeting: Automatically excludes nodes from InPlace NodePools to prevent MCD conflicts</li> </ul>"},{"location":"how-to/azure/global-pull-secret/#inplace-nodepool-handling","title":"InPlace NodePool Handling","text":"<p>To prevent conflicts with Machine Config Daemon operations, the implementation includes intelligent node targeting:</p>"},{"location":"how-to/azure/global-pull-secret/#node-labeling-process","title":"Node Labeling Process","text":"<ol> <li>MachineSets Discovery: The controller queries the management cluster for MachineSets with InPlace-specific annotations (<code>hypershift.openshift.io/nodePoolTargetConfigVersion</code>)</li> <li>Machine Enumeration: For each InPlace MachineSets, it lists all associated Machines</li> <li>Node Identification: Maps Machine objects to their corresponding nodes via <code>machine.Status.NodeRef.Name</code></li> <li>Labeling: Applies <code>hypershift.openshift.io/nodepool-inplace-strategy=true</code> label to identified nodes</li> </ol>"},{"location":"how-to/azure/global-pull-secret/#daemonset-scheduling-configuration","title":"DaemonSet Scheduling Configuration","text":"<p>The DaemonSet uses NodeAffinity to exclude InPlace nodes:</p> <pre><code>spec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: hypershift.openshift.io/nodepool-inplace-strategy\n                operator: DoesNotExist\n</code></pre> <p>This ensures that: - Nodes without the label: \u2705 Are eligible for DaemonSet scheduling - Nodes with the label (any value): \u274c Are excluded from DaemonSet scheduling</p>"},{"location":"how-to/azure/global-pull-secret/#conflict-prevention-benefits","title":"Conflict Prevention Benefits","text":"<ul> <li>Prevents MCD failures: Avoids conflicts when MCD expects specific kubelet configuration during InPlace upgrades</li> <li>Maintains upgrade reliability: InPlace upgrade processes are not interrupted by Global Pull Secret modifications</li> <li>Automatic detection: No manual intervention required - the system automatically identifies and handles InPlace nodes</li> </ul>"},{"location":"how-to/azure/global-pull-secret/#error-handling","title":"Error Handling","text":"<p>The system includes comprehensive error handling:</p> <ul> <li>Validation errors: Invalid DockerConfigJSON format is caught early</li> <li>Restart failures: If kubelet restart fails after 3 attempts, the file is rolled back</li> <li>Resource cleanup: If the additional pull secret is deleted, the HCCO automatically removes the globalPullSecret</li> </ul> <p>This implementation provides a secure, autonomous solution that allows HostedCluster administrators to add private registry credentials without requiring Management Cluster administrator intervention.</p>"},{"location":"how-to/azure/scheduler/","title":"Using Azure Scheduler","text":"<p>The Azure Scheduler works with the default <code>ClusterSizingConfiguration</code> resource and the <code>HostedClusterSizing</code> controller.</p>"},{"location":"how-to/azure/scheduler/#clustersizingconfiguration","title":"ClusterSizingConfiguration","text":"<p>The <code>ClusterSizingConfiguration</code> is an API used for setting tshirt sizes based on the number of nodes a <code>HostedCluster</code> has. Each tshirt size can configure different effects that control various aspects of the cluster, such as the Kube API Server (KAS), etcd, etc. Additionally, it allows controlling the frequency of transitions between cluster sizes.</p>"},{"location":"how-to/azure/scheduler/#effects","title":"Effects","text":"<ul> <li><code>kasGoMemLimit</code>: Specifies the memory limit for the Kube API Server.</li> <li><code>controlPlanePriorityClassName</code>: The priority class for most control plane pods.</li> <li><code>etcdPriorityClassName</code>: The priority class for etcd pods.</li> <li><code>apiCriticalPriorityClassName</code>: The priority class for pods in the API request serving path, including Kube API Server and OpenShift APIServer.</li> <li><code>resourceRequests</code>: Allows specifying resource requests for control plane pods.</li> <li><code>machineHealthCheckTimeout</code>: Specifies an optional timeout for machine health checks created for <code>HostedClusters</code> with this specific size.</li> <li><code>maximumRequestsInFlight</code>: Specifies the maximum requests in flight for Kube API Server.</li> <li><code>maximumMutatingRequestsInflight</code>: Specifies the maximum mutating requests in flight for Kube API Server.</li> </ul>"},{"location":"how-to/azure/scheduler/#concurrencyconfiguration","title":"ConcurrencyConfiguration","text":"<p>The <code>ConcurrencyConfiguration</code> defines the bounds of allowed behavior for clusters transitioning between sizes. It includes:</p> <ul> <li><code>SlidingWindow</code>: The window over which the concurrency bound is enforced. This is a duration (e.g., <code>10m</code> for 10 minutes) that specifies the time frame within which the concurrency limit is applied.</li> <li><code>Limit</code>: The maximum allowed number of cluster size transitions during the sliding window. This is an integer (e.g., <code>5</code>) that specifies how many transitions can occur within the sliding window.</li> </ul>"},{"location":"how-to/azure/scheduler/#transitiondelayconfiguration","title":"TransitionDelayConfiguration","text":"<p>The <code>TransitionDelayConfiguration</code> defines the lag between cluster size changing and the assigned tshirt size class being applied. It includes:</p> <ul> <li><code>Increase</code>: The minimum period of time to wait between a cluster's size increasing and the tshirt size assigned to it being updated to reflect the new size. This is a duration (e.g., <code>30s</code> for 30 seconds).</li> <li><code>Decrease</code>: The minimum period of time to wait between a cluster's size decreasing and the tshirt size assigned to it being updated to reflect the new size. This is a duration (e.g., <code>10m</code> for 10 minutes).</li> </ul>"},{"location":"how-to/azure/scheduler/#hostedclustersizing-controller","title":"HostedClusterSizing Controller","text":"<p>The <code>HostedClusterSizing</code> controller determines the number of nodes associated with a <code>HostedCluster</code> either from the <code>HostedControlPlane.Status</code> or by iterating through the nodepools and counting the nodepools associated with the <code>HostedCluster</code>. It then compares the number of nodes against the minimum and maximum sizes set for each tshirt size in the <code>ClusterSizingConfiguration</code>. Based on this comparison, it applies a label to the <code>HostedCluster</code> with the appropriate tshirt size. Depending on the settings in the <code>ClusterSizingConfiguration</code>, it can wait a specified amount of time before transitioning between tshirt sizes using a sliding window, ensuring that only a limited number of transitions (e.g., 5 transitions) can occur within a specified time frame (e.g., 20 minutes).</p> <p>The controller also updates the status of the <code>HostedCluster</code>, reporting the computed cluster size, indicating if a tshirt size transition is pending, and specifying if the cluster requires a transition to a different size.</p>"},{"location":"how-to/azure/scheduler/#azure-scheduler-controller","title":"Azure Scheduler Controller","text":"<p>The Azure scheduler controller is straightforward. It checks the label set by the <code>HostedClusterSizing</code> controller and retrieves the cluster sizing configuration associated with the tshirt size. Based on the configuration, it can modify the <code>HostedCluster</code> with annotations for the specified fields. These annotations are then used by different controllers to propagate the required changes to the appropriate pods and containers.</p>"},{"location":"how-to/azure/scheduler/#how-to-use","title":"How to Use","text":""},{"location":"how-to/azure/scheduler/#prerequisites","title":"Prerequisites","text":"<ul> <li>AKS cluster with cluster-autoscaler enabled and using Standard_D4s_v4 VMs for this example. (--enable-cluster-autoscaler flag when installing AKS cluster, with --min-count 2 --max-count 6)</li> <li>Hypershift operator with size tagging enabled. (--enable-size-tagging flag when installing hypershift operator)</li> <li>ClusterSizingConfiguration resource created. (A default clusterSizingConfiguration resource is created by the hypershift operator)</li> <li>A HostedCluster in the Completed state.</li> <li>A Nodepool with 2 nodes associated with the HostedCluster.</li> </ul>"},{"location":"how-to/azure/scheduler/#steps","title":"Steps","text":"<p>In the example below we will use a HostedCluster with the name 'pstefans-3' in the 'clusters' namespace and the nodepool 'pstefans-3' in the 'clusters' namespace.</p> <ol> <li> <p>The AKS cluster should have only 2 nodes at this point.</p> <pre><code>oc get nodes\nNAME                                STATUS   ROLES    AGE     VERSION\naks-nodepool1-11371333-vmss000000   Ready    &lt;none&gt;   3h43m   v1.31.1\naks-nodepool1-11371333-vmss000002   Ready    &lt;none&gt;   3h43m   v1.31.1\n</code></pre> </li> <li> <p>Edit the <code>ClusterSizingConfiguration</code> resource with the following spec:</p> <pre><code>oc edit clustersizingconfiguration cluster\n</code></pre> <pre><code>spec:\n  concurrency:\n    limit: 5\n    slidingWindow: 0s\n  sizes:\n  - criteria:\n      from: 0\n      to: 2\n    name: small\n  - criteria:\n      from: 3\n      to: 4\n    effects:\n      resourceRequests:\n      - containerName: kube-apiserver\n        cpu: 3\n        deploymentName: kube-apiserver\n      - containerName: control-plane-operator\n        cpu: 3\n        deploymentName: control-plane-operator\n    name: medium\n  - criteria:\n      from: 5\n    name: large\n  transitionDelay:\n    decrease: 0s\n    increase: 0s\n</code></pre> </li> <li> <p>Scale nodepool up to 3 nodes:</p> <pre><code>oc scale nodepool pstefans-3 \\\n  --namespace clusters \\\n  --replicas 3\n</code></pre> </li> <li> <p>Once node pool scales successfully, the <code>HostedCluster</code> will be updated with the new tshirt size label and should have the resource request overrides annotations applied to the HC and the relevant controllers should pick this up and set it on the specified pods.</p> <pre><code>oc get deployment kube-apiserver -n clusters-pstefans-3 -o json | jq '.spec.template.spec.containers[] | select(.name == \"kube-apiserver\") | .resources'\n</code></pre> <pre><code>{\n  \"requests\": {\n    \"cpu\": \"3\",\n    \"memory\": \"2Gi\"\n  }\n}\n</code></pre> <pre><code>oc get deployment control-plane-operator -n clusters-pstefans-3 -o json | jq '.spec.template.spec.containers[] | select(.name == \"control-plane-operator\") | .resources'\n</code></pre> <pre><code>{\n  \"requests\": {\n    \"cpu\": \"3\",\n    \"memory\": \"80Mi\"\n  }\n}\n</code></pre> <pre><code>oc get hc pstefans-3 -n clusters  -o yaml | grep resource-request-override.hypershift.openshift.io\nresource-request-override.hypershift.openshift.io/control-plane-operator.control-plane-operator: cpu=3\nresource-request-override.hypershift.openshift.io/kube-apiserver.kube-apiserver: cpu=3\n</code></pre> </li> <li> <p>You should now see the autoscaler scaled the nodes on the AKS cluster to 3 as we requested 3 CPU cores for the kube-apiserver and control-plane-operator on a nodepool with max 4 cores. So each deployment will nearly request nearly a full node to itself.</p> <pre><code>oc get nodes\nNAME                                STATUS   ROLES    AGE     VERSION\naks-nodepool1-11371333-vmss000000   Ready    &lt;none&gt;   4h8m    v1.31.1\naks-nodepool1-11371333-vmss000002   Ready    &lt;none&gt;   4h8m    v1.31.1\naks-nodepool1-11371333-vmss000003   Ready    &lt;none&gt;   9m31s   v1.31.1\n</code></pre> </li> <li> <p>You should now see that each of the deployments we changed the resource requests for are running on a different node with sufficient compute.</p> <pre><code>kubectl get pods --all-namespaces --field-selector spec.nodeName=aks-nodepool1-11371333-vmss000003\n</code></pre> <pre><code>NAMESPACE             NAME                                     READY   STATUS    RESTARTS   AGE\nclusters-pstefans-3   kube-apiserver-549c75cb99-jj964          4/4     Running   0          12m\n</code></pre> <pre><code>kubectl get pods --all-namespaces --field-selector spec.nodeName=aks-nodepool1-11371333-vmss000002\n</code></pre> <pre><code>NAMESPACE             NAME                                      READY   STATUS    RESTARTS   AGE\nclusters-pstefans-3   control-plane-operator-69b894d9dd-cxv2z   1/1     Running   0          14m\n</code></pre> </li> </ol>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/","title":"self managed azure getting started DEPRECATED","text":"<p>DEPRECATED - Documentation Restructured</p> <p>This documentation has been reorganized into a modular structure for better navigation and clarity.</p> <p>Please use the new documentation:</p> <ul> <li>Understanding HyperShift on Azure</li> <li>Planning Your Deployment</li> <li>Azure Foundation Setup</li> <li>Management Cluster Setup</li> <li>Create Hosted Clusters</li> <li>Day 2 Operations</li> <li>Troubleshooting</li> <li>Reference</li> </ul> <p>This file is kept for reference during content migration and will be removed.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#getting-started-with-self-managed-azure-hypershift-deprecated","title":"Getting Started with Self-Managed Azure HyperShift (DEPRECATED)","text":"<p>Developer Preview in OCP 4.21</p> <p>Self-managed Azure HostedClusters are available as a Developer Preview feature in OpenShift Container Platform 4.21.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#introduction","title":"Introduction","text":""},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#what-is-self-managed-azure-hypershift","title":"What is Self-Managed Azure HyperShift?","text":"<p>Self-managed Azure HyperShift enables you to deploy and manage OpenShift hosted control planes on an OpenShift management cluster running in Azure. This architecture allows you to:</p> <ul> <li>Reduce costs: Run multiple OpenShift control planes as pods on a shared management cluster</li> <li>Improve density: Host dozens of control planes on the same infrastructure</li> <li>Simplify management: Centralize control plane operations while isolating workload data planes</li> <li>Increase flexibility: Choose your own management cluster platform and customize networking</li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#who-is-this-guide-for","title":"Who is This Guide For?","text":"<p>This guide is designed for two primary personas working together to deploy self-managed Azure HyperShift:</p> <ol> <li>Azure Administrator: Responsible for Azure subscription configuration, identity and access management (IAM), DNS setup, and resource group management</li> <li>HyperShift Administrator: Responsible for deploying and managing the management cluster, installing the HyperShift operator, and creating/managing hosted clusters</li> </ol> <p>Division of Responsibilities</p> <p>While some organizations may have a single person handling both roles, this guide separates tasks by persona to help you understand which parts require Azure-level permissions versus cluster-level permissions.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#official-product-name","title":"Official Product Name","text":"<ul> <li>Project: HyperShift (open source project)</li> <li>Product (when generally available): Hosted Control Planes for self-managed OpenShift on Azure</li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#architecture-overview","title":"Architecture Overview","text":""},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#high-level-architecture","title":"High-Level Architecture","text":"<p>Self-managed Azure HyperShift deployments consist of three key layers:</p> <ol> <li>Management Cluster: An existing OpenShift cluster running in Azure that hosts the HyperShift operator and control plane pods for your hosted clusters</li> <li>Control Plane: Kubernetes control plane components (API server, etcd, controllers) running as pods on the management cluster</li> <li>Data Plane: Worker nodes running as Azure Virtual Machines in your Azure subscription, managed by the control plane</li> </ol> graph TB     subgraph Azure[\"Azure Subscription\"]         direction TB          subgraph MgmtCluster[\"Management Cluster (OpenShift)\"]             direction LR             Operator[\"HyperShiftOperator\"]             ClusterACP[\"Cluster AControl Plane\"]             ClusterBCP[\"Cluster BControl Plane\"]         end          WorkerA[\"Cluster A Worker Nodes(Azure VMs)\"]         WorkerB[\"Cluster B Worker Nodes(Azure VMs)\"]          subgraph Shared[\"Shared Azure Resources\"]             SharedItems[\"\u2022 Workload Identities (Managed Identities)\u2022 OIDC Issuer (Blob Storage)\u2022 DNS Zones (Optional)\"]         end          ClusterACP -.-&gt;|manages| WorkerA         ClusterBCP -.-&gt;|manages| WorkerB     end"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#key-components","title":"Key Components","text":""},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#management-cluster","title":"Management Cluster","text":"<ul> <li>Must be an OpenShift cluster running in Azure</li> <li>Hosts the HyperShift operator which manages the lifecycle of hosted clusters</li> <li>Runs control plane pods for multiple hosted clusters</li> <li>Must have sufficient capacity for control plane workloads</li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#hosted-control-plane","title":"Hosted Control Plane","text":"<ul> <li>Runs as a set of pods on the management cluster</li> <li>Includes API server, etcd, controller manager, scheduler</li> <li>Isolated per hosted cluster for security and multi-tenancy</li> <li>Communicates with worker nodes via Azure networking</li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#worker-nodes","title":"Worker Nodes","text":"<ul> <li>Standard Azure Virtual Machines in your subscription</li> <li>Join the hosted cluster via machine provisioning</li> <li>Run your application workloads</li> <li>Managed by the control plane components</li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#authentication-identity","title":"Authentication &amp; Identity","text":"<ul> <li>Uses Azure Workload Identity for secure, credential-free authentication</li> <li>Federated identity credentials enable OpenShift service accounts to authenticate with Azure APIs</li> <li>Eliminates the need for long-lived service principal credentials</li> <li>Each OpenShift component (storage, networking, etc.) gets its own managed identity with minimal required permissions</li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#security-architecture","title":"Security Architecture","text":"<p>Self-managed Azure HyperShift implements several security best practices:</p> <ol> <li>Workload Identity Federation: Uses OIDC-based authentication to eliminate long-lived credentials</li> <li>Least Privilege Access: Each component gets its own managed identity with minimal required permissions</li> <li>Network Isolation: Custom VNets and NSGs allow you to implement network segmentation and security policies</li> <li>Federated Credentials: Trust relationships are scoped to specific service accounts, preventing unauthorized access</li> <li>Control Plane Isolation: Each hosted cluster's control plane runs in isolated pods on the management cluster</li> </ol>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#prerequisites","title":"Prerequisites","text":"<p>This section outlines the prerequisites needed before beginning your deployment, organized by the responsible persona.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#for-azure-administrators","title":"For Azure Administrators","text":"<p>Before the HyperShift Administrator can begin deploying hosted clusters, the Azure Administrator must have:</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#azure-subscription-permissions","title":"Azure Subscription &amp; Permissions","text":"<ul> <li>Active Azure subscription</li> <li>Subscription-level roles:<ul> <li><code>Contributor</code> role</li> <li><code>User Access Administrator</code> role</li> </ul> </li> <li>Microsoft Graph API permissions:<ul> <li><code>Application.ReadWrite.OwnedBy</code> permission (for creating service principals)</li> </ul> </li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#azure-resources","title":"Azure Resources","text":"<ul> <li>A persistent resource group for shared resources (e.g., <code>os4-common</code>)<ul> <li>This resource group will contain workload identities, OIDC issuer, and optionally DNS zones</li> <li>Should not be deleted when individual hosted clusters are removed</li> </ul> </li> <li>(Optional) Parent DNS zone in Azure DNS for delegating cluster DNS records<ul> <li>Required only if using External DNS for automatic DNS management</li> <li>Must allow NS record delegation for subdomain zones</li> </ul> </li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#tools-access","title":"Tools &amp; Access","text":"<ul> <li>Azure CLI (<code>az</code>) installed and configured with the subscription</li> <li><code>jq</code> command-line JSON processor</li> <li>Cloud Credential Operator (CCO) tool installed</li> <li>Access to create and manage:<ul> <li>Managed identities</li> <li>Storage accounts (for OIDC issuer)</li> <li>DNS zones (if using External DNS)</li> <li>Service principals (if using External DNS)</li> </ul> </li> </ul> <p>Resource Reaper Warning</p> <p>In Red Hat development/testing environments, a periodic Azure resource \"reaper\" deletes untagged resources or resources not in approved resource groups. If you're in such an environment, use <code>os4-common</code> for shared resources or ensure your organization's required tags/policies are applied. If you're outside Red Hat infrastructure, use any persistent resource group name that fits your organization's naming conventions.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#for-hypershift-administrators","title":"For HyperShift Administrators","text":"<p>The HyperShift Administrator needs:</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#management-cluster_1","title":"Management Cluster","text":"<ul> <li>An existing Kubernetes cluster (OpenShift or upstream Kubernetes) running in Azure</li> <li>Cluster must have:<ul> <li>Sufficient capacity for hosting control plane pods</li> <li>Network connectivity to Azure APIs</li> <li>Ability to create LoadBalancer services (if not using External DNS)</li> </ul> </li> <li>Administrative access (<code>cluster-admin</code> permissions) to the management cluster</li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#tools-cli","title":"Tools &amp; CLI","text":"<ul> <li>OpenShift CLI (<code>oc</code>) or Kubernetes CLI (<code>kubectl</code>)</li> <li>HyperShift CLI binary (download)</li> <li>Valid OpenShift pull secret from cloud.redhat.com</li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#azure-access-credentials","title":"Azure Access Credentials","text":"<ul> <li>Service principal credentials with appropriate permissions (provided by Azure Administrator)</li> <li>These credentials file will be used for cluster creation operations</li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#configuration-files-from-azure-administrator","title":"Configuration Files (from Azure Administrator)","text":"<ul> <li>Workload identities configuration file (<code>workload-identities.json</code>)</li> <li>OIDC issuer URL</li> <li>Service account signer private key</li> <li>Azure credentials file</li> <li>(Optional) DNS configuration details if using External DNS</li> </ul> <p>Azure Credentials File Format</p> <p>The <code>azure-credentials.json</code> file should contain service principal credentials in this format:</p> <pre><code>{\n  \"subscriptionId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n  \"tenantId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n  \"clientId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n  \"clientSecret\": \"your-service-principal-secret\"\n}\n</code></pre> <p>How to create this file:</p> <pre><code># Create service principal with required permissions\nSP_DETAILS=$(az ad sp create-for-rbac \\\n    --name \"hypershift-cluster-ops\" \\\n    --role Contributor \\\n    --scopes \"/subscriptions/$(az account show --query id -o tsv)\")\n\n# Extract values\nCLIENT_ID=$(echo \"$SP_DETAILS\" | jq -r '.appId')\nCLIENT_SECRET=$(echo \"$SP_DETAILS\" | jq -r '.password')\nTENANT_ID=$(az account show --query tenantId -o tsv)\nSUBSCRIPTION_ID=$(az account show --query id -o tsv)\n\n# Create credentials file\ncat &lt;&lt;EOF &gt; azure-credentials.json\n{\n  \"subscriptionId\": \"$SUBSCRIPTION_ID\",\n  \"tenantId\": \"$TENANT_ID\",\n  \"clientId\": \"$CLIENT_ID\",\n  \"clientSecret\": \"$CLIENT_SECRET\"\n}\nEOF\n</code></pre> <p>Your Azure administrator may provide this file, or you can create it yourself if you have the necessary permissions.</p> <p>Understanding the Different Credential Files</p> <p>This guide uses three different credential/configuration files:</p> File Purpose Used By Format azure-credentials.json Cluster operations (create/destroy) HyperShift CLI <code>{\"subscriptionId\", \"tenantId\", \"clientId\", \"clientSecret\"}</code> workload-identities.json Cluster component authentication OpenShift components <code>{\"imageRegistry\": {\"clientID\": \"...\"}, ...}</code> azure_mgmt.json DNS record management (optional) External DNS operator <code>{\"tenantId\", \"subscriptionId\", \"resourceGroup\", \"aadClientId\", \"aadClientSecret\"}</code> <ul> <li>azure-credentials.json: Created by you or provided by Azure admin (see above)</li> <li>workload-identities.json: Created in Phase 1, Step 2</li> <li>azure_mgmt.json: Created in Phase 1, Step 6 (only if using External DNS)</li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#planning-your-deployment","title":"Planning Your Deployment","text":"<p>Before beginning the installation, make several key decisions that will affect your deployment architecture.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#dns-management-strategy","title":"DNS Management Strategy","text":"<p>One of the first decisions you need to make is how to handle DNS for your hosted clusters. This choice affects both complexity and operational characteristics.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#option-1-with-external-dns-recommended-for-production","title":"Option 1: With External DNS (Recommended for Production)","text":"<p>Best For: Production environments, multiple clusters, custom domains</p> <p>Characteristics: - Automatic DNS record management via External DNS operator - Custom domain names (e.g., <code>api-cluster.example.com</code>) - Requires: DNS zones, service principal with DNS permissions, External DNS operator - Higher initial setup complexity, but simpler ongoing operations</p> <p>Example DNS: - API Server: <code>api.my-cluster.azure.example.com</code> - Apps: <code>*.apps.my-cluster.azure.example.com</code></p> <p>Decision Criteria: - Choose this if you need custom, branded domain names - Choose this if you plan to manage multiple clusters - Choose this if you want fully automated DNS provisioning</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#option-2-without-external-dns-simpler-for-devtest","title":"Option 2: Without External DNS (Simpler for Dev/Test)","text":"<p>Best For: Development, testing, proof-of-concept environments</p> <p>Characteristics: - Manual DNS management or Azure-provided LoadBalancer DNS - API server uses Azure LoadBalancer DNS (e.g., <code>abc123-xyz789.eastus.cloudapp.azure.com</code>) - No DNS zones or service principals needed - Lower initial setup complexity, but requires manual DNS work for production use</p> <p>Example DNS: - API Server: <code>my-cluster-abc123.eastus.cloudapp.azure.com</code> - Apps: <code>my-cluster-apps-xyz789.eastus.cloudapp.azure.com</code></p> <p>Decision Criteria: - Choose this for quick testing or POC environments - Choose this if you don't control your DNS infrastructure - Choose this if you only need a single cluster temporarily</p> <p>Making the Choice</p> <p>Start with Option 2 (Without External DNS) for your first cluster to learn the basics. Once comfortable, you can deploy production clusters with Option 1 (External DNS) for better operational characteristics.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#resource-group-strategy","title":"Resource Group Strategy","text":"<p>Plan your resource group structure to separate long-lived shared resources from cluster-specific resources:</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#persistent-resource-group","title":"Persistent Resource Group","text":"<ul> <li>Name example: <code>os4-common</code>, <code>hypershift-shared</code>, or per your organization's conventions</li> <li>Lifecycle: Long-lived, not deleted when clusters are removed</li> <li>Contents:<ul> <li>Workload identities (managed identities) - reusable across clusters</li> <li>OIDC issuer storage account - reusable across clusters</li> <li>Azure DNS zones (if using External DNS)</li> <li>External DNS service principal (if using External DNS)</li> </ul> </li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#cluster-specific-resource-groups","title":"Cluster-Specific Resource Groups","text":"<ul> <li>Lifecycle: Created and destroyed with each hosted cluster</li> <li>Contents:<ul> <li>Managed resource group for cluster infrastructure (automatically created by HyperShift)</li> <li>VNet resource group (if using custom networking)</li> <li>NSG resource group (if using custom networking)</li> </ul> </li> </ul> <p>Benefit: By separating shared resources, you can: - Reuse workload identities across multiple clusters - Reduce cluster creation time (no need to recreate federated credentials) - Simplify cleanup (delete cluster-specific groups, keep shared resources) - Reduce costs (one OIDC issuer serves multiple clusters)</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#network-architecture","title":"Network Architecture","text":"<p>Decide on your networking approach:</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#option-1-custom-vnets-and-nsgs","title":"Option 1: Custom VNets and NSGs","text":"<ul> <li>Full control over network topology</li> <li>Can integrate with existing Azure networking</li> <li>Specify VNet, subnet, and NSG during cluster creation</li> <li>Best for production environments with specific security requirements</li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#option-2-auto-generated-networking","title":"Option 2: Auto-Generated Networking","text":"<ul> <li>HyperShift creates VNet and networking automatically</li> <li>Simpler for testing and POC</li> <li>Less control over IP ranges and security rules</li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#deployment-phases-overview","title":"Deployment Phases Overview","text":"<p>The deployment process consists of three sequential phases:</p> <ol> <li> <p>Phase 1: Azure Setup (Azure Administrator)</p> <ul> <li>Create workload identities (managed identities)</li> <li>Configure OIDC issuer</li> <li>Set up federated identity credentials</li> <li>(Optional) Create DNS zones and service principal</li> </ul> </li> <li> <p>Phase 2: Management Cluster Setup (HyperShift Administrator)</p> <ul> <li>Install HyperShift operator on management cluster</li> <li>(Optional) Install and configure External DNS</li> </ul> </li> <li> <p>Phase 3: Create Hosted Clusters (HyperShift Administrator)</p> <ul> <li>Create cluster-specific infrastructure (VNets, NSGs)</li> <li>Deploy hosted clusters with workload identities</li> <li>Verify cluster operation</li> </ul> </li> </ol> <p>Sequential Execution Required</p> <p>These phases must be completed in order. You cannot skip Phase 1 or Phase 2, as each phase builds on the previous one.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#phase-1-azure-setup","title":"Phase 1: Azure Setup","text":"<p>Persona: Azure Administrator</p> <p>This phase is typically performed by an Azure Administrator who has subscription-level permissions to create identities, storage accounts, and configure federated credentials.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#overview","title":"Overview","text":"<p>This phase establishes the foundational security infrastructure for your hosted clusters:</p> <ul> <li>Managed Identities: Creates Azure managed identities for each OpenShift component</li> <li>OIDC Issuer: Configures an OIDC issuer in Azure Blob Storage for service account token validation</li> <li>Federated Credentials: Establishes trust relationships between Azure Entra ID and OpenShift service accounts</li> </ul> <p>Why This Matters: Without these identities and federated credentials, your hosted cluster components cannot authenticate with Azure APIs to provision storage, manage load balancers, configure networking, or perform other essential cloud operations. Using workload identities instead of traditional service principals provides better security through automatic credential rotation and follows Azure's modern authentication best practices.</p> <p>When to Complete: This is a one-time setup that can be reused across multiple hosted clusters. Complete this before the HyperShift Administrator proceeds to Phase 2.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#step-1-create-azure-workload-identities","title":"Step 1: Create Azure Workload Identities","text":"<p>Create managed identities for each OpenShift component that needs Azure access:</p> <pre><code># Set environment variables\nPREFIX=\"myprefix\"  # Choose a unique prefix for your cluster\nPERSISTENT_RG_NAME=\"os4-common\"  # Use persistent resource group\nLOCATION=\"eastus\"\nCLUSTER_NAME=\"${PREFIX}-hc\"\nSUBSCRIPTION_ID=$(az account show --query id -o tsv)\n\n# Create persistent resource group (if it doesn't exist)\naz group create --name $PERSISTENT_RG_NAME --location $LOCATION\n\n# Create managed identities for each component\ndeclare -A COMPONENTS=(\n    [\"image-registry\"]=\"cluster-image-registry-operator\"\n    [\"ingress\"]=\"cluster-ingress-operator\"\n    [\"file-csi\"]=\"cluster-storage-operator-file\"\n    [\"disk-csi\"]=\"cluster-storage-operator-disk\"\n    [\"nodepool-mgmt\"]=\"cluster-api-provider-azure\"\n    [\"cloud-provider\"]=\"azure-cloud-provider\"\n    [\"network\"]=\"cluster-network-operator\"\n)\n\n# Create managed identities and capture client IDs\ndeclare -A CLIENT_IDS\nfor component in \"${!COMPONENTS[@]}\"; do\n    echo \"Creating managed identity for $component...\"\n    CLIENT_ID=$(az identity create \\\n        --name \"${CLUSTER_NAME}-${component}\" \\\n        --resource-group $PERSISTENT_RG_NAME \\\n        --query clientId -o tsv)\n    CLIENT_IDS[$component]=$CLIENT_ID\n    echo \"Created identity ${CLUSTER_NAME}-${component} with client ID: $CLIENT_ID\"\ndone\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#step-2-create-workload-identities-configuration-file","title":"Step 2: Create Workload Identities Configuration File","text":"<p>Create a JSON file with all the workload identity client IDs. This file will be provided to the HyperShift Administrator:</p> <pre><code>cat &lt;&lt;EOF &gt; workload-identities.json\n{\n  \"imageRegistry\": {\n    \"clientID\": \"${CLIENT_IDS[image-registry]}\"\n  },\n  \"ingress\": {\n    \"clientID\": \"${CLIENT_IDS[ingress]}\"\n  },\n  \"file\": {\n    \"clientID\": \"${CLIENT_IDS[file-csi]}\"\n  },\n  \"disk\": {\n    \"clientID\": \"${CLIENT_IDS[disk-csi]}\"\n  },\n  \"nodePoolManagement\": {\n    \"clientID\": \"${CLIENT_IDS[nodepool-mgmt]}\"\n  },\n  \"cloudProvider\": {\n    \"clientID\": \"${CLIENT_IDS[cloud-provider]}\"\n  },\n  \"network\": {\n    \"clientID\": \"${CLIENT_IDS[network]}\"\n  }\n}\nEOF\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#step-3-configure-oidc-issuer","title":"Step 3: Configure OIDC Issuer","text":"<p>Use the Cloud Credential Operator (CCO) tool to create the OIDC issuer:</p> <pre><code># Set OIDC issuer variables (reusing variables from previous steps)\nOIDC_STORAGE_ACCOUNT_NAME=\"${PREFIX}oidc$(date +%s)\"  # Must be globally unique\nTENANT_ID=$(az account show --query tenantId -o tsv)\n# SUBSCRIPTION_ID, PERSISTENT_RG_NAME, LOCATION, and PREFIX already set from previous section\n\n# Create an RSA key pair and save the private and public key\nccoctl azure create-key-pair\n\nSA_TOKEN_ISSUER_PRIVATE_KEY_PATH=\"serviceaccount-signer.private\"\nSA_TOKEN_ISSUER_PUBLIC_KEY_PATH=\"serviceaccount-signer.public\"\n\n# Create OIDC issuer using CCO tool in os4-common resource group\nccoctl azure create-oidc-issuer \\\n    --oidc-resource-group-name ${PERSISTENT_RG_NAME} \\\n    --tenant-id ${TENANT_ID} \\\n    --region ${LOCATION} \\\n    --name ${OIDC_STORAGE_ACCOUNT_NAME} \\\n    --subscription-id ${SUBSCRIPTION_ID} \\\n    --public-key-file ${SA_TOKEN_ISSUER_PUBLIC_KEY_PATH}\n\n# Set OIDC issuer URL (provide this to the HyperShift Administrator)\nOIDC_ISSUER_URL=\"https://${OIDC_STORAGE_ACCOUNT_NAME}.blob.core.windows.net/${OIDC_STORAGE_ACCOUNT_NAME}\"\necho \"OIDC Issuer URL: ${OIDC_ISSUER_URL}\"\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#step-4-set-up-federated-identity-credentials","title":"Step 4: Set Up Federated Identity Credentials","text":"<p>Configure federated identity credentials for each workload identity. These establish trust between Azure Entra ID and the specific service accounts in your hosted clusters:</p> <pre><code># Define workload identity names (matching those created earlier)\nAZURE_DISK_MI_NAME=\"${CLUSTER_NAME}-disk-csi\"\nAZURE_FILE_MI_NAME=\"${CLUSTER_NAME}-file-csi\"\nIMAGE_REGISTRY_MI_NAME=\"${CLUSTER_NAME}-image-registry\"\nINGRESS_MI_NAME=\"${CLUSTER_NAME}-ingress\"\nCLOUD_PROVIDER_MI_NAME=\"${CLUSTER_NAME}-cloud-provider\"\nNODE_POOL_MANAGEMENT_MI_NAME=\"${CLUSTER_NAME}-nodepool-mgmt\"\nNETWORK_MI_NAME=\"${CLUSTER_NAME}-network\"\n\n# Azure Disk CSI Driver federated credentials\naz identity federated-credential create \\\n    --name \"${AZURE_DISK_MI_NAME}-fed-id-node\" \\\n    --identity-name \"${AZURE_DISK_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-cluster-csi-drivers:azure-disk-csi-driver-node-sa \\\n    --audience openshift\n\naz identity federated-credential create \\\n    --name \"${AZURE_DISK_MI_NAME}-fed-id-operator\" \\\n    --identity-name \"${AZURE_DISK_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-cluster-csi-drivers:azure-disk-csi-driver-operator \\\n    --audience openshift\n\naz identity federated-credential create \\\n    --name \"${AZURE_DISK_MI_NAME}-fed-id-controller\" \\\n    --identity-name \"${AZURE_DISK_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-cluster-csi-drivers:azure-disk-csi-driver-controller-sa \\\n    --audience openshift\n\n# Azure File CSI Driver federated credentials\naz identity federated-credential create \\\n    --name \"${AZURE_FILE_MI_NAME}-fed-id-node\" \\\n    --identity-name \"${AZURE_FILE_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-cluster-csi-drivers:azure-file-csi-driver-node-sa \\\n    --audience openshift\n\naz identity federated-credential create \\\n    --name \"${AZURE_FILE_MI_NAME}-fed-id-operator\" \\\n    --identity-name \"${AZURE_FILE_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-cluster-csi-drivers:azure-file-csi-driver-operator \\\n    --audience openshift\n\naz identity federated-credential create \\\n    --name \"${AZURE_FILE_MI_NAME}-fed-id-controller\" \\\n    --identity-name \"${AZURE_FILE_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-cluster-csi-drivers:azure-file-csi-driver-controller-sa \\\n    --audience openshift\n\n# Image Registry federated credentials\naz identity federated-credential create \\\n    --name \"${IMAGE_REGISTRY_MI_NAME}-fed-id-registry\" \\\n    --identity-name \"${IMAGE_REGISTRY_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-image-registry:registry \\\n    --audience openshift\n\naz identity federated-credential create \\\n    --name \"${IMAGE_REGISTRY_MI_NAME}-fed-id-operator\" \\\n    --identity-name \"${IMAGE_REGISTRY_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-image-registry:cluster-image-registry-operator \\\n    --audience openshift\n\n# Ingress Operator federated credential\naz identity federated-credential create \\\n    --name \"${INGRESS_MI_NAME}-fed-id\" \\\n    --identity-name \"${INGRESS_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-ingress-operator:ingress-operator \\\n    --audience openshift\n\n# Cloud Provider federated credential\naz identity federated-credential create \\\n    --name \"${CLOUD_PROVIDER_MI_NAME}-fed-id\" \\\n    --identity-name \"${CLOUD_PROVIDER_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:kube-system:azure-cloud-provider \\\n    --audience openshift\n\n# Node Pool Management federated credential\naz identity federated-credential create \\\n    --name \"${NODE_POOL_MANAGEMENT_MI_NAME}-fed-id\" \\\n    --identity-name \"${NODE_POOL_MANAGEMENT_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:kube-system:capi-provider \\\n    --audience openshift\n\n# Network Operator federated credential\naz identity federated-credential create \\\n    --name \"${NETWORK_MI_NAME}-fed-id\" \\\n    --identity-name \"${NETWORK_MI_NAME}\" \\\n    --resource-group \"${PERSISTENT_RG_NAME}\" \\\n    --issuer \"${OIDC_ISSUER_URL}\" \\\n    --subject system:serviceaccount:openshift-cloud-network-config-controller:cloud-network-config-controller \\\n    --audience openshift\n</code></pre> <p>Service Account Mapping</p> <p>Each federated identity credential maps a specific Azure managed identity to an OpenShift service account. The service accounts listed above are the default service accounts used by various OpenShift components for Azure integration. These mappings are automatically configured by HyperShift when you create a hosted cluster - you don't need to manually configure them on the cluster side.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#step-5-optional-dns-zone-configuration-for-external-dns","title":"Step 5: (Optional) DNS Zone Configuration for External DNS","text":"<p>External DNS Only</p> <p>This step is only required if you chose to use External DNS for automatic DNS management. If you're using the simpler approach without External DNS, skip to Phase 1 Verification.</p> <p>If you're using External DNS, create DNS zones and delegate DNS records:</p> <pre><code># Set DNS configuration variables\nEXTRN_DNS_PARENT_RG=\"myparent-dns-rg\"  # Set to your parent DNS resource group\nEXTRN_DNS_PARENT_ZONE=\"example.com\"    # Set to your parent DNS zone (same as base domain)\nEXTRN_DNS_RECORD_NAME=\"${PREFIX}\"\nEXTRN_DNS_ZONE_NAME=\"${PREFIX}.${EXTRN_DNS_PARENT_ZONE}\"\n\n# Create DNS zone in persistent resource group\naz network dns zone create \\\n    --resource-group $PERSISTENT_RG_NAME \\\n    --name $EXTRN_DNS_ZONE_NAME\n\n# Delete existing NS record if it exists\naz network dns record-set ns delete \\\n    --resource-group $EXTRN_DNS_PARENT_RG \\\n    --zone-name $EXTRN_DNS_PARENT_ZONE \\\n    --name $EXTRN_DNS_RECORD_NAME -y\n\n# Get name servers from your DNS zone\nname_servers=$(az network dns zone show \\\n    --resource-group $PERSISTENT_RG_NAME \\\n    --name $EXTRN_DNS_ZONE_NAME \\\n    --query nameServers \\\n    --output tsv)\n\n# Create array of name servers\nns_array=()\nwhile IFS= read -r ns; do\n    ns_array+=(\"$ns\")\ndone &lt;&lt;&lt; \"$name_servers\"\n\n# Add NS records to parent zone\nfor ns in \"${ns_array[@]}\"; do\n    az network dns record-set ns add-record \\\n        --resource-group $EXTRN_DNS_PARENT_RG \\\n        --zone-name $EXTRN_DNS_PARENT_ZONE \\\n        --record-set-name $EXTRN_DNS_RECORD_NAME \\\n        --nsdname \"$ns\"\ndone\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#step-6-optional-create-external-dns-service-principal","title":"Step 6: (Optional) Create External DNS Service Principal","text":"<p>External DNS Only</p> <p>This step is only required if you're using External DNS. Skip to Phase 1 Verification if not.</p> <p>Create a dedicated service principal for External DNS:</p> <pre><code># Set External DNS configuration\nEXTERNAL_DNS_NEW_SP_NAME=\"ExternalDnsServicePrincipal\"\nEXTERNAL_DNS_CREDS_FILE=\"azure_mgmt.json\"\n\n# Create service principal for External DNS\nEXTRN_DNS_SP=$(az ad sp create-for-rbac --name ${EXTERNAL_DNS_NEW_SP_NAME})\nEXTERNAL_DNS_SP_APP_ID=$(echo \"$EXTRN_DNS_SP\" | jq -r '.appId')\nEXTERNAL_DNS_SP_PASSWORD=$(echo \"$EXTRN_DNS_SP\" | jq -r '.password')\n\n# Get DNS zone ID\nEXTRN_DNS_ZONE_ID=$(az network dns zone show \\\n    --name ${EXTRN_DNS_ZONE_NAME} \\\n    --resource-group ${PERSISTENT_RG_NAME} \\\n    --query \"id\" \\\n    --output tsv)\n\n# Assign roles to the service principal\naz role assignment create \\\n    --role \"Reader\" \\\n    --assignee \"${EXTERNAL_DNS_SP_APP_ID}\" \\\n    --scope \"${EXTRN_DNS_ZONE_ID}\"\n\naz role assignment create \\\n    --role \"Contributor\" \\\n    --assignee \"${EXTERNAL_DNS_SP_APP_ID}\" \\\n    --scope \"${EXTRN_DNS_ZONE_ID}\"\n\n# Create Azure credentials file (provide this to HyperShift Administrator)\ncat &lt;&lt;-EOF &gt; ${EXTERNAL_DNS_CREDS_FILE}\n{\n  \"tenantId\": \"$(az account show --query tenantId -o tsv)\",\n  \"subscriptionId\": \"$(az account show --query id -o tsv)\",\n  \"resourceGroup\": \"$PERSISTENT_RG_NAME\",\n  \"aadClientId\": \"$EXTERNAL_DNS_SP_APP_ID\",\n  \"aadClientSecret\": \"$EXTERNAL_DNS_SP_PASSWORD\"\n}\nEOF\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#phase-1-verification","title":"Phase 1 Verification","text":"<p>Verify the Azure setup:</p> <pre><code># List created managed identities\naz identity list --resource-group $PERSISTENT_RG_NAME --output table\n\n# Verify federated credentials for one identity\naz identity federated-credential list \\\n    --identity-name \"${AZURE_DISK_MI_NAME}\" \\\n    --resource-group $PERSISTENT_RG_NAME\n\n# Test OIDC issuer accessibility\ncurl -s \"${OIDC_ISSUER_URL}/.well-known/openid-configuration\" | jq .\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#phase-1-summary","title":"Phase 1 Summary","text":"<p>Save these values for use in Phase 3:</p> <pre><code># Print Phase 1 configuration summary\necho \"\"\necho \"=========================================\"\necho \"Phase 1 Configuration Summary\"\necho \"=========================================\"\necho \"PREFIX=${PREFIX}\"\necho \"CLUSTER_NAME=${CLUSTER_NAME}\"\necho \"PERSISTENT_RG_NAME=${PERSISTENT_RG_NAME}\"\necho \"LOCATION=${LOCATION}\"\necho \"OIDC_STORAGE_ACCOUNT_NAME=${OIDC_STORAGE_ACCOUNT_NAME}\"\necho \"OIDC_ISSUER_URL=${OIDC_ISSUER_URL}\"\necho \"SA_TOKEN_ISSUER_PRIVATE_KEY_PATH=${SA_TOKEN_ISSUER_PRIVATE_KEY_PATH}\"\necho \"\"\necho \"Files created in current directory:\"\necho \"  \u2713 workload-identities.json\"\necho \"  \u2713 ${SA_TOKEN_ISSUER_PRIVATE_KEY_PATH}\"\necho \"  \u2713 ${SA_TOKEN_ISSUER_PUBLIC_KEY_PATH}\"\n\n# If External DNS was configured\nif [[ -n \"${EXTRN_DNS_ZONE_NAME:-}\" ]]; then\n    echo \"  \u2713 azure_mgmt.json\"\n    echo \"\"\n    echo \"DNS Configuration:\"\n    echo \"EXTRN_DNS_PARENT_ZONE=${EXTRN_DNS_PARENT_ZONE}\"\n    echo \"EXTRN_DNS_ZONE_NAME=${EXTRN_DNS_ZONE_NAME}\"\nfi\n\necho \"\"\necho \"Copy these values for Phase 3!\"\necho \"=========================================\"\n</code></pre> <p>Save These Values</p> <p>You will need these exact values when setting up your hosted cluster in Phase 3. Copy the output above to a secure location.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#handoff-to-hypershift-administrator","title":"Handoff to HyperShift Administrator","text":"<p>Once Phase 1 is complete, provide the following to the HyperShift Administrator:</p> <p>Required Files: - <code>workload-identities.json</code> - Workload identity configuration - Service account signer private key (<code>serviceaccount-signer.private</code>) - <code>azure-credentials.json</code> - Service principal credentials for cluster operations</p> <p>Required Information (from Phase 1 summary above): - <code>PREFIX</code> value (e.g., <code>myprefix</code>) - <code>CLUSTER_NAME</code> value (e.g., <code>myprefix-hc</code>) - <code>OIDC_ISSUER_URL</code> (full URL) - <code>OIDC_STORAGE_ACCOUNT_NAME</code> (storage account name) - <code>PERSISTENT_RG_NAME</code> (e.g., <code>os4-common</code>) - <code>LOCATION</code> (e.g., <code>eastus</code>)</p> <p>Optional (if using External DNS): - <code>azure_mgmt.json</code> - External DNS service principal credentials - <code>EXTRN_DNS_ZONE_NAME</code> (e.g., <code>myprefix.example.com</code>) - <code>EXTRN_DNS_PARENT_ZONE</code> (e.g., <code>example.com</code>)</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#phase-2-management-cluster-setup","title":"Phase 2: Management Cluster Setup","text":"<p>Persona: HyperShift Administrator</p> <p>This phase is typically performed by a HyperShift Administrator who has cluster-admin access to the Kubernetes/OpenShift management cluster.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#overview_1","title":"Overview","text":"<p>This phase prepares your management cluster to host HyperShift control planes:</p> <ul> <li>HyperShift Operator: Installs the HyperShift operator that manages hosted cluster lifecycles</li> <li>External DNS (Optional): Deploys and configures External DNS for automatic DNS record management</li> </ul> <p>Why This Matters: The HyperShift operator is the core component that orchestrates all hosted cluster operations. It watches for HostedCluster custom resources and provisions the corresponding control plane components.</p> <p>When to Complete: After the Azure Administrator completes Phase 1.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#step-1-access-your-management-cluster","title":"Step 1: Access Your Management Cluster","text":"<p>Ensure you have access to your management cluster:</p> <pre><code># Verify you can access the management cluster\nkubectl cluster-info\n\n# Or with oc\noc cluster-info\n\n# Verify you have cluster-admin permissions\nkubectl auth can-i '*' '*'  # Should return \"yes\"\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#step-2-create-kubernetes-secret-for-external-dns-optional","title":"Step 2: Create Kubernetes Secret for External DNS (Optional)","text":"<p>External DNS Only</p> <p>This step is only required if you're using External DNS. Skip to Step 3 if not.</p> <p>Create a Kubernetes secret for the Azure credentials:</p> <pre><code># Use the EXTERNAL_DNS_CREDS_FILE from Phase 1, Step 6\nEXTERNAL_DNS_CREDS_FILE=\"azure_mgmt.json\"\n\n# Create Kubernetes secret for Azure credentials\nkubectl delete secret/azure-config-file --namespace \"default\" || true\nkubectl create secret generic azure-config-file \\\n    --namespace \"default\" \\\n    --from-file ${EXTERNAL_DNS_CREDS_FILE}\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#step-3-install-hypershift-operator","title":"Step 3: Install HyperShift Operator","text":"<p>Choose the installation method based on your DNS strategy:</p> With External DNSWithout External DNS <p>Install the HyperShift operator with External DNS configuration:</p> <pre><code># Set installation variables\nPULL_SECRET=\"pull-secret.json\"\nHYPERSHIFT_BINARY_PATH=\"./bin\"\n\n# DNS configuration from Phase 1 (must match Phase 1 values)\nPREFIX=\"myprefix\"  # Must match Phase 1\nEXTRN_DNS_PARENT_ZONE=\"example.com\"  # Must match Phase 1, Step 5\nEXTRN_DNS_ZONE_NAME=\"${PREFIX}.${EXTRN_DNS_PARENT_ZONE}\"\nEXTERNAL_DNS_CREDS_FILE=\"azure_mgmt.json\"  # From Phase 1, Step 6\n\n# Install HyperShift operator with External DNS\n${HYPERSHIFT_BINARY_PATH}/hypershift install \\\n    --external-dns-provider=azure \\\n    --external-dns-credentials ${EXTERNAL_DNS_CREDS_FILE} \\\n    --pull-secret ${PULL_SECRET} \\\n    --external-dns-domain-filter ${EXTRN_DNS_ZONE_NAME} \\\n    --limit-crd-install Azure\n</code></pre> <p>Custom HyperShift Image</p> <p>Add <code>--hypershift-image quay.io/hypershift/hypershift:TAG</code> if using a custom operator image.</p> <p>Install the HyperShift operator without External DNS:</p> <pre><code># Set installation variables\nPULL_SECRET=\"pull-secret.json\"\nHYPERSHIFT_BINARY_PATH=\"./bin\"\n\n# Install HyperShift operator (no External DNS)\n${HYPERSHIFT_BINARY_PATH}/hypershift install \\\n    --pull-secret ${PULL_SECRET} \\\n    --limit-crd-install Azure\n</code></pre> <p>Custom HyperShift Image</p> <p>Add <code>--hypershift-image quay.io/hypershift/hypershift:TAG</code> if using a custom operator image.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#phase-2-verification","title":"Phase 2 Verification","text":"<p>Verify your installation:</p> With External DNSWithout External DNS <pre><code># Check for both operator and external-dns\nkubectl get pods -n hypershift\n\n# Expected output:\n# NAME                           READY   STATUS    RESTARTS   AGE\n# external-dns-xxxxx-xxxxx       1/1     Running   0          1m\n# operator-xxxxx-xxxxx           1/1     Running   0          1m\n</code></pre> <pre><code># Check for operator only\nkubectl get pods -n hypershift\n\n# Expected output:\n# NAME                     READY   STATUS    RESTARTS   AGE\n# operator-xxxxx-xxxxx     1/1     Running   0          1m\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#phase-3-create-your-first-hosted-cluster","title":"Phase 3: Create Your First Hosted Cluster","text":"<p>Persona: HyperShift Administrator</p> <p>This phase creates your actual OpenShift hosted clusters that run application workloads.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#overview_2","title":"Overview","text":"<p>This phase deploys a hosted OpenShift cluster:</p> <ul> <li>Infrastructure Provisioning: Creates Azure resource groups, VNets, subnets, and network security groups</li> <li>HostedCluster Creation: Deploys the control plane on the management cluster and provisions worker nodes as Azure VMs</li> <li>Workload Identity Integration: Automatically links the hosted cluster to the workload identities created in Phase 1</li> </ul> <p>Why This Matters: This creates the actual OpenShift cluster where your applications will run. The control plane runs as pods on the management cluster, while worker nodes run as Azure VMs. The cluster uses the workload identities from Phase 1 to securely access Azure services without storing credentials.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#step-1-prepare-cluster-specific-azure-infrastructure","title":"Step 1: Prepare Cluster-Specific Azure Infrastructure","text":"<p>Create the Azure infrastructure for your hosted cluster:</p> <p>Copy Values from Phase 1 Summary</p> <p>You MUST copy the exact values from the Phase 1 Configuration Summary output.</p> <p>The following variables must match Phase 1: - <code>PREFIX</code> - Must be identical (e.g., <code>myprefix</code>) - <code>CLUSTER_NAME</code> - Must be identical (e.g., <code>myprefix-hc</code>) - <code>OIDC_STORAGE_ACCOUNT_NAME</code> - Copy exact value from Phase 1 summary - <code>PERSISTENT_RG_NAME</code>, <code>LOCATION</code> - Copy from Phase 1</p> <p>If these don't match, workload identity names will be incorrect and cluster creation will fail.</p> <pre><code># Set cluster configuration variables (MUST match Phase 1 values)\nPREFIX=\"myprefix\"  # MUST match the PREFIX used in Phase 1\nCLUSTER_NAME=\"${PREFIX}-hc\"  # MUST match Phase 1\nRELEASE_IMAGE=\"quay.io/openshift-release-dev/ocp-release:4.21.0-x86_64\"\n\n# Resource group names for this cluster\nMANAGED_RG_NAME=\"${PREFIX}-managed-rg\"\nVNET_RG_NAME=\"${PREFIX}-customer-vnet-rg\"\nNSG_RG_NAME=\"${PREFIX}-customer-nsg-rg\"\nVNET_NAME=\"${PREFIX}-customer-vnet\"\nVNET_SUBNET1=\"${PREFIX}-customer-subnet-1\"\nNSG=\"${PREFIX}-customer-nsg\"\n\n# Values from Phase 1 Azure Administrator (use exact values from Phase 1 summary)\n# IMPORTANT: Replace the placeholder below with actual value from Phase 1 summary\n# Example: OIDC_STORAGE_ACCOUNT_NAME=\"myprefixoidc1705432123\"\nOIDC_STORAGE_ACCOUNT_NAME=\"&lt;paste-from-phase-1&gt;\"  # \u2190 REPLACE THIS before proceeding!\nOIDC_ISSUER_URL=\"https://${OIDC_STORAGE_ACCOUNT_NAME}.blob.core.windows.net/${OIDC_STORAGE_ACCOUNT_NAME}\"\nSA_TOKEN_ISSUER_PRIVATE_KEY_PATH=\"serviceaccount-signer.private\"  # From Phase 1\nPERSISTENT_RG_NAME=\"os4-common\"  # From Phase 1\nLOCATION=\"eastus\"  # From Phase 1\n\n# DNS Configuration\n# For External DNS users: use values from Phase 1, Step 5\n# For non-External DNS users: set your base domain (cluster API will use Azure LB DNS)\nPARENT_DNS_ZONE=\"example.com\"  # Your base domain (needed by all clusters)\nEXTRN_DNS_ZONE_NAME=\"${PREFIX}.${PARENT_DNS_ZONE}\"  # For External DNS users only\n\n# Standard configuration\nCLUSTER_NAMESPACE=\"clusters\"\nAZURE_CREDS=\"azure-credentials.json\"\nPULL_SECRET=\"pull-secret.json\"\nHYPERSHIFT_BINARY_PATH=\"./bin\"\n\n# Clean up any previous instances (optional, for testing)\naz group delete -n \"${VNET_RG_NAME}\" --yes --no-wait || true\naz group delete -n \"${NSG_RG_NAME}\" --yes --no-wait || true\n\n# Create managed resource group\naz group create --name \"${MANAGED_RG_NAME}\" --location ${LOCATION}\n\n# Create VNET &amp; NSG resource groups\naz group create --name \"${VNET_RG_NAME}\" --location ${LOCATION}\naz group create --name \"${NSG_RG_NAME}\" --location ${LOCATION}\n\n# Create network security group\naz network nsg create \\\n    --resource-group \"${NSG_RG_NAME}\" \\\n    --name \"${NSG}\"\n\n# Get NSG ID\nGetNsgID=$(az network nsg list --query \"[?name=='${NSG}'].id\" -o tsv)\n\n# Create VNet with subnet\naz network vnet create \\\n    --name \"${VNET_NAME}\" \\\n    --resource-group \"${VNET_RG_NAME}\" \\\n    --address-prefix 10.0.0.0/16 \\\n    --subnet-name \"${VNET_SUBNET1}\" \\\n    --subnet-prefixes 10.0.0.0/24 \\\n    --nsg \"${GetNsgID}\"\n\n# Get VNet and Subnet IDs\nGetVnetID=$(az network vnet list --query \"[?name=='${VNET_NAME}'].id\" -o tsv)\nGetSubnetID=$(az network vnet subnet show \\\n    --vnet-name \"${VNET_NAME}\" \\\n    --name \"${VNET_SUBNET1}\" \\\n    --resource-group \"${VNET_RG_NAME}\" \\\n    --query id --output tsv)\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#step-2-create-the-hostedcluster","title":"Step 2: Create the HostedCluster","text":"<p>Federated Identity Prerequisites</p> <p>Before creating the cluster, ensure that the Azure Administrator has completed all federated identity credential setup in Phase 1. The cluster creation will fail if these are not properly configured.</p> <p>Azure Marketplace Images</p> <p>For OpenShift 4.20 and later, HyperShift automatically selects the appropriate Azure Marketplace image from the release payload. You no longer need to specify <code>--marketplace-*</code> flags unless you want to use a specific custom image. See the Marketplace Images section for details.</p> <p>Create the HostedCluster using the appropriate command for your DNS strategy:</p> With External DNSWithout External DNS <pre><code># Create the HostedCluster with External DNS\n${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \\\n    --name \"$CLUSTER_NAME\" \\\n    --namespace \"$CLUSTER_NAMESPACE\" \\\n    --azure-creds $AZURE_CREDS \\\n    --location ${LOCATION} \\\n    --node-pool-replicas 2 \\\n    --base-domain $PARENT_DNS_ZONE \\\n    --pull-secret $PULL_SECRET \\\n    --generate-ssh \\\n    --release-image ${RELEASE_IMAGE} \\\n    --external-dns-domain ${EXTRN_DNS_ZONE_NAME} \\\n    --resource-group-name \"${MANAGED_RG_NAME}\" \\\n    --vnet-id \"${GetVnetID}\" \\\n    --subnet-id \"${GetSubnetID}\" \\\n    --network-security-group-id \"${GetNsgID}\" \\\n    --sa-token-issuer-private-key-path \"${SA_TOKEN_ISSUER_PRIVATE_KEY_PATH}\" \\\n    --oidc-issuer-url \"${OIDC_ISSUER_URL}\" \\\n    --dns-zone-rg-name ${PERSISTENT_RG_NAME} \\\n    --assign-service-principal-roles \\\n    --workload-identities-file ./workload-identities.json \\\n    --diagnostics-storage-account-type Managed\n</code></pre> <pre><code># Create the HostedCluster without External DNS\n${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \\\n    --name \"$CLUSTER_NAME\" \\\n    --namespace \"$CLUSTER_NAMESPACE\" \\\n    --azure-creds $AZURE_CREDS \\\n    --location ${LOCATION} \\\n    --node-pool-replicas 2 \\\n    --base-domain $PARENT_DNS_ZONE \\\n    --pull-secret $PULL_SECRET \\\n    --generate-ssh \\\n    --release-image ${RELEASE_IMAGE} \\\n    --resource-group-name \"${MANAGED_RG_NAME}\" \\\n    --vnet-id \"${GetVnetID}\" \\\n    --subnet-id \"${GetSubnetID}\" \\\n    --network-security-group-id \"${GetNsgID}\" \\\n    --sa-token-issuer-private-key-path \"${SA_TOKEN_ISSUER_PRIVATE_KEY_PATH}\" \\\n    --oidc-issuer-url \"${OIDC_ISSUER_URL}\" \\\n    --assign-service-principal-roles \\\n    --workload-identities-file ./workload-identities.json \\\n    --diagnostics-storage-account-type Managed\n</code></pre> <p>Key Configuration Options</p> <ul> <li><code>--workload-identities-file</code>: References the workload identities configuration from Phase 1</li> <li><code>--assign-service-principal-roles</code>: Automatically assigns required Azure roles to workload identities</li> <li><code>--sa-token-issuer-private-key-path</code>: Path to the private key for service account token signing</li> <li><code>--oidc-issuer-url</code>: URL of the OIDC issuer created in Phase 1</li> <li><code>--vnet-id</code>, <code>--subnet-id</code>, <code>--network-security-group-id</code>: Custom networking infrastructure</li> <li><code>--dns-zone-rg-name</code>: Resource group containing the DNS zone (only with External DNS)</li> <li><code>--external-dns-domain</code>: DNS zone name from EXTRN_DNS_ZONE_NAME (only with External DNS)</li> <li><code>--diagnostics-storage-account-type Managed</code>: Use Azure managed storage for diagnostics</li> </ul> <p>Control Plane Operator Image</p> <p>The Control Plane Operator (CPO) image is automatically selected from the release image's \"hypershift\" component. You typically don't need to specify <code>--control-plane-operator-image</code> unless you're:</p> <ul> <li>Testing a custom build</li> <li>Using a specific CPO version for debugging</li> </ul> <p>When specified, use: <code>--control-plane-operator-image=quay.io/hypershift/hypershift:&lt;tag&gt;</code></p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#configuring-azure-marketplace-images-optional","title":"Configuring Azure Marketplace Images (Optional)","text":"<p>HyperShift supports multiple approaches for configuring Azure Marketplace images for your cluster nodes. The recommended approach varies based on your OpenShift version.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#for-openshift-420-and-later-recommended","title":"For OpenShift 4.20 and Later (Recommended)","text":"<p>Pattern 1: Use Release Payload Defaults (Simplest)</p> <p>For OpenShift 4.20+, HyperShift automatically selects the appropriate Azure Marketplace image from the release payload. Simply omit all marketplace-related flags:</p> <pre><code># No marketplace flags needed - HyperShift will auto-select the image\n# Gen2 VM generation is used by default\n${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \\\n    --name \"$CLUSTER_NAME\" \\\n    # ... other flags from Step 2 ...\n</code></pre> <p>This is the recommended approach as it ensures your nodes use the officially tested and supported image for your OpenShift version.</p> <p>Pattern 2: Specify VM Generation Only</p> <p>If you need to use a specific VM generation (Gen1 or Gen2):</p> <pre><code>${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \\\n    --name \"$CLUSTER_NAME\" \\\n    --image-generation Gen2 \\  # Or Gen1 (case-sensitive)\n    # ... other flags from Step 2 ...\n</code></pre> <p>VM Generation</p> <ul> <li>Valid values: <code>Gen1</code> or <code>Gen2</code> (case-sensitive)</li> <li>Default: <code>Gen2</code> (recommended for new clusters)</li> <li>Gen2 VMs offer better performance and support for newer Azure features</li> </ul> <p>Pattern 3: Use Custom Marketplace Image</p> <p>If you need to use a specific custom marketplace image:</p> <pre><code>${HYPERSHIFT_BINARY_PATH}/hypershift create cluster azure \\\n    --name \"$CLUSTER_NAME\" \\\n    --marketplace-publisher azureopenshift \\\n    --marketplace-offer aro4 \\\n    --marketplace-sku aro_419 \\\n    --marketplace-version 419.6.20250523 \\\n    --image-generation Gen2 \\  # Optional, defaults to Gen2\n    # ... other flags from Step 2 ...\n</code></pre> <p>Marketplace Flag Requirements</p> <p>When specifying marketplace details, you must provide all four flags (<code>--marketplace-publisher</code>, <code>--marketplace-offer</code>, <code>--marketplace-sku</code>, <code>--marketplace-version</code>) together. Partial specification is not allowed.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#for-openshift-versions-before-420","title":"For OpenShift Versions Before 4.20","text":"<p>For OpenShift versions prior to 4.20, you must explicitly specify marketplace image details (Pattern 3 above). The automatic image selection from release payload is not available.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#step-3-verify-cluster-creation","title":"Step 3: Verify Cluster Creation","text":"<p>Monitor the cluster creation process:</p> <pre><code># Check HostedCluster status\nkubectl get hostedcluster $CLUSTER_NAME -n $CLUSTER_NAMESPACE\n\n# Watch cluster become available (this may take 15-30 minutes)\nkubectl wait --for=condition=Available hostedcluster/$CLUSTER_NAME -n $CLUSTER_NAMESPACE --timeout=30m\n\n# Check NodePool status\nkubectl get nodepool -n $CLUSTER_NAMESPACE\n\n# Verify control plane pods are running\nkubectl get pods -n clusters-${CLUSTER_NAME}\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#step-4-access-your-cluster","title":"Step 4: Access Your Cluster","text":"<p>Once the cluster is available, generate a kubeconfig and access it:</p> <pre><code># Generate kubeconfig\n${HYPERSHIFT_BINARY_PATH}/hypershift create kubeconfig --name $CLUSTER_NAME &gt; $CLUSTER_NAME-kubeconfig\n\n# Set KUBECONFIG to access your hosted cluster\nexport KUBECONFIG=$CLUSTER_NAME-kubeconfig\n\n# Verify cluster access\nkubectl get nodes\nkubectl get clusterversion\nkubectl get co  # Check cluster operators status\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#day-2-operations","title":"Day 2 Operations","text":""},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#adding-additional-nodepools","title":"Adding Additional NodePools","text":"<p>Create additional nodepools with different VM sizes or configurations:</p> <pre><code># Variables from Phase 3 (ensure these are set)\n# PREFIX=\"myprefix\"\n# CLUSTER_NAME=\"${PREFIX}-hc\"\n# CLUSTER_NAMESPACE=\"clusters\"\n# AZURE_CREDS=\"azure-credentials.json\"\n# HYPERSHIFT_BINARY_PATH=\"./bin\"\n\n# Create a new nodepool with different VM size\n${HYPERSHIFT_BINARY_PATH}/hypershift create nodepool azure \\\n    --cluster-name \"$CLUSTER_NAME\" \\\n    --namespace \"$CLUSTER_NAMESPACE\" \\\n    --name \"${CLUSTER_NAME}-workers\" \\\n    --node-count 3 \\\n    --azure-instance-type Standard_D4s_v3 \\\n    --azure-creds $AZURE_CREDS\n</code></pre> <p>You can also specify marketplace image configuration for nodepools:</p> <pre><code># Use default from release payload (OCP 4.20+)\n${HYPERSHIFT_BINARY_PATH}/hypershift create nodepool azure \\\n    --cluster-name \"$CLUSTER_NAME\" \\\n    # ... other flags ...\n\n# Or specify VM generation\n${HYPERSHIFT_BINARY_PATH}/hypershift create nodepool azure \\\n    --cluster-name \"$CLUSTER_NAME\" \\\n    --image-generation Gen1 \\\n    # ... other flags ...\n\n# Or use custom marketplace image\n${HYPERSHIFT_BINARY_PATH}/hypershift create nodepool azure \\\n    --cluster-name \"$CLUSTER_NAME\" \\\n    --marketplace-publisher azureopenshift \\\n    --marketplace-offer aro4 \\\n    --marketplace-sku aro_419 \\\n    --marketplace-version 419.6.20250523 \\\n    # ... other flags ...\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#scaling-nodepools","title":"Scaling NodePools","text":"<p>Scale existing nodepools up or down:</p> <pre><code># Scale a nodepool\nkubectl scale nodepool/${CLUSTER_NAME} -n $CLUSTER_NAMESPACE --replicas=5\n\n# Or edit the nodepool directly\nkubectl edit nodepool/${CLUSTER_NAME} -n $CLUSTER_NAMESPACE\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#upgrading-clusters","title":"Upgrading Clusters","text":"<p>Upgrade the cluster to a new OpenShift version:</p> <pre><code># Update the release image\nkubectl patch hostedcluster/$CLUSTER_NAME -n $CLUSTER_NAMESPACE \\\n    --type merge \\\n    --patch '{\"spec\":{\"release\":{\"image\":\"quay.io/openshift-release-dev/ocp-release:NEW_VERSION\"}}}'\n\n# Monitor the upgrade\nkubectl get hostedcluster $CLUSTER_NAME -n $CLUSTER_NAMESPACE -w\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#deleting-nodepools","title":"Deleting NodePools","text":"<p>Remove a nodepool when no longer needed:</p> <pre><code># Delete a specific nodepool\nkubectl delete nodepool/${CLUSTER_NAME}-workers -n $CLUSTER_NAMESPACE\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#cleanup","title":"Cleanup","text":""},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#delete-a-hosted-cluster","title":"Delete a Hosted Cluster","text":"<p>To delete a hosted cluster while preserving shared Azure resources:</p> <pre><code># Variables from Phase 3 (ensure these are set)\n# PREFIX=\"myprefix\"\n# CLUSTER_NAME=\"${PREFIX}-hc\"\n# AZURE_CREDS=\"azure-credentials.json\"\n# HYPERSHIFT_BINARY_PATH=\"./bin\"\n# MANAGED_RG_NAME=\"${PREFIX}-managed-rg\"\n# VNET_RG_NAME=\"${PREFIX}-customer-vnet-rg\"\n# NSG_RG_NAME=\"${PREFIX}-customer-nsg-rg\"\n\n# Delete the HostedCluster\n${HYPERSHIFT_BINARY_PATH}/hypershift destroy cluster azure \\\n    --name $CLUSTER_NAME \\\n    --azure-creds $AZURE_CREDS \\\n    --resource-group-name $MANAGED_RG_NAME\n\n# Optionally delete cluster-specific resource groups\naz group delete -n \"${VNET_RG_NAME}\" --yes --no-wait\naz group delete -n \"${NSG_RG_NAME}\" --yes --no-wait\naz group delete -n \"${MANAGED_RG_NAME}\" --yes --no-wait\n</code></pre> <p>Shared Resources Preserved</p> <p>The HyperShift destroy command will clean up cluster-specific resources. Workload identities, OIDC issuer, and DNS zones in the persistent resource group (<code>$PERSISTENT_RG_NAME</code>) are preserved and can be reused for other clusters.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#delete-shared-azure-resources","title":"Delete Shared Azure Resources","text":"<p>Only delete these if you're completely removing HyperShift from your environment:</p> <p>Destructive Operation</p> <p>Only perform these steps if you're certain you want to remove all HyperShift infrastructure. This will affect all hosted clusters using these shared resources.</p> <pre><code># Variables from Phase 1 (ensure these are set)\n# PREFIX=\"myprefix\"\n# CLUSTER_NAME=\"${PREFIX}-hc\"\n# PERSISTENT_RG_NAME=\"os4-common\"\n# OIDC_STORAGE_ACCOUNT_NAME=\"${PREFIX}oidc&lt;TIMESTAMP&gt;\"\n# EXTRN_DNS_ZONE_NAME=\"${PREFIX}.${PARENT_DNS_ZONE}\" (if using External DNS)\n# EXTERNAL_DNS_SP_APP_ID=\"&lt;app-id&gt;\" (if using External DNS)\n\n# Delete all managed identities\nfor component in image-registry ingress file-csi disk-csi nodepool-mgmt cloud-provider network; do\n    az identity delete \\\n        --name \"${CLUSTER_NAME}-${component}\" \\\n        --resource-group $PERSISTENT_RG_NAME\ndone\n\n# Delete OIDC issuer storage account\naz storage account delete \\\n    --name $OIDC_STORAGE_ACCOUNT_NAME \\\n    --resource-group $PERSISTENT_RG_NAME\n\n# Delete DNS zones (if using External DNS)\naz network dns zone delete \\\n    --name $EXTRN_DNS_ZONE_NAME \\\n    --resource-group $PERSISTENT_RG_NAME\n\n# Delete service principal (if using External DNS)\naz ad sp delete --id $EXTERNAL_DNS_SP_APP_ID\n\n# Optionally delete the entire persistent resource group\n# Only do this if you're absolutely sure!\n# az group delete -n $PERSISTENT_RG_NAME --yes\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#common-issues","title":"Common Issues","text":""},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#cluster-creation-fails-with-authentication-errors","title":"Cluster Creation Fails with Authentication Errors","text":"<p>Symptom: Cluster creation fails with \"failed to authenticate\" or federated credential errors.</p> <p>Cause: Federated identity credentials not properly configured in Phase 1.</p> <p>Solution: <pre><code># Variables needed: CLUSTER_NAME, PERSISTENT_RG_NAME, OIDC_ISSUER_URL from Phase 1\n\n# Verify federated credentials exist\naz identity federated-credential list \\\n    --identity-name \"${CLUSTER_NAME}-disk-csi\" \\\n    --resource-group $PERSISTENT_RG_NAME\n\n# Verify OIDC issuer is accessible\ncurl -s \"${OIDC_ISSUER_URL}/.well-known/openid-configuration\" | jq .\n</code></pre></p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#control-plane-pods-not-starting","title":"Control Plane Pods Not Starting","text":"<p>Symptom: Control plane pods remain in <code>Pending</code> or <code>CrashLoopBackOff</code> state.</p> <p>Cause: Insufficient resources on management cluster or configuration issues.</p> <p>Solution: <pre><code># Check pod events\nkubectl describe pod &lt;pod-name&gt; -n clusters-${CLUSTER_NAME}\n\n# Check management cluster resources\nkubectl top nodes\n\n# Ensure management cluster has adequate resources\n# Control plane typically needs: 4 CPU, 8GB RAM per hosted cluster\n</code></pre></p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#worker-nodes-not-joining-cluster","title":"Worker Nodes Not Joining Cluster","text":"<p>Symptom: Azure VMs created but nodes don't appear in <code>kubectl get nodes</code>.</p> <p>Cause: Networking issues, NSG rules blocking traffic, or bootstrap problems.</p> <p>Solution: <pre><code># Variables needed: CLUSTER_NAME, CLUSTER_NAMESPACE from Phase 3\n\n# Check nodepool status\nkubectl get nodepool -n $CLUSTER_NAMESPACE\n\n# Check machine status\nkubectl get machine -n clusters-${CLUSTER_NAME}\n\n# Verify NSG allows required traffic:\n# - Port 6443 (API server)\n# - Port 22623 (machine config server)\n# - Port 443 (ingress)\n</code></pre></p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#dns-resolution-issues","title":"DNS Resolution Issues","text":"<p>Symptom: Cannot access cluster API or apps via DNS.</p> <p>Cause: DNS records not created or propagated.</p> <p>Solution For External DNS: <pre><code># Variables needed: PERSISTENT_RG_NAME, EXTRN_DNS_ZONE_NAME from Phase 1\n\n# Check External DNS logs\nkubectl logs -n hypershift deployment/external-dns\n\n# Verify DNS records were created\naz network dns record-set list \\\n    --resource-group $PERSISTENT_RG_NAME \\\n    --zone-name $EXTRN_DNS_ZONE_NAME\n</code></pre></p> <p>Solution Without External DNS: <pre><code># Variables needed: CLUSTER_NAME from Phase 3\n\n# Get LoadBalancer IP/hostname\nkubectl get svc -n clusters-${CLUSTER_NAME}\n\n# Manually create DNS records or use the LoadBalancer DNS directly\n</code></pre></p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#diagnostic-commands","title":"Diagnostic Commands","text":"<pre><code># Variables needed: CLUSTER_NAME, CLUSTER_NAMESPACE from Phase 3\n\n# Check overall cluster health\nkubectl get hostedcluster $CLUSTER_NAME -n $CLUSTER_NAMESPACE -o yaml\n\n# Check hosted control plane status\nkubectl get hostedcontrolplane -n clusters-${CLUSTER_NAME}\n\n# View operator logs\nkubectl logs -n hypershift deployment/operator -f\n\n# Check all resources for a cluster\nkubectl get all -n clusters-${CLUSTER_NAME}\n\n# Describe the hostedcluster for events and conditions\nkubectl describe hostedcluster $CLUSTER_NAME -n $CLUSTER_NAMESPACE\n</code></pre>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here:</p> <ol> <li>Check HyperShift GitHub Issues</li> <li>Review HyperShift Documentation</li> <li>Consult Azure Workload Identity Documentation</li> <li>For supported deployments, contact Red Hat Support</li> </ol>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#appendices","title":"Appendices","text":""},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#appendix-a-azure-permissions-reference","title":"Appendix A: Azure Permissions Reference","text":"<p>Detailed Azure permissions required for each role:</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#azure-administrator-permissions","title":"Azure Administrator Permissions","text":"<ul> <li>Subscription Level:<ul> <li><code>Contributor</code> - For creating resources</li> <li><code>User Access Administrator</code> - For assigning roles to managed identities</li> </ul> </li> <li>Microsoft Graph API:<ul> <li><code>Application.ReadWrite.OwnedBy</code> - For creating service principals</li> </ul> </li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#service-principal-permissions-for-cluster-operations","title":"Service Principal Permissions (for cluster operations)","text":"<ul> <li>Subscription Level:<ul> <li><code>Contributor</code> - For managing cluster infrastructure</li> <li><code>User Access Administrator</code> - For role assignments</li> </ul> </li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#appendix-b-workload-identity-service-account-mapping","title":"Appendix B: Workload Identity Service Account Mapping","text":"<p>Complete mapping of Azure managed identities to OpenShift service accounts:</p> Component Managed Identity Service Accounts Purpose Disk CSI <code>${CLUSTER_NAME}-disk-csi</code> <code>azure-disk-csi-driver-node-sa</code><code>azure-disk-csi-driver-operator</code><code>azure-disk-csi-driver-controller-sa</code> Provision and attach Azure Disk volumes File CSI <code>${CLUSTER_NAME}-file-csi</code> <code>azure-file-csi-driver-node-sa</code><code>azure-file-csi-driver-operator</code><code>azure-file-csi-driver-controller-sa</code> Provision and mount Azure Files Image Registry <code>${CLUSTER_NAME}-image-registry</code> <code>registry</code><code>cluster-image-registry-operator</code> Manage image registry storage Ingress <code>${CLUSTER_NAME}-ingress</code> <code>ingress-operator</code> Configure Azure Load Balancers for ingress Cloud Provider <code>${CLUSTER_NAME}-cloud-provider</code> <code>azure-cloud-provider</code> Integrate with Azure cloud APIs NodePool Management <code>${CLUSTER_NAME}-nodepool-mgmt</code> <code>capi-provider</code> Provision and manage Azure VMs Network <code>${CLUSTER_NAME}-network</code> <code>cloud-network-config-controller</code> Configure Azure networking"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#appendix-c-resource-group-lifecycle-summary","title":"Appendix C: Resource Group Lifecycle Summary","text":"<p>Understanding resource group lifecycles:</p> graph TB     subgraph Persistent[\"Persistent Resource Group (os4-common)\"]         PersistentInfo[\"Lifecycle: Long-lived, survives cluster deletionsContents:\u2022 Workload Identities (7 managed identities)\u2022 OIDC Issuer (storage account)\u2022 DNS Zones (if External DNS)\u2022 External DNS Service Principal (if External DNS)Shared across: Multiple hosted clusters\"]     end      subgraph ClusterA[\"Cluster A RGs\"]         ClusterAInfo[\"\u2022 Managed RG\u2022 VNet RG\u2022 NSG RGLifecycle:Cluster-bound\"]     end      subgraph ClusterB[\"Cluster B RGs\"]         ClusterBInfo[\"\u2022 Managed RG\u2022 VNet RG\u2022 NSG RGLifecycle:Cluster-bound\"]     end      subgraph ClusterC[\"Cluster C RGs\"]         ClusterCInfo[\"\u2022 Managed RG\u2022 VNet RG\u2022 NSG RGLifecycle:Cluster-bound\"]     end"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#appendix-d-reference-links","title":"Appendix D: Reference Links","text":"<p>HyperShift Documentation: - HyperShift Project Home - HyperShift GitHub Repository - HyperShift Architecture</p> <p>Azure Documentation: - Azure Workload Identity - Azure Managed Identities - Azure DNS Zones - Azure Virtual Networks</p> <p>OpenShift Documentation: - OpenShift Container Platform - Cloud Credential Operator</p> <p>Tools: - Azure CLI - OpenShift CLI (oc) - kubectl - jq</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#appendix-e-glossary","title":"Appendix E: Glossary","text":"<ul> <li>HostedCluster: Custom resource representing a hosted OpenShift cluster</li> <li>NodePool: Custom resource representing a group of worker nodes</li> <li>Control Plane: Kubernetes components (API server, etcd, controllers) that manage the cluster</li> <li>Data Plane: Worker nodes where application workloads run</li> <li>Workload Identity: Azure managed identity used by OpenShift components</li> <li>OIDC Issuer: OpenID Connect endpoint used for service account token validation</li> <li>Federated Credential: Trust relationship between Azure Entra ID and OpenShift service accounts</li> <li>External DNS: Kubernetes operator that automatically manages DNS records</li> <li>Management Cluster: Kubernetes cluster that hosts HyperShift and control planes</li> <li>Persistent Resource Group: Azure resource group containing shared, long-lived resources</li> </ul>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#appendix-f-dns-configuration-reference","title":"Appendix F: DNS Configuration Reference","text":"<p>This appendix explains how the DNS-related flags affect cluster creation and operation.</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#overview_3","title":"Overview","text":"<p>HyperShift uses three DNS-related configuration flags that control different aspects of DNS management:</p> <ol> <li><code>--base-domain</code> - Determines the ingress (apps) domain for application routes</li> <li><code>--external-dns-domain</code> - Controls control plane service publishing and DNS record creation</li> <li><code>--external-dns-domain-filter</code> - Restricts External DNS to a specific domain scope</li> </ol>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#flag-details","title":"Flag Details","text":""},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#-base-domain-cluster-creation","title":"<code>--base-domain</code> (Cluster Creation)","text":"<p>Used in: <code>hypershift create cluster azure</code> command</p> <p>Purpose: Constructs the ingress domain where application routes are exposed.</p> <p>Formula: <pre><code>Ingress Domain = \"apps.\" + BaseDomain\n\nWhere BaseDomain is:\n- \"&lt;cluster-name&gt;.&lt;base-domain&gt;\"           (default)\n- \"&lt;base-domain-prefix&gt;.&lt;base-domain&gt;\"     (if --base-domain-prefix is set)\n- \"&lt;base-domain&gt;\"                          (if --base-domain-prefix is empty)\n</code></pre></p> <p>Example: <pre><code>--base-domain \"example.com\"\n--name \"mycluster\"\n\n# Results in:\n# - BaseDomain: mycluster.example.com\n# - Ingress Domain: apps.mycluster.example.com\n# - Application routes: myapp-default.apps.mycluster.example.com\n</code></pre></p> <p>Requirement: Always required for all clusters (with or without External DNS)</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#-external-dns-domain-cluster-creation","title":"<code>--external-dns-domain</code> (Cluster Creation)","text":"<p>Used in: <code>hypershift create cluster azure</code> command</p> <p>Purpose: Changes service publishing strategy and sets control plane hostnames.</p> <p>Effect on Service Publishing:</p> Configuration API Server OAuth Konnectivity Ingress Result Without <code>--external-dns-domain</code> LoadBalancer Route Route Route Azure creates LoadBalancer with Azure-provided DNS (e.g., <code>*.eastus.cloudapp.azure.com</code>) With <code>--external-dns-domain</code> Route Route Route Route External DNS creates DNS records in your managed zone <p>Hostnames Created (when set): <pre><code>API Server:    api-&lt;cluster-name&gt;.&lt;external-dns-domain&gt;\nOAuth Server:  oauth-&lt;cluster-name&gt;.&lt;external-dns-domain&gt;\nKonnectivity:  konnectivity-&lt;cluster-name&gt;.&lt;external-dns-domain&gt;\nIgnition:      ignition-&lt;cluster-name&gt;.&lt;external-dns-domain&gt;\n</code></pre></p> <p>Example: <pre><code>--external-dns-domain \"myprefix.example.com\"\n--name \"mycluster\"\n\n# Results in:\n# - API Server: api-mycluster.myprefix.example.com\n# - OAuth Server: oauth-mycluster.myprefix.example.com\n# - Publishing: All services use Route (not LoadBalancer)\n</code></pre></p> <p>Requirement: Optional - only needed when using External DNS for automatic DNS management</p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#-external-dns-domain-filter-hypershift-operator-install","title":"<code>--external-dns-domain-filter</code> (HyperShift Operator Install)","text":"<p>Used in: <code>hypershift install</code> command</p> <p>Purpose: Restricts External DNS to only manage DNS records within a specific domain.</p> <p>Effect: Passed to the External DNS deployment as the <code>--domain-filter</code> argument. External DNS will: - Create DNS records for services/routes matching this domain - Ignore any services/routes outside this domain</p> <p>Example: <pre><code>hypershift install \\\n  --external-dns-provider=azure \\\n  --external-dns-domain-filter \"myprefix.example.com\"\n\n# Results in:\n# - External DNS only manages *.myprefix.example.com records\n# - Services outside this domain are ignored\n# - Prevents accidental DNS record creation in other zones\n</code></pre></p> <p>Requirement: Required when using <code>--external-dns-provider</code></p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#typical-configuration-patterns","title":"Typical Configuration Patterns","text":""},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#pattern-1-with-external-dns-production","title":"Pattern 1: With External DNS (Production)","text":"<p>Operator Install: <pre><code>hypershift install \\\n  --external-dns-provider=azure \\\n  --external-dns-domain-filter \"myprefix.example.com\"\n</code></pre></p> <p>Cluster Creation: <pre><code>hypershift create cluster azure \\\n  --base-domain \"example.com\" \\\n  --external-dns-domain \"myprefix.example.com\" \\\n  --name \"mycluster\"\n</code></pre></p> <p>Resulting DNS Records (created automatically by External DNS): <pre><code>Control Plane (in myprefix.example.com zone):\n  api-mycluster.myprefix.example.com\n  oauth-mycluster.myprefix.example.com\n  konnectivity-mycluster.myprefix.example.com\n  ignition-mycluster.myprefix.example.com\n\nApplications (in example.com zone):\n  *.apps.mycluster.example.com\n</code></pre></p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#pattern-2-without-external-dns-devtest","title":"Pattern 2: Without External DNS (Dev/Test)","text":"<p>Operator Install: <pre><code>hypershift install\n# No --external-dns-* flags\n</code></pre></p> <p>Cluster Creation: <pre><code>hypershift create cluster azure \\\n  --base-domain \"example.com\" \\\n  --name \"mycluster\"\n# No --external-dns-domain flag\n</code></pre></p> <p>Resulting Configuration: <pre><code>Control Plane (Azure LoadBalancer DNS):\n  API Server: mycluster-abc123.eastus.cloudapp.azure.com\n\nApplications (requires manual DNS or wildcard):\n  *.apps.mycluster.example.com (must configure manually)\n</code></pre></p>"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#relationship-between-flags","title":"Relationship Between Flags","text":"graph TB     subgraph \"HyperShift Operator Level\"         EDFilter[\"--external-dns-domain-filter(e.g., myprefix.example.com)\"]     end      subgraph \"Cluster Level\"         BaseDomain[\"--base-domain(e.g., example.com)\"]         EDDomain[\"--external-dns-domain(e.g., myprefix.example.com)\"]     end      subgraph \"Results\"         Apps[\"Apps Domain:apps.mycluster.example.com\"]         API[\"API Domain:api-mycluster.myprefix.example.com\"]         Publishing[\"Service Publishing:Route vs LoadBalancer\"]     end      BaseDomain --&gt; Apps     EDDomain --&gt; API     EDDomain --&gt; Publishing     EDFilter -.-&gt;|restricts| API      style EDFilter fill:#e1f5ff     style EDDomain fill:#e1f5ff     style BaseDomain fill:#fff4e1"},{"location":"how-to/azure/self-managed-azure-getting-started-DEPRECATED/#key-insights","title":"Key Insights","text":"<ol> <li> <p><code>--base-domain</code> is always required: Both External DNS and non-External DNS deployments need this for application routes</p> </li> <li> <p><code>--external-dns-domain</code> changes publishing strategy: When set, the API server switches from LoadBalancer to Route, enabling DNS-based access instead of Azure LoadBalancer IPs</p> </li> <li> <p>Domain hierarchy should match: Typically, <code>--external-dns-domain</code> is a subdomain of <code>--base-domain</code>:    <pre><code>--base-domain \"example.com\"\n--external-dns-domain \"myprefix.example.com\"\n</code></pre></p> </li> <li> <p>Filter must match domain: The <code>--external-dns-domain-filter</code> in the operator install should match the <code>--external-dns-domain</code> used in cluster creation</p> </li> <li> <p>External DNS scope: External DNS only manages records for:</p> </li> <li>Control plane services (when <code>--external-dns-domain</code> is set)</li> <li>Application routes with publishing type Route</li> <li>Only within the domain specified by <code>--external-dns-domain-filter</code></li> </ol>"},{"location":"how-to/azure/self-managed-azure-index/","title":"Self-Managed Azure HyperShift Overview","text":"<p>Developer Preview in OCP 4.21</p> <p>Self-managed Azure HostedClusters are available as a Developer Preview feature in OpenShift Container Platform 4.21.</p>"},{"location":"how-to/azure/self-managed-azure-index/#introduction","title":"Introduction","text":"<p>Self-managed Azure HyperShift enables you to deploy and manage OpenShift hosted control planes on an OpenShift management cluster running in Azure. The key difference from managed Azure HyperShift deployments is the management cluster platform: self-managed uses an OpenShift cluster you provision and manage, while managed Azure uses Azure Kubernetes Service (AKS).</p>"},{"location":"how-to/azure/self-managed-azure-index/#architecture-overview","title":"Architecture Overview","text":"<p>Self-managed Azure HyperShift deployments consist of three key layers:</p> <ol> <li>Management Cluster: An existing Azure OpenShift cluster that hosts the HyperShift operator and control planes for your hosted clusters</li> <li>Control Plane: Kubernetes control plane components running as pods on the management cluster</li> <li>Data Plane: Worker nodes running as Azure Virtual Machines in your Azure subscription</li> </ol> <p>The architecture uses Azure Workload Identity for secure, credential-free authentication between OpenShift components and Azure services. This eliminates the need to manage long-lived service principal credentials and provides better security through federated identity credentials.</p>"},{"location":"how-to/azure/self-managed-azure-index/#key-differences-from-managed-azure","title":"Key Differences from Managed Azure","text":"<p>Unlike managed Azure HyperShift deployments, self-managed Azure:</p> <ul> <li>Uses an OpenShift cluster as the management platform instead of AKS</li> <li>Requires you to provision and manage the lifecycle of the management cluster</li> </ul>"},{"location":"how-to/azure/self-managed-azure-index/#dns-management-options","title":"DNS Management Options","text":"<p>Self-managed Azure HyperShift supports two DNS management approaches, and both are documented in the same guides:</p> Aspect With External DNS Without External DNS Best For Production, multi-cluster Development, testing API Server DNS Custom (e.g., <code>api-cluster.example.com</code>) Azure LoadBalancer (e.g., <code>abc123.region.cloudapp.azure.com</code>) Setup Complexity Higher (requires DNS zones, service principal) Lower (minimal configuration) Management Fully automatic Manual or Azure-provided <p>Both Approaches in One Guide</p> <p>Our setup guides include instructions for both approaches. Simply follow the sections that match your choice:</p> <ul> <li>With External DNS: Follow all sections including DNS Zone Configuration and External DNS Service Principal Setup</li> <li>Without External DNS: Skip DNS sections and use the simplified operator installation command</li> </ul>"},{"location":"how-to/azure/self-managed-azure-index/#deployment-workflow","title":"Deployment Workflow","text":"<p>Setting up self-managed Azure HyperShift is a three-phase process that must be completed in order:</p>"},{"location":"how-to/azure/self-managed-azure-index/#phase-1-azure-workload-identity-setup","title":"Phase 1: Azure Workload Identity Setup","text":"<p>Purpose: Establish secure authentication infrastructure for OpenShift components to access Azure services.</p> <p>This phase creates the foundational security infrastructure required for your hosted clusters:</p> <ul> <li>Managed Identities: Creates Azure managed identities for each OpenShift component (image registry, ingress, CSI drivers, cloud provider, network operator, etc.)</li> <li>OIDC Issuer: Configures an OIDC issuer in Azure Blob Storage for service account token validation</li> <li>Federated Credentials: Establishes trust relationships between Azure Entra ID and OpenShift service accounts</li> </ul> <p>Why This Matters: Without these identities and federated credentials, your hosted cluster components cannot authenticate with Azure APIs to provision storage, manage load balancers, configure networking, or perform other essential cloud operations. Using workload identities instead of traditional service principals provides better security, automatic credential rotation, and follows Azure's modern authentication best practices.</p> <p>When to Complete: This is a one-time setup that can be reused across multiple hosted clusters. Complete this before proceeding to Phase 2.</p> <p>\ud83d\udc49 Guide: Azure Workload Identity Setup</p>"},{"location":"how-to/azure/self-managed-azure-index/#phase-2-management-cluster-setup","title":"Phase 2: Management Cluster Setup","text":"<p>Purpose: Prepare your Azure OpenShift management cluster to host and manage HyperShift control planes.</p> <p>This phase installs the HyperShift operator with optional External DNS configuration:</p> <ul> <li>DNS Zone Configuration (Optional): Creates Azure DNS zones for automatic DNS record management</li> <li>External DNS (Optional): Sets up a service principal and deploys ExternalDNS operator</li> <li>HyperShift Operator: Installs the HyperShift operator</li> </ul> <p>Why This Matters: The HyperShift operator orchestrates the entire lifecycle of hosted control planes. If you choose External DNS, it automatically provisions DNS records for each hosted cluster's endpoints.</p> <p>When to Complete: After Phase 1 is complete, but before creating any hosted clusters.</p> <p>\ud83d\udc49 Guide: Setup Azure Management Cluster for HyperShift - Includes both DNS approaches</p>"},{"location":"how-to/azure/self-managed-azure-index/#phase-3-create-hosted-clusters","title":"Phase 3: Create Hosted Clusters","text":"<p>Purpose: Deploy self-managed Azure hosted clusters with custom networking and configuration.</p> <p>This phase creates your actual hosted OpenShift clusters:</p> <ul> <li>Infrastructure Provisioning: Creates resource groups, VNets, subnets, and network security groups</li> <li>HostedCluster Creation: Deploys the control plane on the management cluster and worker nodes in your Azure subscription</li> <li>Workload Identity Integration: Links the hosted cluster to the workload identities created in Phase 1</li> </ul> <p>Why This Matters: This is where you deploy the actual OpenShift clusters that your applications will run on. Each hosted cluster gets its own control plane running on the management cluster and its own set of worker node VMs in Azure. The cluster uses the workload identities from Phase 1 to securely access Azure services without storing credentials.</p> <p>When to Complete: After Phases 1 and 2 are fully configured and verified.</p> <p>\ud83d\udc49 Guide: Create a Self-Managed Azure HostedCluster - Includes both DNS approaches</p>"},{"location":"how-to/azure/self-managed-azure-index/#prerequisites-summary","title":"Prerequisites Summary","text":"<p>Before beginning the deployment process, ensure you have:</p> <ul> <li> <p>Azure Resources:</p> <ul> <li>An existing Azure OpenShift management cluster</li> <li>Azure subscription with appropriate permissions (Contributor + User Access Administrator)</li> <li>(Optional) A parent DNS zone in Azure DNS for delegating cluster DNS records (required only if using External DNS)</li> </ul> </li> <li> <p>Tools and Access:</p> <ul> <li>Azure CLI (<code>az</code>) configured with your subscription</li> <li>OpenShift CLI (<code>oc</code>) or Kubernetes CLI (<code>kubectl</code>)</li> <li>HyperShift CLI binary</li> <li><code>jq</code> command-line JSON processor</li> <li>Cloud Credential Operator (CCO) tool</li> <li>Valid OpenShift pull secret</li> </ul> </li> <li> <p>Permissions:</p> <ul> <li>Subscription-level Contributor and User Access Administrator roles</li> <li>Microsoft Graph API permissions (Application.ReadWrite.OwnedBy) for creating service principals</li> </ul> </li> </ul>"},{"location":"how-to/azure/self-managed-azure-index/#resource-group-strategy","title":"Resource Group Strategy","text":"<p>Self-managed Azure deployments use multiple resource groups with different lifecycles:</p> <ul> <li> <p>Persistent Resource Group (e.g., <code>os4-common</code>): Long-lived resources shared across multiple clusters</p> <ul> <li>Workload identities (managed identities)</li> <li>OIDC issuer storage account</li> <li>Azure DNS zones (if using External DNS)</li> <li>External DNS service principal (if using External DNS)</li> </ul> </li> <li> <p>Cluster-Specific Resource Groups: Created and destroyed with each hosted cluster</p> <ul> <li>Managed resource group for cluster infrastructure</li> <li>VNet resource group (if using custom networking)</li> <li>NSG resource group (if using custom networking)</li> </ul> </li> </ul> <p>Resource Reuse</p> <p>By placing workload identities and OIDC issuer in a persistent resource group, you can:</p> <ul> <li>Reuse the same identities across multiple hosted clusters</li> <li>Avoid recreating federated credentials for each cluster</li> <li>Reduce cluster creation time</li> <li>Simplify cleanup when deleting individual clusters</li> </ul>"},{"location":"how-to/azure/self-managed-azure-index/#security-considerations","title":"Security Considerations","text":"<p>Self-managed Azure HyperShift implements several security best practices:</p> <ol> <li>Workload Identity Federation: Eliminates long-lived credentials by using OIDC-based authentication</li> <li>Least Privilege Access: Each component gets its own managed identity with minimal required permissions</li> <li>Network Isolation: Custom VNets and NSGs allow you to implement network segmentation and security policies</li> <li>Federated Credentials: Trust relationships are scoped to specific service accounts, preventing unauthorized access</li> </ol>"},{"location":"how-to/azure/self-managed-azure-index/#next-steps","title":"Next Steps","text":"<p>Begin your self-managed Azure HyperShift deployment by following the guides in order:</p> <ol> <li>Azure Workload Identity Setup - Set up managed identities and OIDC federation</li> <li>Setup Azure Management Cluster for HyperShift - Install HyperShift operator (with or without External DNS)</li> <li>Create a Self-Managed Azure HostedCluster - Deploy your first hosted cluster</li> </ol> <p>Each guide includes sections for both DNS approaches - simply follow the sections that match your choice.</p>"},{"location":"how-to/azure/self-managed-azure-index/#additional-resources","title":"Additional Resources","text":"<ul> <li>Azure Workload Identity Documentation</li> <li>HyperShift Architecture Reference</li> <li>Managed Azure Documentation - For comparison with AKS-based deployments</li> </ul>"},{"location":"how-to/azure/setup-management-cluster/","title":"Setup Azure Management Cluster for HyperShift","text":"<p>Developer Preview in OCP 4.21</p> <p>Self-managed Azure HostedClusters are available as a Developer Preview feature in OpenShift Container Platform 4.21.</p> <p>This document describes how to install the HyperShift operator on your Azure management cluster with optional External DNS configuration.</p>"},{"location":"how-to/azure/setup-management-cluster/#dns-management-options","title":"DNS Management Options","text":"<p>You can set up your management cluster with or without External DNS:</p> <p>Choose Your Approach</p> <p>With External DNS (Recommended for Production)</p> <ul> <li>Automatic DNS record management</li> <li>Custom domain names (e.g., <code>api-cluster.example.com</code>)</li> <li>Requires: DNS zones, service principal, External DNS operator</li> <li>Follow sections: DNS Zone Configuration \u2192 External DNS Service Principal Setup \u2192 HyperShift Operator Installation (With External DNS)</li> </ul> <p>Without External DNS (Simpler for Dev/Test)</p> <ul> <li>Manual or Azure-provided DNS</li> <li>API server uses Azure LoadBalancer DNS (e.g., <code>abc123.region.cloudapp.azure.com</code>)</li> <li>No DNS zones or service principals needed</li> <li>Skip to: HyperShift Operator Installation (Without External DNS)</li> </ul>"},{"location":"how-to/azure/setup-management-cluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure CLI (<code>az</code>) installed and configured</li> <li>OpenShift CLI (<code>oc</code>) or Kubernetes CLI (<code>kubectl</code>)</li> <li>An Azure OpenShift management cluster</li> <li>For External DNS only: <code>jq</code> command-line JSON processor</li> </ul>"},{"location":"how-to/azure/setup-management-cluster/#dns-zone-configuration-external-dns-only","title":"DNS Zone Configuration (External DNS Only)","text":"<p>External DNS Only</p> <p>This section is only required if you want automatic DNS management with External DNS. If you prefer a simpler setup, skip to HyperShift Operator Installation (Without External DNS).</p> <p>Before creating HostedClusters with External DNS, you need to set up DNS zones and delegate DNS records for your clusters.</p> <p>About PERSISTENT_RG_NAME</p> <p>In Red Hat environments, a periodic Azure resource \"reaper\" deletes resources that are not properly tagged or not located in an approved resource group. We frequently use the <code>os4-common</code> resource group for shared, long-lived assets (for example, public DNS zones) to avoid accidental cleanup. If you are not in Red Hat infrastructure, set <code>PERSISTENT_RG_NAME</code> to any long-lived resource group in your subscription that will not be automatically reaped, or ensure your organization\u2019s required tags/policies are applied. The name does not have to be <code>os4-common</code>\u2014use whatever persistent resource group fits your environment.</p> <pre><code># Set DNS configuration variables\nPARENT_DNS_RG=\"your-parent-dns-rg\"\nPARENT_DNS_ZONE=\"your-parent.dns.zone.com\"\nDNS_RECORD_NAME=\"your-subdomain\"\nPERSISTENT_RG_NAME=\"os4-common\"  # Use persistent resource group\nDNS_ZONE_NAME=\"your-subdomain.your-parent.dns.zone.com\"\n\naz group create \\  \n      --name $PERSISTENT_RG_NAME \\  \n      --location $LOCATION  \n\naz network dns zone create \\  \n        --resource-group $PERSISTENT_RG_NAME \\  \n        --name $DNS_ZONE_NAME  \n\n# Delete existing NS record if it exists\naz network dns record-set ns delete \\\n    --resource-group $PARENT_DNS_RG \\\n    --zone-name $PARENT_DNS_ZONE \\\n    --name $DNS_RECORD_NAME -y\n\n# Get name servers from your DNS zone\nname_servers=$(az network dns zone show \\\n    --resource-group $PERSISTENT_RG_NAME \\\n    --name $DNS_ZONE_NAME \\\n    --query nameServers \\\n    --output tsv)\n\n# Create array of name servers\nns_array=()\nwhile IFS= read -r ns; do\n    ns_array+=(\"$ns\")\ndone &lt;&lt;&lt; \"$name_servers\"\n\n# Add NS records to parent zone\nfor ns in \"${ns_array[@]}\"; do\n    az network dns record-set ns add-record \\\n        --resource-group $PARENT_DNS_RG \\\n        --zone-name $PARENT_DNS_ZONE \\\n        --record-set-name $DNS_RECORD_NAME \\\n        --nsdname \"$ns\"\ndone\n</code></pre>"},{"location":"how-to/azure/setup-management-cluster/#external-dns-service-principal-setup-external-dns-only","title":"External DNS Service Principal Setup (External DNS Only)","text":"<p>External DNS Only</p> <p>This section is only required if you're using External DNS. Skip to HyperShift Operator Installation (Without External DNS) if not.</p> <p>Create a dedicated service principal for External DNS:</p> <pre><code># Set External DNS configuration\nEXTERNAL_DNS_NEW_SP_NAME=\"ExternalDnsServicePrincipal\"\nSERVICE_PRINCIPAL_FILEPATH=\"/path/to/azure_mgmt.json\"\nPERSISTENT_RG_NAME=\"hypershift-shared-resources\"  # Use persistent resource group\n\n# Create service principal for External DNS\nDNS_SP=$(az ad sp create-for-rbac --name ${EXTERNAL_DNS_NEW_SP_NAME})\nEXTERNAL_DNS_SP_APP_ID=$(echo \"$DNS_SP\" | jq -r '.appId')\nEXTERNAL_DNS_SP_PASSWORD=$(echo \"$DNS_SP\" | jq -r '.password')\n\n# Get DNS zone ID\nDNS_ID=$(az network dns zone show \\\n    --name ${DNS_ZONE_NAME} \\\n    --resource-group ${PERSISTENT_RG_NAME} \\\n    --query \"id\" \\\n    --output tsv)\n\n# Assign roles to the service principal\naz role assignment create \\\n    --role \"Reader\" \\\n    --assignee \"${EXTERNAL_DNS_SP_APP_ID}\" \\\n    --scope \"${DNS_ID}\"\n\naz role assignment create \\\n    --role \"Contributor\" \\\n    --assignee \"${EXTERNAL_DNS_SP_APP_ID}\" \\\n    --scope \"${DNS_ID}\"\n\n# Create Azure credentials file\ncat &lt;&lt;-EOF &gt; ${SERVICE_PRINCIPAL_FILEPATH}\n{\n  \"tenantId\": \"$(az account show --query tenantId -o tsv)\",\n  \"subscriptionId\": \"$(az account show --query id -o tsv)\",\n  \"resourceGroup\": \"$PERSISTENT_RG_NAME\",\n  \"aadClientId\": \"$EXTERNAL_DNS_SP_APP_ID\",\n  \"aadClientSecret\": \"$EXTERNAL_DNS_SP_PASSWORD\"\n}\nEOF\n\n# Create Kubernetes secret for Azure credentials\nkubectl delete secret/azure-config-file --namespace \"default\" || true\nkubectl create secret generic azure-config-file \\\n    --namespace \"default\" \\\n    --from-file ${SERVICE_PRINCIPAL_FILEPATH}\n</code></pre>"},{"location":"how-to/azure/setup-management-cluster/#hypershift-operator-installation","title":"HyperShift Operator Installation","text":"<p>Choose the installation method that matches your DNS approach:</p>"},{"location":"how-to/azure/setup-management-cluster/#with-external-dns","title":"With External DNS","text":"<p>Install the HyperShift operator with External DNS configuration:</p> <pre><code># Set installation variables\nPULL_SECRET=\"/path/to/pull-secret.json\"\nHYPERSHIFT_BINARY_PATH=\"/path/to/hypershift/bin\"\n\n# Install HyperShift operator with External DNS\n${HYPERSHIFT_BINARY_PATH}/hypershift install \\\n    --external-dns-provider=azure \\\n    --external-dns-credentials ${SERVICE_PRINCIPAL_FILEPATH} \\\n    --pull-secret ${PULL_SECRET} \\\n    --external-dns-domain-filter ${DNS_ZONE_NAME} \\\n    --limit-crd-install Azure\n</code></pre> <p>Custom HyperShift Image</p> <p>Add <code>--hypershift-image quay.io/hypershift/hypershift:TAG</code> if using a custom operator image.</p> <p>After installation, verify both the HyperShift operator and External DNS are running:</p> <pre><code># Check HyperShift operator and ExternalDNS pods\noc get pods -n hypershift\n</code></pre> <p>You should see both <code>operator-*</code> and <code>external-dns-*</code> pods running.</p>"},{"location":"how-to/azure/setup-management-cluster/#without-external-dns","title":"Without External DNS","text":"<p>Install the HyperShift operator without External DNS:</p> <pre><code># Set installation variables\nPULL_SECRET=\"/path/to/pull-secret.json\"\nHYPERSHIFT_BINARY_PATH=\"/path/to/hypershift/bin\"\n\n# Install HyperShift operator (no External DNS)\n${HYPERSHIFT_BINARY_PATH}/hypershift install \\\n    --pull-secret ${PULL_SECRET} \\\n    --limit-crd-install Azure\n</code></pre> <p>Custom HyperShift Image</p> <p>Add <code>--hypershift-image quay.io/hypershift/hypershift:TAG</code> if using a custom operator image.</p> <p>After installation, verify the HyperShift operator is running:</p> <pre><code># Check HyperShift operator pods\noc get pods -n hypershift\n</code></pre> <p>You should see the <code>operator-*</code> pod running (no <code>external-dns</code> pod).</p>"},{"location":"how-to/azure/setup-management-cluster/#verification","title":"Verification","text":"<p>Verify your installation:</p> With External DNSWithout External DNS <pre><code># Check for both operator and external-dns\noc get pods -n hypershift\n\n# Expected output:\n# NAME                           READY   STATUS    RESTARTS   AGE\n# external-dns-xxxxx-xxxxx       1/1     Running   0          1m\n# operator-xxxxx-xxxxx           1/1     Running   0          1m\n</code></pre> <pre><code># Check for operator only\noc get pods -n hypershift\n\n# Expected output:\n# NAME                     READY   STATUS    RESTARTS   AGE\n# operator-xxxxx-xxxxx     1/1     Running   0          1m\n</code></pre>"},{"location":"how-to/azure/setup-management-cluster/#next-steps","title":"Next Steps","text":"<p>Once the management cluster is set up, create hosted clusters:</p> <ul> <li>Create a Self-Managed Azure HostedCluster - Includes guidance for both DNS approaches</li> </ul>"},{"location":"how-to/azure/self-managed/01-understanding/","title":"Understanding HyperShift on Azure","text":"<p>Developer Preview in OCP 4.21</p> <p>Self-managed Azure HostedClusters are available as a Developer Preview feature in OpenShift Container Platform 4.21.</p>"},{"location":"how-to/azure/self-managed/01-understanding/#what-is-self-managed-azure-hypershift","title":"What is Self-Managed Azure HyperShift?","text":"<p>Self-managed Azure HyperShift enables you to deploy and manage OpenShift hosted control planes on an OpenShift management cluster running in Azure. This architecture allows you to:</p> <ul> <li>Reduce costs: Run multiple OpenShift control planes as pods on a shared management cluster</li> <li>Improve density: Host dozens of control planes on the same infrastructure</li> <li>Simplify management: Centralize control plane operations while isolating workload data planes</li> <li>Increase flexibility: Choose your own management cluster platform and customize networking</li> </ul>"},{"location":"how-to/azure/self-managed/01-understanding/#architecture-overview","title":"Architecture Overview","text":""},{"location":"how-to/azure/self-managed/01-understanding/#high-level-architecture","title":"High-Level Architecture","text":"<p>Self-managed Azure HyperShift deployments consist of three key layers:</p> <ol> <li>Management Cluster: An existing OpenShift cluster running in Azure that hosts the HyperShift operator and control plane pods for your hosted clusters</li> <li>Control Plane: Kubernetes control plane components (API server, etcd, controllers) running as pods on the management cluster</li> <li>Data Plane: Worker nodes running as Azure Virtual Machines in your Azure subscription, managed by the control plane</li> </ol> graph TB     subgraph Azure[\"Azure Subscription\"]         direction TB          subgraph MgmtCluster[\"Management Cluster (OpenShift)\"]             direction LR             Operator[\"HyperShiftOperator\"]             ClusterACP[\"Cluster AControl Plane\"]             ClusterBCP[\"Cluster BControl Plane\"]         end          WorkerA[\"Cluster A Worker Nodes(Azure VMs)\"]         WorkerB[\"Cluster B Worker Nodes(Azure VMs)\"]          subgraph Shared[\"Shared Azure Resources\"]             SharedItems[\"\u2022 Workload Identities (Managed Identities)\u2022 OIDC Issuer (Blob Storage)\u2022 DNS Zones (Optional)\"]         end          ClusterACP -.-&gt;|manages| WorkerA         ClusterBCP -.-&gt;|manages| WorkerB     end"},{"location":"how-to/azure/self-managed/01-understanding/#key-components","title":"Key Components","text":""},{"location":"how-to/azure/self-managed/01-understanding/#management-cluster","title":"Management Cluster","text":"<ul> <li>Must be an OpenShift cluster running in Azure</li> <li>Hosts the HyperShift operator which manages the lifecycle of hosted clusters</li> <li>Runs control plane pods for multiple hosted clusters</li> <li>Must have sufficient capacity for control plane workloads</li> </ul>"},{"location":"how-to/azure/self-managed/01-understanding/#hosted-control-plane","title":"Hosted Control Plane","text":"<ul> <li>Runs as a set of pods on the management cluster</li> <li>Includes API server, etcd, controller manager, scheduler</li> <li>Isolated per hosted cluster for security and multi-tenancy</li> <li>Communicates with worker nodes via Azure networking</li> </ul>"},{"location":"how-to/azure/self-managed/01-understanding/#worker-nodes","title":"Worker Nodes","text":"<ul> <li>Standard Azure Virtual Machines in your subscription</li> <li>Join the hosted cluster via machine provisioning</li> <li>Run your application workloads</li> <li>Managed by the control plane components</li> </ul>"},{"location":"how-to/azure/self-managed/01-understanding/#authentication-identity","title":"Authentication &amp; Identity","text":"<ul> <li>Uses Azure Workload Identity for secure, credential-free authentication</li> <li>Federated identity credentials enable OpenShift service accounts to authenticate with Azure APIs</li> <li>Eliminates the need for long-lived service principal credentials</li> <li>Each OpenShift component (storage, networking, etc.) gets its own managed identity with minimal required permissions</li> </ul>"},{"location":"how-to/azure/self-managed/01-understanding/#security-architecture","title":"Security Architecture","text":"<p>Self-managed Azure HyperShift implements several security best practices:</p> <ol> <li>Workload Identity Federation: Uses OIDC-based authentication to eliminate long-lived credentials</li> <li>Least Privilege Access: Each component gets its own managed identity with minimal required permissions</li> <li>Network Isolation: Custom VNets and NSGs allow you to implement network segmentation and security policies</li> <li>Federated Credentials: Trust relationships are scoped to specific service accounts, preventing unauthorized access</li> <li>Control Plane Isolation: Each hosted cluster's control plane runs in isolated pods on the management cluster</li> </ol>"},{"location":"how-to/azure/self-managed/01-understanding/#resource-lifecycle","title":"Resource Lifecycle","text":"<p>Understanding the lifecycle of Azure resources is critical for proper management:</p>"},{"location":"how-to/azure/self-managed/01-understanding/#one-time-shared-resources","title":"One-Time Shared Resources","text":"<p>These resources are created once and shared across all hosted clusters:</p> <ul> <li>OIDC Issuer (Azure Blob Storage): Provides the OpenID Connect endpoint for workload identity federation</li> <li>Service Account Signing Keys: Used to sign service account tokens for workload identity</li> </ul>"},{"location":"how-to/azure/self-managed/01-understanding/#per-cluster-persistent-resources","title":"Per-Cluster Persistent Resources","text":"<p>These resources are created per hosted cluster and persist even after cluster deletion:</p> <ul> <li>Managed Identities (7 per cluster): One for each OpenShift component (image-registry, ingress, disk-csi, file-csi, nodepool-mgmt, cloud-provider, network)</li> <li>Federated Identity Credentials (14 per cluster): Link managed identities to specific OpenShift service accounts</li> </ul> <p>Why Persistent?</p> <p>Managed identities and federated credentials persist after cluster deletion to:</p> <ul> <li>Allow cluster recreation without recreating identities</li> <li>Maintain Azure IAM role assignments</li> <li>Preserve audit trails and access logs</li> <li>Support disaster recovery scenarios</li> </ul>"},{"location":"how-to/azure/self-managed/01-understanding/#per-cluster-temporary-resources","title":"Per-Cluster Temporary Resources","text":"<p>These resources are created and deleted with each hosted cluster:</p> <ul> <li>Virtual Network (VNet): Provides network isolation for cluster resources</li> <li>Network Security Group (NSG): Controls network traffic rules</li> <li>Subnets: Network segments within the VNet</li> <li>Resource Groups: Container for cluster networking resources</li> <li>Virtual Machines: Worker nodes for the cluster</li> </ul>"},{"location":"how-to/azure/self-managed/01-understanding/#when-to-use-hypershift","title":"When to Use HyperShift","text":"<p>HyperShift is ideal for:</p> <ul> <li>Multi-tenancy scenarios: Providing isolated OpenShift clusters for different teams or customers</li> <li>Development/testing environments: Quickly spinning up and tearing down clusters</li> <li>Edge deployments: Running lightweight control planes with distributed worker nodes</li> <li>Cost optimization: Consolidating control planes to reduce infrastructure costs</li> <li>Simplified operations: Managing multiple clusters from a single management plane</li> </ul>"},{"location":"how-to/azure/self-managed/01-understanding/#how-it-differs-from-standalone-openshift","title":"How it Differs from Standalone OpenShift","text":"Aspect Standalone OpenShift HyperShift Control Plane Runs on dedicated master nodes Runs as pods on management cluster Infrastructure Requires dedicated control plane VMs Shares management cluster resources Deployment Time Longer (provisions control plane VMs) Faster (deploys control plane pods) Density One cluster per set of master nodes Many clusters per management cluster Management Each cluster managed independently Centralized management via HyperShift operator Cost Higher (dedicated control plane infra) Lower (shared control plane infra) Isolation Network and compute isolation Pod-level isolation in management cluster"},{"location":"how-to/azure/self-managed/01-understanding/#next-steps","title":"Next Steps","text":"<p>Now that you understand the architecture, proceed to:</p> <ul> <li>Planning Your Deployment - Determine your prerequisites and make key decisions</li> <li>Azure Foundation Setup - Set up the Azure infrastructure components</li> </ul>"},{"location":"how-to/azure/self-managed/02-planning/","title":"Planning Your Deployment","text":"<p>This guide helps you prepare for a successful HyperShift on Azure deployment by outlining prerequisites, key decisions, and planning considerations.</p>"},{"location":"how-to/azure/self-managed/02-planning/#target-audience","title":"Target Audience","text":"<p>This guide is designed for veteran OpenShift administrators and SREs who are new to HyperShift. We assume you have:</p> <ul> <li>Strong knowledge of OpenShift/Kubernetes operations</li> <li>Experience with Azure infrastructure and services</li> <li>Understanding of networking, storage, and identity management concepts</li> </ul>"},{"location":"how-to/azure/self-managed/02-planning/#prerequisites","title":"Prerequisites","text":""},{"location":"how-to/azure/self-managed/02-planning/#azure-requirements","title":"Azure Requirements","text":""},{"location":"how-to/azure/self-managed/02-planning/#azure-subscription-permissions","title":"Azure Subscription &amp; Permissions","text":"<ul> <li>Active Azure subscription</li> <li>Subscription-level roles:<ul> <li><code>Contributor</code> role</li> <li><code>User Access Administrator</code> role</li> </ul> </li> <li>Microsoft Graph API permissions:<ul> <li><code>Application.ReadWrite.OwnedBy</code> permission (for creating service principals)</li> </ul> </li> </ul>"},{"location":"how-to/azure/self-managed/02-planning/#azure-resources","title":"Azure Resources","text":"<ul> <li>A persistent resource group for shared resources (e.g., <code>hypershift-shared</code>)<ul> <li>This resource group will contain workload identities, OIDC issuer, and optionally DNS zones</li> <li>Should not be deleted when individual hosted clusters are removed</li> </ul> </li> <li>(Optional) Parent DNS zone in Azure DNS for delegating cluster DNS records<ul> <li>Required only if using External DNS for automatic DNS management</li> <li>Must allow NS record delegation for subdomain zones</li> </ul> </li> </ul>"},{"location":"how-to/azure/self-managed/02-planning/#tools-cli","title":"Tools &amp; CLI","text":"<ul> <li>Azure CLI (<code>az</code>) installed and configured with the subscription</li> <li><code>jq</code> command-line JSON processor</li> <li>Cloud Credential Operator (CCO) tool (<code>ccoctl</code>) installed</li> <li>go-task/task for running automation</li> </ul>"},{"location":"how-to/azure/self-managed/02-planning/#management-cluster-requirements","title":"Management Cluster Requirements","text":""},{"location":"how-to/azure/self-managed/02-planning/#cluster-prerequisites","title":"Cluster Prerequisites","text":"<ul> <li>An existing OpenShift cluster running in Azure</li> <li>Cluster must have:<ul> <li>Sufficient capacity for hosting control plane pods</li> <li>Network connectivity to Azure APIs</li> <li>Ability to create LoadBalancer services (if not using External DNS)</li> </ul> </li> <li>Administrative access (<code>cluster-admin</code> permissions) to the management cluster</li> </ul>"},{"location":"how-to/azure/self-managed/02-planning/#tools","title":"Tools","text":"<ul> <li>OpenShift CLI (<code>oc</code>) or Kubernetes CLI (<code>kubectl</code>)</li> <li>HyperShift CLI binary (download)</li> <li>Valid OpenShift pull secret from cloud.redhat.com</li> </ul>"},{"location":"how-to/azure/self-managed/02-planning/#required-configuration-files","title":"Required Configuration Files","text":"<p>You'll need these files during deployment:</p> File Purpose Created By Format azure-credentials.json Cluster operations (create/destroy) You or Azure admin Service principal credentials workload-identities.json Cluster component authentication <code>task azure:identities</code> Managed identity client IDs serviceaccount-signer.private/public Service account token signing <code>task prereq:keys</code> RSA key pair .azure-net-ids Network resource IDs <code>task azure:infra</code> Exported environment variables <p>Azure Credentials File Format</p> <p>The <code>azure-credentials.json</code> file should contain service principal credentials:</p> <pre><code>{\n  \"subscriptionId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n  \"tenantId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n  \"clientId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n  \"clientSecret\": \"your-service-principal-secret\"\n}\n</code></pre> <p>How to create:</p> <pre><code># Create service principal with required permissions\nSP_DETAILS=$(az ad sp create-for-rbac \\\n    --name \"hypershift-cluster-ops\" \\\n    --role Contributor \\\n    --scopes \"/subscriptions/$(az account show --query id -o tsv)\")\n\n# Extract values and create file\ncat &lt;&lt;EOF &gt; azure-credentials.json\n{\n  \"subscriptionId\": \"$(az account show --query id -o tsv)\",\n  \"tenantId\": \"$(az account show --query tenantId -o tsv)\",\n  \"clientID\": \"$(echo \"$SP_DETAILS\" | jq -r '.appId')\",\n  \"clientSecret\": \"$(echo \"$SP_DETAILS\" | jq -r '.password')\"\n}\nEOF\n</code></pre>"},{"location":"how-to/azure/self-managed/02-planning/#key-planning-decisions","title":"Key Planning Decisions","text":""},{"location":"how-to/azure/self-managed/02-planning/#dns-management-strategy","title":"DNS Management Strategy","text":"<p>Choose how to handle DNS for your hosted clusters:</p>"},{"location":"how-to/azure/self-managed/02-planning/#option-1-with-external-dns-recommended-for-production","title":"Option 1: With External DNS (Recommended for Production)","text":"<p>Best For: Production environments, multiple clusters, custom domains</p> <p>Characteristics: - Automatic DNS record management via External DNS operator - Custom domain names (e.g., <code>api.my-cluster.example.com</code>) - Requires: DNS zones, service principal with DNS permissions, External DNS operator - Higher initial setup complexity, but simpler ongoing operations</p> <p>Example DNS: - API Server: <code>api.my-cluster.azure.example.com</code> - Apps: <code>*.apps.my-cluster.azure.example.com</code></p> <p>Decision Criteria: - Choose this if you need custom, branded domain names - Choose this if you plan to manage multiple clusters - Choose this if you want fully automated DNS provisioning</p>"},{"location":"how-to/azure/self-managed/02-planning/#option-2-without-external-dns-simpler-for-devtest","title":"Option 2: Without External DNS (Simpler for Dev/Test)","text":"<p>Best For: Development, testing, proof-of-concept environments</p> <p>Characteristics: - Manual DNS management or Azure-provided LoadBalancer DNS - API server uses Azure LoadBalancer DNS (e.g., <code>abc123.eastus.cloudapp.azure.com</code>) - No DNS zones or service principals needed - Lower initial setup complexity, but requires manual DNS work for production use</p> <p>Example DNS: - API Server: <code>my-cluster-api.eastus.cloudapp.azure.com</code> - Apps: <code>my-cluster-apps.eastus.cloudapp.azure.com</code></p> <p>Decision Criteria: - Choose this for quick testing or POC environments - Choose this if you don't control your DNS infrastructure - Choose this if you only need a single cluster temporarily</p> <p>Recommendation</p> <p>Start with Option 2 (Without External DNS) for your first cluster to learn the basics. Once comfortable, you can deploy production clusters with Option 1 (External DNS).</p>"},{"location":"how-to/azure/self-managed/02-planning/#resource-group-strategy","title":"Resource Group Strategy","text":"<p>Plan your resource group structure to separate long-lived shared resources from cluster-specific resources:</p>"},{"location":"how-to/azure/self-managed/02-planning/#persistent-resource-group","title":"Persistent Resource Group","text":"<ul> <li>Name example: <code>hypershift-shared</code>, <code>openshift-common</code></li> <li>Lifecycle: Long-lived, not deleted when clusters are removed</li> <li>Contains:<ul> <li>OIDC issuer (storage account)</li> <li>Managed identities (per cluster, but persistent)</li> <li>Federated identity credentials (per cluster, but persistent)</li> <li>(Optional) DNS zones</li> </ul> </li> </ul>"},{"location":"how-to/azure/self-managed/02-planning/#per-cluster-resource-groups","title":"Per-Cluster Resource Groups","text":"<ul> <li>Name pattern: <code>&lt;prefix&gt;-vnet-rg</code>, <code>&lt;prefix&gt;-nsg-rg</code>, <code>&lt;prefix&gt;-managed-rg</code></li> <li>Lifecycle: Created and deleted with each cluster</li> <li>Contains:<ul> <li>Virtual Network (VNet)</li> <li>Network Security Group (NSG)</li> <li>Subnets</li> <li>Virtual Machines (worker nodes)</li> </ul> </li> </ul> <p>Understanding Resource Persistence</p> <p>Managed identities and federated credentials are per-cluster but persistent. They:</p> <ul> <li>Are created once per cluster</li> <li>Remain after cluster deletion</li> <li>Allow cluster recreation without identity recreation</li> <li>Preserve Azure IAM role assignments</li> <li>Support disaster recovery scenarios</li> </ul> <p>Network infrastructure is temporary and deleted with the cluster.</p>"},{"location":"how-to/azure/self-managed/02-planning/#naming-conventions","title":"Naming Conventions","text":"<p>Establish consistent naming patterns:</p> <pre><code># Example naming pattern\nPREFIX=\"myorg-dev\"\nCLUSTER_NAME=\"${PREFIX}-hc\"           # e.g., myorg-dev-hc\nPERSISTENT_RG=\"hypershift-shared\"     # Shared across all clusters\nVNET_RG_NAME=\"${PREFIX}-vnet-rg\"      # e.g., myorg-dev-vnet-rg\n</code></pre>"},{"location":"how-to/azure/self-managed/02-planning/#network-planning","title":"Network Planning","text":""},{"location":"how-to/azure/self-managed/02-planning/#vnet-and-subnet-design","title":"VNet and Subnet Design","text":"<ul> <li>Default: <code>/16</code> VNet with <code>/24</code> subnet</li> <li>Plan for sufficient IP addresses for worker nodes</li> <li>Consider network security group (NSG) rules for your environment</li> <li>Ensure network connectivity to Azure APIs and management cluster</li> </ul>"},{"location":"how-to/azure/self-managed/02-planning/#network-security","title":"Network Security","text":"<ul> <li>NSG rules are created automatically</li> <li>Review and customize based on your security requirements</li> <li>Consider private endpoint requirements for production</li> </ul>"},{"location":"how-to/azure/self-managed/02-planning/#configuration-taskfile-vs-environment-variables","title":"Configuration: Taskfile vs Environment Variables","text":"<p>This repository uses Taskfile automation as the primary approach:</p>"},{"location":"how-to/azure/self-managed/02-planning/#using-taskfile-variables-recommended","title":"Using Taskfile Variables (Recommended)","text":"<p>All configuration is defined in <code>hack/dev-preview/Taskfile.yml</code>:</p> <pre><code>vars:\n  PREFIX: 'myorg-dev'\n  SUBSCRIPTION_ID: 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n  TENANT_ID: 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n  LOCATION: 'eastus'\n  PERSISTENT_RG_NAME: 'hypershift-shared'\n  OIDC_STORAGE_ACCOUNT_NAME: 'myorgoidc1234567890'\n  # ... etc\n</code></pre> <p>This approach provides: - Self-documenting configuration: All variables visible in one file - Validation: Taskfile validates required variables - Consistency: Same variables used across all tasks</p>"},{"location":"how-to/azure/self-managed/02-planning/#using-envrc-optional-convenience","title":"Using .envrc (Optional Convenience)","text":"<p>An <code>envrc.example</code> file is provided for convenience with tools like direnv:</p> <pre><code>export PREFIX=\"myorg-dev\"\nexport SUBSCRIPTION_ID=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n# ... etc\n</code></pre> <p>Primary vs Secondary</p> <ul> <li>Primary: Edit variables in <code>Taskfile.yml</code></li> <li>Secondary: Use <code>.envrc</code> for convenience if you prefer environment variables</li> <li>The Taskfile reads from environment variables when available</li> </ul>"},{"location":"how-to/azure/self-managed/02-planning/#deployment-flow","title":"Deployment Flow","text":"<p>Understanding the deployment sequence:</p> graph TD     A[Prerequisites] --&gt;|task prereq:validate| B[Generate Keys]     B --&gt;|task prereq:keys| C[OIDC Setup]     C --&gt;|task azure:oidc| D[Create Identities]     D --&gt;|task azure:identities| E[Federated Credentials]     E --&gt;|task azure:federated-creds| F[Network Infrastructure]     F --&gt;|task azure:infra| G[Create Cluster]     G --&gt;|task cluster:create| H[Hosted Cluster Running]  <ol> <li>Validate prerequisites - Ensure all tools and credentials are available</li> <li>Generate signing keys - Create service account token signing key pair</li> <li>OIDC setup - Create shared OIDC issuer (one-time)</li> <li>Create identities - Create managed identities (per cluster, persistent)</li> <li>Federated credentials - Link identities to service accounts (per cluster, persistent)</li> <li>Network infrastructure - Create VNet, NSG, subnets (per cluster, temporary)</li> <li>Create cluster - Deploy the hosted cluster</li> </ol>"},{"location":"how-to/azure/self-managed/02-planning/#next-steps","title":"Next Steps","text":"<p>Now that you've planned your deployment:</p> <ul> <li>Azure Foundation Setup - Set up Azure infrastructure using Taskfile automation</li> <li>Management Cluster Setup - Prepare your management cluster</li> </ul>"},{"location":"how-to/azure/self-managed/03-azure-foundation/","title":"Azure Foundation Setup","text":"<p>This guide covers the one-time Azure infrastructure setup required before creating hosted clusters. These resources are shared across all hosted clusters in your environment.</p> <p>One-Time Setup</p> <p>This setup is performed ONCE and shared across all hosted clusters. You do NOT repeat this for each cluster.</p>"},{"location":"how-to/azure/self-managed/03-azure-foundation/#overview","title":"Overview","text":"<p>The Azure foundation consists of:</p> <ol> <li>Service Account Signing Keys: RSA key pair for signing service account tokens</li> <li>OIDC Issuer: Azure Blob Storage container providing OpenID Connect endpoint</li> <li>Persistent Resource Group: Container for shared resources</li> </ol>"},{"location":"how-to/azure/self-managed/03-azure-foundation/#prerequisites","title":"Prerequisites","text":"<p>Before beginning, ensure you have:</p> <ul> <li>Azure CLI (<code>az</code>) authenticated to your subscription</li> <li>Cloud Credential Operator tool (<code>ccoctl</code>) installed</li> <li>go-task/task installed</li> <li>Taskfile configuration completed in <code>hack/dev-preview/Taskfile.yml</code></li> </ul> <p>Required Taskfile variables: <pre><code>SUBSCRIPTION_ID: 'your-subscription-id'\nTENANT_ID: 'your-tenant-id'\nLOCATION: 'eastus'\nPERSISTENT_RG_NAME: 'hypershift-shared'\nOIDC_STORAGE_ACCOUNT_NAME: 'unique-storage-name'\n</code></pre></p>"},{"location":"how-to/azure/self-managed/03-azure-foundation/#step-1-validate-prerequisites","title":"Step 1: Validate Prerequisites","text":"<p>Validate that all required tools and variables are set:</p> <pre><code>task prereq:validate\n</code></pre> <p>This checks for: - Required environment variables - Azure CLI installation and authentication - ccoctl installation - Required credential files</p>"},{"location":"how-to/azure/self-managed/03-azure-foundation/#step-2-generate-service-account-signing-keys","title":"Step 2: Generate Service Account Signing Keys","text":"<p>Generate the RSA key pair used to sign service account tokens for workload identity:</p> <pre><code>task prereq:keys\n</code></pre> <p>What this does: - Creates <code>serviceaccount-signer.private</code> (4096-bit RSA private key) - Creates <code>serviceaccount-signer.public</code> (corresponding public key) - These keys are used to sign JWTs for workload identity federation</p> <p>Under the hood (see <code>tasks/prereq.yml</code>): <pre><code>ccoctl azure create-key-pair --output-dir .\n</code></pre></p> <p>Output: <pre><code>Generating service account token issuer key pair...\n\u2713 Generated RSA key pair\n  Private key: serviceaccount-signer.private\n  Public key: serviceaccount-signer.public\n</code></pre></p> <p>Key Security</p> <p>The private key must be kept secure. It's used to sign service account tokens that authenticate with Azure.</p> <ul> <li>Store securely (e.g., secrets management system)</li> <li>Do not commit to version control</li> <li>Back up safely</li> </ul>"},{"location":"how-to/azure/self-managed/03-azure-foundation/#step-3-create-oidc-issuer","title":"Step 3: Create OIDC Issuer","text":"<p>Create the Azure Blob Storage account that serves as the OIDC issuer:</p> <pre><code>task azure:oidc\n</code></pre> <p>What this does: - Creates a storage account in the persistent resource group - Uploads the public signing key to the storage account - Configures the OIDC discovery endpoint - Sets up the required blob container and files</p> <p>Under the hood (see <code>tasks/azure.yml</code>): <pre><code>ccoctl azure create-oidc-issuer \\\n  --oidc-resource-group-name ${PERSISTENT_RG_NAME} \\\n  --tenant-id ${TENANT_ID} \\\n  --region ${LOCATION} \\\n  --name ${OIDC_STORAGE_ACCOUNT_NAME} \\\n  --subscription-id ${SUBSCRIPTION_ID} \\\n  --public-key-file serviceaccount-signer.public\n</code></pre></p> <p>Output: <pre><code>Creating OIDC issuer (one-time setup)...\n\u2713 OIDC issuer created at https://yourstorageaccount.blob.core.windows.net/yourstorageaccount\n</code></pre></p>"},{"location":"how-to/azure/self-managed/03-azure-foundation/#what-gets-created","title":"What Gets Created","text":"<p>The OIDC issuer consists of:</p> <ol> <li>Azure Storage Account: Named <code>${OIDC_STORAGE_ACCOUNT_NAME}</code></li> <li>Blob Container: Same name as storage account</li> <li>OIDC Discovery Files:</li> <li><code>.well-known/openid-configuration</code>: OIDC discovery document</li> <li><code>openid/v1/jwks</code>: JSON Web Key Set (public keys)</li> </ol>"},{"location":"how-to/azure/self-managed/03-azure-foundation/#verification","title":"Verification","text":"<p>Verify the OIDC issuer is accessible:</p> <pre><code># Check storage account exists\naz storage account show \\\n  --name ${OIDC_STORAGE_ACCOUNT_NAME} \\\n  --resource-group ${PERSISTENT_RG_NAME}\n\n# Test OIDC endpoint (should return JSON)\ncurl https://${OIDC_STORAGE_ACCOUNT_NAME}.blob.core.windows.net/${OIDC_STORAGE_ACCOUNT_NAME}/.well-known/openid-configuration\n</code></pre>"},{"location":"how-to/azure/self-managed/03-azure-foundation/#understanding-oidc-issuer-scope","title":"Understanding OIDC Issuer Scope","text":"<p>The OIDC issuer is shared infrastructure:</p>"},{"location":"how-to/azure/self-managed/03-azure-foundation/#what-its-shared-across","title":"What It's Shared Across","text":"<ul> <li>All hosted clusters using this management cluster</li> <li>All clusters in the same environment/region</li> <li>Multiple teams using the same Azure subscription</li> </ul>"},{"location":"how-to/azure/self-managed/03-azure-foundation/#why-its-shared","title":"Why It's Shared","text":"<ul> <li>Cost efficiency: One storage account vs many</li> <li>Simplified management: Single OIDC endpoint to manage</li> <li>Consistency: Same authentication mechanism across clusters</li> </ul>"},{"location":"how-to/azure/self-managed/03-azure-foundation/#when-to-create-multiple-issuers","title":"When to Create Multiple Issuers","text":"<ul> <li>Different Azure subscriptions</li> <li>Compliance requirements for isolation</li> <li>Different security zones or environments</li> </ul> <p>Production Recommendation</p> <p>Use one OIDC issuer per:</p> <ul> <li>Environment (dev, staging, prod) - For change control</li> <li>Security zone - For compliance requirements</li> <li>Azure subscription - For billing separation</li> </ul>"},{"location":"how-to/azure/self-managed/03-azure-foundation/#resource-lifecycle","title":"Resource Lifecycle","text":"<p>Understanding what happens to these resources:</p> Resource Lifecycle Deletion Service Account Keys One-time, manual Manual only (keep for disaster recovery) OIDC Issuer One-time, persistent Manual via <code>task azure:delete-oidc</code> Persistent Resource Group Long-lived Manual only <p>Deleting the OIDC Issuer</p> <p>Deleting the OIDC issuer will break authentication for ALL clusters using it. Only delete when:</p> <ul> <li>All clusters using it have been decommissioned</li> <li>You're performing a complete environment teardown</li> </ul> <p>Delete with: <code>task azure:delete-oidc</code></p>"},{"location":"how-to/azure/self-managed/03-azure-foundation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/azure/self-managed/03-azure-foundation/#storage-account-name-already-taken","title":"Storage Account Name Already Taken","text":"<p>Storage account names are globally unique across all of Azure.</p> <p>Error: <pre><code>The storage account named 'myoidc' is already taken.\n</code></pre></p> <p>Solution: Add a random suffix to your storage account name: <pre><code>OIDC_STORAGE_ACCOUNT_NAME: 'myoidc1234567890'\n</code></pre></p>"},{"location":"how-to/azure/self-managed/03-azure-foundation/#authentication-errors","title":"Authentication Errors","text":"<p>Error: <pre><code>ERROR: Please run 'az login' to setup account.\n</code></pre></p> <p>Solution: <pre><code>az login\naz account set --subscription ${SUBSCRIPTION_ID}\n</code></pre></p>"},{"location":"how-to/azure/self-managed/03-azure-foundation/#insufficient-permissions","title":"Insufficient Permissions","text":"<p>Error: <pre><code>The client does not have authorization to perform action 'Microsoft.Storage/storageAccounts/write'\n</code></pre></p> <p>Solution: Ensure your Azure account has: - <code>Contributor</code> role on the subscription or resource group - <code>User Access Administrator</code> for managed identity operations</p>"},{"location":"how-to/azure/self-managed/03-azure-foundation/#next-steps","title":"Next Steps","text":"<p>With the Azure foundation in place, proceed to:</p> <ul> <li>Management Cluster Setup - Install HyperShift operator</li> <li>Or jump to Create Hosted Cluster if your management cluster is ready</li> </ul>"},{"location":"how-to/azure/self-managed/04-management-cluster/","title":"Management Cluster Setup","text":"<p>This guide covers installing the HyperShift operator on your management cluster using Taskfile automation. The operator manages the lifecycle of hosted clusters and orchestrates control plane deployments.</p> <p>Persona: HyperShift Administrator</p> <p>This phase requires cluster-admin access to the OpenShift management cluster.</p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#overview","title":"Overview","text":"<p>Preparing your management cluster involves:</p> <ol> <li>Verification: Confirm cluster access and capacity</li> <li>HyperShift Operator Installation: Deploy the operator that manages HostedCluster resources</li> <li>External DNS Configuration (Optional): Enable automatic DNS record management for production</li> <li>Verification: Ensure the operator is ready to create hosted clusters</li> </ol> <p>Why This Matters: The HyperShift operator is the core component that orchestrates all hosted cluster operations. It watches for HostedCluster custom resources and provisions the corresponding control plane components.</p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#prerequisites","title":"Prerequisites","text":"<p>Before beginning, ensure you have:</p> <ul> <li>Cluster-admin access to your OpenShift management cluster</li> <li>Completed Azure Foundation Setup</li> <li>Taskfile configuration in <code>hack/dev-preview/Taskfile.yml</code></li> <li>Valid OpenShift pull secret from cloud.redhat.com</li> <li>HyperShift CLI binary (installation covered by <code>task prereq:validate</code>)</li> </ul> <p>Required Taskfile variables: <pre><code>PULL_SECRET: 'pull-secret.json'\nPARENT_DNS_ZONE: 'example.com'  # For External DNS option\n</code></pre></p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#installing-the-hypershift-operator","title":"Installing the HyperShift Operator","text":""},{"location":"how-to/azure/self-managed/04-management-cluster/#step-1-verify-management-cluster-access","title":"Step 1: Verify Management Cluster Access","text":"<p>Verify you have the required access to the management cluster:</p> <pre><code>task mgmt:verify-access\n</code></pre> <p>What this does: - Tests kubectl connectivity to the cluster - Verifies cluster-admin permissions (<code>kubectl auth can-i '*' '*'</code>) - Confirms you can deploy resources</p> <p>Expected output: <pre><code>Verifying management cluster access...\nKubernetes control plane is running at https://api.management-cluster.example.com:6443\n\u2713 Cluster-admin permissions confirmed\n</code></pre></p> <p>Under the hood (see <code>tasks/mgmt-cluster.yml</code>): <pre><code>kubectl cluster-info\nkubectl auth can-i '*' '*'\n</code></pre></p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#step-2-choose-your-dns-strategy","title":"Step 2: Choose Your DNS Strategy","text":"<p>Select one of two installation paths based on your DNS requirements:</p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#option-a-with-external-dns-recommended-for-production","title":"Option A: With External DNS (Recommended for Production)","text":"<p>Best for: Production environments, multiple clusters, custom domains</p> <p>Characteristics: - Automatic DNS record creation/deletion - Custom branded domains (e.g., <code>api.my-cluster.example.com</code>) - Requires Azure DNS zone and service principal</p> <p>What you'll need: - Azure DNS zone configured in <code>PARENT_DNS_ZONE</code> variable - Taskfile will automatically create service principal and credentials</p> <p>Proceed to: Step 3A</p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#option-b-without-external-dns-simpler-for-devtest","title":"Option B: Without External DNS (Simpler for Dev/Test)","text":"<p>Best for: Development, testing, proof-of-concept</p> <p>Characteristics: - Uses Azure LoadBalancer DNS names - No DNS zones or additional credentials needed - Simpler setup with fewer moving parts</p> <p>What you'll need: - Only pull secret (already configured) - No additional Azure resources</p> <p>Proceed to: Step 3B</p> <p>Recommendation</p> <p>Start with Option B (Without External DNS) for your first cluster to learn the basics. Once comfortable, reinstall with Option A for production deployments.</p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#step-3a-install-with-external-dns-production-mode","title":"Step 3A: Install with External DNS (Production Mode)","text":"<p>Install the HyperShift operator with External DNS for automatic DNS management:</p> <pre><code>task mgmt:setup-with-dns\n</code></pre> <p>What this does: 1. Verifies management cluster access 2. Creates Azure service principal with DNS Zone Contributor role 3. Generates <code>azure_mgmt.json</code> credentials file 4. Creates Kubernetes secret <code>azure-config-file</code> in default namespace 5. Installs HyperShift operator with External DNS configuration 6. Verifies both operator and External DNS pods are running</p> <p>Under the hood (see <code>tasks/mgmt-cluster.yml</code>): <pre><code># Create service principal\naz ad sp create-for-rbac \\\n  --name \"hypershift-external-dns-${PREFIX}\" \\\n  --role \"DNS Zone Contributor\" \\\n  --scopes \"/subscriptions/${SUBSCRIPTION_ID}/resourceGroups/${PERSISTENT_RG_NAME}\"\n\n# Create Kubernetes secret\nkubectl create secret generic azure-config-file \\\n  -n default \\\n  --from-file=azure_mgmt.json\n\n# Install operator with External DNS\nhypershift install \\\n  --external-dns-provider=azure \\\n  --external-dns-credentials azure_mgmt.json \\\n  --pull-secret ${PULL_SECRET} \\\n  --external-dns-domain-filter ${PARENT_DNS_ZONE} \\\n  --limit-crd-install Azure\n</code></pre></p> <p>Expected output: <pre><code>Verifying management cluster access...\n\u2713 Cluster-admin permissions confirmed\nCreating service principal for External DNS...\n\u2713 Created azure_mgmt.json\nCreating azure-config-file secret in default namespace...\n\u2713 Created secret azure-config-file\nInstalling HyperShift operator with External DNS...\n  DNS Zone: example.com\n\u2713 HyperShift operator and External DNS installed in namespace hypershift\nVerifying HyperShift operator installation...\n\nPods in hypershift namespace:\nNAME                           READY   STATUS    RESTARTS   AGE\nexternal-dns-xxxxx-xxxxx       1/1     Running   0          30s\noperator-xxxxx-xxxxx           1/1     Running   0          30s\n\n\u2713 HyperShift operator verification complete\n\u2713 External DNS verification complete\n</code></pre></p> <p>Advanced configuration:</p> <p>To use a custom operator image: <pre><code># In Taskfile.yml\nvars:\n  HYPERSHIFT_IMAGE: 'quay.io/hypershift/hypershift:custom-tag'\n</code></pre></p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#step-3b-install-without-external-dns-simple-mode","title":"Step 3B: Install without External DNS (Simple Mode)","text":"<p>Install the HyperShift operator for simpler dev/test deployments:</p> <pre><code>task mgmt:setup\n</code></pre> <p>What this does: 1. Verifies management cluster access 2. Installs HyperShift operator in <code>hypershift</code> namespace 3. Limits CRD installation to Azure-specific resources only 4. Verifies operator pod is running</p> <p>Under the hood (see <code>tasks/mgmt-cluster.yml</code>): <pre><code>hypershift install \\\n  --pull-secret ${PULL_SECRET} \\\n  --limit-crd-install Azure\n</code></pre></p> <p>Expected output: <pre><code>Verifying management cluster access...\n\u2713 Cluster-admin permissions confirmed\nInstalling HyperShift operator (without External DNS)...\n\u2713 HyperShift operator installed in namespace hypershift\nVerifying HyperShift operator installation...\n\nPods in hypershift namespace:\nNAME                     READY   STATUS    RESTARTS   AGE\noperator-xxxxx-xxxxx     1/1     Running   0          30s\n\nOperator deployment:\nNAME       READY   UP-TO-DATE   AVAILABLE   AGE\noperator   1/1     1            1           30s\n\n\u2713 HyperShift operator verification complete\n</code></pre></p> <p>Advanced configuration:</p> <p>To use a custom operator image or namespace: <pre><code># In Taskfile.yml\nvars:\n  HYPERSHIFT_IMAGE: 'quay.io/hypershift/hypershift:custom-tag'\n  HYPERSHIFT_NAMESPACE: 'custom-namespace'\n</code></pre></p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#verifying-the-installation","title":"Verifying the Installation","text":""},{"location":"how-to/azure/self-managed/04-management-cluster/#check-operator-status","title":"Check Operator Status","text":"<p>Verify the operator is running correctly:</p> <pre><code>task mgmt:verify\n</code></pre> <p>What this shows: - Pods in hypershift namespace - Operator deployment status - HyperShift CRDs installed - Recent operator logs</p> <p>Example output: <pre><code>Verifying HyperShift operator installation...\n\nPods in hypershift namespace:\nNAME                     READY   STATUS    RESTARTS   AGE\noperator-xxxxx-xxxxx     1/1     Running   0          2m\n\nOperator deployment:\nNAME       READY   UP-TO-DATE   AVAILABLE   AGE\noperator   1/1     1            1           2m\n\nHyperShift CRDs:\ncertificates.hypershift.openshift.io\nclustersizingconfigurations.hypershift.openshift.io\nhostedclusters.hypershift.openshift.io\nhostedcontrolplanes.hypershift.openshift.io\nnodepools.hypershift.openshift.io\n...\n\nOperator logs (last 20 lines):\n2025-01-17T10:30:00.000Z INFO Starting workers\n2025-01-17T10:30:00.000Z INFO Started controllers\n\n\u2713 HyperShift operator verification complete\n</code></pre></p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#check-external-dns-if-installed","title":"Check External DNS (if installed)","text":"<p>For installations with External DNS:</p> <pre><code>task mgmt:verify-external-dns\n</code></pre> <p>What this shows: - External DNS deployment status - Recent External DNS logs - Connection to Azure DNS</p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#check-management-cluster-capacity","title":"Check Management Cluster Capacity","text":"<p>Verify the management cluster has sufficient resources:</p> <pre><code>task mgmt:check-capacity\n</code></pre> <p>What this shows: - Current node resource usage - Available capacity for hosted clusters</p> <p>Resource requirements: Each hosted cluster needs approximately: - 4 CPU cores - 8 GB memory</p> <p>Plan your management cluster capacity accordingly.</p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#understanding-the-installation","title":"Understanding the Installation","text":""},{"location":"how-to/azure/self-managed/04-management-cluster/#what-gets-deployed","title":"What Gets Deployed","text":"<p>The installation creates:</p> <p>1. Namespace: <code>hypershift</code> (configurable via <code>HYPERSHIFT_NAMESPACE</code>)</p> <p>2. HyperShift Operator Deployment: - Watches for HostedCluster and NodePool resources - Provisions control plane pods - Manages cluster lifecycle</p> <p>3. CRDs (Custom Resource Definitions): - <code>hostedclusters.hypershift.openshift.io</code> - <code>nodepools.hypershift.openshift.io</code> - <code>certificates.hypershift.openshift.io</code> - Related scheduling and configuration resources</p> <p>4. RBAC Resources: - ServiceAccounts for operator - ClusterRoles and ClusterRoleBindings - Permissions to manage cluster-wide resources</p> <p>5. External DNS Deployment (if enabled): - Watches services and routes in hosted cluster namespaces - Creates/updates DNS records in Azure DNS - Uses service principal credentials from Kubernetes secret</p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#operator-responsibilities","title":"Operator Responsibilities","text":"<p>The HyperShift operator: - Watches for HostedCluster custom resources in configured namespaces - Provisions control plane components as pods on the management cluster - Manages control plane lifecycle (creation, upgrades, deletion) - Coordinates with Azure APIs for infrastructure provisioning - Handles PKI and certificate rotation for control planes</p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#external-dns-responsibilities-if-enabled","title":"External DNS Responsibilities (if enabled)","text":"<p>External DNS: - Watches Kubernetes services and OpenShift routes - Creates DNS A records in Azure DNS for:   - API server endpoints   - OAuth server endpoints   - Konnectivity server endpoints   - Application ingress routes - Removes DNS records when services are deleted - Uses Azure credentials from <code>azure-config-file</code> secret</p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/azure/self-managed/04-management-cluster/#operator-pod-not-starting","title":"Operator Pod Not Starting","text":"<p>Symptom: Operator pod stuck in <code>Pending</code> or <code>CrashLoopBackOff</code></p> <p>Check: <pre><code>kubectl describe pod -n hypershift -l name=operator\n</code></pre></p> <p>Common causes: - Insufficient resources on management cluster - Image pull errors (invalid pull secret) - Admission webhook failures</p> <p>Solutions: - Verify pull secret is valid - Check management cluster has available CPU/memory - Review pod events and logs</p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#external-dns-not-creating-records","title":"External DNS Not Creating Records","text":"<p>Symptom: DNS records not appearing in Azure DNS zone</p> <p>Check: <pre><code>task mgmt:verify-external-dns\n</code></pre></p> <p>Common causes: - Service principal lacks DNS Zone Contributor permissions - DNS zone name doesn't match <code>PARENT_DNS_ZONE</code> - Secret <code>azure-config-file</code> not created or incorrect</p> <p>Solutions: <pre><code># Verify secret exists\nkubectl get secret azure-config-file -n default\n\n# Check service principal permissions\naz role assignment list \\\n  --assignee $(jq -r '.aadClientId' &lt; azure_mgmt.json) \\\n  --output table\n\n# Recreate DNS resources if needed\ntask mgmt:delete-dns-resources\ntask mgmt:setup-with-dns\n</code></pre></p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#crd-installation-issues","title":"CRD Installation Issues","text":"<p>Symptom: Errors about unknown resource types</p> <p>Check: <pre><code>kubectl get crd | grep hypershift\n</code></pre></p> <p>Solution: Reinstall the operator: <pre><code>task mgmt:cleanup\ntask mgmt:setup\n</code></pre></p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#uninstalling-the-operator","title":"Uninstalling the Operator","text":""},{"location":"how-to/azure/self-managed/04-management-cluster/#before-uninstalling","title":"Before Uninstalling","text":"<p>Delete Hosted Clusters First</p> <p>Never uninstall the operator while hosted clusters exist!</p> <p>The operator manages the lifecycle of hosted clusters. Removing it while clusters exist will leave orphaned resources.</p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#check-for-existing-clusters","title":"Check for Existing Clusters","text":"<p>The uninstall task automatically checks for existing clusters:</p> <pre><code>task mgmt:cleanup\n</code></pre> <p>If hosted clusters exist, you'll see: <pre><code>\u274c WARNING: Found 2 HostedCluster resource(s)\n\nNAMESPACE   NAME          VERSION   ...\nclusters    myprefix-hc   4.21.0    ...\n\nDelete all hosted clusters before uninstalling the operator!\n</code></pre></p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#complete-cleanup","title":"Complete Cleanup","text":"<p>To uninstall the operator and remove DNS resources:</p> <pre><code># This will:\n# 1. Check for existing HostedClusters (fails if any exist)\n# 2. Delete the hypershift namespace\n# 3. Remove External DNS Kubernetes secret\n# 4. Delete local azure_mgmt.json file\ntask mgmt:cleanup\n</code></pre> <p>What gets removed: - HyperShift operator deployment - External DNS deployment (if installed) - All pods in hypershift namespace - CRDs remain (to prevent accidental data loss) - Kubernetes secret <code>azure-config-file</code> - Local <code>azure_mgmt.json</code> file</p> <p>What persists: - CRDs (Custom Resource Definitions) - Service principal in Azure AD (must delete manually if not needed)</p> <p>To remove service principal (if no longer needed): <pre><code>az ad sp delete --id $(az ad sp list --display-name 'hypershift-external-dns-${PREFIX}' --query '[0].appId' -o tsv)\n</code></pre></p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#next-steps","title":"Next Steps","text":"<p>With the HyperShift operator installed and verified, you're ready to:</p> <ul> <li>Create Hosted Cluster - Deploy your first hosted OpenShift cluster</li> <li>Configure cluster-specific settings in Taskfile.yml</li> <li>Plan your hosted cluster deployment strategy</li> </ul> <p>What's Next: The hosted cluster creation process will use the operator you just installed to provision Azure infrastructure and deploy a complete OpenShift control plane on your management cluster.</p>"},{"location":"how-to/azure/self-managed/04-management-cluster/#quick-reference","title":"Quick Reference","text":""},{"location":"how-to/azure/self-managed/04-management-cluster/#essential-commands","title":"Essential Commands","text":"<pre><code># Simple installation (dev/test)\ntask mgmt:setup\n\n# Production installation with External DNS\ntask mgmt:setup-with-dns\n\n# Verify installation\ntask mgmt:verify\ntask mgmt:verify-external-dns  # If using External DNS\n\n# Check capacity\ntask mgmt:check-capacity\n\n# Cleanup (requires no hosted clusters exist)\ntask mgmt:cleanup\n</code></pre>"},{"location":"how-to/azure/self-managed/04-management-cluster/#task-summary","title":"Task Summary","text":"Task Purpose When to Use <code>task mgmt:verify-access</code> Verify cluster access and permissions Before installation <code>task mgmt:setup</code> Install operator without External DNS Dev/test environments <code>task mgmt:setup-with-dns</code> Install operator with External DNS Production environments <code>task mgmt:verify</code> Verify operator installation After installation <code>task mgmt:verify-external-dns</code> Verify External DNS installation After installation with DNS <code>task mgmt:check-capacity</code> Check management cluster capacity Before creating clusters <code>task mgmt:cleanup</code> Uninstall operator (safety checks) Environment teardown"},{"location":"how-to/azure/self-managed/05-hosted-cluster/","title":"Create and Manage Hosted Clusters","text":"<p>This guide covers the end-to-end process of creating, accessing, and destroying HyperShift hosted clusters on Azure using Taskfile automation.</p> <p>Persona: HyperShift Administrator</p> <p>This phase creates your actual OpenShift hosted clusters that run application workloads.</p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#overview","title":"Overview","text":"<p>Deploying a hosted cluster involves:</p> <ol> <li>Workload Identity Creation: Per-cluster managed identities and federated credentials</li> <li>Infrastructure Provisioning: Azure VNets, subnets, network security groups, and resource groups</li> <li>HostedCluster Deployment: Control plane on management cluster, worker nodes as Azure VMs</li> <li>Cluster Access: Generate kubeconfig and verify cluster operation</li> </ol> <p>Why This Matters: This creates the actual OpenShift cluster where your applications will run. The control plane runs as pods on the management cluster, while worker nodes run as Azure VMs. The cluster uses workload identities for secure, credential-free access to Azure services.</p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#prerequisites","title":"Prerequisites","text":"<p>Before creating a hosted cluster, ensure:</p> <ul> <li>Azure Foundation Setup completed (OIDC issuer, signing keys)</li> <li>Management Cluster Setup completed (HyperShift operator installed)</li> <li>Taskfile configuration completed in <code>hack/dev-preview/Taskfile.yml</code></li> <li>Azure credentials file created (<code>azure-credentials.json</code>)</li> <li>OpenShift pull secret available (<code>pull-secret.json</code>)</li> </ul> <p>Required Taskfile variables: <pre><code>PREFIX: 'myprefix'\nCLUSTER_NAME: '${PREFIX}-hc'\nRELEASE_IMAGE: 'quay.io/openshift-release-dev/ocp-release:4.21.0-x86_64'\nAZURE_CREDS: 'azure-credentials.json'\nPULL_SECRET: 'pull-secret.json'\n</code></pre></p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#creating-a-hosted-cluster","title":"Creating a Hosted Cluster","text":""},{"location":"how-to/azure/self-managed/05-hosted-cluster/#step-1-create-workload-identities","title":"Step 1: Create Workload Identities","text":"<p>Create the managed identities and federated credentials for your cluster:</p> <pre><code># Create 7 managed identities for cluster components\ntask azure:identities\n\n# Create 14 federated identity credentials\ntask azure:federated-creds\n</code></pre> <p>What this does: - Creates 7 managed identities (one per OpenShift component) - Creates 14 federated credentials (2 per component for redundancy) - Generates <code>workload-identities.json</code> file with client IDs - These resources persist after cluster deletion</p> <p>Components with identities: - Image Registry - Ingress - Disk CSI Driver - File CSI Driver - NodePool Management - Cloud Provider - Network</p> <p>Output: <pre><code>Creating 7 managed identities for cluster myprefix-hc...\n\u2713 Created identity: myprefix-hc-image-registry\n\u2713 Created identity: myprefix-hc-ingress\n...\nCreating 14 federated identity credentials...\n\u2713 Created federated credential for openshift-image-registry/installer\n...\n</code></pre></p> <p>Identity Persistence</p> <p>Managed identities and federated credentials are persistent. They:</p> <ul> <li>Remain after cluster deletion</li> <li>Allow cluster recreation without identity recreation</li> <li>Preserve Azure IAM role assignments</li> <li>Support disaster recovery scenarios</li> </ul> <p>To delete identities: <code>task azure:delete-identities</code> (only when cluster is permanently removed)</p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#step-2-provision-azure-infrastructure","title":"Step 2: Provision Azure Infrastructure","text":"<p>Create the VNet, NSG, subnets, and resource groups:</p> <pre><code>task azure:infra\n</code></pre> <p>What this does: - Creates managed resource group for cluster resources - Creates VNet resource group - Creates NSG resource group - Provisions VNet with <code>/16</code> address space - Creates subnet with <code>/24</code> address space - Creates and configures Network Security Group - Exports resource IDs to <code>.azure-net-ids</code> file</p> <p>Under the hood (see <code>tasks/azure.yml</code>): <pre><code># Resource group creation\naz group create --name ${MANAGED_RG_NAME} --location ${LOCATION}\naz group create --name ${VNET_RG_NAME} --location ${LOCATION}\naz group create --name ${NSG_RG_NAME} --location ${LOCATION}\n\n# NSG creation\naz network nsg create \\\n    --resource-group ${NSG_RG_NAME} \\\n    --name ${NSG}\n\n# VNet and subnet creation\naz network vnet create \\\n    --name ${VNET_NAME} \\\n    --resource-group ${VNET_RG_NAME} \\\n    --address-prefix 10.0.0.0/16 \\\n    --subnet-name ${VNET_SUBNET1} \\\n    --subnet-prefixes 10.0.0.0/24 \\\n    --nsg ${NSG_ID}\n</code></pre></p> <p>Output: <pre><code>Creating Azure infrastructure for cluster...\n\u2713 Created resource group: myprefix-managed-rg\n\u2713 Created resource group: myprefix-vnet-rg\n\u2713 Created resource group: myprefix-nsg-rg\n\u2713 Created NSG: myprefix-nsg\n\u2713 Created VNet: myprefix-vnet\n\u2713 Created subnet: myprefix-subnet-1\nExported resource IDs to .azure-net-ids\n</code></pre></p> <p>Infrastructure Lifecycle</p> <p>This infrastructure is temporary and deleted with the cluster. Use <code>task azure:delete-infra</code> to remove it.</p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#step-3-create-the-hostedcluster","title":"Step 3: Create the HostedCluster","text":"<p>Create the hosted cluster using all the prepared resources:</p> <pre><code>task cluster:create\n</code></pre> <p>What this does: - Validates all prerequisites (resource groups, identities, credentials) - Reads network resource IDs from <code>.azure-net-ids</code> - Reads workload identities from <code>workload-identities.json</code> - Invokes <code>hypershift create cluster azure</code> with all required parameters - Deploys control plane pods on management cluster - Provisions worker nodes as Azure VMs</p> <p>Under the hood (see <code>tasks/cluster.yml</code>): <pre><code>hypershift create cluster azure \\\n    --name ${CLUSTER_NAME} \\\n    --namespace ${CLUSTER_NAMESPACE} \\\n    --azure-creds ${AZURE_CREDS} \\\n    --location ${LOCATION} \\\n    --node-pool-replicas ${NODE_POOL_REPLICAS} \\\n    --base-domain ${PARENT_DNS_ZONE} \\\n    --dns-zone-rg-name ${PERSISTENT_RG_NAME} \\\n    --pull-secret ${PULL_SECRET} \\\n    --generate-ssh \\\n    --release-image ${RELEASE_IMAGE} \\\n    --resource-group-name ${MANAGED_RG_NAME} \\\n    --vnet-id ${VNET_ID} \\\n    --subnet-id ${SUBNET_ID} \\\n    --network-security-group-id ${NSG_ID} \\\n    --sa-token-issuer-private-key-path ${SA_TOKEN_ISSUER_PRIVATE_KEY_PATH} \\\n    --oidc-issuer-url ${OIDC_ISSUER_URL} \\\n    --assign-service-principal-roles \\\n    --workload-identities-file ./workload-identities.json \\\n    --diagnostics-storage-account-type Managed\n</code></pre></p> <p>Expected output: <pre><code>Creating hosted cluster myprefix-hc\n========================================\nOpenShift Details:\n  Release: quay.io/openshift-release-dev/ocp-release:4.21.0-x86_64\n========================================\nAzure Details:\n  Resource Group: myprefix-managed-rg\n  Location: eastus\n  Base Domain: example.com\n  DNS Zone RG: hypershift-shared\n========================================\n\u2713 Hosted cluster myprefix-hc created successfully\n</code></pre></p> <p>Azure Marketplace Images</p> <p>For OpenShift 4.20+, HyperShift automatically selects the appropriate Azure Marketplace image from the release payload. No <code>--marketplace-*</code> flags needed.</p> <p>For custom images or VM generation control, see Advanced Configuration.</p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#step-4-verify-cluster-creation","title":"Step 4: Verify Cluster Creation","text":"<p>Monitor cluster creation progress:</p> <pre><code># Check HostedCluster status\nkubectl get hostedcluster ${CLUSTER_NAME} -n ${CLUSTER_NAMESPACE}\n\n# Watch cluster become available (15-30 minutes)\nkubectl wait --for=condition=Available \\\n    hostedcluster/${CLUSTER_NAME} \\\n    -n ${CLUSTER_NAMESPACE} \\\n    --timeout=30m\n\n# Check NodePool status\nkubectl get nodepool -n ${CLUSTER_NAMESPACE}\n\n# Verify control plane pods are running\nkubectl get pods -n clusters-${CLUSTER_NAME}\n</code></pre> <p>Healthy cluster indicators: <pre><code># HostedCluster should show Available=True\nNAME          VERSION   KUBECONFIG                      PROGRESS    AVAILABLE   PROGRESSING   MESSAGE\nmyprefix-hc   4.21.0    myprefix-hc-admin-kubeconfig   Completed   True        False         The hosted cluster is available\n\n# NodePool should show all replicas ready\nNAME                 CLUSTER       DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE\nmyprefix-hc          myprefix-hc   2               2               False         False        4.21.0\n\n# Control plane pods all Running\nNAME                                          READY   STATUS    RESTARTS   AGE\ncapi-provider-5d4c6b9d8-xxxxx                1/1     Running   0          10m\ncatalog-operator-6b8c9f8b7-xxxxx             2/2     Running   0          8m\ncluster-api-7b9c8d7f6-xxxxx                  1/1     Running   0          10m\n...\n</code></pre></p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#step-5-access-your-cluster","title":"Step 5: Access Your Cluster","text":"<p>Generate kubeconfig and access the hosted cluster:</p> <pre><code># Generate kubeconfig for your cluster\nhypershift create kubeconfig \\\n    --name ${CLUSTER_NAME} \\\n    --namespace ${CLUSTER_NAMESPACE} \\\n    &gt; ${CLUSTER_NAME}-kubeconfig\n\n# Set KUBECONFIG to access hosted cluster\nexport KUBECONFIG=${CLUSTER_NAME}-kubeconfig\n\n# Verify cluster access\nkubectl get nodes\nkubectl get clusterversion\nkubectl get co  # Check cluster operators status\n</code></pre> <p>Expected output: <pre><code># Nodes should be Ready\nNAME                                     STATUS   ROLES    AGE   VERSION\nmyprefix-hc-nodepool-1-xxxxx            Ready    worker   5m    v1.31.0\nmyprefix-hc-nodepool-1-yyyyy            Ready    worker   5m    v1.31.0\n\n# ClusterVersion should show correct version\nNAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS\nversion   4.21.0    True        False         3m      Cluster version is 4.21.0\n\n# Cluster operators should be Available\nNAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nauthentication                             4.21.0    True        False         False      5m\nconsole                                    4.21.0    True        False         False      3m\n...\n</code></pre></p> <p>Cluster Ready</p> <p>Your hosted cluster is now operational! You can deploy applications, configure cluster settings, and perform all standard OpenShift operations.</p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#destroying-a-hosted-cluster","title":"Destroying a Hosted Cluster","text":"<p>When you're done with a cluster, clean it up to avoid unnecessary costs.</p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#option-1-complete-cleanup-recommended","title":"Option 1: Complete Cleanup (Recommended)","text":"<p>Delete the cluster and all temporary infrastructure:</p> <pre><code># 1. Destroy the HostedCluster (this deletes VMs and cluster resources)\ntask cluster:destroy\n\n# 2. Delete Azure infrastructure (VNet, NSG, resource groups)\ntask azure:delete-infra\n</code></pre> <p>What this does: - Deletes HostedCluster and NodePool resources from management cluster - Removes worker node VMs from Azure - Deletes control plane pods from management cluster - Removes VNet, NSG, subnets, and resource groups - Preserves: Managed identities and federated credentials (for recreation)</p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#option-2-delete-cluster-only-keep-infrastructure","title":"Option 2: Delete Cluster Only (Keep Infrastructure)","text":"<p>Delete just the cluster, preserving infrastructure for quick recreation:</p> <pre><code>task cluster:destroy\n</code></pre> <p>When to use: Testing scenarios where you want to quickly recreate clusters with the same network infrastructure.</p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#option-3-complete-removal-including-identities","title":"Option 3: Complete Removal Including Identities","text":"<p>Permanently remove all cluster-related resources including identities:</p> <pre><code># 1. Destroy the cluster\ntask cluster:destroy\n\n# 2. Delete infrastructure\ntask azure:delete-infra\n\n# 3. Delete workload identities (CAUTION: permanent)\ntask azure:delete-identities\n</code></pre> <p>Deleting Identities</p> <p>Only delete identities when the cluster is permanently removed and will never be recreated. Deleting identities:</p> <ul> <li>Removes all Azure role assignments</li> <li>Requires full identity recreation for new clusters with same name</li> <li>Cannot be undone</li> </ul>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"how-to/azure/self-managed/05-hosted-cluster/#using-custom-control-plane-operator-image","title":"Using Custom Control Plane Operator Image","text":"<p>Override the CPO image for testing or debugging:</p> <pre><code># In Taskfile.yml\nvars:\n  CPO_IMAGE: 'quay.io/hypershift/hypershift:custom-tag'\n</code></pre> <p>The cluster creation task automatically detects and uses the custom image.</p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#configuring-azure-marketplace-images","title":"Configuring Azure Marketplace Images","text":"<p>For OpenShift 4.20+ (Recommended):</p> <p>HyperShift auto-selects the appropriate image. No configuration needed.</p> <p>For Custom Images:</p> <p>Add to your <code>hypershift create cluster azure</code> command: <pre><code>--marketplace-publisher azureopenshift \\\n--marketplace-offer aro4 \\\n--marketplace-sku aro_421 \\\n--marketplace-version 421.0.20250101 \\\n--image-generation Gen2  # Gen1 or Gen2\n</code></pre></p> <p>Marketplace Flag Requirements</p> <p>When specifying marketplace details, provide all four flags together.</p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#customizing-network-configuration","title":"Customizing Network Configuration","text":"<p>Modify network settings in <code>Taskfile.yml</code>:</p> <pre><code>vars:\n  VNET_CIDR: '10.0.0.0/16'          # Default VNet CIDR\n  SUBNET_CIDR: '10.0.0.0/24'         # Default subnet CIDR\n  VNET_NAME: '${PREFIX}-custom-vnet'  # Custom VNet name\n</code></pre> <p>Then regenerate infrastructure: <pre><code>task azure:delete-infra  # Clean up old infrastructure\ntask azure:infra         # Create with new configuration\n</code></pre></p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#using-external-dns","title":"Using External DNS","text":"<p>If you configured External DNS during management cluster setup:</p> <p>In Taskfile.yml, ensure: <pre><code>vars:\n  PARENT_DNS_ZONE: 'example.com'\n  # External DNS will create: api.myprefix-hc.example.com\n</code></pre></p> <p>The cluster creation task automatically includes DNS configuration when External DNS is available.</p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#scaling-nodepools","title":"Scaling NodePools","text":"<p>Scale worker nodes after cluster creation:</p> <pre><code># Scale existing NodePool\nkubectl scale nodepool/${CLUSTER_NAME} \\\n    -n ${CLUSTER_NAMESPACE} \\\n    --replicas=5\n\n# Verify scaling\nkubectl get nodepool -n ${CLUSTER_NAMESPACE}\n</code></pre>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#troubleshooting-cluster-creation","title":"Troubleshooting Cluster Creation","text":""},{"location":"how-to/azure/self-managed/05-hosted-cluster/#cluster-stuck-in-provisioning","title":"Cluster Stuck in Provisioning","text":"<p>Symptom: HostedCluster remains in <code>Progressing</code> state for &gt;30 minutes</p> <p>Check: <pre><code># Check HostedCluster status\nkubectl get hostedcluster ${CLUSTER_NAME} -n ${CLUSTER_NAMESPACE} -o yaml\n\n# Check control plane pods\nkubectl get pods -n clusters-${CLUSTER_NAME}\n\n# Check operator logs\nkubectl logs -n hypershift deployment/operator\n</code></pre></p> <p>Common causes: - Insufficient resources on management cluster - Image pull errors (check pull secret) - Azure API rate limiting - Network connectivity issues</p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#worker-nodes-not-joining","title":"Worker Nodes Not Joining","text":"<p>Symptom: NodePool shows desired nodes, but nodes don't appear as Ready</p> <p>Check: <pre><code># Check NodePool status\nkubectl get nodepool -n ${CLUSTER_NAMESPACE} -o yaml\n\n# Check machine resources\nkubectl get machines -n clusters-${CLUSTER_NAME}\n\n# Check Azure VMs\naz vm list \\\n    --resource-group ${MANAGED_RG_NAME} \\\n    --output table\n</code></pre></p> <p>Common causes: - Workload identity misconfiguration - NSG blocking required ports - Ignition server unreachable - Azure quota limits</p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#workload-identity-authentication-failures","title":"Workload Identity Authentication Failures","text":"<p>Symptom: Control plane pods reporting Azure authentication errors</p> <p>Check: <pre><code># Verify workload-identities.json exists and has valid client IDs\ncat workload-identities.json | jq\n\n# Check federated credentials in Azure\ntask azure:list-federated-creds\n\n# Verify OIDC issuer is accessible\ncurl ${OIDC_ISSUER_URL}/.well-known/openid-configuration\n</code></pre></p> <p>Solutions: - Recreate federated credentials: <code>task azure:delete-federated-creds &amp;&amp; task azure:federated-creds</code> - Verify OIDC issuer URL matches in Taskfile and Azure - Check that PREFIX and CLUSTER_NAME are identical to identity creation</p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#dns-resolution-issues","title":"DNS Resolution Issues","text":"<p>For External DNS deployments:</p> <p>Check: <pre><code># Check External DNS logs\nkubectl logs -n hypershift deployment/external-dns\n\n# Verify DNS records in Azure\naz network dns record-set list \\\n    --resource-group ${PERSISTENT_RG_NAME} \\\n    --zone-name ${PARENT_DNS_ZONE}\n</code></pre></p> <p>For deployments without External DNS:</p> <p>API server uses Azure LoadBalancer DNS. Get the DNS name: <pre><code>kubectl get svc -n clusters-${CLUSTER_NAME} kube-apiserver\n\n# Use the EXTERNAL-IP or LoadBalancer hostname\n</code></pre></p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#precondition-failures","title":"Precondition Failures","text":"<p>If <code>task cluster:create</code> reports missing preconditions:</p> <pre><code># Check managed resource group exists\ntask azure:infra\n\n# Check workload identities file exists\ntask azure:identities\n\n# Check network IDs file exists\nls -l .azure-net-ids\n\n# Check credentials files\nls -l ${AZURE_CREDS} ${PULL_SECRET}\n</code></pre>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#understanding-cluster-components","title":"Understanding Cluster Components","text":""},{"location":"how-to/azure/self-managed/05-hosted-cluster/#control-plane-architecture","title":"Control Plane Architecture","text":"<p>The hosted control plane consists of pods running on the management cluster:</p> <pre><code># View control plane namespace\nkubectl get ns | grep clusters-${CLUSTER_NAME}\n\n# View control plane pods\nkubectl get pods -n clusters-${CLUSTER_NAME}\n</code></pre> <p>Key components: - kube-apiserver: Cluster API endpoint - etcd: Control plane data store - kube-controller-manager: Cluster controllers - kube-scheduler: Pod scheduling - cluster-version-operator: Manages cluster version and upgrades - openshift-controller-manager: OpenShift-specific controllers</p>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#data-plane-architecture","title":"Data Plane Architecture","text":"<p>Worker nodes run as standard Azure VMs:</p> <pre><code># View VMs in Azure\naz vm list \\\n    --resource-group ${MANAGED_RG_NAME} \\\n    --output table\n\n# SSH to a node (if SSH key was generated)\nssh -i ${CLUSTER_NAME}-ssh-key core@&lt;node-ip&gt;\n</code></pre>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#workload-identity-integration","title":"Workload Identity Integration","text":"<p>Each component authenticates to Azure using federated credentials:</p> graph LR     A[Service Account Token] --&gt;|Signed by| B[SA Signing Key]     B --&gt;|Presented to| C[OIDC Issuer]     C --&gt;|Validated by| D[Azure AD]     D --&gt;|Grants| E[Managed Identity Token]     E --&gt;|Accesses| F[Azure APIs]"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#next-steps","title":"Next Steps","text":"<p>With your hosted cluster running:</p> <ul> <li>Day 2 Operations - Upgrades, scaling, configuration</li> <li>Troubleshooting - Diagnostics and common issues</li> <li>Deploy your applications using standard OpenShift/Kubernetes workflows</li> </ul>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#quick-reference","title":"Quick Reference","text":""},{"location":"how-to/azure/self-managed/05-hosted-cluster/#essential-commands","title":"Essential Commands","text":"<pre><code># Create cluster (full workflow)\ntask azure:identities\ntask azure:federated-creds\ntask azure:infra\ntask cluster:create\n\n# Access cluster\nhypershift create kubeconfig --name ${CLUSTER_NAME} &gt; kubeconfig\nexport KUBECONFIG=kubeconfig\n\n# Check cluster status\nkubectl get hostedcluster,nodepool -n ${CLUSTER_NAMESPACE}\n\n# Destroy cluster\ntask cluster:destroy\ntask azure:delete-infra\n</code></pre>"},{"location":"how-to/azure/self-managed/05-hosted-cluster/#task-summary","title":"Task Summary","text":"Task Purpose Lifecycle <code>task azure:identities</code> Create managed identities Persistent <code>task azure:federated-creds</code> Create federated credentials Persistent <code>task azure:infra</code> Provision VNet, NSG, resource groups Temporary <code>task cluster:create</code> Deploy HostedCluster Temporary <code>task cluster:destroy</code> Delete HostedCluster - <code>task azure:delete-infra</code> Remove VNet, NSG, resource groups - <code>task azure:delete-identities</code> Remove managed identities (CAUTION) - <code>task azure:delete-federated-creds</code> Remove federated credentials -"},{"location":"how-to/azure/self-managed/06-day2-operations/","title":"Day 2 Operations","text":"<p>This guide covers common operational tasks for managing HyperShift hosted clusters after initial deployment.</p> <p>Persona: HyperShift Administrator</p> <p>Day 2 operations are performed on running clusters to manage capacity, upgrade versions, and maintain cluster health.</p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#prerequisites","title":"Prerequisites","text":"<ul> <li>At least one HostedCluster successfully deployed</li> <li>kubectl/oc access to the management cluster</li> <li>HyperShift CLI binary available</li> </ul>"},{"location":"how-to/azure/self-managed/06-day2-operations/#managing-nodepools","title":"Managing NodePools","text":"<p>NodePools define the worker node configuration and count for your hosted cluster. You can add, modify, scale, and delete NodePools as your workload requirements change.</p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#understanding-nodepools","title":"Understanding NodePools","text":"<p>What is a NodePool? - Defines a group of worker nodes with identical configuration - Specifies VM size, node count, and Azure-specific settings - Can have multiple NodePools per cluster with different configurations - Each NodePool manages its own set of Azure VMs</p> <p>Use cases for multiple NodePools: - Different VM sizes for different workload types (CPU vs memory optimized) - Separate pools for different availability zones - Testing new VM configurations before migrating workloads - Gradual node upgrades with blue/green approaches</p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#adding-additional-nodepools","title":"Adding Additional NodePools","text":"<p>Create a new NodePool with different configuration:</p> <pre><code># Set variables (from your Taskfile or environment)\nCLUSTER_NAME=\"myprefix-hc\"\nCLUSTER_NAMESPACE=\"clusters\"\nAZURE_CREDS=\"azure-credentials.json\"\n\n# Create a new NodePool with larger VMs\nhypershift create nodepool azure \\\n    --cluster-name \"${CLUSTER_NAME}\" \\\n    --namespace \"${CLUSTER_NAMESPACE}\" \\\n    --name \"${CLUSTER_NAME}-large-workers\" \\\n    --node-count 3 \\\n    --azure-instance-type Standard_D8s_v3 \\\n    --azure-creds ${AZURE_CREDS}\n</code></pre> <p>Common VM sizes: - <code>Standard_D2s_v3</code>: 2 vCPU, 8GB RAM (default, balanced) - <code>Standard_D4s_v3</code>: 4 vCPU, 16GB RAM (larger workloads) - <code>Standard_D8s_v3</code>: 8 vCPU, 32GB RAM (high-performance) - <code>Standard_E4s_v3</code>: 4 vCPU, 32GB RAM (memory-optimized) - <code>Standard_F8s_v2</code>: 8 vCPU, 16GB RAM (compute-optimized)</p> <p>See Azure VM sizes for complete list.</p> <p>Verify NodePool creation: <pre><code># Check NodePool status\nkubectl get nodepool -n ${CLUSTER_NAMESPACE}\n\n# Watch nodes join the cluster (switch to hosted cluster context)\nexport KUBECONFIG=${CLUSTER_NAME}-kubeconfig\nkubectl get nodes -w\n</code></pre></p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#configuring-nodepool-marketplace-images","title":"Configuring NodePool Marketplace Images","text":"<p>For OpenShift 4.20+, NodePools automatically use the release payload defaults. For custom configurations:</p> <p>Option 1: Specify VM Generation Only <pre><code>hypershift create nodepool azure \\\n    --cluster-name \"${CLUSTER_NAME}\" \\\n    --namespace \"${CLUSTER_NAMESPACE}\" \\\n    --name \"${CLUSTER_NAME}-gen1-workers\" \\\n    --node-count 2 \\\n    --image-generation Gen1 \\\n    --azure-creds ${AZURE_CREDS}\n</code></pre></p> <p>Option 2: Use Custom Marketplace Image <pre><code>hypershift create nodepool azure \\\n    --cluster-name \"${CLUSTER_NAME}\" \\\n    --namespace \"${CLUSTER_NAMESPACE}\" \\\n    --name \"${CLUSTER_NAME}-custom-workers\" \\\n    --node-count 2 \\\n    --marketplace-publisher azureopenshift \\\n    --marketplace-offer aro4 \\\n    --marketplace-sku aro_421 \\\n    --marketplace-version 421.0.20250101 \\\n    --azure-creds ${AZURE_CREDS}\n</code></pre></p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#scaling-nodepools","title":"Scaling NodePools","text":"<p>Adjust the number of worker nodes in a NodePool:</p> <p>Scale using kubectl: <pre><code># Scale to 5 replicas\nkubectl scale nodepool/${CLUSTER_NAME} \\\n    -n ${CLUSTER_NAMESPACE} \\\n    --replicas=5\n\n# Verify scaling\nkubectl get nodepool -n ${CLUSTER_NAMESPACE}\n</code></pre></p> <p>Scale using patch: <pre><code># Scale specific NodePool\nkubectl patch nodepool/${CLUSTER_NAME}-large-workers \\\n    -n ${CLUSTER_NAMESPACE} \\\n    --type merge \\\n    --patch '{\"spec\":{\"replicas\":10}}'\n</code></pre></p> <p>Scale using edit: <pre><code># Edit NodePool interactively\nkubectl edit nodepool/${CLUSTER_NAME} -n ${CLUSTER_NAMESPACE}\n\n# Modify spec.replicas field and save\n</code></pre></p> <p>Monitor scaling progress: <pre><code># Watch NodePool status\nkubectl get nodepool -n ${CLUSTER_NAMESPACE} -w\n\n# Watch machines being created\nkubectl get machines -n clusters-${CLUSTER_NAME} -w\n\n# In hosted cluster context, watch nodes joining\nexport KUBECONFIG=${CLUSTER_NAME}-kubeconfig\nkubectl get nodes -w\n</code></pre></p> <p>Scaling Best Practices</p> <ul> <li>Scale gradually in production (add/remove 1-2 nodes at a time)</li> <li>Monitor cluster resource usage before scaling down</li> <li>Ensure workloads have PodDisruptionBudgets configured</li> <li>Drain nodes before scaling down to avoid disruption</li> </ul>"},{"location":"how-to/azure/self-managed/06-day2-operations/#deleting-nodepools","title":"Deleting NodePools","text":"<p>Remove a NodePool when no longer needed:</p> <pre><code># Delete a specific NodePool\nkubectl delete nodepool/${CLUSTER_NAME}-large-workers \\\n    -n ${CLUSTER_NAMESPACE}\n\n# Verify deletion\nkubectl get nodepool -n ${CLUSTER_NAMESPACE}\n</code></pre> <p>NodePool Deletion</p> <p>Deleting a NodePool:</p> <ul> <li>Immediately deletes the NodePool resource</li> <li>Terminates all Azure VMs in that NodePool</li> <li>Evicts all pods running on those nodes</li> <li>Cannot be undone</li> </ul> <p>Before deleting: - Drain workloads to other nodes - Ensure sufficient capacity remains - Verify no critical workloads are pinned to specific nodes</p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#upgrading-clusters","title":"Upgrading Clusters","text":"<p>Upgrade hosted clusters to new OpenShift versions by changing the release image.</p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#understanding-hypershift-upgrades","title":"Understanding HyperShift Upgrades","text":"<p>How upgrades work: 1. Update the HostedCluster <code>spec.release.image</code> field 2. Control plane components upgrade first (pods on management cluster) 3. NodePools upgrade next (worker nodes replaced or upgraded) 4. Cluster operators coordinate the upgrade process</p> <p>Upgrade characteristics: - Control plane upgrades are fast (pod restarts) - Worker node upgrades create new VMs and drain old ones - Minimal downtime with proper planning - Can upgrade control plane and NodePools separately</p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#upgrading-the-control-plane","title":"Upgrading the Control Plane","text":"<p>Update the cluster release image:</p> <pre><code># Variables\nCLUSTER_NAME=\"myprefix-hc\"\nCLUSTER_NAMESPACE=\"clusters\"\nNEW_RELEASE_IMAGE=\"quay.io/openshift-release-dev/ocp-release:4.21.1-x86_64\"\n\n# Patch the HostedCluster\nkubectl patch hostedcluster/${CLUSTER_NAME} \\\n    -n ${CLUSTER_NAMESPACE} \\\n    --type merge \\\n    --patch \"{\\\"spec\\\":{\\\"release\\\":{\\\"image\\\":\\\"${NEW_RELEASE_IMAGE}\\\"}}}\"\n</code></pre> <p>Monitor control plane upgrade: <pre><code># Watch HostedCluster status\nkubectl get hostedcluster ${CLUSTER_NAME} \\\n    -n ${CLUSTER_NAMESPACE} \\\n    -w\n\n# Check control plane pod rollouts\nkubectl get pods -n clusters-${CLUSTER_NAME} -w\n\n# In hosted cluster context, check cluster version\nexport KUBECONFIG=${CLUSTER_NAME}-kubeconfig\nkubectl get clusterversion\n</code></pre></p> <p>Expected progression: <pre><code># HostedCluster shows progressing\nNAME          VERSION   AVAILABLE   PROGRESSING   MESSAGE\nmyprefix-hc   4.21.0    True        True          Upgrading to 4.21.1\n\n# After completion\nNAME          VERSION   AVAILABLE   PROGRESSING   MESSAGE\nmyprefix-hc   4.21.1    True        False         The hosted cluster is available\n</code></pre></p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#upgrading-nodepools","title":"Upgrading NodePools","text":"<p>Upgrade worker nodes to match the control plane version:</p> <pre><code># Option 1: Patch the NodePool release\nkubectl patch nodepool/${CLUSTER_NAME} \\\n    -n ${CLUSTER_NAMESPACE} \\\n    --type merge \\\n    --patch \"{\\\"spec\\\":{\\\"release\\\":{\\\"image\\\":\\\"${NEW_RELEASE_IMAGE}\\\"}}}\"\n\n# Option 2: Edit NodePool interactively\nkubectl edit nodepool/${CLUSTER_NAME} -n ${CLUSTER_NAMESPACE}\n# Update spec.release.image and save\n</code></pre> <p>Monitor NodePool upgrade: <pre><code># Watch NodePool status\nkubectl get nodepool -n ${CLUSTER_NAMESPACE} -w\n\n# Watch machines being replaced\nkubectl get machines -n clusters-${CLUSTER_NAME} -w\n\n# In hosted cluster context, watch node versions\nexport KUBECONFIG=${CLUSTER_NAME}-kubeconfig\nkubectl get nodes -w\n</code></pre></p> <p>Upgrade strategies:</p> <p>Replace Strategy (default): - Creates new VMs with new version - Drains and deletes old VMs - Rolling replacement maintains capacity - Safer but slower</p> <p>InPlace Strategy: - Upgrades existing VMs in place - Faster but requires node reboots - Less common in cloud environments</p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#upgrade-best-practices","title":"Upgrade Best Practices","text":"<p>Pre-Upgrade Checklist</p> <p>Before upgrading:</p> <ul> <li>[ ] Review release notes for breaking changes</li> <li>[ ] Test upgrade in non-production cluster first</li> <li>[ ] Ensure cluster operators are healthy: <code>kubectl get co</code></li> <li>[ ] Verify sufficient capacity in management cluster</li> <li>[ ] Back up critical data and configurations</li> <li>[ ] Notify users of potential disruption</li> <li>[ ] Plan maintenance window if required</li> </ul> <p>Upgrade sequence: 1. Upgrade control plane first 2. Wait for control plane to become Available 3. Verify all cluster operators are healthy 4. Upgrade NodePools (one at a time in production) 5. Verify cluster functionality</p> <p>Rollback: Downgrades are not supported. If issues occur: - Fix issues in upgraded version - Or restore from backup/recreate cluster</p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#monitoring-cluster-health","title":"Monitoring Cluster Health","text":""},{"location":"how-to/azure/self-managed/06-day2-operations/#checking-cluster-status","title":"Checking Cluster Status","text":"<p>From management cluster: <pre><code># HostedCluster health\nkubectl get hostedcluster ${CLUSTER_NAME} -n ${CLUSTER_NAMESPACE}\n\n# NodePool health\nkubectl get nodepool -n ${CLUSTER_NAMESPACE}\n\n# Control plane pods\nkubectl get pods -n clusters-${CLUSTER_NAME}\n\n# Control plane resource usage\nkubectl top pods -n clusters-${CLUSTER_NAME}\n</code></pre></p> <p>From hosted cluster: <pre><code># Switch to hosted cluster\nexport KUBECONFIG=${CLUSTER_NAME}-kubeconfig\n\n# Cluster version and available updates\nkubectl get clusterversion\n\n# Cluster operators\nkubectl get co\n\n# Node health\nkubectl get nodes\n\n# Node resource usage\nkubectl top nodes\n</code></pre></p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#key-health-indicators","title":"Key Health Indicators","text":"<p>Healthy HostedCluster: <pre><code>status:\n  conditions:\n  - type: Available\n    status: \"True\"\n  - type: Progressing\n    status: \"False\"\n  - type: Degraded\n    status: \"False\"\n</code></pre></p> <p>Healthy NodePool: <pre><code>status:\n  replicas: 3        # Matches desired count\n  readyReplicas: 3   # All nodes ready\n  conditions:\n  - type: Ready\n    status: \"True\"\n</code></pre></p> <p>Healthy Cluster Operators: <pre><code># All operators should show AVAILABLE=True, DEGRADED=False\nkubectl get co\n\nNAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED\nauthentication                             4.21.0    True        False         False\ncloud-credential                           4.21.0    True        False         False\ncluster-autoscaler                         4.21.0    True        False         False\n...\n</code></pre></p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#cluster-metrics-and-logging","title":"Cluster Metrics and Logging","text":"<p>View control plane metrics (from management cluster): <pre><code># CPU and memory usage\nkubectl top pods -n clusters-${CLUSTER_NAME}\n\n# Node resource allocation on management cluster\nkubectl top nodes\n</code></pre></p> <p>View hosted cluster metrics: <pre><code># Switch to hosted cluster\nexport KUBECONFIG=${CLUSTER_NAME}-kubeconfig\n\n# Worker node metrics\nkubectl top nodes\n\n# Pod metrics across cluster\nkubectl top pods --all-namespaces\n</code></pre></p> <p>Access control plane logs: <pre><code># From management cluster\nkubectl logs -n clusters-${CLUSTER_NAME} deployment/kube-apiserver\nkubectl logs -n clusters-${CLUSTER_NAME} deployment/kube-controller-manager\nkubectl logs -n clusters-${CLUSTER_NAME} statefulset/etcd\n</code></pre></p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#modifying-cluster-configuration","title":"Modifying Cluster Configuration","text":""},{"location":"how-to/azure/self-managed/06-day2-operations/#changing-cluster-networking","title":"Changing Cluster Networking","text":"<p>Update cluster network settings:</p> <pre><code># Edit HostedCluster\nkubectl edit hostedcluster/${CLUSTER_NAME} -n ${CLUSTER_NAMESPACE}\n\n# Modify spec.networking fields\n# Note: Some changes require cluster recreation\n</code></pre> <p>Network Configuration Changes</p> <p>Many networking changes cannot be applied to running clusters and require recreation:</p> <ul> <li>Service network CIDR</li> <li>Pod network CIDR</li> <li>Network type (OVNKubernetes vs others)</li> </ul> <p>Changes that can be applied: - DNS configuration - Proxy settings</p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#managing-cluster-credentials","title":"Managing Cluster Credentials","text":"<p>Rotate kubeadmin password: <pre><code># From management cluster\nkubectl delete secret kubeadmin -n clusters-${CLUSTER_NAME}\n\n# Controller will regenerate with new password\n# Retrieve new password\nkubectl get secret kubeadmin \\\n    -n clusters-${CLUSTER_NAME} \\\n    -o jsonpath='{.data.password}' | base64 -d\n</code></pre></p> <p>Add cluster administrators: <pre><code># From hosted cluster\nexport KUBECONFIG=${CLUSTER_NAME}-kubeconfig\n\n# Grant cluster-admin to user\nkubectl create clusterrolebinding admin-user \\\n    --clusterrole=cluster-admin \\\n    --user=admin@example.com\n</code></pre></p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#nodepool-advanced-configuration","title":"NodePool Advanced Configuration","text":""},{"location":"how-to/azure/self-managed/06-day2-operations/#configuring-node-labels-and-taints","title":"Configuring Node Labels and Taints","text":"<p>Add labels to NodePool: <pre><code>kubectl patch nodepool/${CLUSTER_NAME} \\\n    -n ${CLUSTER_NAMESPACE} \\\n    --type merge \\\n    --patch '{\"spec\":{\"nodeLabels\":{\"workload-type\":\"database\"}}}'\n</code></pre></p> <p>Add taints to NodePool: <pre><code>kubectl edit nodepool/${CLUSTER_NAME} -n ${CLUSTER_NAMESPACE}\n\n# Add to spec:\nspec:\n  taints:\n  - key: dedicated\n    value: database\n    effect: NoSchedule\n</code></pre></p> <p>Verify labels and taints: <pre><code># From hosted cluster\nexport KUBECONFIG=${CLUSTER_NAME}-kubeconfig\nkubectl get nodes --show-labels\nkubectl describe node &lt;node-name&gt; | grep Taints\n</code></pre></p>"},{"location":"how-to/azure/self-managed/06-day2-operations/#configuring-root-volume-size","title":"Configuring Root Volume Size","text":"<p>Increase root volume size for NodePool VMs:</p> <pre><code>kubectl patch nodepool/${CLUSTER_NAME} \\\n    -n ${CLUSTER_NAMESPACE} \\\n    --type merge \\\n    --patch '{\"spec\":{\"platform\":{\"azure\":{\"diskSizeGB\":256}}}}'\n</code></pre> <p>Volume Size Changes</p> <ul> <li>Only affects new nodes created after the change</li> <li>Existing nodes keep their current disk size</li> <li>Scale down and up to replace nodes with new disk size</li> </ul>"},{"location":"how-to/azure/self-managed/06-day2-operations/#next-steps","title":"Next Steps","text":"<ul> <li>Troubleshooting - Diagnose and resolve common issues</li> <li>Reference - CLI commands and configuration reference</li> </ul>"},{"location":"how-to/azure/self-managed/06-day2-operations/#quick-reference","title":"Quick Reference","text":""},{"location":"how-to/azure/self-managed/06-day2-operations/#common-operations","title":"Common Operations","text":"<pre><code># Add NodePool\nhypershift create nodepool azure \\\n    --cluster-name ${CLUSTER_NAME} \\\n    --namespace ${CLUSTER_NAMESPACE} \\\n    --name ${CLUSTER_NAME}-new-pool \\\n    --node-count 3 \\\n    --azure-instance-type Standard_D4s_v3 \\\n    --azure-creds ${AZURE_CREDS}\n\n# Scale NodePool\nkubectl scale nodepool/${CLUSTER_NAME} \\\n    -n ${CLUSTER_NAMESPACE} \\\n    --replicas=5\n\n# Upgrade cluster\nNEW_VERSION=\"quay.io/openshift-release-dev/ocp-release:4.21.1-x86_64\"\nkubectl patch hostedcluster/${CLUSTER_NAME} \\\n    -n ${CLUSTER_NAMESPACE} \\\n    --type merge \\\n    --patch \"{\\\"spec\\\":{\\\"release\\\":{\\\"image\\\":\\\"${NEW_VERSION}\\\"}}}\"\n\n# Delete NodePool\nkubectl delete nodepool/${CLUSTER_NAME}-new-pool \\\n    -n ${CLUSTER_NAMESPACE}\n\n# Check cluster health\nkubectl get hostedcluster,nodepool -n ${CLUSTER_NAMESPACE}\nkubectl get co  # From hosted cluster context\n</code></pre>"},{"location":"how-to/azure/self-managed/07-troubleshooting/","title":"Troubleshooting","text":"<p>This guide provides diagnostic procedures and solutions for common issues encountered with HyperShift hosted clusters on Azure.</p>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"how-to/azure/self-managed/07-troubleshooting/#cluster-creation-failures","title":"Cluster Creation Failures","text":""},{"location":"how-to/azure/self-managed/07-troubleshooting/#authentication-errors-with-workload-identities","title":"Authentication Errors with Workload Identities","text":"<p>Symptom: Cluster creation fails with \"failed to authenticate\" or Azure authentication errors in control plane pods.</p> <p>Common error messages: <pre><code>Failed to get managed identity credential: AADSTS70021: No matching federated identity record found\n</code></pre></p> <p>Root causes: - Federated credentials not created or misconfigured - OIDC issuer URL mismatch - Service account namespace/name mismatch - Managed identity doesn't exist</p> <p>Diagnostic steps: <pre><code># 1. Verify managed identities exist\naz identity list \\\n    --resource-group ${PERSISTENT_RG_NAME} \\\n    --query \"[?contains(name, '${CLUSTER_NAME}')].{Name:name, ClientId:clientId}\" \\\n    --output table\n\n# 2. Check federated credentials for a specific identity\ntask azure:list-federated-creds\n\n# Or manually check:\naz identity federated-credential list \\\n    --identity-name \"${CLUSTER_NAME}-disk-csi\" \\\n    --resource-group ${PERSISTENT_RG_NAME} \\\n    --output table\n\n# 3. Verify OIDC issuer is accessible\ncurl -s \"${OIDC_ISSUER_URL}/.well-known/openid-configuration\" | jq .\n\n# 4. Verify OIDC issuer URL matches\nkubectl get hostedcluster ${CLUSTER_NAME} \\\n    -n ${CLUSTER_NAMESPACE} \\\n    -o jsonpath='{.spec.issuerURL}'\n</code></pre></p> <p>Solutions:</p> <p>Solution 1: Recreate federated credentials <pre><code># Delete and recreate federated credentials\ntask azure:delete-federated-creds\ntask azure:federated-creds\n</code></pre></p> <p>Solution 2: Verify configuration matches <pre><code># Ensure PREFIX and CLUSTER_NAME are identical between:\n# - Identity creation (task azure:identities)\n# - Federated credential creation (task azure:federated-creds)\n# - Cluster creation (task cluster:create)\n\n# Check Taskfile.yml values\ngrep -E 'PREFIX|CLUSTER_NAME' hack/dev-preview/Taskfile.yml\n</code></pre></p> <p>Solution 3: Verify OIDC issuer accessibility <pre><code># Test OIDC endpoint\ncurl -v \"${OIDC_ISSUER_URL}/.well-known/openid-configuration\"\n\n# Ensure storage account allows public access\naz storage account show \\\n    --name ${OIDC_STORAGE_ACCOUNT_NAME} \\\n    --resource-group ${PERSISTENT_RG_NAME} \\\n    --query \"allowBlobPublicAccess\"\n</code></pre></p>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#precondition-failures","title":"Precondition Failures","text":"<p>Symptom: <code>task cluster:create</code> fails with missing precondition errors.</p> <p>Error example: <pre><code>task: precondition not met: Managed resource group myprefix-managed-rg not found\n</code></pre></p> <p>Solutions: <pre><code># Missing managed resource group\ntask azure:infra\n\n# Missing workload identities\ntask azure:identities\n\n# Missing federated credentials\ntask azure:federated-creds\n\n# Missing network IDs file\n# This is created by task azure:infra\nls -l .azure-net-ids\n\n# Missing credential files\nls -l ${AZURE_CREDS} ${PULL_SECRET}\n</code></pre></p>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#azure-quota-or-capacity-issues","title":"Azure Quota or Capacity Issues","text":"<p>Symptom: Cluster creation fails with Azure quota exceeded errors.</p> <p>Error messages: <pre><code>Operation could not be completed as it results in exceeding approved Total Regional vCPUs quota\n</code></pre></p> <p>Solutions: <pre><code># Check current quota usage\naz vm list-usage \\\n    --location ${LOCATION} \\\n    --output table\n\n# Request quota increase through Azure portal:\n# Portal \u2192 Subscriptions \u2192 Usage + quotas \u2192 Request increase\n\n# Workaround: Use smaller VM size\n# In cluster creation, reduce node count or VM size\nNODE_POOL_REPLICAS=1  # Reduce from 2+\n</code></pre></p>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#control-plane-issues","title":"Control Plane Issues","text":""},{"location":"how-to/azure/self-managed/07-troubleshooting/#control-plane-pods-not-starting","title":"Control Plane Pods Not Starting","text":"<p>Symptom: Control plane pods stuck in <code>Pending</code>, <code>ImagePullBackOff</code>, or <code>CrashLoopBackOff</code>.</p> <p>Diagnostic steps: <pre><code># Check pod status\nkubectl get pods -n clusters-${CLUSTER_NAME}\n\n# Describe problematic pod\nkubectl describe pod &lt;pod-name&gt; -n clusters-${CLUSTER_NAME}\n\n# Check pod logs\nkubectl logs &lt;pod-name&gt; -n clusters-${CLUSTER_NAME}\n\n# Check events\nkubectl get events -n clusters-${CLUSTER_NAME} --sort-by='.lastTimestamp'\n</code></pre></p> <p>Common causes and solutions:</p> <p>Insufficient Resources on Management Cluster: <pre><code># Check node resources\nkubectl top nodes\n\n# Check pod resource requests\nkubectl describe pod &lt;pod-name&gt; -n clusters-${CLUSTER_NAME} | grep -A 5 Requests\n\n# Each hosted cluster needs approximately:\n# - 4 vCPU\n# - 8 GB memory\n\n# Solutions:\n# - Add more nodes to management cluster\n# - Delete unused hosted clusters\n# - Reduce resource requests (not recommended for production)\n</code></pre></p> <p>Image Pull Errors: <pre><code># Check pull secret\nkubectl get secret ${CLUSTER_NAME}-pull-secret \\\n    -n clusters-${CLUSTER_NAME} \\\n    -o jsonpath='{.data.\\.dockerconfigjson}' | base64 -d | jq .\n\n# Verify pull secret is valid at cloud.redhat.com\n# Recreate cluster with valid pull secret if needed\n</code></pre></p> <p>etcd Issues: <pre><code># Check etcd pods\nkubectl get pods -n clusters-${CLUSTER_NAME} -l app=etcd\n\n# Check etcd logs\nkubectl logs -n clusters-${CLUSTER_NAME} statefulset/etcd\n\n# Check etcd persistent volumes\nkubectl get pvc -n clusters-${CLUSTER_NAME}\n</code></pre></p>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#control-plane-performance-issues","title":"Control Plane Performance Issues","text":"<p>Symptom: Slow API response times or timeouts.</p> <p>Diagnostic steps: <pre><code># Check control plane pod CPU/memory usage\nkubectl top pods -n clusters-${CLUSTER_NAME}\n\n# Check API server logs for slow requests\nkubectl logs -n clusters-${CLUSTER_NAME} deployment/kube-apiserver | grep -i \"slow\\|timeout\"\n\n# Check etcd performance\nkubectl logs -n clusters-${CLUSTER_NAME} statefulset/etcd | grep -i \"slow\"\n</code></pre></p> <p>Solutions: - Scale up management cluster nodes - Reduce load on hosted cluster - Check management cluster storage performance</p>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#worker-node-issues","title":"Worker Node Issues","text":""},{"location":"how-to/azure/self-managed/07-troubleshooting/#worker-nodes-not-joining-cluster","title":"Worker Nodes Not Joining Cluster","text":"<p>Symptom: Azure VMs are created but don't appear in <code>kubectl get nodes</code> (from hosted cluster context).</p> <p>Diagnostic steps: <pre><code># 1. Check NodePool status\nkubectl get nodepool -n ${CLUSTER_NAMESPACE} -o yaml\n\n# 2. Check Machine resources\nkubectl get machines -n clusters-${CLUSTER_NAME}\n\n# 3. Check Azure VMs\naz vm list \\\n    --resource-group ${MANAGED_RG_NAME} \\\n    --output table\n\n# 4. Check ignition server logs\nkubectl logs -n clusters-${CLUSTER_NAME} deployment/ignition-server\n\n# 5. Get VM boot diagnostics (if enabled)\nVM_NAME=$(az vm list --resource-group ${MANAGED_RG_NAME} --query \"[0].name\" -o tsv)\naz vm boot-diagnostics get-boot-log \\\n    --resource-group ${MANAGED_RG_NAME} \\\n    --name ${VM_NAME}\n</code></pre></p> <p>Common causes and solutions:</p> <p>NSG Blocking Required Ports: <pre><code># Check NSG rules\naz network nsg show \\\n    --name ${NSG} \\\n    --resource-group ${NSG_RG_NAME}\n\n# Required inbound ports:\n# - 6443 (API server)\n# - 22623 (machine config server / ignition)\n# - 443 (ingress)\n# - 10250-10259 (kubelet, kube-proxy)\n\n# Fix: Update NSG rules\naz network nsg rule create \\\n    --resource-group ${NSG_RG_NAME} \\\n    --nsg-name ${NSG} \\\n    --name allow-ignition \\\n    --priority 1000 \\\n    --source-address-prefixes '*' \\\n    --destination-port-ranges 22623 \\\n    --access Allow \\\n    --protocol Tcp\n</code></pre></p> <p>Ignition Server Not Reachable: <pre><code># Check ignition server service\nkubectl get svc -n clusters-${CLUSTER_NAME} ignition-server\n\n# From a node, test connectivity (requires VM access)\n# curl -k https://ignition-server.clusters-${CLUSTER_NAME}.svc:443/config/worker\n</code></pre></p> <p>Workload Identity Issues: <pre><code># Check nodepool-mgmt identity has proper roles\naz role assignment list \\\n    --assignee $(az identity show \\\n        --name \"${CLUSTER_NAME}-nodepool-mgmt\" \\\n        --resource-group ${PERSISTENT_RG_NAME} \\\n        --query clientId -o tsv) \\\n    --output table\n</code></pre></p>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#node-status-shows-notready","title":"Node Status Shows NotReady","text":"<p>Symptom: Nodes appear in <code>kubectl get nodes</code> but show status <code>NotReady</code>.</p> <p>Diagnostic steps: <pre><code># From hosted cluster context\nexport KUBECONFIG=${CLUSTER_NAME}-kubeconfig\n\n# Check node status\nkubectl get nodes -o wide\n\n# Describe the NotReady node\nkubectl describe node &lt;node-name&gt;\n\n# Check node conditions\nkubectl get node &lt;node-name&gt; -o jsonpath='{.status.conditions}'  | jq .\n</code></pre></p> <p>Common causes: - Container runtime issues - Network plugin not ready - Insufficient resources on node - Disk pressure</p> <p>Solutions: <pre><code># Check kubelet logs (requires SSH access to node)\n# ssh -i ${CLUSTER_NAME}-ssh-key core@&lt;node-ip&gt;\n# sudo journalctl -u kubelet -f\n\n# Delete and recreate the node\nkubectl delete node &lt;node-name&gt;\n# NodePool controller will create replacement VM\n</code></pre></p>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#dns-and-networking-issues","title":"DNS and Networking Issues","text":""},{"location":"how-to/azure/self-managed/07-troubleshooting/#dns-records-not-created-external-dns","title":"DNS Records Not Created (External DNS)","text":"<p>Symptom: Cannot resolve cluster API or app hostnames via DNS.</p> <p>Diagnostic steps: <pre><code># Check External DNS pod\nkubectl get pods -n hypershift -l app=external-dns\n\n# Check External DNS logs\nkubectl logs -n hypershift deployment/external-dns\n\n# Check Route resources\nkubectl get routes -n clusters-${CLUSTER_NAME}\n\n# Verify DNS zone exists\naz network dns zone show \\\n    --name ${PARENT_DNS_ZONE} \\\n    --resource-group ${PERSISTENT_RG_NAME}\n\n# Check DNS records\naz network dns record-set list \\\n    --resource-group ${PERSISTENT_RG_NAME} \\\n    --zone-name ${PARENT_DNS_ZONE} \\\n    --output table\n</code></pre></p> <p>Common issues:</p> <p>External DNS Authentication Failures: <pre><code># Check External DNS secret\nkubectl get secret azure-config-file -n default\n\n# Verify service principal permissions\naz role assignment list \\\n    --assignee $(jq -r '.aadClientId' &lt; azure_mgmt.json) \\\n    --output table\n\n# Should have \"DNS Zone Contributor\" role on DNS zone resource group\n</code></pre></p> <p>External DNS Not Watching Correct Namespace: <pre><code># Check External DNS configuration\nkubectl get deployment external-dns -n hypershift -o yaml | grep -A 10 args\n\n# Ensure it's watching routes in hosted cluster namespaces\n</code></pre></p> <p>Solution: Recreate External DNS configuration <pre><code># Reinstall HyperShift operator with correct External DNS config\nhypershift install \\\n    --external-dns-provider=azure \\\n    --external-dns-credentials azure_mgmt.json \\\n    --pull-secret ${PULL_SECRET} \\\n    --external-dns-domain-filter ${EXTRN_DNS_ZONE_NAME} \\\n    --limit-crd-install Azure \\\n    --render | kubectl apply -f -\n</code></pre></p>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#loadbalancer-service-stuck-pending-without-external-dns","title":"LoadBalancer Service Stuck Pending (Without External DNS)","text":"<p>Symptom: API server LoadBalancer service doesn't get external IP.</p> <p>Diagnostic steps: <pre><code># Check service\nkubectl get svc -n clusters-${CLUSTER_NAME} kube-apiserver\n\n# Check cloud-controller-manager logs\nkubectl logs -n clusters-${CLUSTER_NAME} deployment/cloud-controller-manager\n\n# Check Azure load balancer\naz network lb list \\\n    --resource-group ${MANAGED_RG_NAME} \\\n    --output table\n</code></pre></p> <p>Solutions: - Verify cloud-provider workload identity has correct permissions - Check Azure subscription quotas for load balancers - Verify network connectivity from management cluster to Azure APIs</p>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#upgrade-issues","title":"Upgrade Issues","text":""},{"location":"how-to/azure/self-managed/07-troubleshooting/#upgrade-stuck-or-fails","title":"Upgrade Stuck or Fails","text":"<p>Symptom: Cluster upgrade doesn't complete or control plane pods crash during upgrade.</p> <p>Diagnostic steps: <pre><code># Check HostedCluster upgrade status\nkubectl get hostedcluster ${CLUSTER_NAME} \\\n    -n ${CLUSTER_NAMESPACE} \\\n    -o jsonpath='{.status.version}'\n\n# Check cluster version operator\nkubectl logs -n clusters-${CLUSTER_NAME} deployment/cluster-version-operator\n\n# Check control plane pod status\nkubectl get pods -n clusters-${CLUSTER_NAME}\n\n# From hosted cluster, check cluster operators\nexport KUBECONFIG=${CLUSTER_NAME}-kubeconfig\nkubectl get co\n</code></pre></p> <p>Solutions: <pre><code># If upgrade is stuck, check for:\n# - Degraded cluster operators before upgrade\n# - Insufficient resources during upgrade\n# - Breaking changes in release notes\n\n# Cannot rollback - must fix forward or recreate cluster\n</code></pre></p>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#diagnostic-commands-reference","title":"Diagnostic Commands Reference","text":""},{"location":"how-to/azure/self-managed/07-troubleshooting/#hostedcluster-health","title":"HostedCluster Health","text":"<pre><code># Overall cluster status\nkubectl get hostedcluster ${CLUSTER_NAME} -n ${CLUSTER_NAMESPACE}\n\n# Detailed status with conditions\nkubectl get hostedcluster ${CLUSTER_NAME} -n ${CLUSTER_NAMESPACE} -o yaml\n\n# Check HostedControlPlane\nkubectl get hostedcontrolplane -n clusters-${CLUSTER_NAME}\n\n# Describe for events\nkubectl describe hostedcluster ${CLUSTER_NAME} -n ${CLUSTER_NAMESPACE}\n</code></pre>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#nodepool-health","title":"NodePool Health","text":"<pre><code># NodePool status\nkubectl get nodepool -n ${CLUSTER_NAMESPACE}\n\n# Detailed NodePool information\nkubectl get nodepool ${CLUSTER_NAME} -n ${CLUSTER_NAMESPACE} -o yaml\n\n# Check Machine resources\nkubectl get machines -n clusters-${CLUSTER_NAME}\n\n# Machine details\nkubectl describe machine &lt;machine-name&gt; -n clusters-${CLUSTER_NAME}\n</code></pre>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#control-plane-diagnostics","title":"Control Plane Diagnostics","text":"<pre><code># All control plane pods\nkubectl get pods -n clusters-${CLUSTER_NAME}\n\n# Pod resource usage\nkubectl top pods -n clusters-${CLUSTER_NAME}\n\n# Specific component logs\nkubectl logs -n clusters-${CLUSTER_NAME} deployment/kube-apiserver -f\nkubectl logs -n clusters-${CLUSTER_NAME} deployment/kube-controller-manager -f\nkubectl logs -n clusters-${CLUSTER_NAME} deployment/kube-scheduler -f\nkubectl logs -n clusters-${CLUSTER_NAME} statefulset/etcd -f\n\n# Check all resources in control plane namespace\nkubectl get all -n clusters-${CLUSTER_NAME}\n\n# Events in control plane namespace\nkubectl get events -n clusters-${CLUSTER_NAME} --sort-by='.lastTimestamp'\n</code></pre>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#hypershift-operator-diagnostics","title":"HyperShift Operator Diagnostics","text":"<pre><code># Operator pod status\nkubectl get pods -n hypershift\n\n# Operator logs\nkubectl logs -n hypershift deployment/operator -f\n\n# Check operator reconciliation\nkubectl logs -n hypershift deployment/operator | grep ${CLUSTER_NAME}\n</code></pre>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#azure-resource-diagnostics","title":"Azure Resource Diagnostics","text":"<pre><code># List all resource groups for cluster\naz group list \\\n    --query \"[?contains(name, '${PREFIX}')].{Name:name, Location:location, State:properties.provisioningState}\" \\\n    --output table\n\n# Check VMs\naz vm list \\\n    --resource-group ${MANAGED_RG_NAME} \\\n    --output table\n\n# Check VM power state\naz vm list \\\n    --resource-group ${MANAGED_RG_NAME} \\\n    --show-details \\\n    --query \"[].{Name:name, PowerState:powerState}\" \\\n    --output table\n\n# Check load balancers\naz network lb list \\\n    --resource-group ${MANAGED_RG_NAME} \\\n    --output table\n\n# Check NSG rules\naz network nsg rule list \\\n    --nsg-name ${NSG} \\\n    --resource-group ${NSG_RG_NAME} \\\n    --output table\n</code></pre>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#workload-identity-diagnostics","title":"Workload Identity Diagnostics","text":"<pre><code># List managed identities\ntask azure:list-identities\n\n# Or manually:\naz identity list \\\n    --resource-group ${PERSISTENT_RG_NAME} \\\n    --query \"[?contains(name, '${CLUSTER_NAME}')]\" \\\n    --output table\n\n# List federated credentials\ntask azure:list-federated-creds\n\n# Check specific component identity\naz identity show \\\n    --name \"${CLUSTER_NAME}-disk-csi\" \\\n    --resource-group ${PERSISTENT_RG_NAME}\n\n# Check role assignments\naz role assignment list \\\n    --assignee $(az identity show \\\n        --name \"${CLUSTER_NAME}-disk-csi\" \\\n        --resource-group ${PERSISTENT_RG_NAME} \\\n        --query principalId -o tsv) \\\n    --output table\n</code></pre>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#collecting-debug-information","title":"Collecting Debug Information","text":""},{"location":"how-to/azure/self-managed/07-troubleshooting/#comprehensive-cluster-dump","title":"Comprehensive Cluster Dump","text":"<pre><code>#!/bin/bash\n# Save to a file: cluster-debug.sh\n\nCLUSTER_NAME=\"myprefix-hc\"\nCLUSTER_NAMESPACE=\"clusters\"\nOUTPUT_DIR=\"debug-$(date +%Y%m%d-%H%M%S)\"\n\nmkdir -p ${OUTPUT_DIR}\n\necho \"Collecting debug information for ${CLUSTER_NAME}...\"\n\n# HostedCluster\nkubectl get hostedcluster ${CLUSTER_NAME} -n ${CLUSTER_NAMESPACE} -o yaml &gt; ${OUTPUT_DIR}/hostedcluster.yaml\n\n# NodePools\nkubectl get nodepool -n ${CLUSTER_NAMESPACE} -o yaml &gt; ${OUTPUT_DIR}/nodepools.yaml\n\n# Control plane pods\nkubectl get pods -n clusters-${CLUSTER_NAME} -o wide &gt; ${OUTPUT_DIR}/control-plane-pods.txt\nkubectl get pods -n clusters-${CLUSTER_NAME} -o yaml &gt; ${OUTPUT_DIR}/control-plane-pods.yaml\n\n# Control plane events\nkubectl get events -n clusters-${CLUSTER_NAME} --sort-by='.lastTimestamp' &gt; ${OUTPUT_DIR}/control-plane-events.txt\n\n# Operator logs\nkubectl logs -n hypershift deployment/operator --tail=1000 &gt; ${OUTPUT_DIR}/operator-logs.txt\n\n# Control plane logs\nfor deployment in kube-apiserver kube-controller-manager kube-scheduler; do\n    kubectl logs -n clusters-${CLUSTER_NAME} deployment/${deployment} --tail=500 &gt; ${OUTPUT_DIR}/${deployment}-logs.txt\ndone\n\n# Machines\nkubectl get machines -n clusters-${CLUSTER_NAME} -o yaml &gt; ${OUTPUT_DIR}/machines.yaml\n\necho \"Debug information collected in ${OUTPUT_DIR}/\"\ntar -czf ${OUTPUT_DIR}.tar.gz ${OUTPUT_DIR}/\necho \"Archive created: ${OUTPUT_DIR}.tar.gz\"\n</code></pre>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"how-to/azure/self-managed/07-troubleshooting/#before-requesting-support","title":"Before Requesting Support","text":"<p>Collect the following information:</p> <ol> <li>Cluster details:</li> <li>OpenShift version</li> <li>HyperShift operator version</li> <li> <p>Azure region</p> </li> <li> <p>Error symptoms:</p> </li> <li>Exact error messages</li> <li>When the issue started</li> <li> <p>Steps to reproduce</p> </li> <li> <p>Debug information (use script above):</p> </li> <li>HostedCluster YAML</li> <li>Control plane pod status and logs</li> <li>NodePool status</li> <li>Operator logs</li> </ol>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#support-channels","title":"Support Channels","text":"<p>For Developer Preview: - HyperShift GitHub Issues - HyperShift Slack (#hypershift channel)</p> <p>For Production (when GA): - Red Hat Support Portal - OpenShift support cases</p>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#useful-resources","title":"Useful Resources","text":"<ul> <li>HyperShift Documentation</li> <li>Azure Workload Identity Troubleshooting</li> <li>OpenShift Documentation</li> <li>Azure CLI Documentation</li> </ul>"},{"location":"how-to/azure/self-managed/07-troubleshooting/#next-steps","title":"Next Steps","text":"<ul> <li>Reference - CLI commands and configuration reference</li> <li>Understanding HyperShift - Review architecture concepts</li> </ul>"},{"location":"how-to/azure/self-managed/08-reference/","title":"Reference","text":"<p>This page provides reference material, appendices, and quick-reference guides for HyperShift on Azure.</p>"},{"location":"how-to/azure/self-managed/08-reference/#taskfile-command-reference","title":"Taskfile Command Reference","text":""},{"location":"how-to/azure/self-managed/08-reference/#prerequisites-and-foundation","title":"Prerequisites and Foundation","text":"Command Purpose When to Run Lifecycle <code>task prereq:validate</code> Validate tools and configuration Before starting N/A <code>task prereq:keys</code> Generate SA signing keys Once per environment Persistent <code>task azure:oidc</code> Create OIDC issuer Once per environment Persistent (shared) <code>task azure:delete-oidc</code> Delete OIDC issuer Environment teardown Destructive"},{"location":"how-to/azure/self-managed/08-reference/#per-cluster-resources","title":"Per-Cluster Resources","text":"Command Purpose When to Run Lifecycle <code>task azure:identities</code> Create managed identities (7) Once per cluster Persistent <code>task azure:federated-creds</code> Create federated credentials (14) Once per cluster Persistent <code>task azure:infra</code> Provision VNet, NSG, RGs Before cluster creation Temporary <code>task cluster:create</code> Deploy HostedCluster After all prerequisites Temporary <code>task cluster:destroy</code> Delete HostedCluster Cluster teardown Destructive <code>task azure:delete-infra</code> Delete VNet, NSG, RGs After cluster destroyed Destructive <code>task azure:delete-federated-creds</code> Delete federated credentials Permanent removal Destructive <code>task azure:delete-identities</code> Delete managed identities Permanent removal Destructive"},{"location":"how-to/azure/self-managed/08-reference/#information-commands","title":"Information Commands","text":"Command Purpose <code>task azure:list-identities</code> List managed identities for cluster <code>task azure:list-federated-creds</code> List federated credentials <code>task --list</code> Show all available tasks"},{"location":"how-to/azure/self-managed/08-reference/#hypershift-cli-reference","title":"HyperShift CLI Reference","text":""},{"location":"how-to/azure/self-managed/08-reference/#cluster-creation","title":"Cluster Creation","text":"<pre><code>hypershift create cluster azure \\\n    --name &lt;cluster-name&gt; \\\n    --namespace &lt;namespace&gt; \\\n    --azure-creds &lt;credentials-file&gt; \\\n    --location &lt;azure-region&gt; \\\n    --node-pool-replicas &lt;count&gt; \\\n    --base-domain &lt;domain&gt; \\\n    --pull-secret &lt;pull-secret-file&gt; \\\n    --release-image &lt;ocp-release-image&gt; \\\n    --resource-group-name &lt;managed-rg&gt; \\\n    --vnet-id &lt;vnet-id&gt; \\\n    --subnet-id &lt;subnet-id&gt; \\\n    --network-security-group-id &lt;nsg-id&gt; \\\n    --sa-token-issuer-private-key-path &lt;private-key&gt; \\\n    --oidc-issuer-url &lt;oidc-url&gt; \\\n    --workload-identities-file &lt;identities-json&gt; \\\n    [--dns-zone-rg-name &lt;dns-rg&gt;] \\\n    [--external-dns-domain &lt;dns-domain&gt;] \\\n    [--control-plane-operator-image &lt;image&gt;] \\\n    [--marketplace-publisher &lt;publisher&gt;] \\\n    [--marketplace-offer &lt;offer&gt;] \\\n    [--marketplace-sku &lt;sku&gt;] \\\n    [--marketplace-version &lt;version&gt;] \\\n    [--image-generation Gen1|Gen2] \\\n    [--assign-service-principal-roles] \\\n    [--diagnostics-storage-account-type Managed] \\\n    [--generate-ssh]\n</code></pre> <p>Key Flags:</p> <ul> <li><code>--name</code>: Cluster name (must match identity creation)</li> <li><code>--namespace</code>: Kubernetes namespace for HostedCluster resource</li> <li><code>--azure-creds</code>: Service principal credentials JSON file</li> <li><code>--location</code>: Azure region (e.g., eastus, westus2)</li> <li><code>--node-pool-replicas</code>: Initial worker node count</li> <li><code>--base-domain</code>: Base domain for cluster (used for ingress)</li> <li><code>--pull-secret</code>: OpenShift pull secret from cloud.redhat.com</li> <li><code>--release-image</code>: OpenShift release image URL</li> <li><code>--resource-group-name</code>: Azure resource group for cluster resources</li> <li><code>--vnet-id</code>: Full Azure resource ID of VNet</li> <li><code>--subnet-id</code>: Full Azure resource ID of subnet</li> <li><code>--network-security-group-id</code>: Full Azure resource ID of NSG</li> <li><code>--sa-token-issuer-private-key-path</code>: Path to SA signing private key</li> <li><code>--oidc-issuer-url</code>: OIDC issuer endpoint URL</li> <li><code>--workload-identities-file</code>: JSON file with managed identity client IDs</li> <li><code>--dns-zone-rg-name</code>: Resource group containing DNS zone (External DNS only)</li> <li><code>--external-dns-domain</code>: DNS domain for External DNS (External DNS only)</li> <li><code>--control-plane-operator-image</code>: Override CPO image (optional)</li> <li><code>--assign-service-principal-roles</code>: Auto-assign Azure roles</li> <li><code>--generate-ssh</code>: Generate SSH key for node access</li> <li><code>--diagnostics-storage-account-type</code>: Boot diagnostics storage (Managed or Disabled)</li> </ul>"},{"location":"how-to/azure/self-managed/08-reference/#cluster-destruction","title":"Cluster Destruction","text":"<pre><code>hypershift destroy cluster azure \\\n    --name &lt;cluster-name&gt; \\\n    --location &lt;azure-region&gt; \\\n    --resource-group-name &lt;managed-rg&gt; \\\n    --azure-creds &lt;credentials-file&gt;\n</code></pre>"},{"location":"how-to/azure/self-managed/08-reference/#nodepool-creation","title":"NodePool Creation","text":"<pre><code>hypershift create nodepool azure \\\n    --cluster-name &lt;cluster-name&gt; \\\n    --namespace &lt;namespace&gt; \\\n    --name &lt;nodepool-name&gt; \\\n    --node-count &lt;count&gt; \\\n    --azure-instance-type &lt;vm-size&gt; \\\n    --azure-creds &lt;credentials-file&gt; \\\n    [--marketplace-publisher &lt;publisher&gt;] \\\n    [--marketplace-offer &lt;offer&gt;] \\\n    [--marketplace-sku &lt;sku&gt;] \\\n    [--marketplace-version &lt;version&gt;] \\\n    [--image-generation Gen1|Gen2]\n</code></pre>"},{"location":"how-to/azure/self-managed/08-reference/#kubeconfig-generation","title":"Kubeconfig Generation","text":"<pre><code>hypershift create kubeconfig \\\n    --name &lt;cluster-name&gt; \\\n    --namespace &lt;namespace&gt; \\\n    &gt; &lt;output-file&gt;\n</code></pre>"},{"location":"how-to/azure/self-managed/08-reference/#operator-installation","title":"Operator Installation","text":"<pre><code>hypershift install \\\n    --pull-secret &lt;pull-secret-file&gt; \\\n    --limit-crd-install Azure \\\n    [--external-dns-provider azure] \\\n    [--external-dns-credentials &lt;azure-creds&gt;] \\\n    [--external-dns-domain-filter &lt;dns-zone&gt;] \\\n    [--hypershift-image &lt;operator-image&gt;] \\\n    [--namespace &lt;namespace&gt;] \\\n    [--development]\n</code></pre>"},{"location":"how-to/azure/self-managed/08-reference/#azure-cli-quick-reference","title":"Azure CLI Quick Reference","text":""},{"location":"how-to/azure/self-managed/08-reference/#identity-management","title":"Identity Management","text":"<pre><code># List managed identities\naz identity list \\\n    --resource-group &lt;rg-name&gt; \\\n    --output table\n\n# Show identity details\naz identity show \\\n    --name &lt;identity-name&gt; \\\n    --resource-group &lt;rg-name&gt;\n\n# List federated credentials\naz identity federated-credential list \\\n    --identity-name &lt;identity-name&gt; \\\n    --resource-group &lt;rg-name&gt; \\\n    --output table\n\n# Show federated credential\naz identity federated-credential show \\\n    --identity-name &lt;identity-name&gt; \\\n    --name &lt;credential-name&gt; \\\n    --resource-group &lt;rg-name&gt;\n</code></pre>"},{"location":"how-to/azure/self-managed/08-reference/#resource-group-management","title":"Resource Group Management","text":"<pre><code># List resource groups\naz group list --output table\n\n# Create resource group\naz group create \\\n    --name &lt;rg-name&gt; \\\n    --location &lt;location&gt;\n\n# Delete resource group\naz group delete \\\n    --name &lt;rg-name&gt; \\\n    --yes [--no-wait]\n</code></pre>"},{"location":"how-to/azure/self-managed/08-reference/#network-resources","title":"Network Resources","text":"<pre><code># Create VNet\naz network vnet create \\\n    --name &lt;vnet-name&gt; \\\n    --resource-group &lt;rg-name&gt; \\\n    --address-prefix &lt;cidr&gt; \\\n    --subnet-name &lt;subnet-name&gt; \\\n    --subnet-prefixes &lt;subnet-cidr&gt;\n\n# Create NSG\naz network nsg create \\\n    --name &lt;nsg-name&gt; \\\n    --resource-group &lt;rg-name&gt;\n\n# List NSG rules\naz network nsg rule list \\\n    --nsg-name &lt;nsg-name&gt; \\\n    --resource-group &lt;rg-name&gt; \\\n    --output table\n</code></pre>"},{"location":"how-to/azure/self-managed/08-reference/#storage-account-oidc","title":"Storage Account (OIDC)","text":"<pre><code># Show storage account\naz storage account show \\\n    --name &lt;account-name&gt; \\\n    --resource-group &lt;rg-name&gt;\n\n# Delete storage account (OIDC issuer)\naz storage account delete \\\n    --name &lt;account-name&gt; \\\n    --resource-group &lt;rg-name&gt;\n</code></pre>"},{"location":"how-to/azure/self-managed/08-reference/#virtual-machines","title":"Virtual Machines","text":"<pre><code># List VMs\naz vm list \\\n    --resource-group &lt;rg-name&gt; \\\n    --output table\n\n# Show VM details with power state\naz vm list \\\n    --resource-group &lt;rg-name&gt; \\\n    --show-details \\\n    --query \"[].{Name:name, PowerState:powerState}\" \\\n    --output table\n\n# Get boot diagnostics\naz vm boot-diagnostics get-boot-log \\\n    --resource-group &lt;rg-name&gt; \\\n    --name &lt;vm-name&gt;\n</code></pre>"},{"location":"how-to/azure/self-managed/08-reference/#kubectloc-quick-reference","title":"kubectl/oc Quick Reference","text":""},{"location":"how-to/azure/self-managed/08-reference/#cluster-resources","title":"Cluster Resources","text":"<pre><code># List HostedClusters\nkubectl get hostedclusters -n &lt;namespace&gt;\n\n# Get HostedCluster details\nkubectl get hostedcluster &lt;name&gt; -n &lt;namespace&gt; -o yaml\n\n# Describe HostedCluster (shows events)\nkubectl describe hostedcluster &lt;name&gt; -n &lt;namespace&gt;\n\n# Patch HostedCluster (upgrade)\nkubectl patch hostedcluster/&lt;name&gt; -n &lt;namespace&gt; \\\n    --type merge \\\n    --patch '{\"spec\":{\"release\":{\"image\":\"&lt;new-image&gt;\"}}}'\n</code></pre>"},{"location":"how-to/azure/self-managed/08-reference/#nodepool-resources","title":"NodePool Resources","text":"<pre><code># List NodePools\nkubectl get nodepools -n &lt;namespace&gt;\n\n# Get NodePool details\nkubectl get nodepool &lt;name&gt; -n &lt;namespace&gt; -o yaml\n\n# Scale NodePool\nkubectl scale nodepool/&lt;name&gt; -n &lt;namespace&gt; --replicas=&lt;count&gt;\n\n# Patch NodePool\nkubectl patch nodepool/&lt;name&gt; -n &lt;namespace&gt; \\\n    --type merge \\\n    --patch '{\"spec\":{\"replicas\":&lt;count&gt;}}'\n\n# Delete NodePool\nkubectl delete nodepool/&lt;name&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"how-to/azure/self-managed/08-reference/#control-plane-pods","title":"Control Plane Pods","text":"<pre><code># List control plane pods\nkubectl get pods -n clusters-&lt;cluster-name&gt;\n\n# Get pod logs\nkubectl logs -n clusters-&lt;cluster-name&gt; &lt;pod-name&gt; [-f]\n\n# Describe pod\nkubectl describe pod -n clusters-&lt;cluster-name&gt; &lt;pod-name&gt;\n\n# Get pod resource usage\nkubectl top pods -n clusters-&lt;cluster-name&gt;\n</code></pre>"},{"location":"how-to/azure/self-managed/08-reference/#machines","title":"Machines","text":"<pre><code># List machines\nkubectl get machines -n clusters-&lt;cluster-name&gt;\n\n# Describe machine\nkubectl describe machine -n clusters-&lt;cluster-name&gt; &lt;machine-name&gt;\n\n# Delete machine (triggers replacement)\nkubectl delete machine -n clusters-&lt;cluster-name&gt; &lt;machine-name&gt;\n</code></pre>"},{"location":"how-to/azure/self-managed/08-reference/#appendices","title":"Appendices","text":""},{"location":"how-to/azure/self-managed/08-reference/#appendix-a-azure-permissions-reference","title":"Appendix A: Azure Permissions Reference","text":""},{"location":"how-to/azure/self-managed/08-reference/#azure-administrator-permissions","title":"Azure Administrator Permissions","text":"<p>Required for setting up foundation resources:</p> <p>Subscription-Level Roles: - <code>Contributor</code>: Create and manage all types of Azure resources - <code>User Access Administrator</code>: Manage user access to Azure resources (for role assignments)</p> <p>Microsoft Graph API Permissions: - <code>Application.ReadWrite.OwnedBy</code>: Create and manage service principals</p>"},{"location":"how-to/azure/self-managed/08-reference/#service-principal-permissions","title":"Service Principal Permissions","text":"<p>For cluster operations (<code>azure-credentials.json</code>):</p> <p>Subscription-Level Roles: - <code>Contributor</code>: Manage cluster infrastructure (VMs, networking, storage) - <code>User Access Administrator</code>: Assign roles to managed identities</p> <p>Alternative: Resource Group Scoped: - <code>Contributor</code> on specific resource groups (tighter security) - <code>User Access Administrator</code> on resource groups containing managed identities</p>"},{"location":"how-to/azure/self-managed/08-reference/#external-dns-service-principal","title":"External DNS Service Principal","text":"<p>For automatic DNS management:</p> <p>DNS Zone Level: - <code>DNS Zone Contributor</code>: Manage DNS records in specific DNS zone</p>"},{"location":"how-to/azure/self-managed/08-reference/#appendix-b-workload-identity-service-account-mapping","title":"Appendix B: Workload Identity Service Account Mapping","text":"<p>Complete mapping of Azure managed identities to OpenShift service accounts and their purposes:</p> Component Managed Identity Service Accounts Namespace Purpose Disk CSI <code>${CLUSTER_NAME}-disk-csi</code> <code>azure-disk-csi-driver-node-sa</code><code>azure-disk-csi-driver-operator</code><code>azure-disk-csi-driver-controller-sa</code> <code>openshift-cluster-csi-drivers</code> Provision and attach Azure Disk volumes as persistent storage File CSI <code>${CLUSTER_NAME}-file-csi</code> <code>azure-file-csi-driver-node-sa</code><code>azure-file-csi-driver-operator</code><code>azure-file-csi-driver-controller-sa</code> <code>openshift-cluster-csi-drivers</code> Provision and mount Azure Files as ReadWriteMany volumes Image Registry <code>${CLUSTER_NAME}-image-registry</code> <code>registry</code><code>cluster-image-registry-operator</code> <code>openshift-image-registry</code> Manage image registry storage backend (Azure Blob Storage) Ingress <code>${CLUSTER_NAME}-ingress</code> <code>ingress-operator</code> <code>openshift-ingress-operator</code> Configure Azure Load Balancers for ingress traffic routing Cloud Provider <code>${CLUSTER_NAME}-cloud-provider</code> <code>azure-cloud-provider</code> <code>openshift-cloud-controller-manager</code> Integrate with Azure cloud APIs for node/volume management NodePool Mgmt <code>${CLUSTER_NAME}-nodepool-mgmt</code> <code>capi-provider</code> Control plane namespace Provision and manage Azure VMs for worker nodes Network <code>${CLUSTER_NAME}-network</code> <code>cloud-network-config-controller</code> <code>openshift-cloud-network-config-controller</code> Configure Azure networking components (routes, load balancer rules) <p>Federated Credential Pattern:</p> <p>Each managed identity has 2 federated credentials for redundancy: - One for the operator service account - One for the controller/node service account</p> <p>Total: 7 managed identities \u00d7 2 federated credentials = 14 federated credentials per cluster</p>"},{"location":"how-to/azure/self-managed/08-reference/#appendix-c-resource-group-lifecycle","title":"Appendix C: Resource Group Lifecycle","text":"<p>Understanding what gets created and when it gets deleted:</p>"},{"location":"how-to/azure/self-managed/08-reference/#persistent-resource-group","title":"Persistent Resource Group","text":"<p>Name: Configurable (e.g., <code>hypershift-shared</code>, <code>os4-common</code>)</p> <p>Contents: - OIDC issuer (storage account) - Managed identities (7 per cluster, but stored centrally) - Federated credentials (14 per cluster) - DNS zones (if using External DNS)</p> <p>Lifecycle: Long-lived, survives cluster deletion</p> <p>Shared Across: All hosted clusters in the environment</p> <p>Deletion: Manual only, when decommissioning entire environment</p>"},{"location":"how-to/azure/self-managed/08-reference/#per-cluster-resource-groups","title":"Per-Cluster Resource Groups","text":"<p>Pattern: <code>${PREFIX}-&lt;type&gt;-rg</code></p> <p>Types: 1. Managed RG (<code>${PREFIX}-managed-rg</code>):    - Worker VMs    - Load balancers    - Managed disks    - NICs</p> <ol> <li>VNet RG (<code>${PREFIX}-vnet-rg</code>):</li> <li>Virtual network</li> <li> <p>Subnets</p> </li> <li> <p>NSG RG (<code>${PREFIX}-nsg-rg</code>):</p> </li> <li>Network security group</li> <li>NSG rules</li> </ol> <p>Lifecycle: Created with cluster, deleted with cluster</p> <p>Deletion: Via <code>task azure:delete-infra</code> or manual <code>az group delete</code></p>"},{"location":"how-to/azure/self-managed/08-reference/#appendix-d-dns-configuration-reference","title":"Appendix D: DNS Configuration Reference","text":""},{"location":"how-to/azure/self-managed/08-reference/#dns-flags-and-their-effects","title":"DNS Flags and Their Effects","text":"<p><code>--base-domain</code> (always required): - Determines the ingress domain for application routes - Formula: <code>apps.&lt;cluster-name&gt;.&lt;base-domain&gt;</code> or <code>apps.&lt;base-domain&gt;</code> - Example: <code>--base-domain example.com</code> \u2192 <code>apps.mycluster.example.com</code></p> <p><code>--external-dns-domain</code> (optional, External DNS only): - Changes service publishing strategy from LoadBalancer to Route - Sets custom DNS names for control plane services - Formula: <code>api-&lt;cluster-name&gt;.&lt;external-dns-domain&gt;</code> - Example: <code>--external-dns-domain azure.example.com</code> \u2192 <code>api-mycluster.azure.example.com</code></p> <p><code>--dns-zone-rg-name</code> (required when using <code>--external-dns-domain</code>): - Resource group containing the Azure DNS zone - External DNS uses this to create DNS records - Example: <code>--dns-zone-rg-name hypershift-shared</code></p>"},{"location":"how-to/azure/self-managed/08-reference/#service-publishing-strategies","title":"Service Publishing Strategies","text":"Scenario API Server OAuth Konnectivity Ingress DNS Management Without External DNS LoadBalancer Route Route Route Manual or Azure-provided LB DNS With External DNS Route Route Route Route Automatic via External DNS operator"},{"location":"how-to/azure/self-managed/08-reference/#appendix-e-glossary","title":"Appendix E: Glossary","text":"<p>HyperShift Terms:</p> <ul> <li>HostedCluster: Custom resource representing a hosted OpenShift cluster with control plane running as pods</li> <li>NodePool: Custom resource representing a group of worker nodes with identical configuration</li> <li>Control Plane: Kubernetes components (API server, etcd, controllers) managing the cluster, running on management cluster</li> <li>Data Plane: Worker nodes where application workloads run, deployed as Azure VMs</li> <li>Management Cluster: OpenShift cluster hosting HyperShift operator and control plane pods</li> <li>HyperShift Operator: Kubernetes operator managing lifecycle of HostedClusters and NodePools</li> </ul> <p>Azure Terms:</p> <ul> <li>Workload Identity: Azure managed identity used by OpenShift components for Azure API authentication</li> <li>Managed Identity: Azure AD identity without credentials, used for Azure service authentication</li> <li>Federated Credential: Trust relationship between Azure AD and external identity provider (OIDC)</li> <li>OIDC Issuer: OpenID Connect endpoint used for service account token validation</li> <li>Service Principal: Azure AD application identity with credentials for programmatic access</li> <li>Resource Group: Logical container for Azure resources with shared lifecycle</li> <li>VNet (Virtual Network): Isolated network in Azure for resource communication</li> <li>NSG (Network Security Group): Firewall rules controlling network traffic</li> <li>Azure Blob Storage: Object storage service used for OIDC issuer hosting</li> </ul> <p>OpenShift Terms:</p> <ul> <li>Service Account: Kubernetes identity for pods to access cluster APIs</li> <li>Cluster Operator: OpenShift component managing specific cluster functionality</li> <li>Machine: Kubernetes resource representing a physical or virtual machine</li> <li>ClusterVersion: Resource tracking OpenShift version and upgrade status</li> <li>Pull Secret: Credentials for pulling OpenShift container images</li> <li>Ignition: System configuration tool for initial node setup</li> </ul>"},{"location":"how-to/azure/self-managed/08-reference/#appendix-f-common-azure-vm-sizes","title":"Appendix F: Common Azure VM Sizes","text":"<p>General Purpose (Dsv3-series): - <code>Standard_D2s_v3</code>: 2 vCPU, 8 GB RAM - Default, balanced workloads - <code>Standard_D4s_v3</code>: 4 vCPU, 16 GB RAM - Medium workloads - <code>Standard_D8s_v3</code>: 8 vCPU, 32 GB RAM - Larger applications - <code>Standard_D16s_v3</code>: 16 vCPU, 64 GB RAM - High-performance apps</p> <p>Memory Optimized (Esv3-series): - <code>Standard_E2s_v3</code>: 2 vCPU, 16 GB RAM - Memory-intensive small - <code>Standard_E4s_v3</code>: 4 vCPU, 32 GB RAM - Databases, caching - <code>Standard_E8s_v3</code>: 8 vCPU, 64 GB RAM - Large databases - <code>Standard_E16s_v3</code>: 16 vCPU, 128 GB RAM - In-memory analytics</p> <p>Compute Optimized (Fsv2-series): - <code>Standard_F2s_v2</code>: 2 vCPU, 4 GB RAM - Batch processing - <code>Standard_F4s_v2</code>: 4 vCPU, 8 GB RAM - Web servers - <code>Standard_F8s_v2</code>: 8 vCPU, 16 GB RAM - Gaming servers - <code>Standard_F16s_v2</code>: 16 vCPU, 32 GB RAM - CPU-intensive workloads</p> <p>See Azure VM sizes documentation for complete list.</p>"},{"location":"how-to/azure/self-managed/08-reference/#reference-links","title":"Reference Links","text":""},{"location":"how-to/azure/self-managed/08-reference/#hypershift-documentation","title":"HyperShift Documentation","text":"<ul> <li>HyperShift Project Home</li> <li>HyperShift GitHub Repository</li> <li>HyperShift Releases</li> <li>HyperShift Architecture Documentation</li> </ul>"},{"location":"how-to/azure/self-managed/08-reference/#azure-documentation","title":"Azure Documentation","text":"<ul> <li>Azure Workload Identity</li> <li>Azure Managed Identities</li> <li>Azure DNS Zones</li> <li>Azure Virtual Networks</li> <li>Azure VM Sizes</li> <li>Azure Network Security Groups</li> </ul>"},{"location":"how-to/azure/self-managed/08-reference/#openshift-documentation","title":"OpenShift Documentation","text":"<ul> <li>OpenShift Container Platform Documentation</li> <li>Cloud Credential Operator</li> <li>OpenShift Release Images</li> <li>Pull Secrets (cloud.redhat.com)</li> </ul>"},{"location":"how-to/azure/self-managed/08-reference/#tools-and-cli","title":"Tools and CLI","text":"<ul> <li>Azure CLI Installation</li> <li>OpenShift CLI (oc) Downloads</li> <li>kubectl Installation</li> <li>jq - Command-line JSON processor</li> <li>go-task/task - Task runner</li> </ul>"},{"location":"how-to/azure/self-managed/08-reference/#community-and-support","title":"Community and Support","text":"<ul> <li>HyperShift Slack Channel (#hypershift)</li> <li>OpenShift Community</li> <li>Red Hat Customer Portal (for supported deployments)</li> </ul>"},{"location":"how-to/azure/self-managed/08-reference/#quick-start-workflow","title":"Quick Start Workflow","text":"<p>For quick reference, here's the complete workflow from scratch to running cluster:</p> <pre><code># 1. Prerequisites (once per environment)\ntask prereq:validate\ntask prereq:keys\ntask azure:oidc\n\n# 2. Per-cluster setup\ntask azure:identities\ntask azure:federated-creds\ntask azure:infra\n\n# 3. Create cluster\ntask cluster:create\n\n# 4. Wait for cluster\nkubectl wait --for=condition=Available \\\n    hostedcluster/${CLUSTER_NAME} \\\n    -n ${CLUSTER_NAMESPACE} \\\n    --timeout=30m\n\n# 5. Access cluster\nhypershift create kubeconfig \\\n    --name ${CLUSTER_NAME} \\\n    --namespace ${CLUSTER_NAMESPACE} \\\n    &gt; kubeconfig\nexport KUBECONFIG=kubeconfig\nkubectl get nodes\nkubectl get co\n\n# 6. Cleanup (when done)\ntask cluster:destroy\ntask azure:delete-infra\n# Optionally: task azure:delete-identities (permanent)\n</code></pre>"},{"location":"how-to/azure/self-managed/08-reference/#configuration-file-examples","title":"Configuration File Examples","text":""},{"location":"how-to/azure/self-managed/08-reference/#taskfileyml-variables","title":"Taskfile.yml Variables","text":"<pre><code>vars:\n  # Cluster identification\n  PREFIX: 'myprefix'\n  CLUSTER_NAME: '${PREFIX}-hc'\n  CLUSTER_NAMESPACE: 'clusters'\n\n  # Azure configuration\n  SUBSCRIPTION_ID: 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n  TENANT_ID: 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n  LOCATION: 'eastus'\n\n  # Resource groups\n  PERSISTENT_RG_NAME: 'hypershift-shared'\n  MANAGED_RG_NAME: '${PREFIX}-managed-rg'\n  VNET_RG_NAME: '${PREFIX}-vnet-rg'\n  NSG_RG_NAME: '${PREFIX}-nsg-rg'\n\n  # Networking\n  VNET_NAME: '${PREFIX}-vnet'\n  VNET_SUBNET1: '${PREFIX}-subnet-1'\n  NSG: '${PREFIX}-nsg'\n\n  # OIDC and identities\n  OIDC_STORAGE_ACCOUNT_NAME: 'myoidc1234567890'\n  OIDC_ISSUER_URL: 'https://${OIDC_STORAGE_ACCOUNT_NAME}.blob.core.windows.net/${OIDC_STORAGE_ACCOUNT_NAME}'\n  SA_TOKEN_ISSUER_PRIVATE_KEY_PATH: 'serviceaccount-signer.private'\n\n  # DNS\n  PARENT_DNS_ZONE: 'example.com'\n\n  # OpenShift\n  RELEASE_IMAGE: 'quay.io/openshift-release-dev/ocp-release:4.21.0-x86_64'\n  NODE_POOL_REPLICAS: '2'\n\n  # Credentials\n  AZURE_CREDS: 'azure-credentials.json'\n  PULL_SECRET: 'pull-secret.json'\n</code></pre>"},{"location":"how-to/azure/self-managed/08-reference/#azure-credentialsjson","title":"azure-credentials.json","text":"<pre><code>{\n  \"subscriptionId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n  \"tenantId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n  \"clientId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n  \"clientSecret\": \"your-service-principal-secret\"\n}\n</code></pre>"},{"location":"how-to/azure/self-managed/08-reference/#workload-identitiesjson-generated","title":"workload-identities.json (generated)","text":"<pre><code>{\n  \"disk_csi\": {\n    \"client_id\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n  },\n  \"file_csi\": {\n    \"client_id\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n  },\n  \"image_registry\": {\n    \"client_id\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n  },\n  \"ingress\": {\n    \"client_id\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n  },\n  \"cloud_provider\": {\n    \"client_id\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n  },\n  \"nodepool_mgmt\": {\n    \"client_id\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n  },\n  \"network\": {\n    \"client_id\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n  }\n}\n</code></pre>"},{"location":"how-to/azure/troubleshooting/","title":"Troubleshooting HyperShift on Azure","text":"<p>This section of the HyperShift documentation contains pages related to troubleshooting specific issues when using the Azure cloud provider.</p> <ul> <li>Debug Missing Nodes</li> </ul>"},{"location":"how-to/azure/troubleshooting/debug-nodes/","title":"Debug Why Azure Nodes Have Not Joined","text":"<p>If your control plane API endpoint has become available, but the nodes have not joined the hosted cluster, check the following:</p>"},{"location":"how-to/azure/troubleshooting/debug-nodes/#verify-machines-were-created","title":"Verify machines were created","text":"<pre><code>HC_NAMESPACE=\"clusters\"\nHC_NAME=\"cluster-name\"\nCONTROL_PLANE_NAMESPACE=\"${HC_NAMESPACE}-${HC_NAME}\"\noc get machines.cluster.x-k8s.io -n $CONTROL_PLANE_NAMESPACE\noc get azuremachines.infrastructure.cluster.x-k8s.io -n $CONTROL_PLANE_NAMESPACE\n</code></pre> <p>If machines don't exist, check that a machinedeployment and machineset have been created: <pre><code>oc get machinedeployment -n $CONTROL_PLANE_NAMESPACE\noc get machineset -n $CONTROL_PLANE_NAMESPACE\n</code></pre></p> <p>In the case that no machinedeployment was created, look at the logs of the hypershift  operator: <pre><code>oc logs -l app=operator -n hypershift --tail=$NUMBER_OF_LINES\n</code></pre></p> <p>If the machines exist but have not been provisioned, check the log of the cluster API provider: <pre><code>oc logs deployment/capi-provider -c manager -n $CONTROL_PLANE_NAMESPACE\n</code></pre></p>"},{"location":"how-to/azure/troubleshooting/debug-nodes/#create-a-bastion-to-ssh-to-a-node","title":"Create a bastion to SSH to a node","text":"<p>If the machines look like they have been provisioned correctly, you can directly access the virtual machines related to your nodes through a bastion. </p>"},{"location":"how-to/azure/troubleshooting/debug-nodes/#prerequisites","title":"Prerequisites","text":"<ul> <li>Download the <code>az</code> cli</li> <li>Add the following extensions to the cli:</li> <li><code>az extension update --name bastionaz extension update --name bastion</code></li> <li><code>az extension add -n ssh</code></li> <li>Extract the private key for the cluster. If you created the cluster with the --generate-ssh flag, a   ssh key for your cluster was placed in the same namespace as the hosted cluster (default <code>clusters</code>). If you   specified your own key and know how to access it, you can skip this step.</li> <li><code>oc get secret -n clusters ${HC_NAME}-ssh-key -o jsonpath='{ .data.id_rsa }' | base64 -d &gt; /tmp/ssh/id_rsa</code></li> <li>Set the permissions on the key <code>chmod 400 /tmp/ssh/id_rsa</code></li> </ul>"},{"location":"how-to/azure/troubleshooting/debug-nodes/#create-a-bastion-machine","title":"Create a bastion machine","text":"<ol> <li>Log into the Azure Portal and go to the resource group where your virtual machine was created</li> <li>Click on the <code>Connect</code> button then <code>Connect to Bastion</code></li> <li>Accept the defaults and click <code>Deploy Bastion</code></li> <li>Once the bastion and its related resources are created, you will need to modify some of its configuration</li> <li>Click on the bastion resource</li> <li>Click Configuration</li> <li>Set the Tier to <code>Standard</code> and check <code>Native client support</code></li> <li>Wait for the new settings to take effect</li> <li>Log into the bastion via terminal through this command <code>az login --scope https://management.core.windows.net//.defaultnetwork bastion ssh --name &lt;bastion-name&gt; --resource-group &lt;resource-group-name&gt; --target-resource-id &lt;vm-id&gt; --auth-type ssh-key --username core --ssh-key &lt;path-to-your-rsa-secret&gt;</code></li> <li>You should now be able to access various directories and logs to debug why the machine did not join</li> <li>One suggestion would be to look at the journal logs and look for a repeating error near the bottom that should indicate why the kubelet has not been able to join the cluster: <code>sudo journalctl</code></li> </ol>"},{"location":"how-to/ci/checking-ci/","title":"Daily CI Health Check Procedures","text":"<p>This document outlines the daily checks that should be performed each morning to ensure the health and stability of our CI systems.</p> <p>Tip</p> <p>Daily CI checks help identify issues early and ensure that our development pipeline remains stable. These checks should be performed at the start of each workday.</p>"},{"location":"how-to/ci/checking-ci/#1-ocp-release-payload-controllers","title":"1. OCP Release Payload Controllers","text":"<p>Check the status of OpenShift Container Platform release payload controllers for the current and previous OCP versions to ensure they are functioning properly.</p> <ul> <li>Review amd64 release payload controller to make sure HyperShift AWS and AKS jobs are passing, thus they are not blocking the CI and nightly payloads (both OCP versions)</li> <li>Review multi-arch release payload controller to make sure HyperShift AKS job is passing, thus it is not blocking the nightly payload (both OCP versions)</li> <li>Alternatively, you can view the same info on the Sippy Payload streams dashboard. Here is an example for OCP 4.21.</li> </ul> <p>HyperShift job is failing and blocking a payload release</p> <p>When either HyperShift job is blocking a payload release:</p> <ul> <li>Open a chat thread in #team-ocp-hypershift to start a dialogue on what is happening and to begin root causing the problem.</li> <li>In addition, alert #forum-ocp-oversight we are aware of the issue and working to root cause the problem.</li> </ul>"},{"location":"how-to/ci/checking-ci/#2-periodic-conformance-jobs","title":"2. Periodic &amp; Conformance Jobs","text":"<p>Review periodic job status for the current and previous OCP versions to ensure long-running validation and maintenance tasks are healthy. We want to be passing 70% or higher.</p> <p>For each OCP version, click on the Jobs link on the left hand side of the screen in Sippy. Here is an example for OCP 4.21 with the jobs filtered on hypershift.</p> <p>We care about the following jobs (you can filter by these names if desired):</p> <ul> <li>AWS<ul> <li>periodic-ci-openshift-hypershift-release-*-periodics-e2e-aws-ovn-conformance</li> <li>periodic-ci-openshift-hypershift-release-*-periodics-e2e-aws-upgrade</li> <li>periodic-ci-openshift-hypershift-release-*-periodics-e2e-aws-multi</li> <li>periodic-ci-openshift-hypershift-release-*-periodics-e2e-aws-ovn</li> </ul> </li> <li>Azure / ARO HCP<ul> <li>periodic-ci-openshift-hypershift-release-*-periodics-e2e-aks</li> <li>periodic-ci-openshift-hypershift-release-*-periodics-e2e-aks-multi-x-ax</li> <li>periodic-ci-openshift-hypershift-release-*-periodics-e2e-azure-aks-ovn-conformance</li> </ul> </li> </ul> <p>Tip - How to check the job test results</p> <p>For any of these jobs, if you click on the running man emblem, Sippy will show you all the test runs.  For each of the test runs, you can click the Prow ship emblem to see the test results of the individual run.</p> <p>What to do when a job is permafailing</p> <p>Open a chat thread in #team-ocp-hypershift to start a dialogue on what is happening and to begin root causing the problem.</p> <p>Alternatively, you can view the job runs in TestGrid.</p>"},{"location":"how-to/ci/checking-ci/#3-presubmit-jobs","title":"3. Presubmit Jobs","text":"<p>Monitor presubmit job health for the current OCP version only to catch any systemic issues that could block development.</p> <p>The best way to check to make sure the presubmit jobs are not permafailing are to look at a recent PR in the HyperShift repo and go to the job history of the specific job you want to review.</p> <p>The presubmit jobs we most care about are:</p> <ul> <li>pull-ci-openshift-hypershift-main-e2e-aws</li> <li>pull-ci-openshift-hypershift-main-e2e-aws-upgrade-hypershift-operator</li> <li>pull-ci-openshift-hypershift-main-e2e-aks</li> <li>pull-ci-openshift-hypershift-main-e2e-aks-4-20</li> <li>pull-ci-openshift-hypershift-main-e2e-kubevirt-aws-ovn-reduced</li> <li>pull-ci-openshift-hypershift-main-verify</li> <li>pull-ci-openshift-hypershift-main-unit</li> </ul> <p>Tip</p> <p>If the job is not solid red, the job is not permafailing.</p> <p>What to do when a job is permafailing</p> <p>Open a chat thread in #team-ocp-hypershift to start a dialogue on what is happening and to begin root causing the problem.</p>"},{"location":"how-to/common/exposing-services-from-hcp/","title":"Exposing the Hosted Control Plane Services","text":"<p>To publish the services from the Hosted Control Plane, we need to understand the available strategies and their implications. Let's explore them.</p>"},{"location":"how-to/common/exposing-services-from-hcp/#service-publishing-strategies","title":"Service Publishing Strategies","text":"<p>Let's delve into the motivations for each of the strategies.</p> <p>The NodePort strategy allows you to expose services without requiring a logical LoadBalancer like MetalLB or similar infrastructure. It is one of the simplest methods to implement. This strategy is supported by all services; however, the limitation arises in high availability (HA) environments where you will be pointing to one of the NodePorts instead of all three.</p> <p>The LoadBalancer strategy enables you to expose certain services through a load balancer. While not all services support this strategy, it is the preferred method for exposing the KubeApiServer, as it allows for a single entry point in an HA configuration without relying in the Ingress Controller of the Manamgement cluster.</p> <p>The Route strategy allows you to expose the HostedControlPlane services using the ingress of the Management OpenShift cluster. This strategy is supported by all services but kubeapi-server.</p>"},{"location":"how-to/common/exposing-services-from-hcp/#nodeport","title":"NodePort","text":"<p>Exposing a service via NodePort is a method used in OpenShift to make a service accessible from outside the cluster. When you expose a service using NodePort, OpenShift allocates a port on each node in the cluster (if the cluster availability policy is set to HighlyAvailable). This port on each node is mapped to the port of the service, allowing external traffic to reach the service by accessing any node's IP address and the allocated NodePort.</p> <p>This is the default configuration when you use <code>Agent</code> and <code>None</code> provider platforms. The services relevant for on-premise platforms are:</p> <ul> <li>APIServer</li> <li>OAuthServer</li> <li>Konnectivity</li> <li>Ignition</li> </ul> <p>Note</p> <p>If any of the services are not relevant for your deployment, it is not necessary to specify them.</p> <p>Here is how it looks in the HostedCluster CR:</p> <pre><code>spec:\n...\n...\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: &lt;IP_ADDRESS&gt;\n        port: &lt;PORT&gt;\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: 10.103.101.101\n      type: NodePort\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: 10.103.101.101\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: 10.103.101.101\n      type: NodePort\n...\n...\n</code></pre>"},{"location":"how-to/common/exposing-services-from-hcp/#route","title":"Route","text":"<p>OpenShift routes provide a way to expose services within the cluster to external clients. A route in OpenShift maps an external request (typically HTTP/HTTPS) to an internal service. The route specifies a hostname that external clients will use to access the service. OpenShift\u2019s router (based on HAProxy) will handle the traffic coming to this hostname.</p> <p>HostedControlPlanes operate in two domains: the Control Plane and the Data Plane. The Control Plane uses routes through the MGMT Cluster ingress to expose services for each of the HostedControlPlanes, and the routes are created in the HostedControlPlane namespace. For the Data Plane, the Ingress handles <code>*.apps.subdomain.tld</code> URLs, and all routes under this wildcard are directed to the Namespace by the OpenShift Router on the worker nodes.</p> <p>The usual configuration for the Hosted Cluster is similar to the LoadBalancer setup we will discuss next.</p>"},{"location":"how-to/common/exposing-services-from-hcp/#loadbalancer","title":"LoadBalancer","text":"<p>The LoadBalancer strategy in OpenShift is used to expose services to external clients using an external load balancer. When you create a service of type LoadBalancer, Kubernetes interacts with the underlying cloud platform or appropriate LoadBalancer controllers to provision an external load balancer, which then routes traffic to the service's endpoints (pods).</p> <p>This is how looks like the most common configuration to expose the services from HCP side:</p> <pre><code>spec:\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      type: LoadBalancer\n  - service: OAuthServer\n    servicePublishingStrategy:\n      type: Route\n  - service: OIDC\n    servicePublishingStrategy:\n      type: Route\n      Route:\n        hostname: &lt;URL&gt;\n  - service: Konnectivity\n    servicePublishingStrate\n      type: Route\n  - service: Ignition\n    servicePublishingStrategy:\n      type: Route\n</code></pre> <p>If you wanna know more about how to expose the ingress service in the Data Plane side, please access the Recipes section to see how to do it with MetalLB.</p>"},{"location":"how-to/common/global-pull-secret/","title":"Global Pull Secret for Hosted Control Planes","text":""},{"location":"how-to/common/global-pull-secret/#overview","title":"Overview","text":"<p>The Global Pull Secret functionality enables Hosted Cluster administrators to include additional pull secrets for accessing container images from private registries without requiring assistance from the Management Cluster administrator. This feature allows you to merge your custom pull secret with the original HostedCluster pull secret, making it available to all nodes in the cluster.</p> <p>The implementation uses a DaemonSet approach that automatically detects when you create an <code>additional-pull-secret</code> in the <code>kube-system</code> namespace of your DataPlane (Hosted Cluster). The system then merges this secret with the original pull secret and deploys the merged result to all nodes via a DaemonSet that updates the kubelet configuration.</p> <p>Note</p> <p>This feature is designed to work autonomously - once you create the additional pull secret, the system automatically handles the rest without requiring Management Cluster administrator intervention.</p>"},{"location":"how-to/common/global-pull-secret/#adding-your-pull-secret","title":"Adding your Pull Secret","text":"<p>Important</p> <p>All actions described in this section must be performed on the HostedCluster's workers (DataPlane), not on the Management Cluster.</p> <p>To use this functionality, follow these steps:</p>"},{"location":"how-to/common/global-pull-secret/#1-create-your-additional-pull-secret","title":"1. Create your additional pull secret","text":"<p>Create a secret named <code>additional-pull-secret</code> in the <code>kube-system</code> namespace of your Hosted Cluster (DataPlane). The secret must contain a valid DockerConfigJSON format:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: additional-pull-secret\n  namespace: kube-system\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: &lt;base64-encoded-docker-config-json&gt;\n</code></pre>"},{"location":"how-to/common/global-pull-secret/#2-example-dockerconfigjson-format","title":"2. Example DockerConfigJSON format","text":"<p>Your <code>.dockerconfigjson</code> should follow this structure:</p> <pre><code>{\n  \"auths\": {\n    \"registry.example.com\": {\n      \"auth\": \"base64-encoded-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"base64-encoded-credentials\"\n    }\n  }\n}\n</code></pre> <p>Using Namespace-Specific Registry Entries</p> <p>For registries like Quay.io that support organization/namespace-specific authentication, you can specify the full path in your registry entry (e.g., <code>quay.io/mycompany</code> instead of just <code>quay.io</code>). This allows you to provide different credentials for different namespaces within the same registry, and helps avoid conflicts with existing registry entries in the original pull secret.</p>"},{"location":"how-to/common/global-pull-secret/#3-apply-the-secret","title":"3. Apply the secret","text":"<pre><code>kubectl apply -f additional-pull-secret.yaml\n</code></pre>"},{"location":"how-to/common/global-pull-secret/#4-verification","title":"4. Verification","text":"<p>After creating the secret, the system will automatically:</p> <ol> <li>Validate the secret format</li> <li>Merge it with the original pull secret</li> <li>Deploy a DaemonSet to all nodes</li> <li>Update the kubelet configuration on each node</li> </ol> <p>You can verify the deployment by checking:</p> <pre><code># Check if the DaemonSet is running\nkubectl get daemonset global-pull-secret-syncer -n kube-system\n\n# Check the merged pull secret\nkubectl get secret global-pull-secret -n kube-system\n\n# Check DaemonSet pods\nkubectl get pods -n kube-system -l name=global-pull-secret-syncer\n</code></pre>"},{"location":"how-to/common/global-pull-secret/#how-it-works","title":"How it works","text":"<p>The Global Pull Secret functionality operates through a multi-component system:</p>"},{"location":"how-to/common/global-pull-secret/#automatic-detection","title":"Automatic Detection","text":"<ul> <li>The Hosted Cluster Config Operator (HCCO) continuously monitors the <code>kube-system</code> namespace</li> <li>When it detects the creation of <code>additional-pull-secret</code>, it triggers the reconciliation process</li> </ul>"},{"location":"how-to/common/global-pull-secret/#validation-and-merging","title":"Validation and Merging","text":"<ul> <li>The system validates that your secret contains a proper DockerConfigJSON format</li> <li>It retrieves the original pull secret from the HostedControlPlane</li> <li>Your additional pull secret is merged with the original one</li> <li>If there are conflicting registry entries, the original pull secret takes precedence (the additional pull secret entry is ignored for conflicting registries)</li> <li>The system supports namespace-specific registry entries (e.g., <code>quay.io/namespace</code>) for better credential specificity</li> </ul>"},{"location":"how-to/common/global-pull-secret/#deployment-process","title":"Deployment Process","text":"<ul> <li>A <code>global-pull-secret</code> is created in the <code>kube-system</code> namespace containing the merged result</li> <li>RBAC resources (ServiceAccount, Role, RoleBinding) are created for the DaemonSet in both <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>We use Role and RoleBinding in both namespaces to access secrets in <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>A DaemonSet named <code>global-pull-secret-syncer</code> is deployed to eligible nodes</li> </ul> <p>NodePool InPlace Strategy Restriction</p> <p>The Global Pull Secret DaemonSet is not deployed to nodes that belong to NodePools using the InPlace upgrade strategy. This restriction prevents conflicts between the DaemonSet's modifications to <code>/var/lib/kubelet/config.json</code> and the Machine Config Daemon (MCD) during InPlace upgrades.</p> <ul> <li>Nodes with Replace strategy: \u2705 Receive Global Pull Secret DaemonSet</li> <li>Nodes with InPlace strategy: \u274c Do not receive Global Pull Secret DaemonSet</li> </ul> <p>This ensures that MCD operations during InPlace upgrades do not fail due to unexpected changes in kubelet configuration files.</p>"},{"location":"how-to/common/global-pull-secret/#node-level-synchronization","title":"Node-Level Synchronization","text":"<ul> <li>Each DaemonSet pod runs a controller that watches the secrets under kube-system namespace</li> <li>When changes are detected, it updates <code>/var/lib/kubelet/config.json</code> on the node</li> <li>The kubelet service is restarted via DBus to apply the new configuration</li> <li>If the restart fails after 3 attempts, the system rolls back the file changes</li> </ul>"},{"location":"how-to/common/global-pull-secret/#automatic-cleanup","title":"Automatic Cleanup","text":"<ul> <li>If you delete the <code>additional-pull-secret</code>, the HCCO automatically removes the <code>global-pull-secret</code> secret</li> <li>The system reverts to using only the original pull secret from the HostedControlPlane</li> <li>The DaemonSet continues running but now syncs only the original pull secret to nodes</li> </ul>"},{"location":"how-to/common/global-pull-secret/#registry-precedence-and-conflict-resolution","title":"Registry Precedence and Conflict Resolution","text":"<p>The Global Pull Secret system uses a specific precedence model when merging your additional pull secret with the original one:</p>"},{"location":"how-to/common/global-pull-secret/#merge-behavior","title":"Merge Behavior","text":"<ul> <li>Original pull secret entries always take precedence over additional pull secret entries for the same registry</li> <li>If both secrets contain an entry for <code>quay.io</code>, the original pull secret's credentials will be used</li> <li>Your additional pull secret entries are only added if they don't conflict with existing entries</li> <li>Warnings are logged when conflicts are detected</li> </ul>"},{"location":"how-to/common/global-pull-secret/#recommended-approach","title":"Recommended Approach","text":"<p>To avoid conflicts and ensure your credentials are used, consider these strategies:</p> <ol> <li>Use namespace-specific entries: Instead of <code>quay.io</code>, use <code>quay.io/your-namespace</code></li> <li>Target specific registries: Add entries only for registries not already in the original pull secret</li> <li>Check existing entries: Review what registries are already configured in the HostedControlPlane</li> </ol>"},{"location":"how-to/common/global-pull-secret/#example-merge-scenario","title":"Example Merge Scenario","text":"<p>Original Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"original-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Your Additional Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"your-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"your-namespace-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Resulting Merged Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"original-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"your-namespace-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Note how the <code>quay.io</code> entry keeps the original credentials, but <code>quay.io/mycompany</code> is added from your additional secret.</p>"},{"location":"how-to/common/global-pull-secret/#implementation-details","title":"Implementation details","text":"<p>The implementation consists of several key components working together:</p>"},{"location":"how-to/common/global-pull-secret/#core-components","title":"Core Components","text":"<ol> <li>Global Pull Secret Controller (<code>globalps</code> package)</li> <li>Handles validation of user-provided pull secrets</li> <li>Manages the merging logic between original and additional pull secrets</li> <li>Creates and manages RBAC resources</li> <li>Deploys and manages the DaemonSet</li> <li> <p>Node eligibility assessment: Labels nodes from InPlace NodePools and configures DaemonSet scheduling restrictions</p> </li> <li> <p>Sync Global Pull Secret Command (<code>sync-global-pullsecret</code> package)</p> </li> <li>Runs as a DaemonSet on each node</li> <li>Watches for changes to the <code>global-pull-secret</code> in <code>kube-system</code> namespace</li> <li>Accesses the original <code>pull-secret</code> in <code>openshift-config</code> namespace</li> <li>Updates the kubelet configuration file</li> <li> <p>Manages kubelet service restarts via DBus</p> </li> <li> <p>Hosted Cluster Config Operator Integration</p> </li> <li>Monitors for the presence of <code>additional-pull-secret</code></li> <li>Orchestrates the entire process</li> <li>Handles cleanup when the secret is removed</li> </ol>"},{"location":"how-to/common/global-pull-secret/#architecture-diagram","title":"Architecture Diagram","text":"graph TB     %% User Input     User[User creates additional-pull-secret] --&gt; |kube-system namespace| AdditionalPS[additional-pull-secret Secret]      %% HCCO Controller     HCCO[Hosted Cluster Config Operator] --&gt; |Watches kube-system secrets| GlobalPSController[Global Pull Secret Controller]     GlobalPSController --&gt; |Validates| AdditionalPS     GlobalPSController --&gt; |Gets original| OriginalPS[Original pull-secret from HCP]      %% Secret Processing     AdditionalPS --&gt; |Validates format| ValidatePS[Validate Additional Pull Secret]     OriginalPS --&gt; |Extracts data| OriginalPSData[Original Pull Secret Data]     ValidatePS --&gt; |Extracts data| AdditionalPSData[Additional Pull Secret Data]      %% Merge Process     OriginalPSData --&gt; MergeSecrets[Merge Pull Secrets]     AdditionalPSData --&gt; MergeSecrets     MergeSecrets --&gt; |Creates merged JSON| GlobalPSData[Global Pull Secret Data]      %% Secret Creation     GlobalPSData --&gt; |Creates in kube-system| GlobalPSSecret[global-pull-secret Secret]      %% RBAC Setup     GlobalPSController --&gt; |Creates RBAC| RBACSetup[Setup RBAC Resources]     RBACSetup --&gt; ServiceAccount[global-pull-secret-syncer ServiceAccount]     RBACSetup --&gt; KubeSystemRole[global-pull-secret-syncer Role in kube-system]     RBACSetup --&gt; KubeSystemRoleBinding[global-pull-secret-syncer RoleBinding in kube-system]     RBACSetup --&gt; OpenshiftConfigRole[global-pull-secret-syncer Role in openshift-config]     RBACSetup --&gt; OpenshiftConfigRoleBinding[global-pull-secret-syncer RoleBinding in openshift-config]      %% DaemonSet Deployment     GlobalPSController --&gt; |Deploys DaemonSet| DaemonSet[global-pull-secret-syncer DaemonSet]     DaemonSet --&gt; |Runs on each node| DaemonSetPod[DaemonSet Pod]      %% DaemonSet Pod Details     DaemonSetPod --&gt; |Mounts host paths| HostMounts[Host Path Mounts]     HostMounts --&gt; KubeletPath[\"/var/lib/kubelet\"]     HostMounts --&gt; DbusPath[\"/var/run/dbus\"]      %% Container Execution     DaemonSetPod --&gt; |Runs command| Container[control-plane-operator Container]     Container --&gt; |Executes| SyncCommand[sync-global-pullsecret command]      %% Sync Process     SyncCommand --&gt; |Watches global-pull-secret| SyncController[Global Pull Secret Reconciler]     SyncController --&gt; |Reads secret| ReadGlobalPS[Read global-pull-secret]     SyncController --&gt; |Reads original| ReadOriginalPS[Read original pull-secret]      %% File Update Process     ReadGlobalPS --&gt; |Gets data| GlobalPSBytes[Global Pull Secret Bytes]     ReadOriginalPS --&gt; |Gets data| OriginalPSBytes[Original Pull Secret Bytes]      %% Decision Logic     GlobalPSBytes --&gt; |If exists| UseGlobalPS[Use Global Pull Secret]     OriginalPSBytes --&gt; |If not exists| UseOriginalPS[Use Original Pull Secret]      %% File Update     UseGlobalPS --&gt; |Updates file| UpdateKubeletConfig[\"Update /var/lib/kubelet/config.json\"]     UseOriginalPS --&gt; |Updates file| UpdateKubeletConfig      %% Kubelet Restart     UpdateKubeletConfig --&gt; |Restarts kubelet| RestartKubelet[Restart kubelet.service via systemd]     RestartKubelet --&gt; |Via dbus| DbusConnection[DBus Connection]      %% Error Handling     UpdateKubeletConfig --&gt; |If restart fails| RollbackProcess[Rollback Process]     RollbackProcess --&gt; |Restore original| RestoreOriginal[Restore Original File Content]      %% Cleanup Process     GlobalPSController --&gt; |If additional PS deleted| CleanupProcess[Cleanup Process]     CleanupProcess --&gt; |Deletes global PS| DeleteGlobalPS[Delete global-pull-secret]     CleanupProcess --&gt; |Removes DaemonSet| RemoveDaemonSet[Remove DaemonSet]      %% Styling     classDef userInput fill:#e1f5fe     classDef controller fill:#f3e5f5     classDef secret fill:#e8f5e8     classDef process fill:#fff3e0     classDef daemonSet fill:#fce4ec     classDef fileSystem fill:#f1f8e9      class User,AdditionalPS userInput     class HCCO,GlobalPSController,SyncController controller     class OriginalPS,GlobalPSSecret,ServiceAccount,KubeSystemRole,KubeSystemRoleBinding,OpenshiftConfigRole,OpenshiftConfigRoleBinding secret     class ValidatePS,MergeSecrets,RBACSetup,UpdateKubeletConfig,RestartKubelet process     class DaemonSet,DaemonSetPod,Container daemonSet     class KubeletPath,DbusPath fileSystem"},{"location":"how-to/common/global-pull-secret/#key-features","title":"Key Features","text":"<ul> <li>Security: Only watches specific secrets in <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>Robustness: Includes automatic rollback in case of failures</li> <li>Efficiency</li> <li>Only updates when there are actual changes</li> <li>The globalPullSecret implementation has their own controller so it cannot interfere with the HCCO reonciliation</li> <li>Security considerations: Uses specific RBAC for only the required resources in each namespace. The DaemonSet containers run in privileged mode due to the need to:</li> <li>Write to <code>/var/lib/kubelet/config.json</code> (kubelet configuration file)</li> <li>Connect to systemd via DBus for service management</li> <li>Restart kubelet.service, which requires root privileges</li> <li>Smart node targeting: Automatically excludes nodes from InPlace NodePools to prevent MCD conflicts</li> </ul>"},{"location":"how-to/common/global-pull-secret/#inplace-nodepool-handling","title":"InPlace NodePool Handling","text":"<p>To prevent conflicts with Machine Config Daemon operations, the implementation includes intelligent node targeting:</p>"},{"location":"how-to/common/global-pull-secret/#node-labeling-process","title":"Node Labeling Process","text":"<ol> <li>MachineSets Discovery: The controller queries the management cluster for MachineSets with InPlace-specific annotations (<code>hypershift.openshift.io/nodePoolTargetConfigVersion</code>)</li> <li>Machine Enumeration: For each InPlace MachineSets, it lists all associated Machines</li> <li>Node Identification: Maps Machine objects to their corresponding nodes via <code>machine.Status.NodeRef.Name</code></li> <li>Labeling: Applies <code>hypershift.openshift.io/nodepool-inplace-strategy=true</code> label to identified nodes</li> </ol>"},{"location":"how-to/common/global-pull-secret/#daemonset-scheduling-configuration","title":"DaemonSet Scheduling Configuration","text":"<p>The DaemonSet uses NodeAffinity to exclude InPlace nodes:</p> <pre><code>spec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: hypershift.openshift.io/nodepool-inplace-strategy\n                operator: DoesNotExist\n</code></pre> <p>This ensures that: - Nodes without the label: \u2705 Are eligible for DaemonSet scheduling - Nodes with the label (any value): \u274c Are excluded from DaemonSet scheduling</p>"},{"location":"how-to/common/global-pull-secret/#conflict-prevention-benefits","title":"Conflict Prevention Benefits","text":"<ul> <li>Prevents MCD failures: Avoids conflicts when MCD expects specific kubelet configuration during InPlace upgrades</li> <li>Maintains upgrade reliability: InPlace upgrade processes are not interrupted by Global Pull Secret modifications</li> <li>Automatic detection: No manual intervention required - the system automatically identifies and handles InPlace nodes</li> </ul>"},{"location":"how-to/common/global-pull-secret/#error-handling","title":"Error Handling","text":"<p>The system includes comprehensive error handling:</p> <ul> <li>Validation errors: Invalid DockerConfigJSON format is caught early</li> <li>Restart failures: If kubelet restart fails after 3 attempts, the file is rolled back</li> <li>Resource cleanup: If the additional pull secret is deleted, the HCCO automatically removes the globalPullSecret</li> </ul> <p>This implementation provides a secure, autonomous solution that allows HostedCluster administrators to add private registry credentials without requiring Management Cluster administrator intervention.</p>"},{"location":"how-to/common/multi-arch-on-hcp/","title":"Multi-arch on Hosted Control Planes","text":""},{"location":"how-to/common/multi-arch-on-hcp/#general","title":"General","text":"<p>Several platforms now support multi-arch capable HostedClusters when a multi-arch release image or stream is used for  the HostedCluster. This means a HostedCluster can manage NodePools with different CPU architectures. </p> <p>Note</p> <p>An individual NodePool only supports one CPU architecture and cannot support multiple CPU architectures within the  same NodePool.</p> <p>The most up-to-date information on what CPU types are supported on a platform can be found  by looking at the NodePool controller function to validate NodePool CPU and platform here.</p> <p>As of September 2024:</p> <ul> <li>AWS supports amd64 &amp; arm64 NodePools</li> <li>Azure supports amd64 &amp; arm64 NodePools (through ARO HCP only)</li> <li>Agent supports amd64, arm64, &amp; ppc64le NodePools</li> </ul>"},{"location":"how-to/common/multi-arch-on-hcp/#multi-arch-validation","title":"Multi-arch Validation","text":""},{"location":"how-to/common/multi-arch-on-hcp/#hypershift-operator","title":"HyperShift Operator","text":"<p>The HyperShift Operator, through the HostedCluster controller, will update a field, HostedCluster.Status.PayloadArch,  with the payload type of the HostedCluster release image. The valid options for this field are: Multi, ARM64, AMD64, or  PPC64LE.</p> <p>When a NodePool is added to a HostedCluster, the HyperShift Operator, through the NodePool controller, will check the  NodePool.Spec.Arch against the HostedCluster.Status.PayloadArch to ensure the NodePool can be managed by the  HostedCluster. If HostedCluster.Status.PayloadArch is not <code>Multi</code> and it does not exactly match NodePool.Spec.Arch, the  NodePool controller will block reconciliation of the NodePool and set a status condition on the NodePool CR stating the  NodePool cannot be supported by the HostedCluster payload type.</p>"},{"location":"how-to/common/multi-arch-on-hcp/#hcp-cli","title":"HCP CLI","text":"<p>Create Cluster CLI commands will check to see if a multi-arch release image or stream is being used for the  HostedCluster payload. If a multi-arch release image or stream is not used, the CLI will check the management cluster  and NodePool CPU architectures match; if they do not match, the CLI will return an error and stop creating the cluster.</p> <p>The Create NodePool CLI commands for AWS and Azure will attempt to validate the NodePool CPU architecture against the  HostedCluster.Status.PayloadArch if the HostedCluster exists. If a HostedCluster doesn't exist, for instance when  creating a new HostedCluster, a warning message will be displayed stating there was a failure to get the HostedCluster  to check the payload status. If the HostedCluster.Status.PayloadArch exists and isn't multi or does not match the  NodePool CPU architecture, the CLI will return an error and stop creating resources.</p>"},{"location":"how-to/configure-ocp-components/","title":"Configure OCP Components","text":""},{"location":"how-to/configure-ocp-components/#overview","title":"Overview","text":"<p>In standalone OpenShift, cluster configuration is achieved via cluster-scoped resources in the <code>config.openshift.io/v1</code> API group. Resources such as APIServer, OAuth, and Proxy allow adding additional named certificates to the Kube APIServer,  adding identity providers, configuring the global proxy, etc. In HyperShift, configuration resources that impact the control plane need to be specified in the HostedCluster resource instead of inside the guest cluster. The resources still exist inside the guest cluster, but their source of truth is the HostedCluster and are continuously reconciled with what is specified in the HostedCluster.</p>"},{"location":"how-to/configure-ocp-components/#configuration-in-a-hostedcluster","title":"Configuration in a HostedCluster","text":"<p>The configuration resources that should be specified in the HostedCluster are:</p> <ul> <li>APIServer - Provides API server configuration such as certificates and certificate authorities.</li> <li>Authentication - Controls the identity provider and authentication configuration for the cluster.</li> <li>FeatureGate - Enables FeatureGates so that you can use Tech Preview features.</li> <li>Ingress - Configuration details related to routing such as the default domain for routes.</li> <li>Image - Configures how specific image registries should be treated (allowed, disallowed, insecure, CA details).</li> <li>OAuth - Configures identity providers and other behavior related to internal OAuth server flows.</li> <li>Proxy - Defines proxies to be used by components needing external network access. Note: not all components currently consume this value.</li> <li>Scheduler - Configures scheduler behavior such as policies and default node selectors.</li> <li> <p>Network - Configures network properties for initial cluster creation. Resources that should still be configured inside the guest cluster are:</p> </li> <li> <p>Build - Controls default and enforced configuration for all builds on the cluster.</p> </li> <li>Console - Configures the behavior of the web console interface, including the logout behavior.</li> <li>Project - Configures how projects are created including the project template.</li> </ul>"},{"location":"how-to/configure-ocp-components/#specifying-configuration-in-a-hostedcluster","title":"Specifying Configuration in a HostedCluster","text":"<p>Configuration should be specified in their corresponding fields under the spec.configuration field of a HostedCluster:</p> <pre><code>apiVersion: hypershift.openshift.io/v1alpha1\nkind: HostedCluster\nmetadata:\n  name: example\n  namespace: clusters\nspec:\n  configuration:\n    apiServer:\n      ...\n    oauth:\n      ...\n    proxy:\n      ...\n</code></pre>"},{"location":"how-to/configure-ocp-components/#referenced-configmaps-and-secrets","title":"Referenced ConfigMaps and Secrets","text":"<p>Some configuration resources contain references to secrets or config maps. In standalone OpenShift, these are normally expected in the <code>openshift-config</code> namespace. For HyperShift clusters, these need to be placed in the same namespace as the <code>HostedCluster</code> resource.</p> <p>For example, when adding an additional serving certificate to the Kube APIServer, the referenced serving certificate is expected to exist in the same namespace as the HostedCluster:</p> <pre><code>apiVersion: hypershift.openshift.io/v1alpha1\nkind: HostedCluster\nmetadata:\n  name: example\n  namespace: clusters\nspec:\n  configuration:\n    apiServer:\n      servingCerts:\n        namedCertificates:\n        - names:\n          - xxx.example.com\n          - yyy.example.com\n          servingCertificate:\n            name: my-serving-cert\n</code></pre>"},{"location":"how-to/configure-ocp-components/#configuration-validation","title":"Configuration Validation","text":"<p>Configuration embedded in a HostedCluster is validated by HyperShift. If there are any issues reading the configuration or there are secret or config map references missing, the <code>ValidConfiguration</code> condition in the <code>HostedCluster</code> status will be set to <code>False</code> and the message will include information on the reason the configuration failed validation.</p>"},{"location":"how-to/configure-ocp-components/#oauth-configuration","title":"OAuth configuration","text":"<p>The Oauth configuration section allows a user to specify the desired Oauth configuration for the internal Openshift Oauth server. Guest cluster kube admin password will be exposed only when user has not explicitly specified the OAuth configuration. An example configuration for an openID identity provider is shown below: <pre><code>apiVersion: hypershift.openshift.io/v1alpha1\nkind: HostedCluster\nmetadata:\n  name: example\n  namespace: master\nspec:\n  configuration:\n    oauth:\n      identityProviders:\n      - openID:\n          claims:\n            email:\n              - email\n            name:\n              - name\n            preferredUsername:\n              - preferred_username\n          clientID: clientid1\n          clientSecret:\n            name: clientid1-secret-name\n          issuer: https://example.com/identity\n        mappingMethod: lookup\n        name: IAM\n        type: OpenID\n</code></pre></p> <p>For more details on the individual identity providers: refer to upstream openshift documentation</p>"},{"location":"how-to/configure-ocp-components/custom-kas-kubeconfig/","title":"Custom Kube API Server DNS Configuration","text":"<p>This guide explains how to configure a custom DNS domain for your HyperShift cluster's Kube API Server and how to consume the generated Kubeconfig that points to this custom domain. This feature is included in OpenShift 4.19 version and MCE 2.9.</p>"},{"location":"how-to/configure-ocp-components/custom-kas-kubeconfig/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running HyperShift cluster</li> <li>Access to modify the HostedCluster resource</li> <li>A custom DNS domain that you want to use for the Kube API Server</li> <li>A custom certificate configured in the Kubeapi-server as we already explained here</li> </ul>"},{"location":"how-to/configure-ocp-components/custom-kas-kubeconfig/#configuration","title":"Configuration","text":"<p>To configure a custom DNS domain for your cluster's Kube API Server, you need to:</p> <ol> <li> <p>First, configure the DNS record in your provider platform (AWS, Azure, etc.). This is your responsibility as the cluster administrator. The DNS record must be properly configured and resolvable from your OpenShift cluster.</p> </li> <li> <p>Then, modify the <code>kubeAPIServerDNSName</code> field in your HostedCluster specification. This field accepts a URI that will be used as the API server endpoint.</p> </li> </ol>"},{"location":"how-to/configure-ocp-components/custom-kas-kubeconfig/#example-configuration","title":"Example Configuration","text":"<pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: HostedCluster\nmetadata:\n  name: example-cluster\n  namespace: clusters\nspec:\n  configuration:\n    apiServer:\n      servingCerts:\n        namedCertificates:\n        - names:\n          - api-custom-cert-sample-hosted.sample-hosted.example.com\n          servingCertificate:\n            name: sample-hosted-kas-custom-cert\n  kubeAPIServerDNSName: api-custom-cert-sample-hosted.sample-hosted.example.com\n  # ... other spec fields ...\n</code></pre>"},{"location":"how-to/configure-ocp-components/custom-kas-kubeconfig/#consuming-the-kubeconfig","title":"Consuming the Kubeconfig","text":"<p>After applying the configuration described above:</p> <ol> <li>The HyperShift operator will generate a new Kubeconfig that points to your custom DNS domain</li> <li>You can retrieve the Kubeconfig using the standard methods, accessing the secret directly: <pre><code>kubectl get secret &lt;cluster-name&gt;-custom-admin-kubeconfig -n &lt;cluster-namespace&gt; -o jsonpath='{.data.kubeconfig}' | base64 -d\n</code></pre></li> </ol>"},{"location":"how-to/configure-ocp-components/custom-kas-kubeconfig/#important-considerations","title":"Important Considerations","text":"<ol> <li>Ensure that your custom DNS domain is properly configured and resolvable</li> <li>The DNS domain should have valid TLS certificates configured</li> <li>Network access to the custom DNS domain should be properly configured in your environment</li> <li>The custom DNS domain should be unique across your HyperShift clusters</li> </ol>"},{"location":"how-to/configure-ocp-components/custom-kas-kubeconfig/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues accessing the cluster using the custom DNS:</p> <ol> <li>Verify that the DNS record is properly configured and resolving</li> <li>Check that the TLS certificates for the custom domain are valid. (Verify the SAN is correct for your domain) <pre><code>oc get secret -n clusters sample-hosted-kas-custom-cert -o jsonpath='{.data.tls\\.crt}' | base64 -d |openssl x509 -text -noout -\n</code></pre></li> <li>Ensure network connectivity to the custom domain is working</li> <li>Verify that the HostedCluster status shows the custom Kubeconfig properly <pre><code>status:\n  customKubeconfig:\n    name: sample-hosted-custom-admin-kubeconfig\n</code></pre></li> <li>Check the <code>kube-apiserver</code> logs in the HostedControlPlane namespace <pre><code>oc logs -n &lt;HostedControlPlane namespace&gt; -l app=kube-apiserver -f -c kube-apiserver\n</code></pre></li> </ol>"},{"location":"how-to/configure-ocp-components/kubeapi-server/","title":"Configuring Custom API Server Certificate in HostedCluster","text":"<p>This guide explains how to configure a custom certificate for the API server in a HostedCluster.</p>"},{"location":"how-to/configure-ocp-components/kubeapi-server/#overview","title":"Overview","text":"<p>You can configure a custom certificate for the API server by specifying the certificate details in the <code>spec.configuration.apiServer</code> section of your HostedCluster configuration.</p>"},{"location":"how-to/configure-ocp-components/kubeapi-server/#considerations-creating-the-certificate","title":"Considerations creating the certificate","text":"<p>When creating a custom certificate for the API server, there are important considerations to keep in mind regarding Subject Alternative Names (SANs):</p>"},{"location":"how-to/configure-ocp-components/kubeapi-server/#san-conflicts-with-internal-api","title":"SAN Conflicts with Internal API","text":"<p>If your HostedCluster configuration includes a service publishing strategy like: <pre><code>services:\n  - service: APIServer\n    servicePublishingStrategy:\n      type: LoadBalancer\n      loadBalancer:\n        hostname: api-custom-cert-sample-hosted.sample-hosted.example.com\n</code></pre></p> <p>You must ensure that the certificate's Subject Alternative Names (SANs) do not conflict with the internal API endpoint (<code>api-int</code>). This is because:</p> <ul> <li>The internal API endpoint is automatically created and managed by the platform</li> <li>Using the same hostname in both the custom certificate and the internal API endpoint can cause routing conflicts</li> </ul> <p>Important</p> <p>The only exception to this rule is when using AWS as the provider with either Private or PublicAndPrivate configurations. In these specific cases, the SAN conflict is allowed and managed by the platform.</p>"},{"location":"how-to/configure-ocp-components/kubeapi-server/#certificate-requirements","title":"Certificate Requirements","text":"<ul> <li>The certificate must be valid for the external API endpoint</li> <li>The certificate should not include SANs that match the internal API endpoint pattern (except for AWS Private/PublicAndPrivate configurations)</li> <li>Ensure the certificate's validity period aligns with your cluster's expected lifecycle</li> </ul>"},{"location":"how-to/configure-ocp-components/kubeapi-server/#configuration-example","title":"Configuration Example","text":"<p>Here's an example of how to configure a custom certificate for the API server:</p> <pre><code>spec:\n  configuration:\n    apiServer:\n      servingCerts:\n        namedCertificates:\n        - names:\n          - api-custom-cert-sample-hosted.sample-hosted.example.com\n          servingCertificate:\n            name: sample-hosted-kas-custom-cert\n</code></pre>"},{"location":"how-to/configure-ocp-components/kubeapi-server/#configuration-fields","title":"Configuration Fields","text":"<ul> <li><code>names</code>: List of DNS names that the certificate should be valid for.</li> <li><code>servingCertificate.name</code>: Name of the secret containing the custom certificate</li> </ul>"},{"location":"how-to/configure-ocp-components/kubeapi-server/#prerequisites","title":"Prerequisites","text":"<p>Before configuring a custom certificate:</p> <ol> <li>Create a Kubernetes secret containing your custom certificate in the management cluster</li> <li>The secret should contain the following keys:<ul> <li><code>tls.crt</code>: The certificate</li> <li><code>tls.key</code>: The private key</li> </ul> </li> </ol>"},{"location":"how-to/configure-ocp-components/kubeapi-server/#steps-to-configure","title":"Steps to Configure","text":"<ol> <li> <p>Create a secret with your custom certificate:    <pre><code>oc create secret tls sample-hosted-kas-custom-cert \\\n  --cert=path/to/cert.crt \\\n  --key=path/to/key.key \\\n  -n &lt;namespace&gt;\n</code></pre></p> </li> <li> <p>Update your HostedCluster configuration with the custom certificate details as shown in the example above.</p> </li> <li> <p>Apply the changes to your HostedCluster.</p> </li> </ol>"},{"location":"how-to/configure-ocp-components/kubeapi-server/#verification","title":"Verification","text":"<p>After applying the configuration, you can verify that the API server is using the custom certificate by:</p> <ol> <li>Checking the API server pods to ensure they have the new certificate mounted</li> <li>Testing the connection to the API server using the custom domain name</li> <li>Verifying the certificate details in your browser or using tools like <code>openssl</code></li> </ol>"},{"location":"how-to/configure-ocp-components/kubeapi-server/#references","title":"References","text":"<ul> <li>Additional validation to prevent the SAN conflict PR</li> <li>Common error creating a custom certificate for the KAS - Solution 6984698</li> <li>OpenShift Documentation</li> </ul>"},{"location":"how-to/disaster-recovery/","title":"Backup and Restore on HostedControlPlanes","text":"<p>This section of the Hypershift documentation contains pages that show how to perform disaster recovery tasks using different methods.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/","title":"Disaster recovery on Hosted Control Planes with OADP-1.5+","text":"<p>Important</p> <p>This procedure differs from the regular disaster recovery (DR) process due to the use of an OADP plugin that automates many tasks. If you\u2019re unable to deploy OADP 1.5 in your management cluster, please refer to the regular DR procedure.</p> <p>This guide outlines the steps for performing disaster recovery on a Hosted Cluster using the OpenShift API for Data Protection (OADP). It distinguishes between:</p> <ul> <li>Control Plane: Pods running in the Management cluster, functioning as a Hosted Control Plane.</li> <li>Data Plane: The Hosted Cluster where customers deploy workloads and develop core business applications.</li> </ul>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#pre-requisites","title":"Pre-requisites","text":"<p>Ensure the following prerequisites are met on the Management cluster (connected or disconnected):</p> <ul> <li>A valid StorageClass.</li> <li>Cluster-admin access.</li> <li>Access to the openshift-adp version 1.5+ subscription via a CatalogSource.</li> <li>Access to online storage compatible with OpenShift ADP cloud storage providers (e.g., S3, Azure, GCP, MinIO).</li> <li>HostedControlPlane pods are accessible and functioning correctly.</li> <li>The HostedCluster should be PublicAndPrivate or Private.</li> <li>The Public only clusters should have at least a hostname.</li> </ul> <p>\u26a0\ufe0f HostedCluster Configuration</p> <p>The HostedCluster must be configured as <code>PublicAndPrivate</code> or <code>Private</code>. Public clusters without a hostname will cause restore failures. See HostedCluster Configuration Requirements for details.</p> <p>Note for Bare Metal Providers</p> <p>Since the InfraEnv has a different lifecycle than the HostedCluster, it should reside in a separate namespace from the HostedControlPlane and must not be deleted during backup or restore procedures.</p> <p>Important</p> <p>Before proceeding further, two crucial points must be noted:</p> <ol> <li> <p>Restoration will occur in a green field environment, signifying that after the HostedCluster has been backed up, it must be destroyed to initiate the restoration process.</p> </li> <li> <p>Node reprovisioning will take place, necessitating the backup of workloads in the Data Plane before deleting the HostedCluster..</p> </li> </ol>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#deploying-openshift-adp","title":"Deploying OpenShift ADP","text":"<p>To deploy the OADP Operator, refer to the official Red Hat documentation, which provides instructions based on your Management cluster version. After installation, create a DataProtectionApplication (DPA) object, which defines backup locations, Velero pod configurations, and more. This process varies depending on your cloud or remote storage provider. Relevant documentation is available here. This guide focuses on the following platforms:</p> <ul> <li>AWS</li> <li>Baremetal</li> <li>Openstack</li> <li>KubeVirt</li> </ul>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#creating-cloud-provider-credentials","title":"Creating Cloud Provider Credentials","text":"<p>Begin by creating credentials for your backup storage platform. Specific instructions are available in the official documentation. For AWS S3 and MinIO, the basic steps are:</p> <pre><code>cat &lt;&lt; EOF &gt; ./credentials\n[default]\naws_access_key_id=&lt;AWS_ACCESS_KEY_ID&gt;\naws_secret_access_key=&lt;AWS_SECRET_ACCESS_KEY&gt;\nEOF\n\noc create secret generic cloud-credentials -n openshift-adp --from-file cloud=credentials\n</code></pre> <p>Note</p> <p>If using AWS S3, additional AWS resources must be created to enable data backup and restoration. Follow these instructions to set up the necessary configurations.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#sample-dpa-configurations","title":"Sample DPA Configurations","text":"<p>Below are some samples of DPA configurations for the mentioned platforms</p> AWSBare MetalOpenstack <pre><code>---\napiVersion: oadp.openshift.io/v1alpha1\nkind: DataProtectionApplication\nmetadata:\n  name: dpa-instance\n  namespace: openshift-adp\nspec:\n  backupLocations:\n    - name: default\n      velero:\n        provider: aws\n        default: true\n        objectStorage:\n          bucket: &lt;bucket_name&gt;\n          prefix: &lt;prefix&gt;\n        config:\n          region: us-east-1\n          profile: \"backupStorage\"\n        credential:\n          key: cloud\n          name: cloud-credentials\n  snapshotLocations:\n    - velero:\n        provider: aws\n        config:\n          region: us-east-1\n          profile: \"volumeSnapshot\"\n        credential:\n          key: cloud\n          name: cloud-credentials\n  configuration:\n    nodeAgent:\n      enable: true\n      uploaderType: kopia\n    velero:\n      defaultPlugins:\n        - openshift\n        - aws\n        - csi\n        - hypershift\n      resourceTimeout: 2h\n</code></pre> <pre><code>---\napiVersion: oadp.openshift.io/v1alpha1\nkind: DataProtectionApplication\nmetadata:\n  name: dpa-instance\n  namespace: openshift-adp\nspec:\n  backupLocations:\n    - name: default\n      velero:\n        provider: aws\n        default: true\n        objectStorage:\n          bucket: oadp-backup\n          prefix: hcp\n        config:\n          region: minio\n          profile: \"default\"\n          s3ForcePathStyle: \"true\"\n          s3Url: \"http://registry.hypershiftbm.lab:9002\"\n          insecureSkipTLSVerify: \"true\"\n        credential:\n          key: cloud\n          name: cloud-credentials\n          default: true\n  snapshotLocations:\n    - velero:\n        provider: aws\n        config:\n          region: minio\n          profile: \"default\"\n        credential:\n          key: cloud\n          name: cloud-credentials\n  configuration:\n    nodeAgent:\n      enable: true\n      uploaderType: kopia\n    velero:\n      defaultPlugins:\n        - openshift\n        - aws\n        - csi\n        - hypershift\n      resourceTimeout: 2h\n</code></pre> <pre><code>---\napiVersion: oadp.openshift.io/v1alpha1\nkind: DataProtectionApplication\nmetadata:\n  name: dpa-instance\n  namespace: openshift-adp\nspec:\n  backupLocations:\n    - name: default\n      velero:\n        provider: aws\n        default: true\n        objectStorage:\n          bucket: example-oadp\n          prefix: backup-objects\n        config:\n          region: region-one\n          profile: \"default\"\n        credential:\n          key: cloud\n          name: cloud-credentials\n  snapshotLocations:\n    - velero:\n        provider: aws\n        config:\n          region: minio\n          profile: \"default\"\n        credential:\n          key: cloud\n          name: cloud-credentials\n  configuration:\n    nodeAgent:\n      enable: true\n      uploaderType: kopia\n    velero:\n      defaultPlugins:\n        - openshift\n        - aws\n        - csi\n        - hypershift\n      resourceTimeout: 2h\n</code></pre> <p>Once you create any of these DPA objects, several pods will be instantiated in the <code>openshift-adp</code> namespace. This includes one <code>node-agent</code> per node in the Management Cluster and the <code>velero</code> deployment.</p> <p>Note</p> <p>To follow backup and restore procedures, you can monitor the logs in the velero pod.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#backup-and-upload","title":"Backup and Upload","text":"AWSBare Metal - CSI SupportBare Metal - Non CSI SupportOpenstackKubeVirt <p>Note</p> <p>If the workloads in the Data Plane are not crucial for you, it's safe to skip this step.</p> <p>If you need to backup the applications running under the HostedCluster, it's advisable to follow the official documentation for backup and restore of OpenShift applications</p> <p>The steps are the following:</p> <ul> <li>Deploy the OADP operator from OLM.</li> <li>Create the DPA (Data Protection Application), with a manifest similar to the one provided earlier. It might be beneficial to adjust the <code>Prefix</code> or/and <code>Bucket</code> fields to keep the ControlPlane and DataPlane backups separated.</li> <li>Create the backup manifest. This step varies depending on the complexity of the workloads in the Data Plane. It's essential to thoroughly examine how to back up the PersistentVolumes, the backend used, and ensure compatibility with our storage provisioner.</li> </ul> <p>We recommend checking if your workloads contain Persistent Volumes and if our StorageClass is compatible with CSI Volume Snapshots, which is one of the simplest ways to handle this aspect.</p> <p>As a standard approach to maintain consistency in the backup layer for the Hosted Control Plane, we will utilize <code>Kopia</code> as the backend tool for data snapshots, along with <code>File System Backup</code>. However, it's possible that your workloads may benefit from a different approach that better aligns with your specific use case.</p> <p>Important</p> <p>The backup of the workloads residing in the Data Plane falls outside the scope of this documentation. Please refer to the official Openshift-ADP backup documentation for further details. Additional links and information can be found in the References section.</p> <p>Once we have completed the backup of the Data Plane layer, we can proceed with the backup of the Hosted Control Plane (HCP).</p> <p>Note</p> <p>If the workloads in the Data Plane are not crucial for you, it's safe to skip this step.</p> <p>If you need to backup the applications running under the HostedCluster, it's advisable to follow the official documentation for backup and restore of OpenShift applications</p> <p>The steps are the following:</p> <ul> <li>Deploy the OADP operator from OLM.</li> <li>Create the DPA (Data Protection Application), with a manifest similar to the one provided earlier. It might be beneficial to adjust the <code>Prefix</code> or/and <code>Bucket</code> fields to keep the ControlPlane and DataPlane backups separated.</li> <li>Create the backup manifest. This step varies depending on the complexity of the workloads in the Data Plane. It's essential to thoroughly examine how to back up the PersistentVolumes, the backend used, and ensure compatibility with our storage provisioner.</li> </ul> <p>We recommend checking if your workloads contain Persistent Volumes and if our StorageClass is compatible with CSI Volume Snapshots, which is one of the simplest ways to handle this aspect.</p> <p>As a standard approach to maintain consistency in the backup layer for the Hosted Control Plane, we will utilize <code>Kopia</code> as the backend tool for data snapshots, along with <code>File System Backup</code>. However, it's possible that your workloads may benefit from a different approach that better aligns with your specific use case.</p> <p>Important</p> <p>The backup of the workloads residing in the Data Plane falls outside the scope of this documentation. Please refer to the official Openshift-ADP backup documentation for further details. Additional links and information can be found in the References section.</p> <p>Once we have completed the backup of the Data Plane layer, we can proceed with the backup of the Hosted Control Plane (HCP).</p> <p>Note</p> <p>If the workloads in the Data Plane are not crucial for you, it's safe to skip this step.</p> <p>If you need to backup the applications running under the HostedCluster, it's advisable to follow the official documentation for backup and restore of OpenShift applications</p> <p>The steps are the following:</p> <ul> <li>Deploy the OADP operator from OLM.</li> <li>Create the DPA (Data Protection Application), with a manifest similar to the one provided earlier. It might be beneficial to adjust the <code>Prefix</code> or/and <code>Bucket</code> fields to keep the ControlPlane and DataPlane backups separated.</li> <li>Create the backup manifest. This step varies depending on the complexity of the workloads in the Data Plane. It's essential to thoroughly examine how to back up the PersistentVolumes, the backend used, and ensure compatibility with our storage provisioner.</li> </ul> <p>We recommend checking if your workloads contain Persistent Volumes and if our StorageClass is compatible with CSI Volume Snapshots, which is one of the simplest ways to handle this aspect.</p> <p>As a standard approach to maintain consistency in the backup layer for the Hosted Control Plane, we will utilize <code>Kopia</code> as the backend tool for data snapshots, along with <code>File System Backup</code>. However, it's possible that your workloads may benefit from a different approach that better aligns with your specific use case.</p> <p>Important</p> <p>The backup of the workloads residing in the Data Plane falls outside the scope of this documentation. Please refer to the official Openshift-ADP backup documentation for further details. Additional links and information can be found in the References section.</p> <p>Once we have completed the backup of the Data Plane layer, we can proceed with the backup of the Hosted Control Plane (HCP).</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#data-plane-workloads-backup","title":"Data Plane workloads backup","text":"<p>Note</p> <p>If the workloads in the Data Plane are not crucial for you, it's safe to skip this step.</p> <p>If you need to backup the applications running under the HostedCluster, it's advisable to follow the official documentation for backup and restore of OpenShift applications</p> <p>The steps are the following:</p> <ul> <li>Deploy the OADP operator from OLM.</li> <li>Create the DPA (Data Protection Application), with a manifest similar to the one provided earlier. It might be beneficial to adjust the <code>Prefix</code> or/and <code>Bucket</code> fields to keep the ControlPlane and DataPlane backups separated.</li> <li>Create the backup manifest. This step varies depending on the complexity of the workloads in the Data Plane. It's essential to thoroughly examine how to back up the PersistentVolumes, the backend used, and ensure compatibility with our storage provisioner.</li> </ul> <p>We recommend checking if your workloads contain Persistent Volumes and if our StorageClass is compatible with CSI Volume Snapshots, which is one of the simplest ways to handle this aspect.</p> <p>As a standard approach to maintain consistency in the backup layer for the Hosted Control Plane, we will utilize <code>Kopia</code> as the backend tool for data snapshots, along with <code>File System Backup</code>. However, it's possible that your workloads may benefit from a different approach that better aligns with your specific use case.</p> <p>Important</p> <p>The backup of the workloads residing in the Data Plane falls outside the scope of this documentation. Please refer to the official Openshift-ADP backup documentation for further details. Additional links and information can be found in the References section.</p> <p>Once we have completed the backup of the Data Plane layer, we can proceed with the backup of the Hosted Control Plane (HCP).</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#control-plane-backup","title":"Control Plane backup","text":"<p>Now, we will apply the backup manifest. Here is how it looks like:</p> <pre><code>---\napiVersion: velero.io/v1\nkind: Backup\nmetadata:\n  name: hc-clusters-hosted-backup\n  namespace: openshift-adp\n  labels:\n    velero.io/storage-location: default\n    spec:\n  hooks: {}\n  includedNamespaces:\n  - clusters\n  - clusters-hosted\n  includedResources:\n  - sa\n  - role\n  - rolebinding\n  - pod\n  - pvc\n  - pv\n  - configmap\n  - priorityclasses\n  - pdb\n  - hostedcluster\n  - nodepool\n  - secrets\n  - services\n  - deployments\n  - statefulsets\n  - hostedcontrolplane\n  - cluster\n  - awscluster\n  - awsmachinetemplate\n  - awsmachine\n  - machinedeployment\n  - machineset\n  - machine\n  - route\n  - clusterdeployment\n  excludedResources: []\n  storageLocation: default\n  ttl: 2h30m0s\n  snapshotMoveData: true\n  datamover: \"velero\"\n  defaultVolumesToFsBackup: false\n  snapshotVolumes: true\n</code></pre> <p>We will emphasize the most important fields:</p> <ul> <li>These two fields enable the CSI VolumeSnapshots to be automatically uploaded to the remote cloud storage.</li> </ul> <pre><code>snapshotMoveData: true\ndatamover: \"velero\"\n</code></pre> <ul> <li>This field selects the namespaces from which objects will be backed up. They should include namespaces from both the HostedCluster (in the example <code>clusters</code>) and the HostedControlPlane (in the example <code>clusters-hosted</code>).</li> </ul> <pre><code>includedNamespaces:\n- clusters\n- clusters-hosted\n</code></pre> <p>Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.</p> <p>The backup process is considered complete when the <code>status.phase</code> is <code>Completed</code>.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#control-plane-backup-for-csi-compatible-storage-class","title":"Control Plane backup - For CSI compatible storage class","text":"<p>Now, we will apply the backup manifest. Here is how it looks like:</p> <pre><code>---\napiVersion: velero.io/v1\nkind: Backup\nmetadata:\n  name: hc-clusters-hosted-backup\n  namespace: openshift-adp\n  labels:\n    velero.io/storage-location: default\n    spec:\n  hooks: {}\n  includedNamespaces:\n  - clusters\n  - clusters-hosted\n  includedResources:\n  - sa\n  - role\n  - rolebinding\n  - pod\n  - pvc\n  - pv\n  - bmh\n  - configmap\n  - infraenv\n  - priorityclasses\n  - pdb\n  - agents\n  - hostedcluster\n  - nodepool\n  - secrets\n  - services\n  - deployments\n  - statefulsets\n  - hostedcontrolplane\n  - cluster\n  - agentcluster\n  - agentmachinetemplate\n  - agentmachine\n  - machinedeployment\n  - machineset\n  - machine\n  - route\n  - clusterdeployment\n  excludedResources: []\n  storageLocation: default\n  ttl: 2h30m0s\n  snapshotMoveData: true\n  datamover: \"velero\"\n  defaultVolumesToFsBackup: false\n  snapshotVolumes: true\n</code></pre> <p>We will emphasize the most important fields:</p> <ul> <li>These two fields enable the CSI VolumeSnapshots to be automatically uploaded to the remote cloud storage.</li> </ul> <pre><code>snapshotMoveData: true\ndatamover: \"velero\"\n</code></pre> <ul> <li>This field selects the namespaces from which objects will be backed up. They should include namespaces from both the HostedCluster (in the example <code>clusters</code>) and the HostedControlPlane (in the example <code>clusters-hosted</code>).</li> </ul> <pre><code>includedNamespaces:\n- clusters\n- clusters-hosted\n</code></pre> <p>Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.</p> <p>The backup process is considered complete when the <code>status.phase</code> is <code>Completed</code>.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#control-plane-backup-for-non-csi-compatible-storage-class","title":"Control Plane backup - For Non CSI compatible storage class","text":"<p>Now, we will apply the backup manifest. Here is how it looks like:</p> <pre><code>---\napiVersion: velero.io/v1\nkind: Backup\nmetadata:\n  name: hc-clusters-hosted-backup\n  namespace: openshift-adp\n  labels:\n    velero.io/storage-location: default\n    spec:\n  hooks: {}\n  includedNamespaces:\n  - clusters\n  - clusters-hosted\n  includedResources:\n  - sa\n  - role\n  - rolebinding\n  - pod\n  - pvc\n  - pv\n  - bmh\n  - configmap\n  - infraenv\n  - priorityclasses\n  - pdb\n  - agents\n  - hostedcluster\n  - nodepool\n  - secrets\n  - services\n  - deployments\n  - statefulsets\n  - hostedcontrolplane\n  - cluster\n  - agentcluster\n  - agentmachinetemplate\n  - agentmachine\n  - machinedeployment\n  - machineset\n  - machine\n  - route\n  - clusterdeployment\n  excludedResources: []\n  defaultVolumesToFsBackup: true\n  storageLocation: default\n  ttl: 2h30m0s\n</code></pre> <p>We will emphasize the most important fields:</p> <ul> <li>This particular field is crucial if you utilize a storage class that does not supported by CSI drivers (e.g NFS). It designates fs-backup as the default method for Persistent Volume backup. If you wish to continue using CSI Volume Snapshot (within the same backup manifest), please check the \"Bare Metal - CSI Support\" tab.</li> </ul> <pre><code>defaultVolumesToFsBackup: true\n</code></pre> <ul> <li>This field selects the namespaces from which objects will be backed up. They should include namespaces from both the HostedCluster (in the example <code>clusters</code>) and the HostedControlPlane (in the example <code>clusters-hosted</code>).</li> </ul> <pre><code>includedNamespaces:\n- clusters\n- clusters-hosted\n</code></pre> <p>Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.</p> <p>The backup process is considered complete when the <code>status.phase</code> is <code>Completed</code>.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#control-plane-backup-openstack","title":"Control Plane backup - Openstack","text":"<p>Now, we will apply the backup manifest. Here is how it looks like:</p> <pre><code>---\napiVersion: velero.io/v1\nkind: Backup\nmetadata:\n  name: hc-clusters-hosted-backup\n  namespace: openshift-adp\n  labels:\n    velero.io/storage-location: default\n    spec:\n  hooks: {}\n  includedNamespaces:\n  - clusters\n  - clusters-hosted\n  includedResources:\n  - sa\n  - role\n  - rolebinding\n  - pod\n  - pvc\n  - pv\n  - configmap\n  - priorityclasses\n  - pdb\n  - hostedcluster\n  - nodepool\n  - secrets\n  - services\n  - deployments\n  - statefulsets\n  - hostedcontrolplane\n  - cluster\n  - openstackclusters\n  - openstackmachinetemplates\n  - openstackmachine\n  - machinedeployment\n  - openstackfloatingippools\n  - machineset\n  - machine\n  - route\n  - clusterdeployment\n  excludedResources: []\n  storageLocation: default\n  ttl: 2h30m0s\n  snapshotMoveData: true\n  datamover: \"velero\"\n  defaultVolumesToFsBackup: false\n  snapshotVolumes: true\n</code></pre> <p>We will emphasize the most important fields:</p> <ul> <li>These two fields enable the CSI VolumeSnapshots to be automatically uploaded to the remote cloud storage.</li> </ul> <pre><code>snapshotMoveData: true\ndatamover: \"velero\"\n</code></pre> <ul> <li>This field selects the namespaces from which objects will be backed up. They should include namespaces from both the HostedCluster (in the example <code>clusters</code>) and the HostedControlPlane (in the example <code>clusters-hosted</code>).</li> </ul> <pre><code>includedNamespaces:\n- clusters\n- clusters-hosted\n</code></pre> <p>Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.</p> <p>The backup process is considered complete when the <code>status.phase</code> is <code>Completed</code>.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#backup-of-a-kubevirt-hosted-cluster","title":"Backup of a kubeVirt Hosted Cluster","text":"<p>Important</p> <p>The restore may only be done on the same management cluster where the backup was created.</p> <p>Backup of a hosted cluster, running on a KubeVirt platform may be done on a running hosted cluster, and there is no need to pause it.</p> <p>The backup will contain the hosted control plane components, the hosted cluster ETCD, and the data stored on the hosted cluster PVCs.</p> <p>The backup will not contain the KubeVirt VMs, used as worker nodes, and they will be automatically recreated after the restore.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#performing-the-backup","title":"Performing the Backup","text":"<p>The backup itself is done by creating a velero Backup resource. Here is how its manifest looks like: <pre><code>apiVersion: velero.io/v1\nkind: Backup\nmetadata:\n  name: hc-clusters-hosted-backup\n  namespace: openshift-adp\n  labels:\n    velero.io/storage-location: default\nspec:\n  includedNamespaces:\n  - clusters\n  - clusters-hosted\n  includedResources:\n  - sa\n  - role\n  - rolebinding\n  - deployment\n  - statefulset\n  - pv\n  - pvc\n  - bmh\n  - configmap\n  - infraenv\n  - priorityclasses\n  - pdb\n  - hostedcluster\n  - nodepool\n  - secrets\n  - hostedcontrolplane\n  - cluster\n  - datavolume\n  - service\n  - route\n  excludedResources: [ ]\n  labelSelector:\n    matchExpressions:\n    - key: 'hypershift.openshift.io/is-kubevirt-rhcos'\n      operator: 'DoesNotExist'\n  storageLocation: default\n  preserveNodePorts: true\n  ttl: 2h30m0s\n  snapshotMoveData: true\n  datamover: \"velero\"\n  defaultVolumesToFsBackup: false\n</code></pre> We will emphasize the most important fields:</p> <ul> <li> <p>These two fields enable the CSI VolumeSnapshots to be automatically uploaded to the remote cloud storage.</p> <pre><code>snapshotMoveData: true\ndatamover: \"velero\"\n</code></pre> </li> <li> <p>We don't want to use this feature. This will allow us to safely backup the PVCs we want</p> <pre><code>defaultVolumesToFsBackup: false\n</code></pre> </li> <li> <p>This field selects the namespaces from which objects will be backed up. They should include namespaces from both   the HostedCluster (in the example <code>clusters</code>) and the HostedControlPlane (in the example <code>clusters-hosted</code>).</p> <pre><code>includedNamespaces:\n- clusters\n- clusters-hosted\n</code></pre> </li> </ul> <p>Hint</p> <p>By default, the HostedControlPlane namespace is <code>clusters-&lt;hosted cluster name&gt;</code>.</p> <ul> <li> <p>The boot image of the KubeVirt VMs, that are used as the hosted cluster nodes, are stored in huge PVCs. We don't   need these PVCs because the VMs are going to be recreated as new VMs. We want tilter these PVCs out of the backup   to gain meaningful reduce in backup time and storage size. We'll filter these PVC using this label selector:</p> <pre><code>labelSelector:\n  matchExpressions:\n  - key: 'hypershift.openshift.io/is-kubevirt-rhcos'\n    operator: 'DoesNotExist'\n</code></pre> </li> </ul> <p>Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.</p> <p>The backup process is considered complete when the <code>status.phase</code> is <code>Completed</code>.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#restore","title":"Restore","text":"<p>For the restoration procedure, ensure that there are no Pods/PVCs running in the HostedControlPlane namespace to facilitate a successful restoration. If restoring the HostedCluster within the same Management cluster, delete the following objects:</p> <ul> <li>HostedCluster</li> <li>Nodepools</li> <li>PVCs</li> </ul> <p>This will remove the Pods/PVCs, allowing for a proper restoration of the environment.</p> <p>Important</p> <p>In the case of the Bare Metal provider (Agent), it's crucial to ensure that we don't delete the InfraEnv object. This object is mandatory for the new nodes that will be reprovisioned, as they need access to it in order to retrieve the Discovery ISO.</p> <p>The restoration object is provider agnostic. Just make sure you point to the right backupName to restore from</p> <p>Now let's take a look to the restoration manifest:</p> <pre><code>---\napiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: hc-clusters-hosted-restore\n  namespace: openshift-adp\nspec:\n  backupName: hc-clusters-hosted-backup\n  restorePVs: true\n  existingResourcePolicy: update\n  excludedResources:\n  - nodes\n  - events\n  - events.events.k8s.io\n  - backups.velero.io\n  - restores.velero.io\n  - resticrepositories.velero.io\n</code></pre> <p>Here we have a couple of important fields</p> <pre><code>restorePVs: true\nexistingResourcePolicy: update\n</code></pre> <ul> <li><code>restorePvs</code> will initiate the recovery of pods with the included persistent volumes.</li> <li>Setting <code>existingResourcePolicy</code> to <code>update</code> ensures that any existing objects are overwritten with the backup content. This may cause issues with objects containing immutable fields, which is why we delete them in a previous step. If this policy is not set, the Velero engine will skip the restoration of objects that already exist.</li> </ul> <p>You can monitor the restoration process by checking the restore status field and following the Velero logs mentioned in the Watching section.</p> <p>The restoration process is considered complete once the <code>status.phase</code> is <code>Completed</code>.</p> <p>Important</p> <p>The restore may only be done on the same management cluster where the backup was created. Depending on the HostedCluster provider you are using, there are important topics to have in mind before the restoration.</p> AWSOpenstackBareMetalKubevirt <ul> <li>Node readoption is not supported in this provider yet, so the worker nodes will be reprovisioned at restoration time</li> </ul> <ul> <li>Node readoption is not supported in this provider yet, so the worker nodes will be reprovisioned at restoration time</li> </ul> <ul> <li>If you are restoring the HostedCluster in the same Management Cluster, make sure:<ul> <li>You don't delete the Infraenv namespace or the objects inside.</li> <li>You don't remove the Assisted Installer Postgres DDBB</li> </ul> </li> <li>If you are restoring the HostedCluster in a different Management Cluster, have in mind that you should backup some things too:<ul> <li>Backup of the infraenv namespace and their objects</li> <li>You need to backup and restore the Postgres DDBB</li> <li>All of this could be included in the Backup manifest to do this automatically</li> </ul> </li> <li>Node readoption is supported in this provider only under certain conditions:<ul> <li>For now only in supported in HostedClusters with 4.19 OCP versions</li> <li>MCE 2.9 or ACM 2.14</li> </ul> </li> </ul> <ul> <li>Restoration in a separated Management cluster is not supported by this provider</li> <li>Node readoption is not supported in this provider yet, so the worker nodes will be reprovisioned at restoration time</li> </ul>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#schedule","title":"Schedule","text":"<p>OADP provides the ability to schedule backups using the Schedule CR. This is fully compatible with the Hosted Control Planes backup procedure; just make sure you are following the official Red Hat documentation.</p> <ul> <li>https://docs.redhat.com/en/documentation/openshift_container_platform/latest/html/backup_and_restore/oadp-application-backup-and-restore#oadp-scheduling-backups-doc</li> </ul>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#watching-and-troubleshooting","title":"Watching and Troubleshooting","text":""},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#watching","title":"Watching","text":"<p>Here we will describe how to monitor and observe a Backup or Restore process:</p> <ul> <li>Watch the Backup process</li> </ul> <pre><code>watch \"oc get backup -n openshift-adp &lt;BACKUP_NAME&gt; -o jsonpath='{.status}' | jq\"\n</code></pre> <ul> <li>Watch the Restore process</li> </ul> <pre><code>watch \"oc get restore -n openshift-adp &lt;BACKUP_NAME&gt; -o jsonpath='{.status}' | jq\"\n</code></pre> <ul> <li>Follow the Velero logs</li> </ul> <pre><code>oc logs -n openshift-adp -ldeploy=velero -f\n</code></pre> <ul> <li>Watch all the OADP Main objects</li> </ul> <pre><code>watch \"echo BackupRepositories:;echo;oc get backuprepositories.velero.io -A;echo; echo BackupStorageLocations: ;echo; oc get backupstoragelocations.velero.io -A;echo;echo DataUploads: ;echo;oc get datauploads.velero.io -A;echo;echo DataDownloads: ;echo;oc get datadownloads.velero.io -n openshift-adp; echo;echo VolumeSnapshotLocations: ;echo;oc get volumesnapshotlocations.velero.io -A;echo;echo Backups:;echo;oc get backup -A; echo;echo Restores:;echo;oc get restore -A\"\n</code></pre>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#troubleshooting","title":"Troubleshooting","text":"<p>For troubleshooting purposes, it's crucial to identify where the process is stuck. Here are some tips:</p> <ul> <li>Review the Velero logs in the <code>openshift-adp</code> namespace.</li> <li>Utilize the <code>velero</code> command with the <code>--details</code> flag to describe the backup/restore objects.</li> <li>Check the backup/restore status directly in the respective object, which can provide valuable hints.</li> <li>Examine the Events in the affected namespaces (in the previous examples, these were <code>clusters</code> and <code>clusters-hosted</code>).</li> <li>Verify the status of all OADP objects using the command provided in the Watching section.</li> </ul>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#tips-and-tricks","title":"Tips and Tricks","text":""},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#use-the-velero-cli","title":"Use the velero CLI","text":"<p>To gain more insights into the backup/restore objects or to perform actions such as deleting a backup/restore object, you can utilize the <code>velero</code> CLI. This tool provides additional context in each case, and the best part is that you don't need to download anything.</p> <ul> <li>Create an alias to use the <code>velero</code> CLI from a container</li> </ul> <pre><code>alias velero='oc -n openshift-adp exec deployment/velero -c velero -it -- ./velero'\n</code></pre> <p>Now you can execute commands using the <code>velero</code> CLI. Here are some sample commands:</p> <ul> <li>Describe a restore called <code>hc-clusters-hosted-restore</code></li> </ul> <pre><code>velero restore describe hc-clusters-hosted-restore --details\n</code></pre> <ul> <li>Describe a backup called <code>hc-clusters-hosted-backup</code></li> </ul> <pre><code>velero backup describe hc-clusters-hosted-backup --details\n</code></pre>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#how-to-handle-backups-properly","title":"How to handle backups properly","text":"<p>To handle the <code>backup</code> and <code>restore</code> objects in the cloud storage effectively, the following command is quite useful. It helps identify issues with the <code>backuprepositories.velero.io</code> object if you manually modify the folder structure in the destination storage. Therefore, we recommend avoiding modifications over that storage and managing the backups/restore objects using the CLI.</p> <ul> <li>Delete a backup created and called <code>hc-clusters-hosted-backup</code></li> </ul> <pre><code>velero delete backup hc-clusters-hosted-backup\n</code></pre> <p>Important</p> <p>If you modify the folder structure of the remote storage where your backups are hosted, you may encounter issues with <code>backuprepositories.velero.io</code>. In such cases, you will need to recreate all the associated objects, including DPAs, backups, restores, etc.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#hostedcluster-configuration-requirements","title":"HostedCluster Configuration Requirements","text":"<p>The HostedCluster must be configured as <code>PublicAndPrivate</code> or <code>Private</code> for backup/restore operations to work correctly. If the HostedCluster is configured as Public (without a hostname in the ServicePublishingStrategy for the kube-api-server), the restore operation will fail with the following consequences:</p> <ul> <li>Nodes remain in NotReady state</li> <li>NodePool scaling fails to generate new nodes</li> </ul>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#root-cause","title":"Root Cause","text":"<p>The issue occurs because:</p> <ul> <li>Nodes store the ELB (Elastic Load Balancer) address in their kubelet configuration, which is ephemeral and changes when the cluster is deleted and restored</li> <li>The SAN (Subject Alternative Name) in the certificate fails because the certificate name no longer matches the new ELB</li> <li>Original nodes cannot connect to the ControlPlane because they point to the old ELB, and even if they pointed to the new one, the certificate would be incorrect</li> </ul>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#solution","title":"Solution","text":"<p>Ensure your HostedCluster is configured with either:</p> <ul> <li><code>PublicAndPrivate</code> service publishing strategy, OR</li> <li><code>Private</code> service publishing strategy, OR</li> <li><code>Public</code> service publishing strategy with a hostname specified for the kube-api-server</li> </ul>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp-1-5/#example-configuration","title":"Example Configuration","text":"<p>Option 1: PublicAndPrivate or Private <pre><code>spec:\n  platform:\n    aws:\n      servicePublishingStrategy:\n        kubeAPIServer:\n          type: PublicAndPrivate  # or Private\n</code></pre></p> <p>Option 2: Public with hostname <pre><code>spec:\n  platform:\n    aws:\n      servicePublishingStrategy:\n        kubeAPIServer:\n          type: Public\n          hostname: \"api.your-cluster.example.com\"\n</code></pre></p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/","title":"Disaster recovery on Hosted Control Planes","text":"<p>Important</p> <p>This procedure uses the Hypershift OADP plugin that automates many tasks. Here we assume you have oadp &lt; 1.5 in your management cluster, if that's not the case, please refer to the OADP-1.5 DR procedure.</p> <p>In this section, we will outline the procedures for performing disaster recovery tasks on a Hosted Cluster using the Openshift API for Data Protection (OADP). We will differentiate between the Control Plane (consisting of pods running in the Management cluster, which function as a Hosted Control Plane) and the Data Plane (the Hosted Cluster where customers add their workloads and develop their core business).</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#pre-requisites","title":"Pre-requisites","text":"<p>The first consideration is to ensure we meet the prerequisites. On the Management cluster, whether it is Connected or Disconnected, we require:</p> <ul> <li>A valid StorageClass.</li> <li>Cluster-admin access.</li> <li>Access to the openshift-adp subscription through a CatalogSource.</li> <li>Access to online storage compatible with the openshift-adp cloud storage providers (S3, Azure, GCP, Minio, etc.).</li> <li>The HostedControlPlane pods should be accessible and functioning correctly.</li> <li>(Bare Metal Provider Only) As the InfraEnv has a different lifecycle than the HostedCluster, it should reside in a namespace separate from that of the HostedControlPlane and should not be deleted during the backup/restore procedures.</li> </ul> <p>\u26a0\ufe0f HostedCluster Configuration</p> <p>The HostedCluster must be configured as <code>PublicAndPrivate</code> or <code>Private</code>. Public clusters without a hostname will cause restore failures. See HostedCluster Configuration Requirements for details.</p> <p>Important</p> <p>Before proceeding further, two crucial points must be noted: 1. Restoration will occur in a green field environment, signifying that after the HostedCluster has been backed up, it must be destroyed to initiate the restoration process. 2. Node reprovisioning will take place, necessitating the backup of workloads in the Data Plane before deleting the HostedCluster..</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#openshift-adp-deployment","title":"Openshift-adp deployment","text":"<p>To deploy the OADP operator, we kindly redirect you to the Official Red Hat documentation, where they provide instructions based on the version of the Management Cluster you're using.</p> <p>Once installed, you'll need to create an object called DPA (Data Protection Application), which essentially describes the backup locations, Velero pod configurations, etc. This process varies depending on the cloud/remote storage location. All relevant documentation is available here.</p> <p>This guide will focus on three main platforms:</p> <ul> <li>AWS</li> <li>Baremetal</li> <li>KubeVirt</li> </ul> <p>The first step is to create credentials for the platform where you'll upload the backups. Specific instructions can be found in the official documentation, but the basic steps are as follows:</p> <pre><code>cat &lt;&lt; EOF &gt; ./credentials\n[default]\naws_access_key_id=&lt;AWS_ACCESS_KEY_ID&gt;\naws_secret_access_key=&lt;AWS_SECRET_ACCESS_KEY&gt;\nEOF\n\noc create secret generic cloud-credentials -n openshift-adp --from-file cloud=credentials\n</code></pre> <p>Note</p> <p>This applies to S3 and Minio platforms. For other providers, you can follow the instructions provided in the official documentation.</p> <p>If you are using the AWS S3 provider, you will need to create additional objects in AWS to enable the push and pullback of data to S3. To accomplish this, follow these instructions</p> <p>Below are some samples of DPA configurations for the mentioned platforms</p> Bare MetalAWSOpenstackKubevirt <pre><code>---\napiVersion: oadp.openshift.io/v1alpha1\nkind: DataProtectionApplication\nmetadata:\n  name: dpa-instance\n  namespace: openshift-adp\nspec:\n  backupLocations:\n    - name: default\n      velero:\n        provider: aws\n        default: true\n        objectStorage:\n          bucket: oadp-backup\n          prefix: hcp\n        config:\n          region: minio\n          profile: \"default\"\n          s3ForcePathStyle: \"true\"\n          s3Url: \"http://registry.hypershiftbm.lab:9002\"\n          insecureSkipTLSVerify: \"true\"\n        credential:\n          key: cloud\n          name: cloud-credentials\n          default: true\n  snapshotLocations:\n    - velero:\n        provider: aws\n        config:\n          region: minio\n          profile: \"default\"\n        credential:\n          key: cloud\n          name: cloud-credentials\n  configuration:\n    nodeAgent:\n      enable: true\n      uploaderType: kopia\n    velero:\n      defaultPlugins:\n        - openshift\n        - aws\n        - csi\n      customPlugins:\n        - name: hypershift-oadp-plugin\n          image: quay.io/redhat-user-workloads/ocp-art-tenant/oadp-hypershift-oadp-plugin-main:main\n      resourceTimeout: 2h\n</code></pre> <pre><code>---\napiVersion: oadp.openshift.io/v1alpha1\nkind: DataProtectionApplication\nmetadata:\n  name: dpa-instance\n  namespace: openshift-adp\nspec:\n  backupLocations:\n    - name: default\n      velero:\n        provider: aws\n        default: true\n        objectStorage:\n          bucket: &lt;bucket_name&gt;\n          prefix: &lt;prefix&gt;\n        config:\n          region: us-east-1\n          profile: \"backupStorage\"\n        credential:\n          key: cloud\n          name: cloud-credentials\n  snapshotLocations:\n    - velero:\n        provider: aws\n        config:\n          region: us-east-1\n          profile: \"volumeSnapshot\"\n        credential:\n          key: cloud\n          name: cloud-credentials\n  configuration:\n    nodeAgent:\n      enable: true\n      uploaderType: kopia\n    velero:\n      defaultPlugins:\n        - openshift\n        - aws\n        - csi\n      customPlugins:\n        - name: hypershift-oadp-plugin\n          image: quay.io/redhat-user-workloads/ocp-art-tenant/oadp-hypershift-oadp-plugin-main:main\n      resourceTimeout: 2h\n</code></pre> <pre><code>---\napiVersion: oadp.openshift.io/v1alpha1\nkind: DataProtectionApplication\nmetadata:\n  name: dpa-instance\n  namespace: openshift-adp\nspec:\n  backupLocations:\n    - name: default\n      velero:\n        config:\n          region: region-one\n          profile: \"default\"\n        credential:\n          key: cloud\n          name: cloud-credentials\n        default: true\n        objectStorage:\n          bucket: example-oadp\n          prefix: backup-objects\n        provider: aws\n  snapshotLocations:\n    - velero:\n        provider: aws\n        config:\n          region: region-one\n          profile: \"default\"\n        credential:\n          key: cloud\n          name: cloud-credentials\n  configuration:\n    nodeAgent:\n      enable: true\n      uploaderType: kopia\n    velero:\n      defaultPlugins:\n        - openshift\n        - aws\n        - kubevirt\n        - csi\n      customPlugins:\n        - name: hypershift-oadp-plugin\n          image: quay.io/redhat-user-workloads/ocp-art-tenant/oadp-hypershift-oadp-plugin-main:main\n      resourceTimeout: 2h\n</code></pre> <pre><code>---\napiVersion: oadp.openshift.io/v1alpha1\nkind: DataProtectionApplication\nmetadata:\n  name: dpa-instance\n  namespace: openshift-adp\nspec:\n  backupLocations:\n    - name: default\n      velero:\n        config:\n          region: minio\n          profile: \"default\"\n          s3ForcePathStyle: 'true'\n          s3Url: 'http://registry.hypershiftbm-2.lab:9002'\n        credential:\n          key: cloud\n          name: cloud-credentials\n        default: true\n        objectStorage:\n          bucket: example-oadp\n          prefix: backup-objects\n        provider: aws\n  snapshotLocations:\n    - velero:\n        provider: aws\n        config:\n          region: region-one\n          profile: \"default\"\n        credential:\n          key: cloud\n          name: cloud-credentials\n  configuration:\n    nodeAgent:\n      enable: true\n      uploaderType: kopia\n    velero:\n      defaultPlugins:\n        - openshift\n        - aws\n        - kubevirt\n        - csi\n      customPlugins:\n        - name: hypershift-oadp-plugin\n          image: quay.io/redhat-user-workloads/ocp-art-tenant/oadp-hypershift-oadp-plugin-main:main\n      resourceTimeout: 2h\n</code></pre> <p>Once you create any of these DPA objects, several pods will be instantiated in the <code>openshift-adp</code> namespace. This includes one <code>node-agent</code> per node in the Management Cluster and the <code>velero</code> deployment.</p> <p>Note</p> <p>To follow backup and restore procedures, you can monitor the logs in the velero pod.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#backup-and-upload","title":"Backup and Upload","text":"Bare MetalBare Metal - Non CSI CompatibleAWSOpenstackKubevirt <pre><code>---\napiVersion: velero.io/v1\nkind: Backup\nmetadata:\n  name: hcp-osp-backup\n  namespace: openshift-adp\n  labels:\n    velero.io/storage-location: default\nspec:\n  includedNamespaces:\n  - clusters\n  - clusters-example-hosted\n  includedResources:\n  - sa\n  - role\n  - rolebinding\n  - pod\n  - pvc\n  - pv\n  - configmap\n  - priorityclasses\n  - pdb\n  - hostedcluster\n  - nodepool\n  - secrets\n  - services\n  - deployments\n  - statefulsets\n  - hostedcontrolplane\n  - cluster\n  - openstackclusters\n  - openstackmachinetemplates\n  - openstackmachine\n  - machinedeployment\n  - openstackfloatingippools\n  - machineset\n  - machine\n  - route\n  - clusterdeployment\n  excludedResources: []\n  storageLocation: default\n  ttl: 2h30m0s\n  snapshotMoveData: true\n  datamover: \"velero\"\n  defaultVolumesToFsBackup: false\n  snapshotVolumes: true\n</code></pre> <p>We will emphasize the most important fields:</p> <ul> <li>These two fields enable the CSI VolumeSnapshots to be automatically uploaded to the remote cloud storage.</li> </ul> <pre><code>snapshotMoveData: true\ndatamover: \"velero\"\ndefaultVolumesToFsBackup: false\n</code></pre> <ul> <li>This field selects the namespaces from which objects will be backed up. They should include namespaces from both the HostedCluster (in the example <code>clusters</code>) and the HostedControlPlane (in the example <code>clusters-example-hosted</code>).</li> </ul> <pre><code>includedNamespaces:\n- clusters\n- clusters-example-hosted\n</code></pre> <p>Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.</p> <p>The backup process is considered complete when the <code>status.phase</code> is <code>Completed</code>.</p> <pre><code>apiVersion: velero.io/v1\nkind: Backup\nmetadata:\n  name: hcp-osp-backup\n  namespace: openshift-adp\n  labels:\n    velero.io/storage-location: default\nspec:\n  includedNamespaces:\n  - clusters\n  - clusters-example-hosted\n  includedResources:\n  - sa\n  - role\n  - rolebinding\n  - deployment\n  - statefulset\n  - pv\n  - pvc\n  - bmh\n  - configmap\n  - infraenv\n  - priorityclasses\n  - pdb\n  - hostedcluster\n  - nodepool\n  - secrets\n  - hostedcontrolplane\n  - cluster\n  - kubevirtcluster\n  - kubevirtmachinetemplate\n  - datavolume\n  - service\n  - route\n  excludedResources: []\n  storageLocation: default\n  preserveNodePorts: true\n  ttl: 2h30m0s\n  snapshotMoveData: true\n  datamover: \"velero\"\n  defaultVolumesToFsBackup: false\n  snapshotVolumes: true\n</code></pre>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#data-plane-workloads-backup","title":"Data Plane workloads backup","text":"<p>Note</p> <p>If the workloads in the Data Plane are not crucial for you, it's safe to skip this step.</p> <p>If you need to backup the applications running under the HostedCluster, it's advisable to follow the official documentation for backup and restore of OpenShift applications</p> <p>The steps will indeed be quite similar:</p> <ul> <li>Deploy the OADP operator from OLM.</li> <li>Create the DPA (Data Protection Application), with a manifest similar to the one provided earlier. It might be beneficial to adjust the <code>Prefix</code> or/and <code>Bucket</code> fields to keep the ControlPlane and DataPlane backups separated.</li> <li>Create the backup manifest. This step varies depending on the complexity of the workloads in the Data Plane. It's essential to thoroughly examine how to back up the PersistentVolumes, the backend used, and ensure compatibility with our storage provisioner.</li> </ul> <p>We recommend checking if your workloads contain Persistent Volumes and if our StorageClass is compatible with CSI Volume Snapshots, which is one of the simplest ways to handle this aspect.</p> <p>As a standard approach to maintain consistency in the backup layer for the Hosted Control Plane, we will utilize <code>Kopia</code> as the backend tool for data snapshots, along with <code>File System Backup</code>. However, it's possible that your workloads may benefit from a different approach that better aligns with your specific use case.</p> <p>Important</p> <p>The backup of the workloads residing in the Data Plane falls outside the scope of this documentation. Please refer to the official Openshift-ADP backup documentation for further details. Additional links and information can be found in the References section.</p> <p>Once we have completed the backup of the Data Plane layer, we can proceed with the backup of the Hosted Control Plane (HCP).</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#control-plane-backup","title":"Control Plane backup","text":"<p>Now, we will apply the backup manifest. Here is how it looks like:</p> <pre><code>---\napiVersion: velero.io/v1\nkind: Backup\nmetadata:\n  name: hcp-minio-backup-csi\n  namespace: openshift-adp\n  labels:\n    velero.io/storage-location: default\nspec:\n  includedNamespaces:\n  - clusters\n  - clusters-hosted-ipv6\n  includedResources:\n  - sa\n  - role\n  - rolebinding\n  - pod\n  - pvc\n  - pv\n  - bmh\n  - configmap\n  - infraenv\n  - priorityclasses\n  - pdb\n  - agents\n  - hostedcluster\n  - nodepool\n  - secrets\n  - services\n  - deployments\n  - statefulsets\n  - hostedcontrolplane\n  - cluster\n  - agentcluster\n  - agentmachinetemplate\n  - agentmachine\n  - machinedeployment\n  - machineset\n  - machine\n  - route\n  - clusterdeployment\n  excludedResources: []\n  storageLocation: default\n  ttl: 2h30m0s\n  snapshotMoveData: true\n  datamover: \"velero\"\n  defaultVolumesToFsBackup: false\n  snapshotVolumes: true\n</code></pre> <p>We will emphasize the most important fields:</p> <ul> <li>These fields enable the CSI VolumeSnapshots to be automatically uploaded to the remote cloud storage.</li> </ul> <pre><code>snapshotMoveData: true\ndatamover: \"velero\"\ndefaultVolumesToFsBackup: false\n</code></pre> <ul> <li>This field selects the namespaces from which objects will be backed up. They should include namespaces from both the HostedCluster (in the example <code>clusters</code>) and the HostedControlPlane (in the example <code>clusters-example-hosted</code>).</li> </ul> <pre><code>includedNamespaces:\n- clusters\n- clusters-hosted\n</code></pre> <p>Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.</p> <p>The backup process is considered complete when the <code>status.phase</code> is <code>Completed</code>.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#data-plane-workloads-backup_1","title":"Data Plane workloads backup","text":"<p>Note</p> <p>If the workloads in the Data Plane are not crucial for you, it's safe to skip this step.</p> <p>If you need to backup the applications running under the HostedCluster, it's advisable to follow the official documentation for backup and restore of OpenShift applications</p> <p>The steps will indeed be quite similar:</p> <ul> <li>Deploy the OADP operator from OLM.</li> <li>Create the DPA (Data Protection Application), with a manifest similar to the one provided earlier. It might be beneficial to adjust the <code>Prefix</code> or/and <code>Bucket</code> fields to keep the ControlPlane and DataPlane backups separated.</li> <li>Create the backup manifest. This step varies depending on the complexity of the workloads in the Data Plane. It's essential to thoroughly examine how to back up the PersistentVolumes, the backend used, and ensure compatibility with our storage provisioner.</li> </ul> <p>We recommend checking if your workloads contain Persistent Volumes and if our StorageClass is compatible with CSI Volume Snapshots, which is one of the simplest ways to handle this aspect.</p> <p>As a standard approach to maintain consistency in the backup layer for the Hosted Control Plane, we will utilize <code>Kopia</code> as the backend tool for data snapshots, along with <code>File System Backup</code>. However, it's possible that your workloads may benefit from a different approach that better aligns with your specific use case.</p> <p>Important</p> <p>The backup of the workloads residing in the Data Plane falls outside the scope of this documentation. Please refer to the official Openshift-ADP backup documentation for further details. Additional links and information can be found in the References section.</p> <p>Once we have completed the backup of the Data Plane layer, we can proceed with the backup of the Hosted Control Plane (HCP).</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#control-plane-backup_1","title":"Control Plane backup","text":"<p>Now, we will apply the backup manifest. Here is how it looks like:</p> <pre><code>---\napiVersion: velero.io/v1\nkind: Backup\nmetadata:\n  name: hcp-minio-backup-no-csi\n  namespace: openshift-adp\n  labels:\n    velero.io/storage-location: default\nspec:\n  includedNamespaces:\n  - clusters\n  - clusters-hosted-ipv6\n  includedResources:\n  - sa\n  - role\n  - rolebinding\n  - pod\n  - pvc\n  - pv\n  - bmh\n  - configmap\n  - infraenv\n  - priorityclasses\n  - pdb\n  - agents\n  - hostedcluster\n  - nodepool\n  - secrets\n  - services\n  - deployments\n  - statefulsets\n  - hostedcontrolplane\n  - cluster\n  - agentcluster\n  - agentmachinetemplate\n  - agentmachine\n  - machinedeployment\n  - machineset\n  - machine\n  - route\n  - clusterdeployment\n  excludedResources: []\n  defaultVolumesToFsBackup: true\n  storageLocation: default\n  ttl: 2h30m0s\n</code></pre> <p>We will emphasize the most important fields:</p> <ul> <li>This particular field is crucial if you utilize a combination of CSI Volume Snapshot and fs-backup. It designates fs-backup as the default method for Persistent Volume backup. If you wish to continue using CSI Volume Snapshot (within the same backup manifest), you will need to add an annotation to the desired pods, including the PVs <code>backup.velero.io/backup-volumes-excludes=&lt;pvc-name&gt;</code>. Further information can be found here.</li> </ul> <pre><code>defaultVolumesToFsBackup: true\n</code></pre> <ul> <li>This field selects the namespaces from which objects will be backed up. They should include namespaces from both the HostedCluster (in the example <code>clusters</code>) and the HostedControlPlane (in the example <code>clusters-example-hosted</code>).</li> </ul> <pre><code>includedNamespaces:\n- clusters\n- clusters-hosted\n</code></pre> <p>Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.</p> <p>The backup process is considered complete when the <code>status.phase</code> is <code>Completed</code>.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#data-plane-workloads-backup_2","title":"Data Plane workloads backup","text":"<p>Note</p> <p>If the workloads in the Data Plane are not crucial for you, it's safe to skip this step.</p> <p>If you need to backup the applications running under the HostedCluster, it's advisable to follow the official documentation for backup and restore of OpenShift applications</p> <p>The steps will indeed be quite similar:</p> <ul> <li>Deploy the OADP operator from OLM.</li> <li>Create the DPA (Data Protection Application), with a manifest similar to the one provided earlier. It might be beneficial to adjust the <code>Prefix</code> or/and <code>Bucket</code> fields to keep the ControlPlane and DataPlane backups separated.</li> <li>Create the backup manifest. This step varies depending on the complexity of the workloads in the Data Plane. It's essential to thoroughly examine how to back up the PersistentVolumes, the backend used, and ensure compatibility with our storage provisioner.</li> </ul> <p>We recommend checking if your workloads contain Persistent Volumes and if our StorageClass is compatible with CSI Volume Snapshots, which is one of the simplest ways to handle this aspect.</p> <p>As a standard approach to maintain consistency in the backup layer for the Hosted Control Plane, we will utilize <code>Kopia</code> as the backend tool for data snapshots, along with <code>File System Backup</code>. However, it's possible that your workloads may benefit from a different approach that better aligns with your specific use case.</p> <p>Important</p> <p>The backup of the workloads residing in the Data Plane falls outside the scope of this documentation. Please refer to the official Openshift-ADP backup documentation for further details. Additional links and information can be found in the References section.</p> <p>Once we have completed the backup of the Data Plane layer, we can proceed with the backup of the Hosted Control Plane (HCP).</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#control-plane-backup_2","title":"Control Plane backup","text":"<p>Now, we will apply the backup manifest. Here is how it looks like:</p> <pre><code>---\napiVersion: velero.io/v1\nkind: Backup\nmetadata:\n  name: hcp-aws-backup\n  namespace: openshift-adp\n  labels:\n    velero.io/storage-location: default\nspec:\n  includedNamespaces:\n  - clusters\n  - clusters-example-hosted\n  includedResources:\n  - sa\n  - role\n  - rolebinding\n  - pod\n  - pvc\n  - pv\n  - configmap\n  - priorityclasses\n  - pdb\n  - hostedcluster\n  - nodepool\n  - secrets\n  - services\n  - deployments\n  - statefulsets\n  - hostedcontrolplane\n  - cluster\n  - awscluster\n  - awsmachinetemplate\n  - awsmachine\n  - machinedeployment\n  - machineset\n  - machine\n  - route\n  - clusterdeployment\n  excludedResources: []\n  storageLocation: default\n  ttl: 2h30m0s\n  snapshotMoveData: true\n  datamover: \"velero\"\n  defaultVolumesToFsBackup: false\n  snapshotVolumes: true\n</code></pre> <p>We will emphasize the most important fields:</p> <ul> <li>These two fields enable the CSI VolumeSnapshots to be automatically uploaded to the remote cloud storage.</li> </ul> <pre><code>snapshotMoveData: true\ndatamover: \"velero\"\ndefaultVolumesToFsBackup: false\n</code></pre> <ul> <li>This field selects the namespaces from which objects will be backed up. They should include namespaces from both the HostedCluster (in the example <code>clusters</code>) and the HostedControlPlane (in the example <code>clusters-example-hosted</code>).</li> </ul> <pre><code>includedNamespaces:\n- clusters\n- clusters-example-hosted\n</code></pre> <p>Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.</p> <p>The backup process is considered complete when the <code>status.phase</code> is <code>Completed</code>.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#backup-of-a-kubevirt-hosted-cluster","title":"Backup of a kubeVirt Hosted Cluster","text":"<p>Important</p> <p>The restore may only be done on the same management cluster where the backup was created.</p> <p>Backup of a hosted cluster, running on a KubeVirt platform may be done on a running hosted cluster, and there is no need to pause it.</p> <p>The backup will contain the hosted control plane components, the hosted cluster ETCD, and the data stored on the hosted cluster PVCs.</p> <p>The backup will not contain the KubeVirt VMs, used as worker nodes, and they will be automatically recreated after the restore.</p> <p>We will emphasize the most important fields:</p> <ul> <li> <p>These two fields enable the CSI VolumeSnapshots to be automatically uploaded to the remote cloud storage.</p> <pre><code>snapshotMoveData: true\ndatamover: \"velero\"\n</code></pre> </li> <li> <p>We don't want to use this feature. This will allow us to safely backup the PVCs we want</p> <pre><code>defaultVolumesToFsBackup: false\n</code></pre> </li> <li> <p>This field selects the namespaces from which objects will be backed up. They should include namespaces from both   the HostedCluster (in the example <code>clusters</code>) and the HostedControlPlane (in the example <code>clusters-example-hosted</code>).</p> <pre><code>includedNamespaces:\n- clusters\n- clusters-example-hosted\n</code></pre> </li> </ul> <p>Hint</p> <p>By default, the HostedControlPlane namespace is <code>clusters-&lt;hosted cluster name&gt;</code>.</p> <ul> <li> <p>The boot image of the KubeVirt VMs, that are used as the hosted cluster nodes, are stored in huge PVCs. We don't   need these PVCs because the VMs are going to be recreated as new VMs. We want tilter these PVCs out of the backup   to gain meaningful reduce in backup time and storage size. We'll filter these PVC using this label selector:</p> <pre><code>labelSelector:\n  matchExpressions:\n  - key: 'hypershift.openshift.io/is-kubevirt-rhcos'\n    operator: 'DoesNotExist'\n</code></pre> </li> </ul> <p>Once you apply the manifest, you can monitor the backup process in two places: the backup object status and the Velero logs. Please refer to the Watching section for more information.</p> <p>The backup process is considered complete when the <code>status.phase</code> is <code>Completed</code>.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#restore","title":"Restore","text":"<p>For the restoration procedure, ensure that there are no Pods/PVCs running in the HostedControlPlane namespace to facilitate a successful restoration. If restoring the HostedCluster within the same Management cluster, delete the following objects:</p> <ul> <li>HostedCluster</li> <li>Nodepools</li> <li>PVCs</li> </ul> <p>This will remove the Pods/PVCs, allowing for a proper restoration of the environment.</p> <p>Important</p> <p>In the case of the Bare Metal provider (Agent), it's crucial to ensure that we don't delete the InfraEnv object. This object is mandatory for the new nodes that will be reprovisioned, as they need access to it in order to retrieve the Discovery ISO.</p> <p>Now let's take a look to the restoration manifest:</p> <pre><code>---\napiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: hc-clusters-example-hosted-restore\n  namespace: openshift-adp\nspec:\n  backupName: &lt;NAME OF THE REFERRED BACKUP&gt;\n  restorePVs: true\n  existingResourcePolicy: update\n  excludedResources:\n  - nodes\n  - events\n  - events.events.k8s.io\n  - backups.velero.io\n  - restores.velero.io\n  - resticrepositories.velero.io\n</code></pre> <p>Here we have a couple of important fields</p> <pre><code>restorePVs: true\nexistingResourcePolicy: update\n</code></pre> <ul> <li><code>restorePvs</code> will initiate the recovery of pods with the included persistent volumes.</li> <li>Setting <code>existingResourcePolicy</code> to <code>update</code> ensures that any existing objects are overwritten with the backup content. This may cause issues with objects containing immutable fields, which is why we delete them in a previous step. If this policy is not set, the Velero engine will skip the restoration of objects that already exist.</li> </ul> <p>You can monitor the restoration process by checking the restore status field and following the Velero logs mentioned in the Watching section.</p> <p>The restoration process is considered complete once the <code>status.phase</code> is <code>Completed</code>.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#watching-and-troubleshooting","title":"Watching and Troubleshooting","text":""},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#watching","title":"Watching","text":"<p>Here we will describe how to monitor and observe a Backup or Restore process:</p> <ul> <li>Watch the Backup process</li> </ul> <pre><code>watch \"oc get backup -n openshift-adp &lt;BACKUP_NAME&gt; -o jsonpath='{.status}' | jq\"\n</code></pre> <ul> <li>Watch the Restore process</li> </ul> <pre><code>watch \"oc get restore -n openshift-adp &lt;BACKUP_NAME&gt; -o jsonpath='{.status}' | jq\"\n</code></pre> <ul> <li>Follow the Velero logs</li> </ul> <pre><code>oc logs -n openshift-adp -ldeploy=velero -f\n</code></pre> <ul> <li>Watch all the OADP Main objects</li> </ul> <pre><code>watch \"echo BackupRepositories:;echo;oc get backuprepositories.velero.io -A;echo; echo BackupStorageLocations: ;echo; oc get backupstoragelocations.velero.io -A;echo;echo DataUploads: ;echo;oc get datauploads.velero.io -A;echo;echo DataDownloads: ;echo;oc get datadownloads.velero.io -n openshift-adp; echo;echo VolumeSnapshotLocations: ;echo;oc get volumesnapshotlocations.velero.io -A;echo;echo Backups:;echo;oc get backup -A; echo;echo Restores:;echo;oc get restore -A\"\n</code></pre>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#troubleshooting","title":"Troubleshooting","text":"<p>For troubleshooting purposes, it's crucial to identify where the process is stuck. Here are some tips:</p> <ul> <li>Review the Velero logs in the <code>openshift-adp</code> namespace.</li> <li>Utilize the <code>velero</code> command with the <code>--details</code> flag to describe the backup/restore objects.</li> <li>Check the backup/restore status directly in the respective object, which can provide valuable hints.</li> <li>Examine the Events in the affected namespaces (in the previous examples, these were <code>clusters</code> and <code>clusters-example-hosted</code>).</li> <li>Verify the status of all OADP objects using the command provided in the Watching section.</li> </ul>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#tips-and-tricks","title":"Tips and Tricks","text":""},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#use-the-velero-cli","title":"Use the velero CLI","text":"<p>To gain more insights into the backup/restore objects or to perform actions such as deleting a backup/restore object, you can utilize the <code>velero</code> CLI. This tool provides additional context in each case, and the best part is that you don't need to download anything.</p> <ul> <li>Create an alias to use the <code>velero</code> CLI from a container</li> </ul> <pre><code>alias velero='oc -n openshift-adp exec deployment/velero -c velero -it -- ./velero'\n</code></pre> <p>Now you can execute commands using the <code>velero</code> CLI. Here are some sample commands:</p> <ul> <li>Describe a restore called <code>hc-clusters-hosted-restore</code></li> </ul> <pre><code>velero restore describe hc-clusters-hosted-restore --details\n</code></pre> <ul> <li>Describe a backup called <code>hc-clusters-hosted-backup</code></li> </ul> <pre><code>velero backup describe hc-clusters-hosted-backup --details\n</code></pre>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#how-to-handle-backups-properly","title":"How to handle backups properly","text":"<p>To handle the <code>backup</code> and <code>restore</code> objects in the cloud storage effectively, the following command is quite useful. It helps identify issues with the <code>backuprepositories.velero.io</code> object if you manually modify the folder structure in the destination storage. Therefore, we recommend avoiding modifications over that storage and managing the backups/restore objects using the CLI.</p> <ul> <li>Delete a backup created and called <code>hc-clusters-hosted-backup</code></li> </ul> <pre><code>velero delete backup hc-clusters-hosted-backup\n</code></pre> <p>Important</p> <p>If you modify the folder structure of the remote storage where your backups are hosted, you may encounter issues with <code>backuprepositories.velero.io</code>. In such cases, you will need to recreate all the associated objects, including DPAs, backups, restores, etc.</p>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#hostedcluster-configuration-requirements-for-aws-provider","title":"HostedCluster Configuration Requirements for AWS provider","text":"<p>The HostedCluster must be configured as <code>PublicAndPrivate</code> or <code>Private</code> for backup/restore operations to work correctly. If the HostedCluster is configured as Public (without a hostname in the ServicePublishingStrategy for the kube-api-server), the restore operation will fail with the following consequences:</p> <ul> <li>Nodes remain in NotReady state</li> <li>NodePool scaling fails to generate new nodes</li> </ul>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#root-cause","title":"Root Cause","text":"<p>The issue occurs because:</p> <ul> <li>Nodes store the ELB (Elastic Load Balancer) address in their kubelet configuration, which is ephemeral and changes when the cluster is deleted and restored</li> <li>The SAN (Subject Alternative Name) in the certificate fails because the certificate name no longer matches the new ELB</li> <li>Original nodes cannot connect to the ControlPlane because they point to the old ELB, and even if they pointed to the new one, the certificate would be incorrect</li> </ul>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#solution","title":"Solution","text":"<p>Ensure your HostedCluster is configured with either:</p> <ul> <li><code>PublicAndPrivate</code> service publishing strategy, OR</li> <li><code>Private</code> service publishing strategy, OR</li> <li><code>Public</code> service publishing strategy with a hostname specified for the kube-api-server</li> </ul>"},{"location":"how-to/disaster-recovery/backup-and-restore-oadp/#example-configuration","title":"Example Configuration","text":"<p>Option 1: PublicAndPrivate or Private <pre><code>spec:\n  platform:\n    aws:\n      endpointAccess: PublicAndPrivate  # or Private\n</code></pre></p> <p>Option 2: Public with hostname <pre><code>spec:\n...\n...\n  platform:\n      aws:\n        endpointAccess: Public\n...\n...\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      type: LoadBalancer\n      loadBalancer:\n        hostname: api.basedomain.tld\n</code></pre></p>"},{"location":"how-to/disaster-recovery/etcd-recovery/","title":"Etcd Recovery","text":""},{"location":"how-to/disaster-recovery/etcd-recovery/#overview","title":"Overview","text":"<p>Etcd pods for hosted clusters run as part of a statefulset (etcd). The statefulset relies on persistent storage to store etcd data per member. In the case of a HighlyAvailable control plane, the size of the statefulset is 3 and each member (etcd-N) has its own PersistentVolumeClaim (etcd-data-N).</p>"},{"location":"how-to/disaster-recovery/etcd-recovery/#automatic-recovery-of-removed-members","title":"Automatic Recovery of Removed Members","text":"<p>In certain circumstances, an etcd member is removed from the cluster. This could be due to networking issues (sdn or dns of management cluster). The hypershift operator can automatically recover from this situation by enabling automatic etcd recovery (--enable-etcd-recovery), which is set to true by default.</p> <p>If this is enabled, then the HyperShift operator will attempt to recover the health of an etcd cluster if the following conditions are met:</p> <ul> <li>The hosted cluster is configured to run HighlyAvailable (<code>spec.controllerAvailabilityPolicy = HighlyAvailable</code>)</li> <li>Etcd is managed by HyperShift (<code>spec.etcd.managementType = Managed</code>)</li> <li>Only one member of the etcd cluster is failing (quorum is not lost)</li> </ul> <p>The recovery procedure consists of the following: * If a member has been removed from the etcd cluster, re-add the missing member by executing the <code>member add</code> command * The administrator should delete the etcd member's pod and pvc, after which the HyperShift operator will automatically provision a replacement etcd member Pod and PersistentVolume.</p> <p>Once this is done, the <code>reset-member</code> init container of the removed pod should be able to complete the recovery.</p> <p>To disable this default behavior, install HyperShift with <code>--enable-etcd-recovery=false</code></p>"},{"location":"how-to/disaster-recovery/etcd-recovery/#checking-cluster-health","title":"Checking cluster health","text":"<p>Execute into a running etcd pod:</p> <pre><code>$ oc rsh -n ${CONTROL_PLANE_NAMESPACE} -c etcd etcd-0\n</code></pre> <p>Setup the etcdctl environment:</p> <pre><code>export ETCDCTL_API=3\nexport ETCDCTL_CACERT=/etc/etcd/tls/etcd-ca/ca.crt\nexport ETCDCTL_CERT=/etc/etcd/tls/client/etcd-client.crt\nexport ETCDCTL_KEY=/etc/etcd/tls/client/etcd-client.key\nexport ETCDCTL_ENDPOINTS=https://etcd-client:2379\n</code></pre> <p>Print out endpoint health for each cluster member: <pre><code>etcdctl endpoint health --cluster -w table\n</code></pre></p>"},{"location":"how-to/disaster-recovery/etcd-recovery/#single-node-recovery","title":"Single Node Recovery","text":"<p>If a single etcd member of a 3-node cluster has corrupted data, it will most likely start crash looping, as in:</p> <pre><code>$ oc get pods -l app=etcd -n ${CONTROL_PLANE_NAMESPACE}\nNAME     READY   STATUS             RESTARTS     AGE\netcd-0   2/2     Running            0            64m\netcd-1   2/2     Running            0            45m\netcd-2   1/2     CrashLoopBackOff   1 (5s ago)   64m\n</code></pre> <p>To recover the etcd member, delete its persistent volume claim (data-etcd-N) as well as the pod (etcd-N):</p> <pre><code>oc delete pvc/data-etcd-2 pod/etcd-2 --wait=false\n</code></pre> <p>When the pod restarts, the member should get re-added to the etcd cluster and become healthy again:</p> <pre><code>$ oc get pods -l app=etcd -n $CONTROL_PLANE_NAMESPACE\nNAME     READY   STATUS    RESTARTS   AGE\netcd-0   2/2     Running   0          67m\netcd-1   2/2     Running   0          48m\netcd-2   2/2     Running   0          2m2s\n</code></pre>"},{"location":"how-to/disaster-recovery/etcd-recovery/#recovery-from-quorum-loss","title":"Recovery from Quorum Loss","text":"<p>If multiple members of the etcd cluster have lost data or are in a crashloop state, then etcd must be restored from a snapshot. The following procedure requires down time for the control plane as the etcd database is restored.</p> <p>NOTE: The following instructions require the <code>oc</code> and <code>jq</code> binaries.</p> <ol> <li>Setup environment variables that point to your hosted cluster:</li> </ol> <pre><code>CLUSTER_NAME=my-cluster\nCLUSTER_NAMESPACE=clusters\nCONTROL_PLANE_NAMESPACE=\"${CLUSTER_NAMESPACE}-${CLUSTER_NAME}\"\n</code></pre> <ol> <li>Pause reconciliation on the HostedCluster (setting CLUSTER_NAME and CLUSTER_NAMESPACE to values that correspond to your hosted cluster):</li> </ol> <pre><code>oc patch -n ${CLUSTER_NAMESPACE} hostedclusters/${CLUSTER_NAME} -p '{\"spec\":{\"pausedUntil\":\"true\"}}' --type=merge\n</code></pre> <ol> <li> <p>Take a snapshot of etcd data using one of the following methods:</p> <p>a. Use a previously backed up snapshot</p> <p>b. Take a snapshot from a running etcd pod (PREFERRED but requires available etcd pod):</p> <pre><code>```\n# List etcd pods\noc get -n ${CONTROL_PLANE_NAMESPACE} pods -l app=etcd\n\n# If a pod is available:\n\n# 1. take a snapshot of its database and save it locally\n# Set ETCD_POD to the name of the pod that is available\nETCD_POD=etcd-0\noc exec -n ${CONTROL_PLANE_NAMESPACE} -c etcd -t ${ETCD_POD} -- env ETCDCTL_API=3 /usr/bin/etcdctl \\\n--cacert /etc/etcd/tls/etcd-ca/ca.crt \\\n--cert /etc/etcd/tls/client/etcd-client.crt \\\n--key /etc/etcd/tls/client/etcd-client.key \\\n--endpoints=https://localhost:2379 \\\nsnapshot save /var/lib/snapshot.db\n\n# 2. Verify that the snapshot is good\noc exec -n ${CONTROL_PLANE_NAMESPACE} -c etcd -t ${ETCD_POD} -- env ETCDCTL_API=3 /usr/bin/etcdctl -w table snapshot status /var/lib/snapshot.db\n\n# 3. Make a local copy of the snapshot\noc cp -c etcd ${CONTROL_PLANE_NAMESPACE}/${ETCD_POD}:/var/lib/snapshot.db /tmp/etcd.snapshot.db\n```\n</code></pre> <p>c. Make a copy of the snapshot db from etcd persistent storage:</p> <pre><code># List etcd pods\noc get -n ${CONTROL_PLANE_NAMESPACE} pods -l app=etcd\n\n# Find a pod that is running and set its name as the value of ETCD_POD\nETCD_POD=etcd-0\n\n# Copy the snapshot db from it\noc cp -c etcd ${CONTROL_PLANE_NAMESPACE}/${ETCD_POD}:/var/lib/data/member/snap/db /tmp/etcd.snapshot.db\n</code></pre> </li> <li> <p>Scale down the etcd statefulset:</p> </li> </ol> <pre><code>oc scale -n ${CONTROL_PLANE_NAMESPACE} statefulset/etcd --replicas=0\n</code></pre> <ol> <li> <p>Delete volumes for 2nd and 3rd members: <pre><code>oc delete -n ${CONTROL_PLANE_NAMESPACE} pvc/data-etcd-1 pvc/data-etcd-2\n</code></pre></p> </li> <li> <p>Create pod to access the first etcd member's data:</p> </li> </ol> <pre><code># Save etcd image\nETCD_IMAGE=$(oc get -n ${CONTROL_PLANE_NAMESPACE} statefulset/etcd -o jsonpath='{ .spec.template.spec.containers[0].image }')\n\n# Create pod that will allow access to etcd data:\ncat &lt;&lt; EOF | oc apply -n ${CONTROL_PLANE_NAMESPACE} -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: etcd-data\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: etcd-data\n  template:\n    metadata:\n      labels:\n        app: etcd-data\n    spec:\n      containers:\n      - name: access\n        image: $ETCD_IMAGE\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib\n        command:\n        - /usr/bin/bash\n        args:\n        - -c\n        - |-\n          while true; do\n            sleep 1000\n          done\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: data-etcd-0\nEOF\n</code></pre> <ol> <li>Clear previous data and restore snapshot</li> </ol> <pre><code># Wait for the etcd-data pod to start running\noc get -n ${CONTROL_PLANE_NAMESPACE} pods -l app=etcd-data\n\n# Get the name of the etcd-data pod\nDATA_POD=$(oc get -n ${CONTROL_PLANE_NAMESPACE} pods --no-headers -l app=etcd-data -o name | cut -d/ -f2)\n\n# Copy local snapshot into the pod\noc cp /tmp/etcd.snapshot.db ${CONTROL_PLANE_NAMESPACE}/${DATA_POD}:/var/lib/restored.snap.db\n\n# Remove old data\noc exec -n ${CONTROL_PLANE_NAMESPACE} ${DATA_POD} -- rm -rf /var/lib/data\noc exec -n ${CONTROL_PLANE_NAMESPACE} ${DATA_POD} -- mkdir -p /var/lib/data\n\n# Restore snapshot\noc exec -n ${CONTROL_PLANE_NAMESPACE} ${DATA_POD} -- etcdutl snapshot restore /var/lib/restored.snap.db \\\n     --data-dir=/var/lib/data --skip-hash-check \\\n     --name etcd-0 \\\n     --initial-cluster-token=etcd-cluster \\\n     --initial-cluster etcd-0=https://etcd-0.etcd-discovery.${CONTROL_PLANE_NAMESPACE}.svc:2380,etcd-1=https://etcd-1.etcd-discovery.${CONTROL_PLANE_NAMESPACE}.svc:2380,etcd-2=https://etcd-2.etcd-discovery.${CONTROL_PLANE_NAMESPACE}.svc:2380 \\\n     --initial-advertise-peer-urls https://etcd-0.etcd-discovery.${CONTROL_PLANE_NAMESPACE}.svc:2380\n\n# Remove snapshot from etcd-0 data directory\noc exec -n ${CONTROL_PLANE_NAMESPACE} ${DATA_POD} -- rm /var/lib/restored.snap.db\n</code></pre> <ol> <li>Delete data access deployment:</li> </ol> <pre><code>oc delete -n ${CONTROL_PLANE_NAMESPACE} deployment/etcd-data\n</code></pre> <ol> <li>Scale up etcd cluster: <pre><code>oc scale -n ${CONTROL_PLANE_NAMESPACE} statefulset/etcd --replicas=3\n</code></pre></li> </ol> <p>Wait for the all etcd member pods to come up and report available: <pre><code>oc get -n ${CONTROL_PLANE_NAMESPACE} pods -l app=etcd -w\n</code></pre></p> <ol> <li>Remove hosted cluster pause:</li> </ol> <pre><code>oc patch -n ${CLUSTER_NAMESPACE} hostedclusters/${CLUSTER_NAME} -p '{\"spec\":{\"pausedUntil\":null}}' --type=merge\n</code></pre>"},{"location":"how-to/disconnected/","title":"Disconnected Deployments","text":"<p>This section of the Hypershift documentation contains pages that explain how disconnected deployments work in the HostedControlPlane area.</p>"},{"location":"how-to/disconnected/#understanding-disconnected-deployments","title":"Understanding Disconnected Deployments","text":"<p>HostedControlPlane deployments in disconnected environments function somewhat differently than in a regular OCP.</p> <p>We have two distinct environments:</p> <ul> <li>ControlPlane: Located in the Management Cluster, where the pods of the HostedControlPlane are running and managed by the ControlPlaneOperator.</li> <li>DataPlane: Located in the workers of the HostedCluster, where the workloads and some other pods run, all managed by the HostedClusterConfigOperator.</li> </ul> <p>Depending on where the pods are running, they will be affected by the <code>IDMS</code>/<code>ICSP</code> created in the Management Cluster (Control Plane) or the <code>ImageContentSources</code> set in the HostedCluster manifest's spec field. The latter will be translated into an <code>IDMS</code> object on the HostedCluster side.</p> <p>These are the relevant resources for disconnected deployments:</p> <ul> <li>Configuring the Registry TLS Certificates</li> <li>Configuring Disconnected HostedControlPlane Deployments</li> <li>Configuring Disconnected Workloads in the HostedCluster through ImageContentSources</li> <li>Disconnected Workarounds</li> <li>Known Issues</li> </ul>"},{"location":"how-to/disconnected/disconnected-workarounds/","title":"Disconnected workarounds","text":""},{"location":"how-to/disconnected/disconnected-workarounds/#make-imagecontentsourcepolicies-work-using-image-tags","title":"Make ImageContentSourcePolicies work using image tags","text":"<p>The ImageContentSourcePolicies (ICSP) have not exposed the API to handle the parameter <code>mirror-by-digest-only</code>, so in order to do that, we need to manually create a MachineConfig change to be applied in all the HostedCluster workers. This change will perform a similar action in the workers like an ICSP.</p> <p>Note</p> <p>You don't need to do this with future versions of Openshift because <code>config.openshift.io/v1</code> has an exposed API to do this, called <code>ImageTagMirrorSet</code>.</p> <p>Note</p> <p>This workaround could also be applied in the management cluster first, prior to deploying a HostedCluster from the management cluster.</p> <p>This is our MachineConfig template:</p> <ul> <li><code>mc-icsp-template.yaml</code> <pre><code>---\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  labels:\n    machineconfiguration.openshift.io/role: master\n  name: 99-worker-mirror-by-digest-registries\nspec:\n  config:\n    ignition:\n      version: 3.1.0\n    storage:\n      files:\n      - contents:\n          source: data:text/plain;charset=utf-8;base64,$B64_RAWICSP\n        filesystem: root\n        mode: 420\n        path: /etc/containers/registries.conf.d/99-mirror-by-digest-registries.conf\n</code></pre></li> </ul> <p>Basically, we create a file inside of <code>/etc/containers/registries.conf.d/</code> called <code>99-mirror-by-digest-registries.conf</code> which tells the runtime to use the custom registry instead of the external one.</p> <p>Also, here we have our final file content:</p> <ul> <li><code>icsp-raw-mc.yaml</code> <pre><code>[[registry]]\n  prefix = \"\"\n  location = \"registry.redhat.io/openshift4/ose-kube-rbac-proxy\"\n  mirror-by-digest-only = false\n\n[[registry.mirror]]\nlocation = \"registry.ocp-edge-cluster-0.qe.lab.redhat.com:5000/openshift4/ose-kube-rbac-proxy\"\n\n[[registry]]\n  prefix = \"\"\n  location = \"quay.io/acm-d\"\n  mirror-by-digest-only = false\n\n[[registry.mirror]]\nlocation = \"registry.ocp-edge-cluster-0.qe.lab.redhat.com:5000/acm-d\"\n\n[[registry]]\n  prefix = \"\"\n  location = \"quay.io/open-cluster-management/addon-manager\"\n  mirror-by-digest-only = false\n\n[[registry.mirror]]\nlocation = \"registry.ocp-edge-cluster-0.qe.lab.redhat.com:5000/open-cluster-management/addon-manager\"\n</code></pre></li> </ul> <p>Now we just need to mix the things up and apply them into our HostedCluster</p> <pre><code>export B64_RAWICSP=$(cat icsp-raw-mc.yaml | base64)\nenvsubst &lt; mc-icsp-template.yaml | oc apply -f -\n</code></pre> <p>These two commands will create the MachineConfig change in the Openshift cluster, so eventually the worker nodes will get rebooted.</p> <p>After applying this change, the worker nodes will be able to consume the mirror when only the tags are involved.</p>"},{"location":"how-to/disconnected/idms-icsp-for-management-clusters/","title":"Idms icsp for management clusters","text":""},{"location":"how-to/disconnected/idms-icsp-for-management-clusters/#configuring-disconnected-hostedcontrolplanes-deployments","title":"Configuring disconnected HostedControlPlanes deployments","text":"<p>Note</p> <p>This section is mainly focused on the Management Cluster side. If you want to see how to configure the HostedCluster's ImageContentSource, please check out this other documentation section.</p> <p>HostedControlPlanes operate in two main domains: the Control Plane, which is part of the Management Cluster, and the Data Plane, which resides in the worker nodes deployed by the customer.</p> <p>The ICSP/IDMS for the Data Plane is managed via the <code>ImageContentSources</code> API in the HostedCluster manifests and this is the only source of truth for the DataPlane. It is also possible to modify the registry files directly in the workers using the MachineConfig method, changing configurations not directly supported via the API of those ICSP/IDMS objects.</p> <p>For the Control Plane, ICSP/IDMS objects are managed in the Management Cluster. These objects are parsed by the Hypershift Operator and shared as <code>registry-overrides</code> with the ControlPlaneOperator. These entries will be injected into any of the deployments in the HostedControlPlane namespace as an argument.</p> <p>In summary, to work with disconnected registries in the HostedControlPlane, you first need to create the appropriate ICSP/IDMS in the Management Cluster. Then, to deploy disconnected workloads in the Data Plane, you need to add the desired entries into the ImageContentSources field inside the HostedCluster manifest.</p>"},{"location":"how-to/disconnected/idms-icsp-for-management-clusters/#mirroring-prerequisites-for-the-management-cluster","title":"Mirroring Prerequisites for the Management Cluster","text":"<p>Deploying HostedControlPlanes in a disconnected environment involves a mirroring process that is straightforward, provided you have the necessary prerequisites:</p> <ol> <li>Private Registry: Ensure a private registry is deployed and operational.</li> <li>Credentials: Have a credentials file to pull from a public registry and push to your private registry.</li> <li>oc-mirror Tool: Install the <code>oc-mirror</code> tool.</li> </ol>"},{"location":"how-to/disconnected/idms-icsp-for-management-clusters/#setting-up-the-private-registry","title":"Setting Up the Private Registry","text":"<p>For the private registry, robust platforms typically use large-scale solutions like Quay or Artifactory. However, for testing or development environments, you can set up a smaller registry by following these steps.</p>"},{"location":"how-to/disconnected/idms-icsp-for-management-clusters/#preparing-the-credentials-file","title":"Preparing the Credentials File","text":"<p>Your credentials file, typically located at <code>${HOME}/.docker/config.json</code>, must include login credentials for at least two registries:</p> <ul> <li>The public registry from which you pull images (e.g., <code>quay.io</code>).</li> <li>Your private registry to which you push the images.</li> </ul> <p>Refer to the official OpenShift documentation for detailed instructions on setting up the registry pull secret here.</p>"},{"location":"how-to/disconnected/idms-icsp-for-management-clusters/#installing-the-oc-mirror-tool","title":"Installing the oc-mirror Tool","text":"<p>Download the <code>oc-mirror</code> plugin for the <code>oc</code> CLI and place it in the correct directory. Follow the instructions in the OpenShift documentation here.</p>"},{"location":"how-to/disconnected/idms-icsp-for-management-clusters/#preparing-the-imagesetconfiguration-file","title":"Preparing the ImageSetConfiguration file","text":"<p>With the prerequisites in place, you need to prepare the <code>ImageSetConfiguration</code> file. This file specifies the OpenShift Container Platform (OCP) releases, Operator Lifecycle Manager (OLM) catalogs, and additional images you wish to mirror.</p> <p>Here is a sample <code>ImageSetConfiguration</code>:</p> <pre><code>apiVersion: mirror.openshift.io/v1alpha2\nkind: ImageSetConfiguration\nstorageConfig:\n  registry:\n    imageURL: registry.sample.lab:5000/openshift/release/metadata:latest\nmirror:\n  platform:\n    channels:\n    - name: stable-4.14\n      minVersion: 4.14.26\n      maxVersion: 4.14.28\n      type: ocp\n    - name: stable-4.15\n      minVersion: 4.15.13\n      maxVersion: 4.15.17\n      type: ocp\n    - name: candidate-4.16\n      minVersion: 4.16.0-rc.2\n      maxVersion: 4.16.0-rc.4\n      type: ocp\n  additionalImages:\n  - name: quay.io/karmab/origin-keepalived-ipfailover:latest\n  - name: registry.redhat.io/openshift4/ose-kube-rbac-proxy:v4.10\n  operators:\n  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.15\n    packages:\n    - name: lvms-operator\n    - name: local-storage-operator\n    - name: odf-csi-addons-operator\n    - name: odf-operator\n    - name: mcg-operator\n    - name: ocs-operator\n    - name: metallb-operator\n</code></pre> <p>For a detailed explanation of each field in the <code>ImageSetConfiguration</code>, consult the official documentation.</p>"},{"location":"how-to/disconnected/idms-icsp-for-management-clusters/#running-the-mirroring-process","title":"Running the Mirroring Process","text":"<p>To start the mirroring process, execute the following command:</p> <pre><code>oc-mirror --config imagesetconfig.yaml docker://${PRIVATE_REGISTRY_ADDRESS}\n</code></pre> <p>Note</p> <p>If your private registry uses a self-signed CA or a CA not recognized by the system running the <code>oc-mirror</code> command, you can use the <code>--source-skip-tls</code> flag to bypass certificate verification.</p> <p>After several minutes, the process will complete, generating output files in the current directory. The key files are <code>ImageContentSourcePolicies</code> and <code>CatalogSource</code>, which must be applied to the management cluster to enable a disconnected deployment of the HostedControlPlanes.</p> <p>Important</p> <p>If you have a <code>Registry-root-url/Namespace</code> or <code>Registry-root-url</code> (E.G: ) IDMS/ICSP entry into the Management cluster for the OCP Metadata and Release images, the Hypershift operator cannot assume where the destination will be, so you need to explicitly create a IDMS/ICSP object with both entries. More info here</p> <p>You must run the <code>oc-mirror</code> command twice. The first run generates a complete <code>ImageContentSourcePolicy</code> file, while the second run provides the differences between the two runs. Always maintain a backup of these files to merge them into a comprehensive <code>ImageContentSourcePolicy</code> file if needed. This backup ensures you have a complete policy file.</p>"},{"location":"how-to/disconnected/idms-icsp-for-management-clusters/#debugging-idmsicsp-propagation","title":"Debugging IDMS/ICSP Propagation","text":"<p>You can check if the entries created in the ICSP/IDMS are being propagated properly checking the deployments in the HostedControlPlane. For instance, if you check the ignition-server deployment you can see the command the pod will execute, you need to check if the <code>--registry-overrides</code> flag is followed by a series of tuples separated by comma:</p> <pre><code>...\n - --registry-overrides\n - quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:922dd705c2e05c88dc328b7ad4651d5af053851ea6a47dc52840d1abdde3926e=registry.sample.net/quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:922dd705c2e05c88dc328b7ad4651d5af053851ea6a47dc52840d1abdde3926e,quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f4196370cd6348094a11d9384bae3c88a05af2fdd0ed32e919bea26943a939c2=registry.sample.net/quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f4196370cd6348094a11d9384bae3c88a05af2fdd0ed32e919bea26943a939c2\n...\n</code></pre> <p>If that so, means the ICSP/IDMS are being propagated properly, otherwise there is some kind of problem. Please if that's the case, check the known-issues section for more info.</p>"},{"location":"how-to/disconnected/idms-icsp-for-management-clusters/#technical-implementation-details","title":"Technical Implementation Details","text":""},{"location":"how-to/disconnected/idms-icsp-for-management-clusters/#general","title":"General","text":"<p>In <code>hypershift-operator/main.go</code>, the HO will look to see if its management cluster has either ICSP or IDMS capabilities. If so, it will retrieve the image registry information from the appropriate ICSP or IDMS instances and store them in a variable called <code>imageRegistryOverrides</code>. This variable is then provided to a custom release image provider called <code>ProviderWithOpenShiftImageRegistryOverrides</code>.</p>"},{"location":"how-to/disconnected/idms-icsp-for-management-clusters/#provider-with-openshift-image-registry-overrides","title":"Provider With OpenShift Image Registry Overrides","text":"<p>The <code>Lookup</code> function for <code>ProviderWithOpenShiftImageRegistryOverrides</code> will use the source and mirror information in <code>imageRegistryOverrides</code> when attempting to look up release images. The <code>ProviderWithOpenShiftImageRegistryOverrides</code> is also provided to the <code>HostedClusterReconciler</code> as its <code>ReleaseProvider</code>. When the <code>HostedClusterReconciler</code> is reconciling, it will pass any image registry overrides to the ignition server reconciler and the CPO deployment specification.</p>"},{"location":"how-to/disconnected/idms-icsp-for-management-clusters/#ignition-server","title":"Ignition Server","text":"<p>The ignition server reconciler forwards this information on to the <code>ignition-server</code> container as an environment variable called <code>OPENSHIFT_IMG_OVERRIDES</code>. <code>hypershift/ignition-server/cmd/start.go</code> retrieves this information for the <code>ReleaseProvider</code> for the <code>TokenSecretReconciler</code>. An important caveat for the ignition server, it cannot follow the ImageContentSourcePolicy or ImageDigestMirrorSet rules because there is not a runtime running inside the pod to do the transformations. So, it's necessary to do the image URL live translation to the custom registry address.</p>"},{"location":"how-to/disconnected/idms-icsp-for-management-clusters/#control-plane-operator","title":"Control Plane Operator","text":"<p>The <code>HostedClusterReconciler</code> passes on the image registry override information as an environment variable in the CPO called <code>OPENSHIFT_IMG_OVERRIDES</code>. The CPO will check for the existence of this environment variable when it runs. If the variable exists, it's used to build the HostedControlPlaneReconciler's <code>releaseProvider</code>.</p>"},{"location":"how-to/disconnected/idms-icsp-for-management-clusters/#olm-catalogs","title":"OLM Catalogs","text":"<p>The imageStream used for the OLM catalogs, when using the <code>management</code> (default) OLMCatalogPlacement mode, is not automatically amended with override information detected from ImageContentSourcePolicy (ICSP) on the management cluster. This because there is not an easy way to validate them in advance for imagestreams. In case the OLM catalogs got properly mirrored to an internal registry (using the original name and tag), the guest cluster owner can use the <code>hypershift.openshift.io/olm-catalogs-is-registry-overrides</code> annotation on the HostedCluster CR. The format is: <code>\"sr1=dr1,sr2=dr2\"</code> having the source registry string as a key and the destination registry string as value. OLM catalog image addresses, before being applied to the imagestream, are scanned for the source registry string and if found the string is replaced with the destination registry one. The cluster admin will also be able to bypass the whole OLM catalogs imagestream mechanism using 4 annotations (<code>hypershift.openshift.io/certified-operators-catalog-image</code>, <code>hypershift.openshift.io/community-operators-catalog-image</code>, <code>hypershift.openshift.io/redhat-marketplace-catalog-image</code>, <code>hypershift.openshift.io/redhat-operators-catalog-image</code>) on the HostedCluster CR to directly specify the address (only by digest) of the 4 images to be used for OLM operator catalogs. In this case the imageStream is not going to be created, and it will be up to the guest cluster owner updating the value of the annotations when the internal mirror will get refreshed to pull in operator updates. Please notice that if this override mechanism is required, all the 4 values for the 4 default catalog sources are needed.</p>"},{"location":"how-to/disconnected/idms-icsp-for-management-clusters/#references","title":"References","text":"<ul> <li>How to configure ICSP/IDMS for the Management clusters</li> <li>Hypershift ImageContentSource details</li> <li>Configuring Machines in HyperShift</li> </ul>"},{"location":"how-to/disconnected/image-content-sources/","title":"Configuring disconnected workloads in the HostedCluster through ImageContentSources","text":""},{"location":"how-to/disconnected/image-content-sources/#what-is-imagecontentsources-and-why-do-we-need-it","title":"What is ImageContentSources and why do we need it?","text":"<p>Users can set up alternative image registry information for their guest clusters to use through a field called <code>ImageContentSources</code> in a hosted control plane's specification. Alternatively, this field can also be set through the HyperShift CLI by specifying a filepath to a file containing the source and mirrors for the alternative registry information, <code>--image-content-sources=/path/to/file/with/sources_and_mirrors.yml</code>.</p> <p>Here is an example of the expected format for this field, <code>ImageContentSources</code>: <pre><code>- mirrors:\n    - brew.registry.redhat.io\n  source: registry.redhat.io\n- mirrors:\n    - brew.registry.redhat.io\n  source: registry.xyz.redhat.io\n- mirrors:\n    - brew.registry.redhat.io\n  source: registry-proxy.engineering.redhat.com\n</code></pre></p> <p>Note</p> <p>This is also the expected format for the file if you choose to use the HyperShift CLI flag.</p>"},{"location":"how-to/disconnected/image-content-sources/#how-imagecontentsources-are-used-in-the-nodes-within-a-nodepool","title":"How ImageContentSources are used in the nodes within a NodePool","text":"<p><code>ImageContentSources</code> are reconciled, through the HostedClusterConfigOperator (HCCO), to either an ImageContentSourcePolicy (ICSP) custom resources (CR) or an ImageDigestMirrorSet (IDMS) CR. The CR is then included in the configuration for the Nodes in a NodePool through the NodePool controller's functions, <code>reconcile &gt; getConfig &gt; defaultAndValidateConfigManifest</code>.</p> <p>Important</p> <p>ICSPs will be deprecated in future OpenShift releases, likely starting with v4.17. IDMSs are the replacement CR for ICSPs but are only available in OpenShift starting in v4.13. The HCCO will automatically delete any ICSPs previously used in the node configuration setup through the Control Plane Operator (CPO) starting in OpenShift v4.13.</p>"},{"location":"how-to/disconnected/known-issues/","title":"Known Issues","text":""},{"location":"how-to/disconnected/known-issues/#olm-default-catalog-sources-in-imagepullbackoff-state","title":"OLM default catalog sources in ImagePullBackOff state","text":"<p>When you work in a disconnected environment the OLM catalog sources will be still pointing to their original source, so all of these container images will keep it in ImagePullBackOff state even if the OLMCatalogPlacement is set to <code>Management</code> or <code>Guest</code>. From this point you have some options ahead:</p> <ol> <li>Disable those OLM default catalog sources and using the oc-mirror binary, mirror the desired images into your private registry, creating a new Custom Catalog Source.</li> <li>Mirror all the Container Images from all the catalog sources and apply an ImageContentSourcePolicy to request those images from the private registry.</li> <li>Mirror all (or part of relying on the operator pruning option, see the image pruning section in https://docs.openshift.com/container-platform/4.14/installing/disconnected_install/installing-mirroring-disconnected.html#oc-mirror-updating-registry-about_installing-mirroring-disconnected ) the Container Images from all the catalog sources to the private registry and annotate the hostedCluster object with hypershift.openshift.io/olm-catalogs-is-registry-overrides annotation to get the ImageStream used for operator catalogs on your hosted-cluster pointing to your internal registry.</li> </ol> <p>The most practical one is the first choice. To proceed with this option, you will need to follow these instructions. The process will make sure all the images get mirrored and also the ICSP will be generated properly.</p> <p>Additionally when you're provisioning the HostedCluster you will need to add a flag to indicate that the OLMCatalogPlacement is set to <code>Guest</code> because if that's not set, you will not be able to disable them.</p>"},{"location":"how-to/disconnected/known-issues/#hypershift-operator-is-failing-to-reconcile-in-disconnected-environments","title":"Hypershift operator is failing to reconcile in Disconnected environments","text":"<p>If you are operating in a disconnected environment and have deployed the Hypershift operator, you may encounter an issue with the UWM telemetry writer. Essentially, it exposes Openshift deployment data in your RedHat account, but this functionality does not operate in a disconnected environments.</p> <p>Symptoms:</p> <ul> <li>The Hypershift operator appears to be running correctly in the <code>hypershift</code> namespace but even if you creates the Hosted Cluster nothing happens.</li> <li>There will be a couple of log entries in the Hypershift operator:</li> </ul> <pre><code>{\"level\":\"error\",\"ts\":\"2023-12-20T15:23:01Z\",\"msg\":\"Reconciler error\",\"controller\":\"deployment\",\"controllerGroup\":\"apps\",\"controllerKind\":\"Deployment\",\"Deployment\":{\"name\":\"operator\",\"namespace\":\"hypershift\"},\"namespace\":\"hypershift\",\"name\":\"operator\",\"reconcileID\":\"451fde3c-eb1b-4cf0-98cb-ad0f8c6a6288\",\"error\":\"cannot get telemeter client secret: Secret \\\"telemeter-client\\\" not found\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\t/hypershift/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:329\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\t/hypershift/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:266\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\t/hypershift/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:227\"}\n\n{\"level\":\"debug\",\"ts\":\"2023-12-20T15:23:01Z\",\"logger\":\"events\",\"msg\":\"Failed to ensure UWM telemetry remote write: cannot get telemeter client secret: Secret \\\"telemeter-client\\\" not found\",\"type\":\"Warning\",\"object\":{\"kind\":\"Deployment\",\"namespace\":\"hypershift\",\"name\":\"operator\",\"uid\":\"c6628a3c-a597-4e32-875a-f5704da2bdbb\",\"apiVersion\":\"apps/v1\",\"resourceVersion\":\"4091099\"},\"reason\":\"ReconcileError\"}\n</code></pre> <p>Solution:</p> <p>To resolve this issue, the solution will depend on how you deployed Hypershift:</p> <ul> <li>The HO was deployed using ACM/MCE: In this case you will need to create a ConfigMap in the <code>local-cluster</code> namespace (the namespace and ConfigMap name cannot be changed) called <code>hypershift-operator-install-flags</code> with this content:</li> </ul> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: hypershift-operator-install-flags\n  namespace: local-cluster\ndata:\n  installFlagsToRemove: --enable-uwm-telemetry-remote-write\n</code></pre> <ul> <li>The HO was deployed using the Hypershift binary: In this case you will just need to remove the flag <code>--enable-uwm-telemetry-remote-write</code> from the hypershift deployment command.</li> </ul>"},{"location":"how-to/disconnected/known-issues/#hcp-cli-failing-to-create-an-hosted-cluster-with-failed-to-extract-release-metadata-failed-to-get-repo-setup-failed-to-create-repository-client-for","title":"HCP CLI failing to create an hosted cluster with `failed to extract release metadata: failed to get repo setup: failed to create repository client for  <p>See: CNV-38194 Currently the HCP client is directly (client side) trying to access the OCP release image, this implies some additional requirements on client side:</p> <ol> <li>the HCP CLI client should be able to directly reach the internal mirror registry, and this could require additional configuration (a proxied access to the api server of the management server is not enough)</li> <li>the HCP CLI client should explicitly consume the pull secret of the internal mirror registry</li> <li>the HCP CLI client should explicitly consume the internal CA used to sign the TLS cert of the internal mirror registry</li> </ol> <p>Workaround: Explicitly set <code>--network-type OVNKubernetes</code> (or other valid SDN provider if needed) when running <code>hcp create cluster</code> to skip the auto-detection of the SDN network type.</p>","text":""},{"location":"how-to/disconnected/known-issues/#hcp-imageregistryoverrides-information-is-extracted-only-once-on-hypershift-operator-initialization-and-never-refreshed","title":"HCP: imageRegistryOverrides information is extracted only once on HyperShift operator initialization and never refreshed <p>See: OCPBUGS-29110 According to https://hypershift-docs.netlify.app/how-to/disconnected/automatically-initialize-registry-overrides/ , the HyperShift Operator (HO) will automatically initialize the control plane operator (CPO) with any image registry override information from any ImageContentSourcePolicy (ICSP) or any ImageDigestMirrorSet (IDMS) instances from an OpenShift management cluster. But due to a bug, the HyperShift Operator (HO) reads the image registry override information only during its startup and never refreshes them at runtime so ICSP and IDMS added after the initialization of the HyperShift Operator are going to get ignored.</p> <p>Workaround: Each time you add or amend and ICSP or an IDMS on your management cluster, you have to explicitly kill the HyperShift Operator pods: <pre><code>$ oc delete pods -n hypershift -l name=operator\n</code></pre></p>","text":""},{"location":"how-to/disconnected/known-issues/#hcp-hypershift-operator-on-disconnected-clusters-ignores-imagecontentsourcepolicies-when-a-imagedigestmirrorset-exist-on-the-management-cluster","title":"HCP: hypershift-operator on disconnected clusters ignores ImageContentSourcePolicies when a ImageDigestMirrorSet exist on the management cluster <p>See: OCPBUGS-29466</p> <p>ICSP (deprecated) and IDMS can coexists on the management cluster but due to a bug if at least one IDMS object exists on the management cluster, the HyperShift Operator will completely ignore all the ICSP.</p> <p>Workaround: If you have at least an IDMS object, explicitly convert all the ICSPs to IDMSs: <pre><code>$ mkdir -p /tmp/idms\n$ mkdir -p /tmp/icsp\n$ for i in $(oc get imageContentSourcePolicy -o name); do oc get ${i} -o yaml &gt; /tmp/icsp/$(basename ${i}).yaml ; done\n$ for f in /tmp/icsp/*; do oc adm migrate icsp ${f} --dest-dir /tmp/idms ; done\n$oc apply -f /tmp/idms || true\n</code></pre></p>","text":""},{"location":"how-to/disconnected/known-issues/#hcp-hypershift-operator-on-disconnected-clusters-ignores-registryoverrides-inspecting-the-control-plane-operator-image","title":"HCP: hypershift-operator on disconnected clusters ignores RegistryOverrides inspecting the control-plane-operator-image <p>See: OCPBUGS-29494</p> <p>Symptoms: Creating an hosted cluster with: hcp create cluster kubevirt --image-content-sources /home/mgmt_iscp.yaml  --additional-trust-bundle /etc/pki/ca-trust/source/anchors/registry.2.crt --name simone3 --node-pool-replicas 2 --memory 16Gi --cores 4 --root-volume-size 64 --namespace local-cluster --release-image virthost.ostest.test.metalkube.org:5000/localimages/local-release-image@sha256:66c6a46013cda0ad4e2291be3da432fdd03b4a47bf13067e0c7b91fb79eb4539 --pull-secret /tmp/.dockerconfigjson --generate-ssh</p> <p>on the hostedCluster object we see: <pre><code>status:\nconditions:\n- lastTransitionTime: \"2024-02-14T22:01:30Z\"\n  message: 'failed to look up image metadata for registry.ci.openshift.org/ocp/4.14-2024-02-14-135111@sha256:84c74cc05250d0e51fe115274cc67ffcf0a4ac86c831b7fea97e484e646072a6:\n  failed to obtain root manifest for registry.ci.openshift.org/ocp/4.14-2024-02-14-135111@sha256:84c74cc05250d0e51fe115274cc67ffcf0a4ac86c831b7fea97e484e646072a6:\n  unauthorized: authentication required'\n  observedGeneration: 3\n  reason: ReconciliationError\n  status: \"False\"\n  type: ReconciliationSucceeded\n</code></pre></p> <p>and in the logs of the hypershift operator: <pre><code>{\"level\":\"info\",\"ts\":\"2024-02-14T22:18:11Z\",\"msg\":\"registry override coincidence not found\",\"controller\":\"hostedcluster\",\"controllerGroup\":\"hypershift.openshift.io\",\"controllerKind\":\"HostedCluster\",\"HostedCluster\":{\"name\":\"simone3\",\"namespace\":\"local-cluster\"},\"namespace\":\"local-cluster\",\"name\":\"simone3\",\"reconcileID\":\"6d6a2479-3d54-42e3-9204-8d0ab1013745\",\"image\":\"4.14-2024-02-14-135111\"}\n{\"level\":\"error\",\"ts\":\"2024-02-14T22:18:12Z\",\"msg\":\"Reconciler error\",\"controller\":\"hostedcluster\",\"controllerGroup\":\"hypershift.openshift.io\",\"controllerKind\":\"HostedCluster\",\"HostedCluster\":{\"name\":\"simone3\",\"namespace\":\"local-cluster\"},\"namespace\":\"local-cluster\",\"name\":\"simone3\",\"reconcileID\":\"6d6a2479-3d54-42e3-9204-8d0ab1013745\",\"error\":\"failed to look up image metadata for registry.ci.openshift.org/ocp/4.14-2024-02-14-135111@sha256:84c74cc05250d0e51fe115274cc67ffcf0a4ac86c831b7fea97e484e646072a6: failed to obtain root manifest for registry.ci.openshift.org/ocp/4.14-2024-02-14-135111@sha256:84c74cc05250d0e51fe115274cc67ffcf0a4ac86c831b7fea97e484e646072a6: unauthorized: authentication required\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\t/remote-source/app/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:326\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\t/remote-source/app/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:273\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\t/remote-source/app/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:234\"}\n</code></pre></p> <p>Workaround: Explicitly use --annotations=hypershift.openshift.io/control-plane-operator-image= when creatng the hosted cluster. <pre><code># this will sync with the release image used on the management cluster, replace if needed\n$ PAYLOADIMAGE=$(oc get clusterversion version -ojsonpath='{.status.desired.image}')\n$ HO_OPERATOR_IMAGE=\"${PAYLOADIMAGE//@sha256:[^ ]*/@\\$(oc adm release info -a /tmp/.dockerconfigjson \"$PAYLOADIMAGE\" | grep hypershift | awk '{print $2}')}\"\n$ hcp create cluster ... --annotations=hypershift.openshift.io/control-plane-operator-image=${HO_OPERATOR_IMAGE} ...\n</code></pre>","text":""},{"location":"how-to/disconnected/known-issues/#kubevirt-specific-oc-mirror-does-not-mirror-the-rhcos-image-for-hypershift-kubevirt-provider-from-the-ocp-release-payload","title":"(KubeVirt specific) oc-mirror does not mirror the RHCOS image for HyperShift KubeVirt provider from the OCP release payload <p>See: OCPBUGS-29408</p> <p>Workaround: Explicitly mirror the RHCOS image for HyperShift KubeVirt provider. This snippet assumes that <code>yq-v4</code> tool is already available on the bastion host. <pre><code>mirror_registry=$(oc get imagecontentsourcepolicy -o json | jq -r '.items[].spec.repositoryDigestMirrors[0].mirrors[0]')\nmirror_registry=${mirror_registry%%/*}\nLOCALIMAGES=localimages\n# this will sync with the release image used on the management cluster, replace if needed\nPAYLOADIMAGE=$(oc get clusterversion version -ojsonpath='{.status.desired.image}')\noc image extract --file /release-manifests/0000_50_installer_coreos-bootimages.yaml ${PAYLOADIMAGE} --confirm\nRHCOS_IMAGE=$(cat 0000_50_installer_coreos-bootimages.yaml | yq -r .data.stream | jq -r '.architectures.x86_64.images.kubevirt.\"digest-ref\"')\nRHCOS_IMAGE_NO_DIGEST=${RHCOS_IMAGE%@sha256*}\nRHCOS_IMAGE_NAME=${RHCOS_IMAGE_NO_DIGEST##*/}\nRHCOS_IMAGE_REPO=${RHCOS_IMAGE_NO_DIGEST%/*}\noc image mirror ${RHCOS_IMAGE} ${mirror_registry}/${LOCALIMAGES}/${RHCOS_IMAGE_NAME}\n</code></pre></p>","text":""},{"location":"how-to/disconnected/known-issues/#hcp-recycler-pods-are-not-starting-on-hostedcontrolplane-in-disconnected-environments-imagepullbackoff-on-quayioopenshiftorigin-toolslatest","title":"HCP: recycler pods are not starting on hostedcontrolplane in disconnected environments ( ImagePullBackOff on quay.io/openshift/origin-tools:latest ) <p>See: OCPBUGS-31398</p> <p>Symptoms: Recycler pods are not starting on hosted control plane with ImagePullBackOff error on <code>quay.io/openshift/origin-tools:latest</code>. The recycler pod template in the recycler-config config map on the hostedcontrolplane namespace refers to <code>quay.io/openshift/origin-tools:latest</code>: <pre><code>$ oc get cm -n clusters-guest recycler-config -o json | jq -r '.data[\"recycler-pod.yaml\"]' | grep \"image\"\nimage: quay.io/openshift/origin-tools:latest\n</code></pre></p> <p>Workaround: Not available. The recycler-config configmap is continuously reconciled by the control plane operator pointing it back to <code>quay.io/openshift/origin-tools:latest</code> which will never be available for a disconnected cluster.</p>","text":""},{"location":"how-to/disconnected/known-issues/#hcp-imagesstreams-on-hosted-clusters-pointing-to-image-on-private-registries-are-failing-due-to-tls-verification-although-the-registry-is-correctly-trusted","title":"HCP: imagesStreams on hosted-clusters pointing to image on private registries are failing due to tls verification, although the registry is correctly trusted <p>See: OCPBUGS-31446 Probably a side effect of https://issues.redhat.com/browse/RFE-3093 - imagestream to trust CA added during the installation</p> <p>Symptoms: On the imageStream conditions on the the hosted cluster we see something like: <pre><code>tags:\n- conditions:\n    - generation: 2\n      lastTransitionTime: \"2024-03-27T12:43:56Z\"\n      message: 'Internal error occurred: virthost.ostest.test.metalkube.org:5000/localimages/local-test-image:e2e-7-registry-k8s-io-e2e-test-images-busybox-1-29-4-4zE9mRvED4RQoUxQ:\n      Get \"https://virthost.ostest.test.metalkube.org:5000/v2/\": tls: failed to\n      verify certificate: x509: certificate signed by unknown authority'\n</code></pre> although the same image can be directly consumed by a pod on the same cluster</p> <p>Workaround: Patch all the image streams on the hostedcluster pointing to the internal registry setting insecure mode. <pre><code># KUBECONFIG points to the hosted cluster\nIMAGESTREAMS=$(oc get imagestreams -n openshift -o=name)\nfor isName in ${IMAGESTREAMS}; do\n  echo \"#### Patching ${isName} using insecure registry\"\n  oc patch -n openshift ${isName} --type json -p '[{\"op\": \"replace\", \"path\": \"/spec/tags/0/importPolicy/insecure\", \"value\": true}]'\ndone\n</code></pre></p>","text":""},{"location":"how-to/disconnected/known-issues/#idmsicsp-with-only-root-registry-are-not-propagated-to-the-registry-override-flag-under-ignition-server","title":"IDMS/ICSP with only root registry are not propagated to the registry-override flag under ignition server <p>See: OCPBUGS-33951</p> <p>Symptoms: Even having the ICSP/IDMS well set in the Management cluster and working fine in that side, the HostedControlPlane deployment is not capable of extract the metadata release from the OCP payload images. Something like this log entry will show up in the ControlPlaneOperator or HypershiftOperator pod.</p> <pre><code>failed to lookup release image: failed to extract release metadata: failed to get repo setup: failed to create repository client for https://registry.sample.net: Get \"https://registry.sample.net/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority\n</code></pre> <p>Root Cause: Once the ICSP/IDMS are created in the Management cluster and they are only using in the \"sources\" side a root registry instead of pointing a registry namespace, the image-overrides are not filled with the explicit destination OCP Metadata and Release images, so the Hypeshift operator cannot infer the exact location of the images in the private registry.</p> <p>This is a sample:</p> <pre><code>apiVersion: config.openshift.io/v1\nkind: ImageDigestMirrorSet\nmetadata:\n  name: image-policy\nspec:\n  imageDigestMirrors:\n  - mirrors:\n    - registry.sample.net/redhat.io\n    source: registry.redhat.io\n  - mirrors:\n    - registry.sample.net/connect.redhat.com\n    source: registry.connect.redhat.com\n  - mirrors:\n    - registry.sample.net/gcr.io\n    source: gcr.io\n  - mirrors:\n    - registry.sample.net/docker.io\n    source: docker.io\n</code></pre> <p>Solution: In order to solve the issue and perform a successful HostedControlPlane disconnected deployment you need at least to create the OCP namespaced reference in a IDMS/ICSP. A sample will look like this:</p> <pre><code>apiVersion: config.openshift.io/v1\nkind: ImageDigestMirrorSet\nmetadata:\n  name: ocp-release\nspec:\n  imageDigestMirrors:\n  - mirrors:\n    - registry.sample.net/quay.io/openshift-release-dev/ocp-v4.0-art-dev\n    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n  - mirrors:\n    - registry.sample.net/quay.io/openshift-release-dev/ocp-release\n    source: quay.io/openshift-release-dev/ocp-release\n</code></pre>","text":""},{"location":"how-to/disconnected/tls-certificates/","title":"How to Configure TLS Certificates for Disconnected Deployments","text":"<p>In the HostedControlPlane area, it is necessary to configure the registry CA certificates in several areas to ensure proper functioning of disconnected deployments.</p>"},{"location":"how-to/disconnected/tls-certificates/#adding-the-registry-ca-to-the-management-cluster","title":"Adding the Registry CA to the Management Cluster","text":"<p>There are numerous methods to accomplish this within the OpenShift environment. However, we have chosen a less intrusive approach.</p> <ol> <li>First, create a ConfigMap with a name of your choosing. In this example, we will use the name <code>registry-config</code>. The content should resemble the following:</li> </ol> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: registry-config\n  namespace: openshift-config\ndata:\n  registry1.hypershiftbm.lab..5000: |\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\n  registry2.hypershiftbm.lab..5000: |\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\n  registry3.hypershiftbm.lab..5000: |\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\n</code></pre> <p>Note</p> <p>The <code>data</code> field should contain the registry name, while the value should include the registry certificate. As shown, the \":\" character is replaced by \"..\"; ensure this correction is made.</p> <p>Ensure your data in the ConfigMap is defined using only <code>\"|\"</code> instead of other methods like <code>\"|-\"</code>. Using other methods can cause issues when the Pod reads the certificates loaded in the system.</p> <ol> <li>Now, we need to patch the cluster-wide object <code>image.config.openshift.io</code> to include this:</li> </ol> <pre><code>spec:\n  additionalTrustedCA:\n    - name: registry-config\n</code></pre> <p>This modification will result in two significant outcomes:</p> <ul> <li>Granting master nodes the capability to retrieve images from the private registry.</li> <li>Allowing the Hypershift Operator to extract the OpenShift payload for the HostedCluster deployments.</li> </ul> <p>Note</p> <p>The modification may take several minutes to be successfully executed.</p>"},{"location":"how-to/disconnected/tls-certificates/#adding-the-registry-ca-to-the-hostedcluster-worker-nodes","title":"Adding the Registry CA to the HostedCluster Worker Nodes","text":"<p>This configuration allows the HostedCluster to inject the CA into the DataPlane workers, which is necessary for pulling images from the private registry.</p> <p>This is defined in the field <code>hc.spec.additionalTrustBundle</code> as follows:</p> <pre><code>spec:\n  additionalTrustBundle:\n    name: user-ca-bundle\n</code></pre> <p>This <code>user-ca-bundle</code> entry is a ConfigMap created by the user in the HostedCluster namespace (the same namespace where the HostedCluster object is created).</p> <p>The ConfigMap should look like this:</p> <pre><code>apiVersion: v1\ndata:\n  ca-bundle.crt: |\n    // Registry1 CA\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\n\n    // Registry2 CA\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\n\n    // Registry3 CA\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\n\nkind: ConfigMap\nmetadata:\n  name: user-ca-bundle\n  namespace: &lt;HOSTEDCLUSTER NAMESPACE&gt;\n</code></pre> <p>This modification will result in two significant outcomes:</p> <ul> <li>Granting HostedCluster workers the capability to retrieve images from the private registry.</li> </ul>"},{"location":"how-to/install/","title":"HyperShift Operator Installation","text":"<p>This document describes different installation flags or methods for HyperShift Operator (HO).</p>"},{"location":"how-to/install/#limiting-the-capi-crds-installed","title":"Limiting the CAPI CRDs installed","text":"<p>The HO uses the Cluster API (CAPI) to manage the nodes in the NodePool. By default, the HO installation will install all  CAPI related CRDs. If you want to limit the CRDs installed, you can set the <code>--limit-crd-install</code> flag to a  comma-separated list of CRDs to install. The valid values for this flag are: AWS, Azure, IBMCloud, KubeVirt, Agent,  OpenStack.</p> <p>For example, to only install the AWS and Azure related CAPI CRDs, you would use  the following flag in your HO install command:</p> <pre><code>--limit-crd-install=AWS,Azure\n</code></pre> <p>Important</p> <p>Limiting the CAPI CRDs installed means the HO will only be able to manage HostedClusters of the same platform. For example, in the above example, if you limit the CRDs to AWS and Azure, the HO will only be able to manage  AWS and Azure HostedClusters.</p>"},{"location":"how-to/kubevirt/configuring-network/","title":"Configuring Network","text":"<p>By default, the NodePools for the KubeVirt platform generate VMs which are attached to the default pod network. It is possible to attach additional networks to the NodePools VMs as well though.</p>"},{"location":"how-to/kubevirt/configuring-network/#attaching-additional-networks-to-kubevirt-vms","title":"Attaching Additional Networks to KubeVirt VMs","text":"<p>Attaching additional networks to a KubeVirt NodePool is accomplished through the usage of multus and NetworkAttachmentDefinitions. More information about how to configure NetworkAttachementDefinitions can be found here. Below is an example of adding multiple additional networks to a guest cluster at creation time using the hcp command line tool with the --additional-network argument.</p> <pre><code>export CLUSTER_NAME=example\nexport PULL_SECRET=\"$HOME/pull-secret\"\nexport MEM=\"6Gi\"\nexport CPU=\"2\"\nexport WORKER_COUNT=\"2\"\n\nhcp create cluster kubevirt \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas $WORKER_COUNT \\\n--pull-secret $PULL_SECRET \\\n--memory $MEM \\\n--cores $CPU \\\n\u2013additional-network name:my-namespace/network1 \\\n\u2013additional-network name:my-namespace/network2\n</code></pre> <p>In this example, the KubeVirt VMs will have interfaces attached to the networks for the NetworkAttachmentDefinitions network1 and network2 which reside in namespace my-namespace.</p>"},{"location":"how-to/kubevirt/configuring-network/#using-secondary-network-as-default","title":"Using Secondary Network as Default","text":"<p>Users managing a network (DHCP, routing, etc...) can use that network  as the default one for the kubevirt hosted clusters, to do so disable pod default network and attach an additional one that connects to it  with the hcp command line tool arguments <code>--attach-default-network=false</code> and  <code>--additional-network</code>.</p> <pre><code>export CLUSTER_NAME=example\nexport PULL_SECRET=\"$HOME/pull-secret\"\nexport MEM=\"6Gi\"\nexport CPU=\"2\"\nexport WORKER_COUNT=\"2\"\n\nhcp create cluster kubevirt \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas $WORKER_COUNT \\\n--pull-secret $PULL_SECRET \\\n--memory $MEM \\\n--cores $CPU \\\n--attach-default-network=false \\\n--additional-network name:my-namespace/network1 \\\n</code></pre>"},{"location":"how-to/kubevirt/configuring-network/#avoiding-network-cidr-collisions-between-hosting-and-guest-clusters","title":"Avoiding Network CIDR Collisions between Hosting and Guest Clusters.","text":"<p>When creating a guest cluster, it's crucial to ensure that the hosting and guest clusters do not share the same subnet (cidr) for the cluster network. This prevents DNS-related conflicts and avoids issues during the deployment of guest clusters.</p>"},{"location":"how-to/kubevirt/configuring-network/#guidelines-to-avoid-cidr-collisions","title":"Guidelines to Avoid CIDR Collisions:","text":"<ol> <li>Detecting Hosting Cluster Network CIDR Range.</li> </ol> <p>Determine the cluster network CIDR range from the underlying Hosting cluster using the command:</p> <pre><code>oc --kubeconfig &lt;hosting cluster kubeconfig&gt; get network cluster -o=jsonpath='{.spec.clusterNetwork}'\n</code></pre> <ol> <li>Identifying CIDR Conflicts with HCP Default Range</li> </ol> <p>Compare the default CIDR range used by HCP, which is typically set to 10.132.0.0/14, with the CIDR range obtained in Step 1 to detect any conflicts with the hosting CIDR range.</p> <ol> <li>Setting Custom Cluster CIDR Range:</li> </ol> <p>If a conflict is detected, use the HCP CLI to specify a custom CIDR range for the guest cluster using the --cluster-cidr flag. The default CIDR range used by the HCP CLI is designed to be offset from the typical OCP deployment range, so under normal operation, this offset should prevent collisions.</p> <p>For example, if the hosting cluster's clusterNetwork CIDR is set to 10.132.0.0/14, it's imperative to select a different CIDR range for the guest cluster to prevent conflicts.</p> <p>For instance, consider specifying a different CIDR range, such as 10.128.0.0/14 (this range is just an example), for the guest cluster.</p> <pre><code>export CLUSTER_NAME=example\nexport PULL_SECRET=\"$HOME/pull-secret\"\nexport MEM=\"6Gi\"\nexport CPU=\"2\"\nexport WORKER_COUNT=\"2\"\n\nhcp create cluster kubevirt \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas $WORKER_COUNT \\\n--pull-secret $PULL_SECRET \\\n--memory $MEM \\\n--cores $CPU \\\n--cluster-cidr 10.128.0.0/14\n</code></pre> <p>By following these guidelines, you can ensure successful deployment of guest clusters, avoid network CIDR collisions, and prevent DNS-related issues between hosting and guest clusters.</p>"},{"location":"how-to/kubevirt/configuring-storage/","title":"Storage Overview","text":"<p>Configuring storage for KubeVirt Hosted Control Planes falls within three categories, each of which have differing requirements.</p> <p>ETCD Storage</p> <p>ETCD requires usage of high performance persistent storage on the management cluster hosting the ETCD pods. Due to the performance requirements, usage of a local storage csi driver such as LVM Storage is preferred. When a guest cluster is created in HighAvailability mode, ETCD is replicated in pods across three separate management cluster nodes. This replication ensures data resiliency even when a local storage csi driver is in use.</p> <p>More information about ETCD storage configuration can be found in the ETCD Storage Configuration section.</p> <p>Node Root Volume Storage</p> <p>In a KubeVirt Hosted Control Plane, the worker nodes are hosted in KubeVirt VMs. It is recommended to use a csi driver capable of providing ReadWriteMany access mode and Block volume mode for the VM root volume storage. This allows the KubeVirt VMs to live migrate and remain available even when the underlying infra cluster nodes are disrupted. Ceph is an example of a csi driver that would meet these requirements.</p> <p>More information about configuring root volume storage can be found in the KubeVirt VM Root Volume Configuration section.</p> <p>KubeVirt CSI Storage</p> <p>The KubeVirt CSI driver allows storage classes present on the infra cluster (the cluster the KubeVirt VMs run on) to be mapped directly to storage classes within the KubeVirt guest cluster. This lets the guest cluster utilize the same storage that is available on the infra cluster.</p> <p>It is recommended to use an underlying infra storage class capable of ReadWriteMany access mode and Block volume mode for KubeVirt CSI. This allows KubeVirt CSI to pass storage to the VMs in a way that still allows for the VMs to live migrate and be portable across infra nodes.</p> <p>Below is a chart that outlines the current features of KubeVirt CSI as they map to the infra cluster's storage class.</p> Infra CSI Capability Guest CSI Capability VM Live Migration Support Notes RWX Block RWO (Block/Filesystem) RWX (Block) Supported RWO Block RWO (Block/Filesystem) Not Supported RWO Filesystem RWO (Block/Filesystem) Not Supported suboptimal guest block volume mode performance. <p>More information about configuring KubeVirt CSI can be found in the KubeVirt CSI StorageClass Mapping section.</p>"},{"location":"how-to/kubevirt/configuring-storage/#configuring-storage","title":"Configuring Storage","text":"<p>By default, if no advanced configuration is provided, the default storageclass is used for the the KubeVirt VM images, the kubevirt csi mapping, and the etcd volumes.</p>"},{"location":"how-to/kubevirt/configuring-storage/#kubevirt-csi-storageclass-mapping","title":"KubeVirt CSI StorageClass Mapping","text":"<p>KubeVirt CSI permits any infra storage class with the <code>ReadWriteMany</code> access mode to be exposed to the guest cluster. This mapping of infra cluster storage class to guest cluster storage class can be configured during cluster creation using the <code>hcp</code> cli tool and the <code>--infra-storage-class-mapping</code> cli argument.</p> <p>Below is an example of how to map two infra storage classes called <code>infra-sc1</code> and <code>infra-sc2</code> to guest storage classes called <code>guest-sc1</code> and <code>guest-sc2</code>. Note that <code>--infra-storage-class-mapping</code> argument can be used multiple times within the create command.</p> <pre><code>export CLUSTER_NAME=example\nexport PULL_SECRET=\"$HOME/pull-secret\"\nexport MEM=\"6Gi\"\nexport CPU=\"2\"\nexport WORKER_COUNT=\"2\"\n\nhcp create cluster kubevirt \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas $WORKER_COUNT \\\n--pull-secret $PULL_SECRET \\\n--memory $MEM \\\n--cores $CPU \\\n--infra-storage-class-mapping=infra-sc1/guest-sc1 \\\n--infra-storage-class-mapping=infra-sc2/guest-sc2\n</code></pre> <p>Once the guest cluster is created, the <code>guest-sc1</code> and <code>guest-sc2</code> storage classes will be visible within the guest cluster. When users create a PVC within the guest cluster that utilizes one of these storage classes, KubeVirt CSI will facilitate provisioning that volume using the infra storage class mapping that was configured during cluster creation.</p> <p>Note</p> <p>KubeVirt CSI only supports mapping an infrastructure storage class that is capable of <code>ReadWriteMany</code> (RWX) access.</p>"},{"location":"how-to/kubevirt/configuring-storage/#kubevirt-csi-volumesnapshotclass-mapping","title":"KubeVirt CSI VolumeSnapshotClass Mapping","text":"<p>KubeVirt CSI permits infra VolumeSnapshotClasses to be exposed to the guest cluster. Since VolumeSnapshotClasses are tied to a particular provisioner the mapping between VolumeSnapshotClasses and StorageClasses needs to expressed to hypershift during guest cluster creation. using the <code>hcp</code> cli tool and the <code>--infra-volumesnapshot-class-mapping</code> cli argument.</p> <p>Below is an example of a simple setup with a single infra storage class and a single matching volume snapshot class in the infra cluster being mapped to a single storage class and single volume snapshot class in the guest cluster.</p> <pre><code>export CLUSTER_NAME=example\nexport PULL_SECRET=\"$HOME/pull-secret\"\nexport MEM=\"6Gi\"\nexport CPU=\"2\"\nexport WORKER_COUNT=\"2\"\n\nhcp create cluster kubevirt \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas $WORKER_COUNT \\\n--pull-secret $PULL_SECRET \\\n--memory $MEM \\\n--cores $CPU \\\n--infra-storage-class-mapping=infra-sc1/guest-sc1 \\\n--infra-volumesnapshot-class-mapping=infra-vsc1/guest-vsc1\n</code></pre> <p>If you omit the <code>--infra-storage-class-mapping</code> and the <code>--infra-volumesnapshot-class-mapping</code>. The system will use the default storage class and the default volume snapshot class in the infra cluster. If the default is not set, then the snapshot functionality will not work and the snapshot request will never reach ready state. This is because it not possible to create a correct snapshot in the infra cluster.</p> <p>A more complex setup could contain multiple storage classes with multiple volume snapshot classes. In particular in this setup one volume snapshot class is only compatible with certain storage classes but not all. So we have infra storage class a and b, and infra snapshot volume class a and b. Only the a's are compatible with each other and only the b's are compatible with each other.</p> <p>To properly group them together use the 'group' option of the <code>--infra-volumesnapshot-class-mapping</code> and <code>group</code> option of the <code>--infra-storage-class-mapping</code>. Below is an example of this setup</p> <pre><code>export CLUSTER_NAME=example\nexport PULL_SECRET=\"$HOME/pull-secret\"\nexport MEM=\"6Gi\"\nexport CPU=\"2\"\nexport WORKER_COUNT=\"2\"\n\nhcp create cluster kubevirt \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas $WORKER_COUNT \\\n--pull-secret $PULL_SECRET \\\n--memory $MEM \\\n--cores $CPU \\\n--infra-storage-class-mapping=infra-sca/guest-sca,group=a \\\n--infra-storage-class-mapping=infra-scb/guest-scb,group=b \\\n--infra-storage-class-mapping=infra-scc/guest-scc,group=a \\\n--infra-volumesnapshot-class-mapping=infra-vsca/guest-vsca,group=a \\\n--infra-volumesnapshot-class-mapping=infra-vscb/guest-vscb,group=b\n</code></pre> <p>Since both storage class <code>infra-sca</code> and volume snapshot class <code>infra-vsca</code> are in the same group, this indicates to KubeVirt CSI that they are compatible and be used to create snapshots of volumes from storage class <code>guest-sca</code> using the guest volume snapshot class <code>guest-vsca</code>. The same is true with with the <code>b</code> grouping as well. Since <code>infra-scc</code> is also in the <code>a</code> group, creating snapshots of volumes from storage class <code>guest-scc</code> will use the same volume snapshot class in the infra cluster as making a snapshot of volumes that use storage class <code>guest-sca</code></p> <p>Note</p> <p>KubeVirt CSI passes snapshot requests to the underlying infra. This means    that snapshots will only work for compatible volumes. Please ensure the    proper mapping is configured before attempting to create a snapshot in the    guest cluster.</p>"},{"location":"how-to/kubevirt/configuring-storage/#disabling-kubevirt-csi","title":"Disabling KubeVirt CSI","text":"<p>By default KubeVirt CSI maps the default storage class on the underlying infrastructure cluster to a storage class in the guest cluster.</p> <p>To disable KubeVirt CSI entirely, set the hostedCluster.spec.platform.kubeVirt.storageDriver.type to the value <code>None</code> at cluster creation time. Below is an example HostedCluster spec that outlines this behavior.</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: HostedCluster\nmetadata:\n  name: example\n  namespace: clusters\nspec:\n  platform:\n    type: KubeVirt\n    storageDriver:\n      type: None\n  pullSecret:\n    name: hcp-pull-secret\n</code></pre>"},{"location":"how-to/kubevirt/configuring-storage/#kubevirt-csi-storage-security-and-isolation","title":"KubeVirt CSI Storage Security and Isolation","text":"<p>While KubeVirt CSI is extending storage capabilities of the underlying infrastructure cluster to guest HCP clusters, the csi driver is doing so in a controlled way. This ensures each guest cluster's storage is both isolated from other guest clusters, and that the guest cluster can't access arbitrary storage volumes on the infrastructure cluster that are not associated with the guest cluster.</p> <p>This isolation is achieved through a depth of security enforcements</p> <ol> <li>Direct API access to the infrastructure cluster is never given directly to the HCP guest cluster worker nodes. This means the guest cluster does not have a means to provision storage on the infrastructure cluster except through the controlled KubeVirt CSI interface.</li> <li>The KubeVirt CSI cluster controller runs in a pod in the HCP namespace, and is not accessible from within the guest cluster. This component ensures PVCs on the infrastructure cluster can only be passed into the guest cluster if those PVCs are associated with the guest cluster.</li> <li>By default, the RBAC provided to the KubeVirt CSI cluster controller limits PVC access only to the HCP namespace. This prevents the possibility of cross namespace storage access by any KubeVirt CSI component.</li> </ol> <p>These security enforcements ensure safe and isolated multitenant access to shared infrastructure storage classes are possible for multiple HCP KubeVirt guest clusters.</p>"},{"location":"how-to/kubevirt/configuring-storage/#kubevirt-vm-root-volume-configuration","title":"KubeVirt VM Root Volume Configuration","text":"<p>The storage class used to host the KubeVirt Virtual Machine root volumes can be configured at cluster creation time using the <code>hcp</code> cli tool and the <code>--root-volume-storage-class</code> argument. Likewise, the size of the volume can be configured using the <code>--root-volume-size</code> cli argument.</p> <p>Below is an example of setting a custom storage class and volume size for the KubeVirt VMs. The result will be a guest cluster with VMs hosted on 64Gi PVCs hosted by the <code>ocs-storagecluster-ceph-rdb</code> storage class</p> <pre><code>export CLUSTER_NAME=example\nexport PULL_SECRET=\"$HOME/pull-secret\"\nexport MEM=\"6Gi\"\nexport CPU=\"2\"\nexport WORKER_COUNT=\"2\"\n\nhcp create cluster kubevirt \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas $WORKER_COUNT \\\n--pull-secret $PULL_SECRET \\\n--memory $MEM \\\n--cores $CPU \\\n--root-volume-storage-class ocs-storagecluster-ceph-rbd \\\n--root-volume-size 64\n</code></pre>"},{"location":"how-to/kubevirt/configuring-storage/#kubevirt-vm-image-caching","title":"KubeVirt VM Image Caching","text":"<p>KubeVirt VM image caching is an advanced feature that can be used to optimize both cluster startup time and storage utilization. This feature requires the usage of a storage class capable of smart cloning and the <code>ReadWriteMany</code> access mode.</p> <p>Image caching works by importing the VM image once to a single PVC associated with the HostedCluster, and then making a unique clone of that PVC for every KubeVirt VM added as a worker node to the cluster. This reduces VM startup time by only requiring a single image import, and can further reduce overall cluster storage usage when the storage class supports copy on write cloning.</p> <p>Image caching can be enabled during cluster creation using the <code>hcp</code> cli tool with the <code>--root-volume-cache-strategy=PVC</code> argument. Below is an example.</p> <pre><code>export CLUSTER_NAME=example\nexport PULL_SECRET=\"$HOME/pull-secret\"\nexport MEM=\"6Gi\"\nexport CPU=\"2\"\nexport WORKER_COUNT=\"2\"\n\nhcp create cluster kubevirt \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas $WORKER_COUNT \\\n--pull-secret $PULL_SECRET \\\n--memory $MEM \\\n--cores $CPU \\\n--root-volume-cache-strategy=PVC\n</code></pre>"},{"location":"how-to/kubevirt/configuring-storage/#etcd-storage-configuration","title":"ETCD Storage Configuration","text":"<p>The storage class used to host the etcd data can be customized at cluster creation time using the <code>hcp</code> cli and the <code>--etcd-storage-class</code> cli argument. When no <code>--etcd-storage-class</code> argument is provided, the default storage class will be used.</p> <p>Below is an example of how to configure usage of a storage class for etcd.</p> <pre><code>export CLUSTER_NAME=example\nexport PULL_SECRET=\"$HOME/pull-secret\"\nexport MEM=\"6Gi\"\nexport CPU=\"2\"\nexport WORKER_COUNT=\"2\"\n\nhcp create cluster kubevirt \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas $WORKER_COUNT \\\n--pull-secret $PULL_SECRET \\\n--memory $MEM \\\n--cores $CPU \\\n--etcd-storage-class=\"local\"\n</code></pre>"},{"location":"how-to/kubevirt/create-kubevirt-cluster/","title":"Create a Kubevirt cluster","text":"<p>Install an OCP cluster running on VMs within a management OCP cluster</p>"},{"location":"how-to/kubevirt/create-kubevirt-cluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>Admin access to an OpenShift cluster (version 4.14+) specified by the <code>KUBECONFIG</code> environment variable.</li> <li>The management OCP cluster must have wildcard dns routes enabled. <code>oc patch ingresscontroller -n openshift-ingress-operator default --type=json -p '[{ \"op\": \"add\", \"path\": \"/spec/routeAdmission\", \"value\": {wildcardPolicy: \"WildcardsAllowed\"}}]'</code></li> <li>The management OCP cluster must have Openshift Virtualization (4.14+) installed on it. Instructions for installing Openshift Virtualization</li> <li>The Management OCP cluster must be configured with OVNKubernetes as the default pod network CNI.</li> <li>The Management OCP cluster must have LoadBalancer service support. Instructions for installing MetalLB</li> <li>The management OCP cluster must have default storage class. Storage Configuration Documentation Example of how to set a default storage class: <code>oc patch storageclass ocs-storagecluster-ceph-rbd -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'</code></li> <li>The OpenShift CLI (<code>oc</code>) or Kubernetes CLI (<code>kubectl</code>).</li> <li>A valid pull secret file for the <code>quay.io/openshift-release-dev</code> repository.</li> <li>A network MTU of 9000 or larger on the OCP cluster hosting the KubeVirt VMs is recommended for optimal network performance. Smaller MTU settings will work, but network latency and throughput of the hosted pods will be impacted. Multiqueue should only be enabled on NodePools when the MTU is 9000 or larger.</li> </ul>"},{"location":"how-to/kubevirt/create-kubevirt-cluster/#installing-hypershift-operator-and-cli-tooling","title":"Installing HyperShift Operator and cli tooling","text":"<p>Before creating a guest cluster, the hcp cli, hypershift cli, and HyperShift Operator must be installed.</p> <p>The <code>hypershift</code> cli tool is a development tool that is used to install developer builds of the HyperShift Operator.</p> <p>The <code>hcp</code> cli tool is used to manage the creation and destruction of guest clusters.</p>"},{"location":"how-to/kubevirt/create-kubevirt-cluster/#build-the-hypershift-and-hcp-cli","title":"Build the HyperShift and HCP CLI","text":"<p>The command below builds latest hypershift and hcp cli tools from source and places the cli tool within the /usr/local/bin directory.</p> <p>Note</p> <p>The command below is the same if you use docker.</p> <pre><code>podman run --rm --privileged -it -v \\\n$PWD:/output docker.io/library/golang:1.20 /bin/bash -c \\\n'git clone https://github.com/openshift/hypershift.git &amp;&amp; \\\ncd hypershift/ &amp;&amp; \\\nmake hypershift product-cli &amp;&amp; \\\nmv bin/hypershift /output/hypershift &amp;&amp; \\\nmv bin/hcp /output/hcp'\n\nsudo install -m 0755 -o root -g root $PWD/hypershift /usr/local/bin/hypershift\nsudo install -m 0755 -o root -g root $PWD/hcp /usr/local/bin/hcp\nrm $PWD/hypershift\nrm $PWD/hcp\n</code></pre>"},{"location":"how-to/kubevirt/create-kubevirt-cluster/#deploy-the-hypershift-operator","title":"Deploy the HyperShift Operator","text":"<p>Use the hypershift cli tool to install the HyperShift operator into the management cluster.</p> <pre><code>hypershift install\n</code></pre> <p>You will see the operator running in the <code>hypershift</code> namespace:</p> <pre><code>oc -n hypershift get pods\n\nNAME                        READY   STATUS    RESTARTS   AGE\noperator-755d587f44-lrtrq   1/1     Running   0          114s\noperator-755d587f44-qj6pz   1/1     Running   0          114s\n</code></pre>"},{"location":"how-to/kubevirt/create-kubevirt-cluster/#create-a-hostedcluster","title":"Create a HostedCluster","text":"<p>Once all the prerequisites are met, and the HyperShift operator is installed, it is now possible to create a guest cluster.</p> <p>Below is an example of how to create a guest cluster using environment variables and the <code>hcp</code> cli tool.</p> <p>Note</p> <p>The --release-image flag could be used to provision the HostedCluster with a specific OpenShift Release (the hypershift operator has a support matrix of releases supported by a given version of the operator)</p> <pre><code>export CLUSTER_NAME=example\nexport PULL_SECRET=\"$HOME/pull-secret\"\nexport MEM=\"6Gi\"\nexport CPU=\"2\"\nexport WORKER_COUNT=\"2\"\n\nhcp create cluster kubevirt \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas $WORKER_COUNT \\\n--pull-secret $PULL_SECRET \\\n--memory $MEM \\\n--cores $CPU\n</code></pre> <p>Note</p> <p>A default NodePool will be created for the cluster with 2 vm worker replicas per the <code>--node-pool-replicas</code> flag.</p> <p>After a few moments we should see our hosted control plane pods up and running:</p> <pre><code>oc -n clusters-$CLUSTER_NAME get pods\n\nNAME                                                  READY   STATUS    RESTARTS   AGE\ncapi-provider-5cc7b74f47-n5gkr                        1/1     Running   0          3m\ncatalog-operator-5f799567b7-fd6jw                     2/2     Running   0          69s\ncertified-operators-catalog-784b9899f9-mrp6p          1/1     Running   0          66s\ncluster-api-6bbc867966-l4dwl                          1/1     Running   0          66s\n.\n.\n.\nredhat-operators-catalog-9d5fd4d44-z8qqk              1/1     Running   0          66s\n</code></pre> <p>A guest cluster backed by KubeVirt virtual machines typically takes around 10-15 minutes to fully provision. The status of the guest cluster can be seen by viewing the corresponding HostedCluster resource. For example, the output below reflects what a fully provisioned HostedCluster object looks like.</p> <pre><code>oc get --namespace clusters hostedclusters\n\nNAME            VERSION   KUBECONFIG                       PROGRESS   AVAILABLE   PROGRESSING   MESSAGE\nexample         4.14.0    example-admin-kubeconfig         Completed  True        False         The hosted control plane is available\n</code></pre>"},{"location":"how-to/kubevirt/create-kubevirt-cluster/#accessing-the-hostedcluster","title":"Accessing the HostedCluster","text":"<p>CLI access to the guest cluster is gained by retrieving the guest cluster's kubeconfig. Below is an example of how to retrieve the guest cluster's kubeconfig using the hcp cli.</p> <pre><code>hcp create kubeconfig --name $CLUSTER_NAME &gt; $CLUSTER_NAME-kubeconfig\n</code></pre> <p>If we access the cluster, we will see we have two nodes.</p> <pre><code>oc --kubeconfig $CLUSTER_NAME-kubeconfig get nodes\n\nNAME                  STATUS   ROLES    AGE   VERSION\nexample-n6prw         Ready    worker   32m   v1.27.4+18eadca\nexample-nc6g4         Ready    worker   32m   v1.27.4+18eadca\n</code></pre> <p>We can also check the ClusterVersion:</p> <pre><code>oc --kubeconfig $CLUSTER_NAME-kubeconfig get clusterversion\n\nNAME      VERSION       AVAILABLE   PROGRESSING   SINCE   STATUS\nversion   4.14.0        True        False         5m39s   Cluster version is 4.14.0\n</code></pre>"},{"location":"how-to/kubevirt/create-kubevirt-cluster/#influencing-vm-scheduling","title":"Influencing VM Scheduling","text":"<p>By default, KubeVirt VMs created by a NodePool are scheduled to any available node with the appropriate capacity to run the VM. A topologySpreadConstraint is used to try to spread VMs for a NodePool out across multiple underlying nodes, but in general it is possible for the VMs to be scheduled to any node that meets the requirements for running KubeVirt VMs. For availability reasons, the topologySpreadConstraint is only a soft constraint (<code>whenUnsatisfiable: ScheduleAnyway</code> policy) so, under certain circumstances, VMs for a single nodepool can \u201cclump\u201d together on a single node due to live migration to available nodes. The De-Scheduler can optionally be used to continuously redistribute VMs so that they again satisfy the topologySpreadConstraint as soon as possible. To achieve that behavior, the cluster-kube-descheduler-operator should be installed and configured with something like: <pre><code>apiVersion: operator.openshift.io/v1\nkind: KubeDescheduler\nmetadata:\n  name: cluster\n  namespace: openshift-kube-descheduler-operator\nspec:\n  mode: Automatic\n  managementState: Managed\n  deschedulingIntervalSeconds: 60\n  profiles:\n  - SoftTopologyAndDuplicates\n  - EvictPodsWithPVC\n  - EvictPodsWithLocalStorage\n  profileCustomizations:\n    devEnableEvictionsInBackground: true\n</code></pre></p> <p>It is possible to influence where the KubeVirt VMs within a NodePool are scheduled through the use of NodeSelectors. Below is an example of using a NodeSelector on the VMs to place the VMs only on nodes with a specific label through usage of the hcp command line and the --vm-node-selector argument.</p> <pre><code>export CLUSTER_NAME=example\nexport PULL_SECRET=\"$HOME/pull-secret\"\nexport MEM=\"6Gi\"\nexport CPU=\"2\"\nexport WORKER_COUNT=\"2\"\n\nhcp create cluster kubevirt \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas $WORKER_COUNT \\\n--pull-secret $PULL_SECRET \\\n--memory $MEM \\\n--cores $CPU \\\n--vm-node-selector labelKey1=labelVal1,labeKey2=labelVal2\n</code></pre> <p>In the example above, the KubeVirt VMs will only be scheduled to nodes that  contain the labels labelKey1=labelVal1 and labelKey2=labelVal2.</p>"},{"location":"how-to/kubevirt/create-kubevirt-cluster/#scaling-an-existing-nodepool","title":"Scaling an existing NodePool","text":"<p>Manually scale a NodePool using the <code>oc scale</code> command:</p> <pre><code>NODEPOOL_NAME=$CLUSTER_NAME\nNODEPOOL_REPLICAS=5\n\noc scale nodepool/$NODEPOOL_NAME --namespace clusters --replicas=$NODEPOOL_REPLICAS\n</code></pre> <p>After a while, in our hosted cluster this is what we will see:</p> <pre><code>oc --kubeconfig $CLUSTER_NAME-kubeconfig get nodes\n\nNAME                  STATUS   ROLES    AGE     VERSION\nexample-9jvnf         Ready    worker   97s     v1.27.4+18eadca\nexample-n6prw         Ready    worker   116m    v1.27.4+18eadca\nexample-nc6g4         Ready    worker   117m    v1.27.4+18eadca\nexample-thp29         Ready    worker   4m17s   v1.27.4+18eadca\nexample-twxns         Ready    worker   88s     v1.27.4+18eadca\n</code></pre>"},{"location":"how-to/kubevirt/create-kubevirt-cluster/#adding-additional-nodepools","title":"Adding Additional NodePools","text":"<p>Create additional NodePools for a guest cluster by specifying a name, number of replicas, and any additional information such as memory and cpu requirements.</p> <p>For example, let's create a NodePool with more CPUs assigned to the VMs (4 vs 2):</p> <pre><code>export NODEPOOL_NAME=$CLUSTER_NAME-extra-cpu\nexport WORKER_COUNT=\"2\"\nexport MEM=\"6Gi\"\nexport CPU=\"4\"\nexport DISK=\"16\"\n\nhcp create nodepool kubevirt \\\n  --cluster-name $CLUSTER_NAME \\\n  --name $NODEPOOL_NAME \\\n  --replicas $WORKER_COUNT \\\n  --memory $MEM \\\n  --cores $CPU \\\n  --root-volume-size $DISK\n</code></pre> <p>Check the status of the NodePool by listing <code>nodepool</code> resources in the <code>clusters</code> namespace:</p> <pre><code>oc get nodepools --namespace clusters\n\nNAME                      CLUSTER         DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE\nexample                   example         5               5               False         False        4.14.0  \nexample-extra-cpu         example         2                               False         False                  True              True             Minimum availability requires 2 replicas, current 0 available\n</code></pre> <p>After a while, in our hosted cluster this is what we will see:</p> <pre><code>oc --kubeconfig $CLUSTER_NAME-kubeconfig get nodes\n\nNAME                      STATUS   ROLES    AGE     VERSION\nexample-9jvnf             Ready    worker   97s     v1.27.4+18eadca\nexample-n6prw             Ready    worker   116m    v1.27.4+18eadca\nexample-nc6g4             Ready    worker   117m    v1.27.4+18eadca\nexample-thp29             Ready    worker   4m17s   v1.27.4+18eadca\nexample-twxns             Ready    worker   88s     v1.27.4+18eadca\nexample-extra-cpu-zh9l5   Ready    worker   2m6s    v1.27.4+18eadca\nexample-extra-cpu-zr8mj   Ready    worker   102s    v1.27.4+18eadca\n</code></pre> <p>And the nodepool will be in the desired state:</p> <pre><code>oc get nodepools --namespace clusters\n\nNAME                      CLUSTER         DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE\nexample                   example         5               5               False         False        4.14.0  \nexample-extra-cpu         example         2               2               False         False        4.14.0  \n</code></pre>"},{"location":"how-to/kubevirt/create-kubevirt-cluster/#delete-a-hostedcluster","title":"Delete a HostedCluster","text":"<p>To delete a HostedCluster:</p> <pre><code>hcp destroy cluster kubevirt --name $CLUSTER_NAME\n</code></pre>"},{"location":"how-to/kubevirt/external-infrastructure/","title":"External Infrastructure","text":"<p>By default, the HyperShift operator hosts both the HostedCluster's control plane pods and KubeVirt worker VMs within the same cluster.</p> <p>With the external infrastructure feature, it possible to place the worker node VMs on a separate cluster from the control plane pods.</p>"},{"location":"how-to/kubevirt/external-infrastructure/#understanding-hosting-cluster-types","title":"Understanding Hosting Cluster Types","text":"<p>Management Cluster: This is the OpenShift cluster that runs the HyperShift operator and hosts the control plane pods for a HostedCluster.</p> <p>Infrastructure Cluster: This is the OpenShift cluster that runs the KubeVirt worker VMs for a HostedCluster.</p> <p>By default, the management cluster also acts as the infrastructure cluster that hosts VMs. However, for the external infrastructure use case, the management and infrastructure clusters are distinctly different.</p>"},{"location":"how-to/kubevirt/external-infrastructure/#create-a-hostedcluster-using-external-infrastructure","title":"Create a HostedCluster using External Infrastructure","text":"<p>Prerequisites  * Creation of a namespace on the external infrastructure cluster for the KubeVirt worker nodes to be hosted in.  * A kubeconfig for the external infrastructure cluster</p> <p>Once the prerequisites are met, the <code>hcp</code> cli tool can be used to create the guest cluster. In order to place the KubeVirt worker VMs on the infrastructure cluster, use the <code>--infra-kubeconfig-file</code> and <code>--infra-namespace</code> arguments.</p> <p>Below is an example of creating a guest cluster using external infrastructure.</p> <pre><code>export CLUSTER_NAME=example\nexport PULL_SECRET=\"$HOME/pull-secret\"\nexport MEM=\"6Gi\"\nexport CPU=\"2\"\nexport WORKER_COUNT=\"2\"\n\nhcp create cluster kubevirt \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas $WORKER_COUNT \\\n--pull-secret $PULL_SECRET \\\n--memory $MEM \\\n--cores $CPU \\\n--infra-namespace=clusters-example \\\n--infra-kubeconfig-file=$HOME/external-infra-kubeconfig\n</code></pre> <p>This command will result in the control plane pods being hosted on the management cluster that the HyperShift Operator runs on, while the KubeVirt VMs will be hosted on a separate infrastructure cluster.</p>"},{"location":"how-to/kubevirt/external-infrastructure/#required-rbac-for-the-external-infrastructure-cluster","title":"Required RBAC for the external infrastructure cluster","text":"<p>It isn't necessary for the user defined in the kubeconfig used for the external infra cluster to be a cluster-admin. The user or service account used in the provided kubeconfig should have full permissions over the following resources: * <code>virtualmachines.kubevirt.io</code> * <code>virtualmachineinstances.kubevirt.io</code> * <code>virtualmachines.kubevirt.io/finalizers</code> * <code>datavolumes.cdi.kubevirt.io</code> * <code>services</code> * <code>endpointslices</code> * <code>endpointslices/restricted</code> * <code>routes</code> The user or service account used in the provided kubeconfig should also have get/create/delete permissions over the following resources: * <code>volumesnapshots</code> As well as get permission for: * <code>persistentvolumeclaims</code></p> <p>All of these permissions are needed only on the target namespace on the infra cluster (passed through the <code>--infra-namespace</code> command-line argument). This can be achieved by binding the following Role to the user used in the external infra kubeconfig: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: kv-external-infra-role\n  namespace: clusters-example\nrules:\n  - apiGroups:\n      - kubevirt.io\n    resources:\n      - virtualmachines\n      - virtualmachines/finalizers\n      - virtualmachineinstances\n    verbs:\n      - '*'\n  - apiGroups:\n      - cdi.kubevirt.io\n    resources:\n      - datavolumes\n    verbs:\n      - '*'\n  - apiGroups:\n      - ''\n    resources:\n      - services\n    verbs:\n      - '*'\n  - apiGroups:\n      - route.openshift.io\n    resources:\n      - routes\n      - routes/custom-host\n    verbs:\n      - '*'\n  - apiGroups:\n      - discovery.k8s.io\n    resources:\n      - endpointslices\n      - endpointslices/restricted\n    verbs:\n      - '*'\n  - apiGroups:\n      - ''\n    resources:\n      - secrets\n    verbs:\n      - '*'\n  - apiGroups:\n    - snapshot.storage.k8s.io\n    resources:\n    - volumesnapshots\n    verbs:\n    - get\n    - create\n    - delete\n  - apiGroups:\n    - ''\n    resources:\n    - persistentvolumeclaims\n    verbs:\n    - get\n</code></pre></p>"},{"location":"how-to/kubevirt/global-pull-secret/","title":"Global Pull Secret for Hosted Control Planes","text":""},{"location":"how-to/kubevirt/global-pull-secret/#overview","title":"Overview","text":"<p>The Global Pull Secret functionality enables Hosted Cluster administrators to include additional pull secrets for accessing container images from private registries without requiring assistance from the Management Cluster administrator. This feature allows you to merge your custom pull secret with the original HostedCluster pull secret, making it available to all nodes in the cluster.</p> <p>The implementation uses a DaemonSet approach that automatically detects when you create an <code>additional-pull-secret</code> in the <code>kube-system</code> namespace of your DataPlane (Hosted Cluster). The system then merges this secret with the original pull secret and deploys the merged result to all nodes via a DaemonSet that updates the kubelet configuration.</p> <p>Note</p> <p>This feature is designed to work autonomously - once you create the additional pull secret, the system automatically handles the rest without requiring Management Cluster administrator intervention.</p>"},{"location":"how-to/kubevirt/global-pull-secret/#adding-your-pull-secret","title":"Adding your Pull Secret","text":"<p>Important</p> <p>All actions described in this section must be performed on the HostedCluster's workers (DataPlane), not on the Management Cluster.</p> <p>To use this functionality, follow these steps:</p>"},{"location":"how-to/kubevirt/global-pull-secret/#1-create-your-additional-pull-secret","title":"1. Create your additional pull secret","text":"<p>Create a secret named <code>additional-pull-secret</code> in the <code>kube-system</code> namespace of your Hosted Cluster (DataPlane). The secret must contain a valid DockerConfigJSON format:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: additional-pull-secret\n  namespace: kube-system\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: &lt;base64-encoded-docker-config-json&gt;\n</code></pre>"},{"location":"how-to/kubevirt/global-pull-secret/#2-example-dockerconfigjson-format","title":"2. Example DockerConfigJSON format","text":"<p>Your <code>.dockerconfigjson</code> should follow this structure:</p> <pre><code>{\n  \"auths\": {\n    \"registry.example.com\": {\n      \"auth\": \"base64-encoded-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"base64-encoded-credentials\"\n    }\n  }\n}\n</code></pre> <p>Using Namespace-Specific Registry Entries</p> <p>For registries like Quay.io that support organization/namespace-specific authentication, you can specify the full path in your registry entry (e.g., <code>quay.io/mycompany</code> instead of just <code>quay.io</code>). This allows you to provide different credentials for different namespaces within the same registry, and helps avoid conflicts with existing registry entries in the original pull secret.</p>"},{"location":"how-to/kubevirt/global-pull-secret/#3-apply-the-secret","title":"3. Apply the secret","text":"<pre><code>kubectl apply -f additional-pull-secret.yaml\n</code></pre>"},{"location":"how-to/kubevirt/global-pull-secret/#4-verification","title":"4. Verification","text":"<p>After creating the secret, the system will automatically:</p> <ol> <li>Validate the secret format</li> <li>Merge it with the original pull secret</li> <li>Deploy a DaemonSet to all nodes</li> <li>Update the kubelet configuration on each node</li> </ol> <p>You can verify the deployment by checking:</p> <pre><code># Check if the DaemonSet is running\nkubectl get daemonset global-pull-secret-syncer -n kube-system\n\n# Check the merged pull secret\nkubectl get secret global-pull-secret -n kube-system\n\n# Check DaemonSet pods\nkubectl get pods -n kube-system -l name=global-pull-secret-syncer\n</code></pre>"},{"location":"how-to/kubevirt/global-pull-secret/#how-it-works","title":"How it works","text":"<p>The Global Pull Secret functionality operates through a multi-component system:</p>"},{"location":"how-to/kubevirt/global-pull-secret/#automatic-detection","title":"Automatic Detection","text":"<ul> <li>The Hosted Cluster Config Operator (HCCO) continuously monitors the <code>kube-system</code> namespace</li> <li>When it detects the creation of <code>additional-pull-secret</code>, it triggers the reconciliation process</li> </ul>"},{"location":"how-to/kubevirt/global-pull-secret/#validation-and-merging","title":"Validation and Merging","text":"<ul> <li>The system validates that your secret contains a proper DockerConfigJSON format</li> <li>It retrieves the original pull secret from the HostedControlPlane</li> <li>Your additional pull secret is merged with the original one</li> <li>If there are conflicting registry entries, the original pull secret takes precedence (the additional pull secret entry is ignored for conflicting registries)</li> <li>The system supports namespace-specific registry entries (e.g., <code>quay.io/namespace</code>) for better credential specificity</li> </ul>"},{"location":"how-to/kubevirt/global-pull-secret/#deployment-process","title":"Deployment Process","text":"<ul> <li>A <code>global-pull-secret</code> is created in the <code>kube-system</code> namespace containing the merged result</li> <li>RBAC resources (ServiceAccount, Role, RoleBinding) are created for the DaemonSet in both <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>We use Role and RoleBinding in both namespaces to access secrets in <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>A DaemonSet named <code>global-pull-secret-syncer</code> is deployed to eligible nodes</li> </ul> <p>NodePool InPlace Strategy Restriction</p> <p>The Global Pull Secret DaemonSet is not deployed to nodes that belong to NodePools using the InPlace upgrade strategy. This restriction prevents conflicts between the DaemonSet's modifications to <code>/var/lib/kubelet/config.json</code> and the Machine Config Daemon (MCD) during InPlace upgrades.</p> <ul> <li>Nodes with Replace strategy: \u2705 Receive Global Pull Secret DaemonSet</li> <li>Nodes with InPlace strategy: \u274c Do not receive Global Pull Secret DaemonSet</li> </ul> <p>This ensures that MCD operations during InPlace upgrades do not fail due to unexpected changes in kubelet configuration files.</p>"},{"location":"how-to/kubevirt/global-pull-secret/#node-level-synchronization","title":"Node-Level Synchronization","text":"<ul> <li>Each DaemonSet pod runs a controller that watches the secrets under kube-system namespace</li> <li>When changes are detected, it updates <code>/var/lib/kubelet/config.json</code> on the node</li> <li>The kubelet service is restarted via DBus to apply the new configuration</li> <li>If the restart fails after 3 attempts, the system rolls back the file changes</li> </ul>"},{"location":"how-to/kubevirt/global-pull-secret/#automatic-cleanup","title":"Automatic Cleanup","text":"<ul> <li>If you delete the <code>additional-pull-secret</code>, the HCCO automatically removes the <code>global-pull-secret</code> secret</li> <li>The system reverts to using only the original pull secret from the HostedControlPlane</li> <li>The DaemonSet continues running but now syncs only the original pull secret to nodes</li> </ul>"},{"location":"how-to/kubevirt/global-pull-secret/#registry-precedence-and-conflict-resolution","title":"Registry Precedence and Conflict Resolution","text":"<p>The Global Pull Secret system uses a specific precedence model when merging your additional pull secret with the original one:</p>"},{"location":"how-to/kubevirt/global-pull-secret/#merge-behavior","title":"Merge Behavior","text":"<ul> <li>Original pull secret entries always take precedence over additional pull secret entries for the same registry</li> <li>If both secrets contain an entry for <code>quay.io</code>, the original pull secret's credentials will be used</li> <li>Your additional pull secret entries are only added if they don't conflict with existing entries</li> <li>Warnings are logged when conflicts are detected</li> </ul>"},{"location":"how-to/kubevirt/global-pull-secret/#recommended-approach","title":"Recommended Approach","text":"<p>To avoid conflicts and ensure your credentials are used, consider these strategies:</p> <ol> <li>Use namespace-specific entries: Instead of <code>quay.io</code>, use <code>quay.io/your-namespace</code></li> <li>Target specific registries: Add entries only for registries not already in the original pull secret</li> <li>Check existing entries: Review what registries are already configured in the HostedControlPlane</li> </ol>"},{"location":"how-to/kubevirt/global-pull-secret/#example-merge-scenario","title":"Example Merge Scenario","text":"<p>Original Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"original-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Your Additional Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"your-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"your-namespace-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Resulting Merged Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"original-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"your-namespace-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Note how the <code>quay.io</code> entry keeps the original credentials, but <code>quay.io/mycompany</code> is added from your additional secret.</p>"},{"location":"how-to/kubevirt/global-pull-secret/#implementation-details","title":"Implementation details","text":"<p>The implementation consists of several key components working together:</p>"},{"location":"how-to/kubevirt/global-pull-secret/#core-components","title":"Core Components","text":"<ol> <li>Global Pull Secret Controller (<code>globalps</code> package)</li> <li>Handles validation of user-provided pull secrets</li> <li>Manages the merging logic between original and additional pull secrets</li> <li>Creates and manages RBAC resources</li> <li>Deploys and manages the DaemonSet</li> <li> <p>Node eligibility assessment: Labels nodes from InPlace NodePools and configures DaemonSet scheduling restrictions</p> </li> <li> <p>Sync Global Pull Secret Command (<code>sync-global-pullsecret</code> package)</p> </li> <li>Runs as a DaemonSet on each node</li> <li>Watches for changes to the <code>global-pull-secret</code> in <code>kube-system</code> namespace</li> <li>Accesses the original <code>pull-secret</code> in <code>openshift-config</code> namespace</li> <li>Updates the kubelet configuration file</li> <li> <p>Manages kubelet service restarts via DBus</p> </li> <li> <p>Hosted Cluster Config Operator Integration</p> </li> <li>Monitors for the presence of <code>additional-pull-secret</code></li> <li>Orchestrates the entire process</li> <li>Handles cleanup when the secret is removed</li> </ol>"},{"location":"how-to/kubevirt/global-pull-secret/#architecture-diagram","title":"Architecture Diagram","text":"graph TB     %% User Input     User[User creates additional-pull-secret] --&gt; |kube-system namespace| AdditionalPS[additional-pull-secret Secret]      %% HCCO Controller     HCCO[Hosted Cluster Config Operator] --&gt; |Watches kube-system secrets| GlobalPSController[Global Pull Secret Controller]     GlobalPSController --&gt; |Validates| AdditionalPS     GlobalPSController --&gt; |Gets original| OriginalPS[Original pull-secret from HCP]      %% Secret Processing     AdditionalPS --&gt; |Validates format| ValidatePS[Validate Additional Pull Secret]     OriginalPS --&gt; |Extracts data| OriginalPSData[Original Pull Secret Data]     ValidatePS --&gt; |Extracts data| AdditionalPSData[Additional Pull Secret Data]      %% Merge Process     OriginalPSData --&gt; MergeSecrets[Merge Pull Secrets]     AdditionalPSData --&gt; MergeSecrets     MergeSecrets --&gt; |Creates merged JSON| GlobalPSData[Global Pull Secret Data]      %% Secret Creation     GlobalPSData --&gt; |Creates in kube-system| GlobalPSSecret[global-pull-secret Secret]      %% RBAC Setup     GlobalPSController --&gt; |Creates RBAC| RBACSetup[Setup RBAC Resources]     RBACSetup --&gt; ServiceAccount[global-pull-secret-syncer ServiceAccount]     RBACSetup --&gt; KubeSystemRole[global-pull-secret-syncer Role in kube-system]     RBACSetup --&gt; KubeSystemRoleBinding[global-pull-secret-syncer RoleBinding in kube-system]     RBACSetup --&gt; OpenshiftConfigRole[global-pull-secret-syncer Role in openshift-config]     RBACSetup --&gt; OpenshiftConfigRoleBinding[global-pull-secret-syncer RoleBinding in openshift-config]      %% DaemonSet Deployment     GlobalPSController --&gt; |Deploys DaemonSet| DaemonSet[global-pull-secret-syncer DaemonSet]     DaemonSet --&gt; |Runs on each node| DaemonSetPod[DaemonSet Pod]      %% DaemonSet Pod Details     DaemonSetPod --&gt; |Mounts host paths| HostMounts[Host Path Mounts]     HostMounts --&gt; KubeletPath[\"/var/lib/kubelet\"]     HostMounts --&gt; DbusPath[\"/var/run/dbus\"]      %% Container Execution     DaemonSetPod --&gt; |Runs command| Container[control-plane-operator Container]     Container --&gt; |Executes| SyncCommand[sync-global-pullsecret command]      %% Sync Process     SyncCommand --&gt; |Watches global-pull-secret| SyncController[Global Pull Secret Reconciler]     SyncController --&gt; |Reads secret| ReadGlobalPS[Read global-pull-secret]     SyncController --&gt; |Reads original| ReadOriginalPS[Read original pull-secret]      %% File Update Process     ReadGlobalPS --&gt; |Gets data| GlobalPSBytes[Global Pull Secret Bytes]     ReadOriginalPS --&gt; |Gets data| OriginalPSBytes[Original Pull Secret Bytes]      %% Decision Logic     GlobalPSBytes --&gt; |If exists| UseGlobalPS[Use Global Pull Secret]     OriginalPSBytes --&gt; |If not exists| UseOriginalPS[Use Original Pull Secret]      %% File Update     UseGlobalPS --&gt; |Updates file| UpdateKubeletConfig[\"Update /var/lib/kubelet/config.json\"]     UseOriginalPS --&gt; |Updates file| UpdateKubeletConfig      %% Kubelet Restart     UpdateKubeletConfig --&gt; |Restarts kubelet| RestartKubelet[Restart kubelet.service via systemd]     RestartKubelet --&gt; |Via dbus| DbusConnection[DBus Connection]      %% Error Handling     UpdateKubeletConfig --&gt; |If restart fails| RollbackProcess[Rollback Process]     RollbackProcess --&gt; |Restore original| RestoreOriginal[Restore Original File Content]      %% Cleanup Process     GlobalPSController --&gt; |If additional PS deleted| CleanupProcess[Cleanup Process]     CleanupProcess --&gt; |Deletes global PS| DeleteGlobalPS[Delete global-pull-secret]     CleanupProcess --&gt; |Removes DaemonSet| RemoveDaemonSet[Remove DaemonSet]      %% Styling     classDef userInput fill:#e1f5fe     classDef controller fill:#f3e5f5     classDef secret fill:#e8f5e8     classDef process fill:#fff3e0     classDef daemonSet fill:#fce4ec     classDef fileSystem fill:#f1f8e9      class User,AdditionalPS userInput     class HCCO,GlobalPSController,SyncController controller     class OriginalPS,GlobalPSSecret,ServiceAccount,KubeSystemRole,KubeSystemRoleBinding,OpenshiftConfigRole,OpenshiftConfigRoleBinding secret     class ValidatePS,MergeSecrets,RBACSetup,UpdateKubeletConfig,RestartKubelet process     class DaemonSet,DaemonSetPod,Container daemonSet     class KubeletPath,DbusPath fileSystem"},{"location":"how-to/kubevirt/global-pull-secret/#key-features","title":"Key Features","text":"<ul> <li>Security: Only watches specific secrets in <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>Robustness: Includes automatic rollback in case of failures</li> <li>Efficiency</li> <li>Only updates when there are actual changes</li> <li>The globalPullSecret implementation has their own controller so it cannot interfere with the HCCO reonciliation</li> <li>Security considerations: Uses specific RBAC for only the required resources in each namespace. The DaemonSet containers run in privileged mode due to the need to:</li> <li>Write to <code>/var/lib/kubelet/config.json</code> (kubelet configuration file)</li> <li>Connect to systemd via DBus for service management</li> <li>Restart kubelet.service, which requires root privileges</li> <li>Smart node targeting: Automatically excludes nodes from InPlace NodePools to prevent MCD conflicts</li> </ul>"},{"location":"how-to/kubevirt/global-pull-secret/#inplace-nodepool-handling","title":"InPlace NodePool Handling","text":"<p>To prevent conflicts with Machine Config Daemon operations, the implementation includes intelligent node targeting:</p>"},{"location":"how-to/kubevirt/global-pull-secret/#node-labeling-process","title":"Node Labeling Process","text":"<ol> <li>MachineSets Discovery: The controller queries the management cluster for MachineSets with InPlace-specific annotations (<code>hypershift.openshift.io/nodePoolTargetConfigVersion</code>)</li> <li>Machine Enumeration: For each InPlace MachineSets, it lists all associated Machines</li> <li>Node Identification: Maps Machine objects to their corresponding nodes via <code>machine.Status.NodeRef.Name</code></li> <li>Labeling: Applies <code>hypershift.openshift.io/nodepool-inplace-strategy=true</code> label to identified nodes</li> </ol>"},{"location":"how-to/kubevirt/global-pull-secret/#daemonset-scheduling-configuration","title":"DaemonSet Scheduling Configuration","text":"<p>The DaemonSet uses NodeAffinity to exclude InPlace nodes:</p> <pre><code>spec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: hypershift.openshift.io/nodepool-inplace-strategy\n                operator: DoesNotExist\n</code></pre> <p>This ensures that: - Nodes without the label: \u2705 Are eligible for DaemonSet scheduling - Nodes with the label (any value): \u274c Are excluded from DaemonSet scheduling</p>"},{"location":"how-to/kubevirt/global-pull-secret/#conflict-prevention-benefits","title":"Conflict Prevention Benefits","text":"<ul> <li>Prevents MCD failures: Avoids conflicts when MCD expects specific kubelet configuration during InPlace upgrades</li> <li>Maintains upgrade reliability: InPlace upgrade processes are not interrupted by Global Pull Secret modifications</li> <li>Automatic detection: No manual intervention required - the system automatically identifies and handles InPlace nodes</li> </ul>"},{"location":"how-to/kubevirt/global-pull-secret/#error-handling","title":"Error Handling","text":"<p>The system includes comprehensive error handling:</p> <ul> <li>Validation errors: Invalid DockerConfigJSON format is caught early</li> <li>Restart failures: If kubelet restart fails after 3 attempts, the file is rolled back</li> <li>Resource cleanup: If the additional pull secret is deleted, the HCCO automatically removes the globalPullSecret</li> </ul> <p>This implementation provides a secure, autonomous solution that allows HostedCluster administrators to add private registry credentials without requiring Management Cluster administrator intervention.</p>"},{"location":"how-to/kubevirt/gpu-devices/","title":"GPU Devices","text":"<p>It is possible to attach one or more GPUs to a HCP KubeVirt clusters through the use of the NodePool api.</p>"},{"location":"how-to/kubevirt/gpu-devices/#prerequisites","title":"Prerequisites","text":"<p>Before a GPU device can be attached to a HCP KubeVirt NodePool, the GPU must first be exposed as a resource on the Node that the GPU resides on. For example this Nvidia Operator documentation outlines how to expose Nvidia GPUs as a resource on an OpenShift cluster. Once the GPU is exposed as an extended resource on the node, it can then be assigned to a NodePool. </p>"},{"location":"how-to/kubevirt/gpu-devices/#attaching-gpu-devices-to-nodepools-using-the-cli","title":"Attaching GPU devices to NodePools Using the CLI","text":"<p>Using the <code>hcp</code> cli, GPU devices can be attached to NodePools through the usage of the <code>--host-device-name</code> cli argument.</p> <p>This argument takes the device's resource name as it has been advertised on the infrastructure node. The argument also takes an optional count that represents the number of GPU devices to attach to each VM in the NodePool. By default, the count is <code>1</code>.</p> <p>In the example below, we are attaching 2 GPUs to a NodePool of size 3. This means each of the 3 VMs in the NodePool will have 2 GPUs each.</p> <pre><code>export CLUSTER_NAME=example\nexport PULL_SECRET=\"$HOME/pull-secret\"\nexport MEM=\"16Gi\"\nexport CPU=\"2\"\nexport WORKER_COUNT=\"3\"\n\nhcp create cluster kubevirt \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas $WORKER_COUNT \\\n--pull-secret $PULL_SECRET \\\n--memory $MEM \\\n--cores $CPU \\\n--host-device-name=\"nvidia-a100,count:2\"\n</code></pre> <p>Note that the <code>--host-device-name</code> argument can be specified multiple times in order to attach multiple devices of different types.</p>"},{"location":"how-to/kubevirt/gpu-devices/#attaching-gpu-devices-via-the-nodepool-api-directly","title":"Attaching GPU Devices via the NodePool API Directly","text":"<p>The same example provided above with the hcp cli can also be expressed directly using the NodePool API. Below is an example of what the yaml would look like to configure a NodePool with 3 replicas with 2 GPU devices for each replica. Take note of the <code>nodepool.spec.platform.kubevirt.hostDevices</code> section. The <code>hostDevices</code> are a list, meaning it is possible to attach multiple GPU devices of different types as well.</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: NodePool\nmetadata:\n  name: example\n  namespace: clusters\nspec:\n  arch: amd64\n  clusterName: example\n  management:\n    autoRepair: false\n    upgradeType: Replace\n  nodeDrainTimeout: 0s\n  nodeVolumeDetachTimeout: 0s\n  platform:\n    kubevirt:\n      attachDefaultNetwork: true\n      compute:\n        cores: 2\n        memory: 16Gi\n      hostDevices:\n      - count: 2\n        deviceName: nvidia-a100\n      networkInterfaceMultiqueue: Enable\n      rootVolume:\n        persistent:\n          size: 32Gi\n        type: Persistent\n    type: KubeVirt\n  replicas: 3\n</code></pre>"},{"location":"how-to/kubevirt/ingress-and-dns/","title":"Ingress and DNS configuration","text":"<p>By default, the HyperShift operator will configure the KubeVirt platform guest cluster's ingress and DNS behavior to reuse what is provided by the underlying infra cluster that the KubeVirt VMs are running on. This section describes that default behavior in greater detail as well as information on advanced usage options.</p>"},{"location":"how-to/kubevirt/ingress-and-dns/#default-ingress-and-dns-behavior","title":"Default Ingress and DNS Behavior","text":"<p>Every OpenShift cluster comes setup with a default application ingress controller which is expected to have an wildcard DNS record associated with it. By default, guest clusters created using the Hypershift KubeVirt provider will automatically become a subdomain of the underlying OCP cluster that the KubeVirt VMs run on.</p> <p>For example, if an OCP cluster has a default ingress DNS entry of <code>*.apps.mgmt-cluster.example.com</code>, then the default ingress of a KubeVirt guest cluster named <code>guest</code> running on that underlying OCP cluster will be <code>*.apps.guest.apps.mgmt-cluster.example.com</code>.</p> <p>Note</p> <p>For this default ingress DNS to work properly, the underlying cluster hosting the KubeVirt VMs must allow wildcard DNS routes. This can be configured using the following cli command. <code>oc patch ingresscontroller -n openshift-ingress-operator default --type=json -p '[{ \"op\": \"add\", \"path\": \"/spec/routeAdmission\", \"value\": {wildcardPolicy: \"WildcardsAllowed\"}}]'</code></p> <p>Note</p> <p>When using the default guest cluster ingress, connectivity is limited to HTTPS traffic over port 443. Plain HTTP traffic over port 80 will be rejected. This limitation only applies to the default ingress behavior and not the custom ingress behavior where manual creation of an ingress LoadBalancer and DNS is performed.</p>"},{"location":"how-to/kubevirt/ingress-and-dns/#customized-ingress-and-dns-behavior","title":"Customized Ingress and DNS Behavior","text":"<p>In lieu of the default ingress and DNS behavior, it is also possible to configure a Hypershift KubeVirt guest cluster with a unique base domain at creation time. This option does require some manual configuration steps during creation though.</p> <p>This process involves three steps:</p> <ol> <li>Cluster creation</li> <li>LoadBalancer creation</li> <li>Wildcard DNS configuration</li> </ol>"},{"location":"how-to/kubevirt/ingress-and-dns/#step-1-deploying-the-hostedcluster-specifying-our-base-domain","title":"Step 1 - Deploying the HostedCluster specifying our base domain","text":"<pre><code>export CLUSTER_NAME=example\nexport PULL_SECRET=\"$HOME/pull-secret\"\nexport MEM=\"6Gi\"\nexport CPU=\"2\"\nexport WORKER_COUNT=\"2\"\nexport BASE_DOMAIN=hypershift.lab\n\nhcp create cluster kubevirt \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas $WORKER_COUNT \\\n--pull-secret $PULL_SECRET \\\n--memory $MEM \\\n--cores $CPU \\\n--base-domain $BASE_DOMAIN\n</code></pre> <p>With above configuration we will end up having a HostedCluster with an ingress wildcard configured for <code>*.apps.example.hypershift.lab</code> (*.apps.&lt;hostedcluster_name&gt;.&lt;base_domain&gt;).</p> <p>This time, the HostedCluster will not finish the deployment (will remain in <code>Partial</code> progress) as we saw in the previous section, since we have configured a base domain we need to make sure that the required DNS records and load balancer are in-place:</p> <pre><code>oc get --namespace clusters hostedclusters\n\nNAME            VERSION   KUBECONFIG                       PROGRESS   AVAILABLE   PROGRESSING   MESSAGE\nexample                   example-admin-kubeconfig         Partial    True        False         The hosted control plane is available\n</code></pre> <p>If we access the HostedCluster this is what we will see:</p> <pre><code>hcp create kubeconfig --name $CLUSTER_NAME &gt; $CLUSTER_NAME-kubeconfig\n</code></pre> <pre><code>oc --kubeconfig $CLUSTER_NAME-kubeconfig get co\n\nNAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nconsole                                    4.14.0    False       False         False      30m     RouteHealthAvailable: failed to GET route (https://console-openshift-console.apps.example.hypershift.lab): Get \"https://console-openshift-console.apps.example.hypershift.lab\": dial tcp: lookup console-openshift-console.apps.example.hypershift.lab on 172.31.0.10:53: no such host\n.\n.\n.\ningress                                    4.14.0    True        False         True       28m     The \"default\" ingress controller reports Degraded=True: DegradedConditions: One or more other status conditions indicate a degraded state: CanaryChecksSucceeding=False (CanaryChecksRepetitiveFailures: Canary route checks for the default ingress controller are failing)\n</code></pre> <p>In the next section we will fix that.</p>"},{"location":"how-to/kubevirt/ingress-and-dns/#step-2-set-up-the-loadbalancer","title":"Step 2 - Set up the LoadBalancer","text":"<p>Note</p> <p>If your cluster is on bare-metal you may need MetalLB to be able to provision functional LoadBalancer services. Take a look at the section Optional MetalLB Configuration Steps.</p> <p>This option requires configuring a new LoadBalancer service that routes to the KubeVirt VMs as well as assign a wildcard DNS entry to the LoadBalancer's IP address.</p> <p>First, we need to create a LoadBalancer Service that routes ingress traffic to the KubeVirt VMs.</p> <p>A NodePort Service exposing the HostedCluster ingress already exists, we will grab the NodePorts and create the LoadBalancer service targeting these ports.</p> <ol> <li> <p>Grab NodePorts</p> <pre><code>export HTTP_NODEPORT=$(oc --kubeconfig $CLUSTER_NAME-kubeconfig get services -n openshift-ingress router-nodeport-default -o jsonpath='{.spec.ports[?(@.name==\"http\")].nodePort}')\nexport HTTPS_NODEPORT=$(oc --kubeconfig $CLUSTER_NAME-kubeconfig get services -n openshift-ingress router-nodeport-default -o jsonpath='{.spec.ports[?(@.name==\"https\")].nodePort}')\n</code></pre> </li> <li> <p>Create LoadBalancer Service</p> <pre><code>cat &lt;&lt; EOF | oc apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: $CLUSTER_NAME\n  name: $CLUSTER_NAME-apps\n  namespace: clusters-$CLUSTER_NAME\nspec:\n  ports:\n  - name: https-443\n    port: 443\n    protocol: TCP\n    targetPort: ${HTTPS_NODEPORT}\n  - name: http-80\n    port: 80\n    protocol: TCP\n    targetPort: ${HTTP_NODEPORT}\n  selector:\n    kubevirt.io: virt-launcher\n  type: LoadBalancer\nEOF\n</code></pre> </li> </ol>"},{"location":"how-to/kubevirt/ingress-and-dns/#step-3-set-up-a-wildcard-dns-record-for-the-apps","title":"Step 3 - Set up a wildcard DNS record for the <code>*.apps</code>","text":"<p>Now that we have the ingress exposed, next step is configure a wildcard DNS A record or CNAME that references the LoadBalancer Service's external IP.</p> <ol> <li>Get the external IP.</li> </ol> <pre><code>export EXTERNAL_IP=$(oc -n clusters-$CLUSTER_NAME get service $CLUSTER_NAME-apps -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n</code></pre> <ol> <li>Configure a wildcard <code>*.apps.&lt;hostedcluster_name\\&gt;.&lt;base_domain\\&gt;.</code> DNS entry referencing the IP stored in $EXTERNAL_IP that is routable both internally and externally of the cluster.</li> </ol> <p>For example, for the cluster used in this example and for an external ip value of <code>192.168.20.30</code> this is what DNS resolutions will look like:</p> <pre><code>dig +short test.apps.example.hypershift.lab\n\n192.168.20.30\n</code></pre>"},{"location":"how-to/kubevirt/ingress-and-dns/#checking-hostedcluster-status-after-having-fixed-the-ingress","title":"Checking HostedCluster status after having fixed the ingress","text":"<p>Now that we fixed the ingress, we should see our HostedCluster progress moved from <code>Partial</code> to <code>Completed</code>.</p> <pre><code>oc get --namespace clusters hostedclusters\n\nNAME            VERSION   KUBECONFIG                       PROGRESS    AVAILABLE   PROGRESSING   MESSAGE\nexample         4.14.0    example-admin-kubeconfig         Completed   True        False         The hosted control plane is available\n</code></pre>"},{"location":"how-to/kubevirt/ingress-and-dns/#optional-metallb-configuration-steps","title":"Optional MetalLB Configuration Steps","text":"<p>LoadBalancer type services are required. If MetalLB is in use, here are some example steps outlining how to configure MetalLB after installing MetalLB using CLI.</p> <ol> <li> <p>Create a MetalLB instance:</p> <pre><code>oc create -f - &lt;&lt;EOF\napiVersion: metallb.io/v1beta1\nkind: MetalLB\nmetadata:\n  name: metallb\n  namespace: metallb-system\nEOF\n</code></pre> </li> <li> <p>Create address pool with an available range of IP addresses within the node network:</p> <pre><code>oc create -f - &lt;&lt;EOF\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: metallb\n  namespace: metallb-system\nspec:\n  addresses:\n  - 192.168.216.32-192.168.216.122\nEOF\n</code></pre> </li> <li> <p>Advertise the address pool using L2 protocol:</p> <pre><code>oc create -f - &lt;&lt;EOF\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: l2advertisement\n  namespace: metallb-system\nspec:\n  ipAddressPools:\n   - metallb\nEOF\n</code></pre> </li> </ol>"},{"location":"how-to/kubevirt/performance-tuning/","title":"Performance Tuning","text":"<p>By default, KubeVirt VMs use best effort resource requests for CPU. This means  it\u2019s possible for a VM to have overcommitted CPU which are in competition with other resources on the host machine. While best effort CPU will provide the best utilization of resources on a node, it could impact performance of the VM in high density applications.</p>"},{"location":"how-to/kubevirt/performance-tuning/#guaranteed-cpu-qos","title":"Guaranteed CPU QOS","text":"<p>To avoid overcommitment of CPU, the KubeVirt NodePool can request guaranteed CPU qos, which will guarantee unshared CPU resources to the KubeVirt VMs. Below is an example of using the hcp command line tool to request a KubeVirt platform guest cluster with guaranteed CPU by using the --qos-class argument.</p> <pre><code>export CLUSTER_NAME=example\nexport PULL_SECRET=\"$HOME/pull-secret\"\nexport MEM=\"6Gi\"\nexport CPU=\"2\"\nexport WORKER_COUNT=\"2\"\n\nhcp create cluster kubevirt \\\n--name $CLUSTER_NAME \\\n--node-pool-replicas $WORKER_COUNT \\\n--pull-secret $PULL_SECRET \\\n--memory $MEM \\\n--cores $CPU \\\n--qos-class Guaranteed\n</code></pre>"},{"location":"how-to/kubevirt/troubleshooting-kubevirt-cluster/","title":"Troubleshooting a KubeVirt cluster","text":"<p>Troubleshooting why a Hosted Control Plane with the KubeVirt platform is not coming fully online should involve starting at the top level HostedCluster and NodePool resources, and then working down through the stack to gain a more detailed understanding of what has occurred until a root cause is found.</p> <p>Below are some steps to help guide that progression and discover the root cause for the most common classes of issues encountered with the HCP KubeVirt platform.</p>"},{"location":"how-to/kubevirt/troubleshooting-kubevirt-cluster/#hostedcluster-stuck-in-partial-state","title":"HostedCluster Stuck in Partial State","text":"<ul> <li>Ensure all prerequisites for the HCP KubeVirt platform are met</li> <li>Look at conditions on HostedCluster and NodePool objects for validation errors that prevent progress from being made.</li> <li>Using the kubeconfig of the guest cluster, inspect the guest cluster\u2019s status. Look at the status of the <code>oc get clusteroperators</code> output to see what cluster operators are pending. Look at the node status <code>oc get nodes</code> to ensure the worker nodes are in a Ready state.</li> </ul>"},{"location":"how-to/kubevirt/troubleshooting-kubevirt-cluster/#hcp-has-no-worker-nodes-registered","title":"HCP has no worker nodes registered","text":"<ul> <li>Look at HostedCluster and NodePool conditions for failures that indicate what the problem could be.</li> <li>Look at the KubeVirt worker node VM status for the NodePool. <code>oc get vm -n &lt;namespace&gt;</code></li> <li>If the VMs are stuck in provisioning, look at the CDI import pods within the VM\u2019s namespace for clues as to why the importer pods have not completed. <code>oc get pods -n &lt;namespace&gt; | grep \"import\"</code></li> <li>If the VMs are stuck in starting, look to see what the status of the virt-launcher pods are. <code>oc get pods -n &lt;namespace&gt; -l kubevirt.io=virt-launcher</code>. If the virt-launcher pods are in a pending state, determine why the pods are not being scheduled. For example, there could be a lack of resources to run the virt-launcher pods.</li> <li>If the VMs are running but they have not registered as worker nodes, use the web console to gain VNC access to one of the impacted VMs. The VNC output should indicate that the ignition config was applied. If the VM is unable to access the HCP ignition server on startup, that will prevent the VM from being provisioned correctly.</li> <li>If the ignition config has been applied but the VM is still not registering as a node, Proceed to the Fetching VM Bootstrap logs section to gain access to the VM console logs during boot.</li> </ul>"},{"location":"how-to/kubevirt/troubleshooting-kubevirt-cluster/#hcp-worker-nodes-stuck-in-notready-state","title":"HCP Worker Nodes Stuck in NotReady State","text":"<ul> <li>During cluster creation, nodes will enter the \"NotReady\" state temporarily while the networking stack is rolling out. This is a part of normal operation. If this time period takes longer than 10-15 minutes, then it's possible an issue has occurred that needs investigation.</li> <li>Look at the conditions on the node object to determine why the node is not ready. <code>oc get nodes -o yaml</code></li> <li>Look for failing pods within the cluster <code>oc get pods -A --field-selector=status.phase!=Running,status.phase!=Succeeded</code></li> </ul>"},{"location":"how-to/kubevirt/troubleshooting-kubevirt-cluster/#ingress-and-console-cluster-operators-are-not-coming-online","title":"Ingress and Console cluster operators are not coming online","text":"<ul> <li>If the cluster is using the default ingress behavior, ensure that wildcard DNS routes are enabled on the OCP cluster the VMs are hosted on. <code>oc patch ingresscontroller -n openshift-ingress-operator default --type=json -p '[{ \"op\": \"add\", \"path\": \"/spec/routeAdmission\", \"value\": {wildcardPolicy: \"WildcardsAllowed\"}}]'</code></li> <li>If a custom base domain is used for the HCP, double check that the Load Balancer is targeting the VM pods accurately, and make sure the wildcard DNS entry is targeting the Load Balancer IP.</li> </ul>"},{"location":"how-to/kubevirt/troubleshooting-kubevirt-cluster/#guest-cluster-load-balancer-services-are-not-becoming-available","title":"Guest Cluster Load Balancer services are not becoming available","text":"<ul> <li>Look for events and details associated with the Load Balancer service within the guest cluster</li> <li>Load Balancers for the guest cluster are by default handled by the kubevirt-cloud-controller-manager within the HCP namespace. Ensure the kccm pod is online and look at the pod\u2019s logs for errors or warnings. The kccm pod can be identified in the HCP namespace with this command <code>oc get pods -n &lt;hcp namespace&gt; -l app=cloud-controller-manager</code></li> </ul>"},{"location":"how-to/kubevirt/troubleshooting-kubevirt-cluster/#guest-cluster-pvcs-are-not-available","title":"Guest Cluster PVCs are not available","text":"<ul> <li>Look for events and details associated with the PVC to understand what errors are occurring</li> <li>If the PVC is failing to attach to a Pod, look at the logs for the kubevirt-csi-node daemonset component within the guest to gain further insight into why that might be occurring. The kubevirt-csi-node pods can be identified for each node with this command <code>oc get pods -n openshift-cluster-csi-drivers -o wide -l app=kubevirt-csi-driver</code></li> <li>If the PVC is unable to bind to a PV, look at the logs of the kubevirt-csi-controller component within the HCP namespace. The kubevirt-csi-controller pod can be identified within the hcp namespace with this command <code>oc get pods -n &lt;hcp namespace&gt; -l app=kubevirt-csi-driver</code></li> </ul>"},{"location":"how-to/kubevirt/troubleshooting-kubevirt-cluster/#fetching-vm-bootstrap-logs","title":"Fetching VM bootstrap logs","text":"<p>The VM console logs are really useful to troubleshoot issues when the HyperShift/KubeVirt VM nodes are not correctly joining the cluster. KubeVirt v1.1.0 will expose logs from the serial console of guest VMs in a k8s native way (<code>kubectl logs -n &lt;namespace&gt; &lt;vmi_pod&gt; -c guest-console-log</code>) but his is still not available with KubeVirt v1.0.0.</p> <p>On KubeVirt v1.0.0 you can use a helper script to stream them from interactive console executed in background.</p> <pre><code>#!/bin/bash\nHC_NAMESPACE=\"${HC_NAMESPACE:-clusters}\"\nNAME=$1\n\nif [[ -z \"${NAME}\" ]]\nthen\n    echo \"Please specify the name of the guest cluster.\"\n    exit 1\nfi\n\nVMNS=\"${HC_NAMESPACE}\"-\"${NAME}\"\nREPLICAS=$(oc get NodePool -n \"${HC_NAMESPACE}\" \"${NAME}\" -o=jsonpath='{.spec.replicas}')\nPLATFORMTYPE=$(oc get NodePool -n \"${HC_NAMESPACE}\" \"${NAME}\" -o=jsonpath='{.spec.platform.type}')\nINFRAID=$(oc get HostedCluster -n \"${HC_NAMESPACE}\" \"${NAME}\" -o=jsonpath='{.spec.infraID}')\n\nif [[ \"${PLATFORMTYPE}\" != \"KubeVirt\" ]]; then\n    echo \"This tool is designed for the KubeVirt provider.\"\n    exit 1\nfi\n\nif ! which tmux &gt;/dev/null 2&gt;&amp;1;\nthen\n    echo \"this tool requires tmux, please install it.\"\n    exit 1\nfi\n\nVMNAMES=()\n\nwhile [[ ${#VMNAMES[@]} &lt; ${REPLICAS}  ]]; do\n  for VMNAME in $(oc get vmi -n \"${VMNS}\" -l hypershift.openshift.io/infra-id=\"${INFRAID}\" -o name 2&gt;/dev/null); do\n    SVMNAME=${VMNAME/virtualmachineinstance.kubevirt.io\\//}\n    if ! [[ \" ${VMNAMES[*]} \" =~ ${SVMNAME} ]]; then\n       VMNAMES+=(\"${SVMNAME}\")\n       tmux new-session -s \"${SVMNAME}\" -d \"virtctl console --timeout 30 -n ${VMNS} ${SVMNAME} | tee -a ${VMNS}_${SVMNAME}.log\"\n       echo \"logs for VM ${SVMNAME} will be appended to ${VMNS}_${SVMNAME}.log\"\n    fi\n  done\n  sleep 3\ndone\n\necho \"Log collection will continue in background while the VMs are running.\"\necho \"Please avoid trying to directly connect to VM console with 'virtctl console' to avoid hijacking open sessions:\"\necho \"you can instead use 'tmux attach -t &lt;vmname&gt;' to reach open session, this will not break file logging.\"\n</code></pre> <p>You can locally save the script and execute it as <code>hypershift_kv_log.sh &lt;guestclustername&gt;</code> as soon as you created the <code>hostedcluster</code> object (the script will not collect logs from the past). The script will loop until all the expected VMs are created and then log collection will continue in background in <code>tmux</code> sessions until the VMs are live. Please avoid directly connecting to VM console with <code>virtctl console</code> to avoid hijacking open sessions braking the logging, you can instead use <code>tmux attach -t &lt;vmname&gt;</code> and interactively use the serial console from there. If the namespace used on the hosted cluster is not named <code>clusters</code>, a custom value could be set with the <code>HC_NAMESPACE</code> env variable.</p>"},{"location":"how-to/none/create-none-cluster/","title":"Create a None cluster","text":"<p>This document explains how to create HostedClusters and NodePools using the 'None' platform to create bare metal worker nodes.</p>"},{"location":"how-to/none/create-none-cluster/#hypershift-operator-requirements","title":"Hypershift Operator requirements","text":"<ul> <li>cluster-admin access to an OpenShift Cluster (We tested it with 4.9.17) to deploy the CRDs + operator</li> <li>1 x filesystem type Persistent Volume to store the <code>etcd</code> database for demo purposes (3x for 'production' environments)</li> </ul>"},{"location":"how-to/none/create-none-cluster/#versions-used","title":"Versions used","text":"<ul> <li>OCP compact cluster (3 masters) version 4.9.17</li> <li>HyperShift Operator built from sources (Commit ID 0371f889)</li> </ul>"},{"location":"how-to/none/create-none-cluster/#prerequisites-building-the-hypershift-operator","title":"Prerequisites: Building the Hypershift Operator","text":"<p>Currently, the HyperShift operator is deployed using the <code>hypershift</code> binary, which needs to be compiled manually. RHEL8 doesn't include go1.18 officially but it can be installed via <code>gvm</code> by following the next steps:</p> <pre><code># Install prerequisites\nsudo dnf install -y curl git make bison gcc glibc-devel\ngit clone https://github.com/openshift/hypershift.git\npushd hypershift\n\n# Install gvm to install go 1.18\nbash &lt; &lt;(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer)\nsource ${HOME}/.gvm/scripts/gvm\ngvm install go1.18\ngvm use go1.18\n\n# build the binary\nmake hypershift\npopd\n</code></pre> <p>Then, the <code>hypershift</code> binary can be moved to a convenient place as:</p> <pre><code>sudo install -m 0755 -o root -g root hypershift/bin/hypershift /usr/local/bin/hypershift\n</code></pre> <p>Alternatively, it can be compiled using a container as:</p> <pre><code># Install prerequisites\nsudo dnf install podman -y\n# Compile hypershift\nmkdir -p ./tmp/ &amp;&amp; \\\npodman run -it -v ${PWD}/tmp:/var/tmp/hypershift-bin/:Z --rm docker.io/golang:1.18 sh -c \\\n  'git clone --depth 1 https://github.com/openshift/hypershift.git /var/tmp/hypershift/ &amp;&amp; \\\n  cd /var/tmp/hypershift &amp;&amp; \\\n  make hypershift &amp;&amp; \\\n  cp bin/hypershift /var/tmp/hypershift-bin/'\nsudo install -m 0755 -o root -g root ./tmp/hypershift /usr/local/bin/hypershift\n</code></pre> <p>WARNING: At the time of writing this document, there were some issues already fixed in HyperShift but unfortunately those weren't included in the latest release of the container.</p>"},{"location":"how-to/none/create-none-cluster/#prerequisite-optional-create-a-custom-hypershift-image","title":"Prerequisite (optional): Create a custom HyperShift image","text":"<p>The official container image containing the HyperShift bits is hosted in quay.io/hypershift/hypershift but if creating a custom HyperShift image is needed, the following steps can be performed:</p> <pre><code>QUAY_ACCOUNT='testuser'\npodman login -u ${QUAY_ACCOUNT} -p testpassword quay.io\nsudo dnf install -y curl git make bison gcc glibc-devel\ngit clone https://github.com/openshift/hypershift.git\npushd hypershift\n\n# Install gvm to install go 1.18\nbash &lt; &lt;(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer)\nsource ${HOME}/.gvm/scripts/gvm\ngvm install go1.18 -B\ngvm use go1.18\n\n# Build the binaries and the container\nmake build\nmake RUNTIME=podman IMG=quay.io/${QUAY_ACCOUNT}/hypershift:latest docker-build docker-push\n\nsudo install -m 0755 -o root -g root hypershift/bin/hypershift /usr/local/bin/hypershift\npopd\n</code></pre>"},{"location":"how-to/none/create-none-cluster/#deploy-hypershift","title":"Deploy HyperShift","text":"<p>Once the binary is in place, the operator deployment is performed as:</p> <pre><code>hypershift install\n</code></pre> <p>Or if using a custom image:</p> <pre><code>hypershift install --hypershift-image quay.io/${QUAY_ACCOUNT}/hypershift:latest\n</code></pre> <p>Using <code>hypershift install --render &gt; hypershift-install.yaml</code> will create a yaml file with all the assets required to deploy HyperShift, then they can be applied as:</p> <pre><code>oc apply -f ./hypershift-install.yaml\n</code></pre>"},{"location":"how-to/none/create-none-cluster/#deploy-a-hosted-cluster","title":"Deploy a hosted cluster","text":"<p>There are two main CRDs to describe a hosted cluster:</p> <ul> <li> <p><code>HostedCluster</code> defines the control plane hosted in the management OpenShift</p> </li> <li> <p><code>NodePool</code> defines the nodes that will be created/attached to a hosted cluster</p> </li> </ul> <p>The <code>hostedcluster.spec.platform</code> specifies the underlying infrastructure provider for the cluster and is used to configure platform specific behavior, so depending on the environment it is required to configure it properly.</p> <p>In this repo we will cover the 'none' provider.</p>"},{"location":"how-to/none/create-none-cluster/#requisites","title":"Requisites","text":"<ul> <li>Proper DNS entries for the workers (if the worker uses <code>localhost</code> it won't work)</li> <li>DNS entry to point <code>api.${cluster}.${domain}</code> to each of the nodes where the hostedcluster will be running. This is because the hosted cluster API is exposed as a <code>nodeport</code>. For example:</li> </ul> <pre><code>api.hosted0.example.com.  IN A  10.19.138.32\napi.hosted0.example.com.  IN A  10.19.138.33\napi.hosted0.example.com.  IN A  10.19.138.37\n</code></pre> <ul> <li> <p>DNS entry to point <code>*.apps.${cluster}.${domain}</code> to a load balancer deployed to redirect incoming traffic to the ingresses pod the OpenShift documentation provides some instructions about this)</p> <p>NOTE: This is not strictly required to deploy a sample cluster but to access the exposed routes there. Also, it can be simply an A record pointing to a worker IP where the ingress pods are running and enabling the <code>hostedcluster.spec.infrastructureAvailabilityPolicy: SingleReplica</code> configuration parameter.</p> </li> <li> <p>Pull-secret (available at cloud.redhat.com)</p> </li> <li>ssh public key already available (it can be created as <code>ssh-keygen -t rsa -f /tmp/sshkey -q -N \"\"</code>)</li> <li>Any httpd server available to host a ignition file (text) and a modified RHCOS iso</li> </ul>"},{"location":"how-to/none/create-none-cluster/#procedure","title":"Procedure","text":"<ul> <li>Create a file containing all the variables depending on the environment:</li> </ul> <pre><code>cat &lt;&lt;'EOF' &gt; ./myvars\nexport CLUSTERS_NAMESPACE=\"clusters\"\nexport HOSTED=\"hosted0\"\nexport HOSTED_CLUSTER_NS=\"clusters-${HOSTED}\"\nexport PULL_SECRET_NAME=\"${HOSTED}-pull-secret\"\nexport MACHINE_CIDR=\"10.19.138.0/24\"\nexport OCP_RELEASE_VERSION=\"4.9.17\"\nexport OCP_ARCH=\"x86_64\"\nexport BASEDOMAIN=\"example.com\"\n\nexport PULL_SECRET_CONTENT=$(cat ~/clusterconfigs/pull-secret.txt)\nexport SSH_PUB=$(cat ~/.ssh/id_rsa.pub)\nEOF\nsource ./myvars\n</code></pre> <ul> <li>Create a namespace to host the HostedCluster and secrets</li> </ul> <pre><code>envsubst &lt;&lt;\"EOF\" | oc apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n name: ${CLUSTERS_NAMESPACE}\nEOF\n\nexport PS64=$(echo -n ${PULL_SECRET_CONTENT} | base64 -w0)\nenvsubst &lt;&lt;\"EOF\" | oc apply -f -\napiVersion: v1\ndata:\n .dockerconfigjson: ${PS64}\nkind: Secret\nmetadata:\n name: ${PULL_SECRET_NAME}\n namespace: ${CLUSTERS_NAMESPACE}\ntype: kubernetes.io/dockerconfigjson\nEOF\n\nenvsubst &lt;&lt;\"EOF\" | oc apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: ${HOSTED}-ssh-key\n  namespace: ${CLUSTERS_NAMESPACE}\nstringData:\n  id_rsa.pub: ${SSH_PUB}\nEOF\n</code></pre> <ul> <li>Create the <code>hostedcluster</code> and the <code>nodepool</code> objects:</li> </ul> <pre><code>envsubst &lt;&lt;\"EOF\" | oc apply -f -\napiVersion: hypershift.openshift.io/v1alpha1\nkind: HostedCluster\nmetadata:\n  name: ${HOSTED}\n  namespace: ${CLUSTERS_NAMESPACE}\nspec:\n  release:\n    image: \"quay.io/openshift-release-dev/ocp-release:${OCP_RELEASE_VERSION}-${OCP_ARCH}\"\n  pullSecret:\n    name: ${PULL_SECRET_NAME}\n  sshKey:\n    name: \"${HOSTED}-ssh-key\"\n  networking:\n    serviceCIDR: \"172.31.0.0/16\"\n    podCIDR: \"10.132.0.0/14\"\n    machineCIDR: \"${MACHINE_CIDR}\"\n  platform:\n    type: None\n  infraID: ${HOSTED}\n  dns:\n    baseDomain: ${BASEDOMAIN}\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: api.${HOSTED}.${BASEDOMAIN}\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: api.${HOSTED}.${BASEDOMAIN}\n      type: NodePort\n  - service: OIDC\n    servicePublishingStrategy:\n      nodePort:\n        address: api.${HOSTED}.${BASEDOMAIN}\n      type: None\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: api.${HOSTED}.${BASEDOMAIN}\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: api.${HOSTED}.${BASEDOMAIN}\n      type: NodePort\nEOF\n\nenvsubst &lt;&lt;\"EOF\" | oc apply -f -\napiVersion: hypershift.openshift.io/v1alpha1\nkind: NodePool\nmetadata:\n  name: ${HOSTED}-workers\n  namespace: ${CLUSTERS_NAMESPACE}\nspec:\n  clusterName: ${HOSTED}\n  replicas: 0\n  management:\n    autoRepair: false\n    upgradeType: Replace\n  platform:\n    type: None\n  release:\n    image: \"quay.io/openshift-release-dev/ocp-release:${OCP_RELEASE_VERSION}-${OCP_ARCH}\"\nEOF\n</code></pre> <p>NOTE: The <code>HostedCluster</code> and <code>NodePool</code> objects can be created using the <code>hypershift</code> binary as <code>hypershift create cluster</code>. See the <code>hypershift create cluster -h</code> output for more information.</p> <p>After a while, a number of pods will be created in the <code>${CLUSTERS_NAMESPACE}</code> namespace. Those pods are the control plane of the hosted cluster.</p> <pre><code>oc get pods -n ${HOSTED_CLUSTER_NS}\n\nNAME                                              READY   STATUS     RESTARTS        AGE\ncatalog-operator-54d47cbbdb-29mzf                 2/2     Running    0               6m24s\ncertified-operators-catalog-78db79f86-6hlk9       1/1     Running    0               6m30s\ncluster-api-655c8ff4fb-598zs                      1/1     Running    1 (5m57s ago)   7m26s\ncluster-autoscaler-86d9474fcf-rmwzr               0/1     Running    0               6m11s\ncluster-policy-controller-bf87c9858-nnlgw         1/1     Running    0               6m37s\ncluster-version-operator-ff9475794-dc9hf          2/2     Running    0               6m37s\ncommunity-operators-catalog-6f5797cdc4-2hlcp      1/1     Running    0               6m29s\ncontrol-plane-operator-749b94cf54-p2lg2           1/1     Running    0               7m23s\netcd-0                                            1/1     Running    0               6m46s\nhosted-cluster-config-operator-6646d8f868-h9r2w   0/1     Running    0               6m34s\nignition-server-7797c5f7-vkb2b                    1/1     Running    0               7m20s\ningress-operator-5dc47b99b7-jttpg                 0/2     Init:0/1   0               6m35s\nkonnectivity-agent-85f979fcb4-67c5h               1/1     Running    0               6m45s\nkonnectivity-server-576dc7b8b7-rxgms              1/1     Running    0               6m46s\nkube-apiserver-66d99fd9fb-dvslc                   2/2     Running    0               6m43s\nkube-controller-manager-68dd9fb75f-mgd22          1/1     Running    0               6m42s\nkube-scheduler-748d9f5bcb-mlk52                   0/1     Running    0               6m42s\nmachine-approver-c8c68ffb9-psc6n                  0/1     Running    0               6m11s\noauth-openshift-7fc7dc9c66-fg258                  1/1     Running    0               6m8s\nolm-operator-54d7d78b89-f9dng                     2/2     Running    0               6m22s\nopenshift-apiserver-64b4669d54-ffpw2              2/2     Running    0               6m41s\nopenshift-controller-manager-7847ddf4fb-x5659     1/1     Running    0               6m38s\nopenshift-oauth-apiserver-554c449b8f-lk97w        1/1     Running    0               6m41s\npackageserver-6fd9f8479-pbvzl                     0/2     Init:0/1   0               6m22s\nredhat-marketplace-catalog-8cc88f5cb-hbxv9        1/1     Running    0               6m29s\nredhat-operators-catalog-b749d6945-2bx8k          1/1     Running    0               6m29s\n</code></pre> <p>The hosted cluster's kubeconfig can be extracted as:</p> <pre><code>oc extract -n ${CLUSTERS_NAMESPACE} secret/${HOSTED}-admin-kubeconfig --to=- &gt; ${HOSTED}-kubeconfig\noc get clusterversion --kubeconfig=${HOSTED}-kubeconfig\n</code></pre>"},{"location":"how-to/none/create-none-cluster/#adding-a-bare-metal-worker","title":"Adding a bare metal worker","text":"<ul> <li>Download the RHCOS live ISO into the httpd server (for example into <code>/var/www/html/hypershift-none/</code> on an apache server hosted at www.example.com)</li> </ul> <pre><code>mkdir -p /var/www/html/hypershift-none/\ncurl -s -o /var/www/html/hypershift-none/rhcos-live.x86_64.iso https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.9/4.9.0/rhcos-live.x86_64.iso\n</code></pre> <ul> <li>Download the ignition generated in the hostedcluster</li> </ul> <pre><code>IGNITION_ENDPOINT=$(oc get hc ${HOSTED} -o json | jq -r '.status.ignitionEndpoint')\nIGNITION_TOKEN_SECRET=$(oc -n clusters-${HOSTED} get secret | grep token-${HOSTED}  | awk '{print $1}')\nset +x\nIGNITION_TOKEN=$(oc -n clusters-${HOSTED} get secret ${IGNITION_TOKEN_SECRET} -o jsonpath={.data.token})\ncurl -s -k -H \"Authorization: Bearer ${IGNITION_TOKEN}\" https://${IGNITION_ENDPOINT}/ignition &gt; /var/www/html/hypershift-none/worker.ign\n</code></pre> <ul> <li>Modify the RHCOS live ISO to install the worker using that ignition file into the <code>/dev/sda</code> device (your milleage may vary)</li> </ul> <pre><code>podman run --rm -it -v /var/www/html/hypershift-none/:/data:z --workdir /data quay.io/coreos/coreos-installer:release iso customize --live-karg-append=\"coreos.inst.ignition_url=http://www.example.com/hypershift-none/worker.ign coreos.inst.install_dev=/dev/sda\" -o ./rhcos.iso ./rhcos-live.x86_64.iso\npodman run --rm -it -v /var/www/html/hypershift-none/:/data:z --workdir /data quay.io/coreos/coreos-installer:release iso kargs show ./rhcos.iso\nchmod a+r /var/www/html/hypershift-none/rhcos.iso\n</code></pre> <ul> <li>(Optionally) Check the ISO can be downloaded</li> </ul> <pre><code>curl -v -o rhcos.iso http://www.example.com/hypershift-none/rhcos.iso\n</code></pre> <ul> <li>Attach the ISO to the BMC and boot from it once.</li> </ul> <p>This step is highly dependent on the hardware used. In this example, using Dell hardware, the following steps can be done, but your milleage may vary:</p> <pre><code>export IDRACIP=10.19.136.22\nexport IDRACUSER=\"root\"\nexport IDRACPASS=\"calvin\"\n\ncurl -s -L -k https://raw.githubusercontent.com/dell/iDRAC-Redfish-Scripting/master/Redfish%20Python/SetNextOneTimeBootVirtualMediaDeviceOemREDFISH.py -O\ncurl -s -L -k https://raw.githubusercontent.com/dell/iDRAC-Redfish-Scripting/master/Redfish%20Python/InsertEjectVirtualMediaREDFISH.py -O\ncurl -s -L -k https://raw.githubusercontent.com/dell/iDRAC-Redfish-Scripting/master/Redfish%20Python/GetSetPowerStateREDFISH.py -O\n\n# Turn the server off\npython3 ./SetPowerStateREDFISH.py -ip ${IDRACIP} -u ${IDRACUSER} -p ${IDRACPASS} -r Off\n# Insert the ISO as virtual media\npython3 ./InsertEjectVirtualMediaREDFISH.py -ip ${IDRACIP} -u ${IDRACUSER} -p ${IDRACPASS} -o 1 -d 1 -i http://www.example.com/hypershift-none/rhcos.iso\n# Set boot once using the Virtual media previously attached\npython3 ./SetNextOneTimeBootVirtualMediaDeviceOemREDFISH.py -ip ${IDRACIP} -u ${IDRACUSER} -p ${IDRACPASS} -d 1 -r y\n</code></pre> <p>After a while, the worker will be installed.</p> <ul> <li>Sign the CSR:</li> </ul> <pre><code>oc get csr --kubeconfig=${HOSTED}-kubeconfig -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{\"\\n\"}}{{end}}{{end}}' | xargs oc adm certificate approve --kubeconfig=${HOSTED}-kubeconfig\n</code></pre> <ul> <li>Then the worker is added to the cluster:</li> </ul> <pre><code>oc get nodes --kubeconfig=${HOSTED}-kubeconfig\nNAME                                         STATUS   ROLES    AGE   VERSION\nkni1-worker-0.cloud.lab.eng.bos.redhat.com   Ready    worker   28m   v1.22.3+e790d7f\n\noc get co --kubeconfig=${HOSTED}-kubeconfig\nNAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nconsole                                    4.9.17    True        False         False      17m\ncsi-snapshot-controller                    4.9.17    True        False         False      24m\ndns                                        4.9.17    True        False         False      24m\nimage-registry                             4.9.17    False       False         False      4m49s   NodeCADaemonAvailable: The daemon set node-ca does not have available replicas...\ningress                                    4.9.17    True        False         False      14m\nkube-apiserver                             4.9.17    True        False         False      3h45m\nkube-controller-manager                    4.9.17    True        False         False      3h45m\nkube-scheduler                             4.9.17    True        False         False      3h45m\nkube-storage-version-migrator              4.9.17    True        False         False      24m\nmonitoring                                           False       True          True       9m      Rollout of the monitoring stack failed and is degraded. Please investigate the degraded status error.\nnetwork                                    4.9.17    True        False         False      25m\nnode-tuning                                4.9.17    True        False         False      24m\nopenshift-apiserver                        4.9.17    True        False         False      3h45m\nopenshift-controller-manager               4.9.17    True        False         False      3h45m\nopenshift-samples                          4.9.17    True        False         False      23m\noperator-lifecycle-manager                 4.9.17    True        False         False      3h45m\noperator-lifecycle-manager-catalog         4.9.17    True        False         False      3h45m\noperator-lifecycle-manager-packageserver   4.9.17    True        False         False      3h45m\nservice-ca                                 4.9.17    True        False         False      25m\nstorage                                    4.9.17    True        False         False      25m\n\noc get clusterversion --kubeconfig=${HOSTED}-kubeconfig\nNAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS\nversion             False       True          3h46m   Unable to apply 4.9.17: some cluster operators have not yet rolled out\n</code></pre> <p>NOTE: Some cluster operators are degraded because there is only a single worker and they require at least 2. However setting the <code>hostedcluster.spec.infrastructureAvailabilityPolicy: SingleReplica</code> configuration parameter disables the requirement and will make the clusters operator available with a single worker.</p> <ul> <li>After adding 2 workers, the hosted cluster is completely available as well as all the cluster operators:</li> </ul> <pre><code>oc get hostedcluster -n clusters hosted0\nNAME      VERSION   KUBECONFIG                 PROGRESS    AVAILABLE   REASON\nhosted0   4.9.17    hosted0-admin-kubeconfig   Completed   True        HostedClusterAsExpected\n\nKUBECONFIG=./hosted0-kubeconfig oc get co\nNAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nconsole                                    4.9.17    True        False         False      64m\ncsi-snapshot-controller                    4.9.17    True        False         False      71m\ndns                                        4.9.17    True        False         False      71m\nimage-registry                             4.9.17    True        False         False      11m\ningress                                    4.9.17    True        False         False      61m\nkube-apiserver                             4.9.17    True        False         False      4h32m\nkube-controller-manager                    4.9.17    True        False         False      4h32m\nkube-scheduler                             4.9.17    True        False         False      4h32m\nkube-storage-version-migrator              4.9.17    True        False         False      71m\nmonitoring                                 4.9.17    True        False         False      6m51s\nnetwork                                    4.9.17    True        False         False      72m\nnode-tuning                                4.9.17    True        False         False      71m\nopenshift-apiserver                        4.9.17    True        False         False      4h32m\nopenshift-controller-manager               4.9.17    True        False         False      4h32m\nopenshift-samples                          4.9.17    True        False         False      70m\noperator-lifecycle-manager                 4.9.17    True        False         False      4h32m\noperator-lifecycle-manager-catalog         4.9.17    True        False         False      4h32m\noperator-lifecycle-manager-packageserver   4.9.17    True        False         False      4h32m\nservice-ca                                 4.9.17    True        False         False      72m\nstorage                                    4.9.17    True        False         False      72m\n\nKUBECONFIG=./hosted0-kubeconfig oc get clusterversion\nNAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS\nversion   4.9.17    True        False         8m42s   Cluster version is 4.9.17\n</code></pre>"},{"location":"how-to/none/exposing-services-from-hcp/","title":"Exposing the Hosted Control Plane Services","text":"<p>To publish the services from the Hosted Control Plane, we need to understand the available strategies and their implications. Let's explore them.</p>"},{"location":"how-to/none/exposing-services-from-hcp/#service-publishing-strategies","title":"Service Publishing Strategies","text":"<p>Let's delve into the motivations for each of the strategies.</p> <p>The NodePort strategy allows you to expose services without requiring a logical LoadBalancer like MetalLB or similar infrastructure. It is one of the simplest methods to implement. This strategy is supported by all services; however, the limitation arises in high availability (HA) environments where you will be pointing to one of the NodePorts instead of all three.</p> <p>The LoadBalancer strategy enables you to expose certain services through a load balancer. While not all services support this strategy, it is the preferred method for exposing the KubeApiServer, as it allows for a single entry point in an HA configuration without relying in the Ingress Controller of the Manamgement cluster.</p> <p>The Route strategy allows you to expose the HostedControlPlane services using the ingress of the Management OpenShift cluster. This strategy is supported by all services but kubeapi-server.</p>"},{"location":"how-to/none/exposing-services-from-hcp/#nodeport","title":"NodePort","text":"<p>Exposing a service via NodePort is a method used in OpenShift to make a service accessible from outside the cluster. When you expose a service using NodePort, OpenShift allocates a port on each node in the cluster (if the cluster availability policy is set to HighlyAvailable). This port on each node is mapped to the port of the service, allowing external traffic to reach the service by accessing any node's IP address and the allocated NodePort.</p> <p>This is the default configuration when you use <code>Agent</code> and <code>None</code> provider platforms. The services relevant for on-premise platforms are:</p> <ul> <li>APIServer</li> <li>OAuthServer</li> <li>Konnectivity</li> <li>Ignition</li> </ul> <p>Note</p> <p>If any of the services are not relevant for your deployment, it is not necessary to specify them.</p> <p>Here is how it looks in the HostedCluster CR:</p> <pre><code>spec:\n...\n...\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: &lt;IP_ADDRESS&gt;\n        port: &lt;PORT&gt;\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: 10.103.101.101\n      type: NodePort\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: 10.103.101.101\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: 10.103.101.101\n      type: NodePort\n...\n...\n</code></pre>"},{"location":"how-to/none/exposing-services-from-hcp/#route","title":"Route","text":"<p>OpenShift routes provide a way to expose services within the cluster to external clients. A route in OpenShift maps an external request (typically HTTP/HTTPS) to an internal service. The route specifies a hostname that external clients will use to access the service. OpenShift\u2019s router (based on HAProxy) will handle the traffic coming to this hostname.</p> <p>HostedControlPlanes operate in two domains: the Control Plane and the Data Plane. The Control Plane uses routes through the MGMT Cluster ingress to expose services for each of the HostedControlPlanes, and the routes are created in the HostedControlPlane namespace. For the Data Plane, the Ingress handles <code>*.apps.subdomain.tld</code> URLs, and all routes under this wildcard are directed to the Namespace by the OpenShift Router on the worker nodes.</p> <p>The usual configuration for the Hosted Cluster is similar to the LoadBalancer setup we will discuss next.</p>"},{"location":"how-to/none/exposing-services-from-hcp/#loadbalancer","title":"LoadBalancer","text":"<p>The LoadBalancer strategy in OpenShift is used to expose services to external clients using an external load balancer. When you create a service of type LoadBalancer, Kubernetes interacts with the underlying cloud platform or appropriate LoadBalancer controllers to provision an external load balancer, which then routes traffic to the service's endpoints (pods).</p> <p>This is how looks like the most common configuration to expose the services from HCP side:</p> <pre><code>spec:\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      type: LoadBalancer\n  - service: OAuthServer\n    servicePublishingStrategy:\n      type: Route\n  - service: OIDC\n    servicePublishingStrategy:\n      type: Route\n      Route:\n        hostname: &lt;URL&gt;\n  - service: Konnectivity\n    servicePublishingStrate\n      type: Route\n  - service: Ignition\n    servicePublishingStrategy:\n      type: Route\n</code></pre> <p>If you wanna know more about how to expose the ingress service in the Data Plane side, please access the Recipes section to see how to do it with MetalLB.</p>"},{"location":"how-to/none/global-pull-secret/","title":"Global Pull Secret for Hosted Control Planes","text":""},{"location":"how-to/none/global-pull-secret/#overview","title":"Overview","text":"<p>The Global Pull Secret functionality enables Hosted Cluster administrators to include additional pull secrets for accessing container images from private registries without requiring assistance from the Management Cluster administrator. This feature allows you to merge your custom pull secret with the original HostedCluster pull secret, making it available to all nodes in the cluster.</p> <p>The implementation uses a DaemonSet approach that automatically detects when you create an <code>additional-pull-secret</code> in the <code>kube-system</code> namespace of your DataPlane (Hosted Cluster). The system then merges this secret with the original pull secret and deploys the merged result to all nodes via a DaemonSet that updates the kubelet configuration.</p> <p>Note</p> <p>This feature is designed to work autonomously - once you create the additional pull secret, the system automatically handles the rest without requiring Management Cluster administrator intervention.</p>"},{"location":"how-to/none/global-pull-secret/#adding-your-pull-secret","title":"Adding your Pull Secret","text":"<p>Important</p> <p>All actions described in this section must be performed on the HostedCluster's workers (DataPlane), not on the Management Cluster.</p> <p>To use this functionality, follow these steps:</p>"},{"location":"how-to/none/global-pull-secret/#1-create-your-additional-pull-secret","title":"1. Create your additional pull secret","text":"<p>Create a secret named <code>additional-pull-secret</code> in the <code>kube-system</code> namespace of your Hosted Cluster (DataPlane). The secret must contain a valid DockerConfigJSON format:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: additional-pull-secret\n  namespace: kube-system\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: &lt;base64-encoded-docker-config-json&gt;\n</code></pre>"},{"location":"how-to/none/global-pull-secret/#2-example-dockerconfigjson-format","title":"2. Example DockerConfigJSON format","text":"<p>Your <code>.dockerconfigjson</code> should follow this structure:</p> <pre><code>{\n  \"auths\": {\n    \"registry.example.com\": {\n      \"auth\": \"base64-encoded-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"base64-encoded-credentials\"\n    }\n  }\n}\n</code></pre> <p>Using Namespace-Specific Registry Entries</p> <p>For registries like Quay.io that support organization/namespace-specific authentication, you can specify the full path in your registry entry (e.g., <code>quay.io/mycompany</code> instead of just <code>quay.io</code>). This allows you to provide different credentials for different namespaces within the same registry, and helps avoid conflicts with existing registry entries in the original pull secret.</p>"},{"location":"how-to/none/global-pull-secret/#3-apply-the-secret","title":"3. Apply the secret","text":"<pre><code>kubectl apply -f additional-pull-secret.yaml\n</code></pre>"},{"location":"how-to/none/global-pull-secret/#4-verification","title":"4. Verification","text":"<p>After creating the secret, the system will automatically:</p> <ol> <li>Validate the secret format</li> <li>Merge it with the original pull secret</li> <li>Deploy a DaemonSet to all nodes</li> <li>Update the kubelet configuration on each node</li> </ol> <p>You can verify the deployment by checking:</p> <pre><code># Check if the DaemonSet is running\nkubectl get daemonset global-pull-secret-syncer -n kube-system\n\n# Check the merged pull secret\nkubectl get secret global-pull-secret -n kube-system\n\n# Check DaemonSet pods\nkubectl get pods -n kube-system -l name=global-pull-secret-syncer\n</code></pre>"},{"location":"how-to/none/global-pull-secret/#how-it-works","title":"How it works","text":"<p>The Global Pull Secret functionality operates through a multi-component system:</p>"},{"location":"how-to/none/global-pull-secret/#automatic-detection","title":"Automatic Detection","text":"<ul> <li>The Hosted Cluster Config Operator (HCCO) continuously monitors the <code>kube-system</code> namespace</li> <li>When it detects the creation of <code>additional-pull-secret</code>, it triggers the reconciliation process</li> </ul>"},{"location":"how-to/none/global-pull-secret/#validation-and-merging","title":"Validation and Merging","text":"<ul> <li>The system validates that your secret contains a proper DockerConfigJSON format</li> <li>It retrieves the original pull secret from the HostedControlPlane</li> <li>Your additional pull secret is merged with the original one</li> <li>If there are conflicting registry entries, the original pull secret takes precedence (the additional pull secret entry is ignored for conflicting registries)</li> <li>The system supports namespace-specific registry entries (e.g., <code>quay.io/namespace</code>) for better credential specificity</li> </ul>"},{"location":"how-to/none/global-pull-secret/#deployment-process","title":"Deployment Process","text":"<ul> <li>A <code>global-pull-secret</code> is created in the <code>kube-system</code> namespace containing the merged result</li> <li>RBAC resources (ServiceAccount, Role, RoleBinding) are created for the DaemonSet in both <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>We use Role and RoleBinding in both namespaces to access secrets in <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>A DaemonSet named <code>global-pull-secret-syncer</code> is deployed to eligible nodes</li> </ul> <p>NodePool InPlace Strategy Restriction</p> <p>The Global Pull Secret DaemonSet is not deployed to nodes that belong to NodePools using the InPlace upgrade strategy. This restriction prevents conflicts between the DaemonSet's modifications to <code>/var/lib/kubelet/config.json</code> and the Machine Config Daemon (MCD) during InPlace upgrades.</p> <ul> <li>Nodes with Replace strategy: \u2705 Receive Global Pull Secret DaemonSet</li> <li>Nodes with InPlace strategy: \u274c Do not receive Global Pull Secret DaemonSet</li> </ul> <p>This ensures that MCD operations during InPlace upgrades do not fail due to unexpected changes in kubelet configuration files.</p>"},{"location":"how-to/none/global-pull-secret/#node-level-synchronization","title":"Node-Level Synchronization","text":"<ul> <li>Each DaemonSet pod runs a controller that watches the secrets under kube-system namespace</li> <li>When changes are detected, it updates <code>/var/lib/kubelet/config.json</code> on the node</li> <li>The kubelet service is restarted via DBus to apply the new configuration</li> <li>If the restart fails after 3 attempts, the system rolls back the file changes</li> </ul>"},{"location":"how-to/none/global-pull-secret/#automatic-cleanup","title":"Automatic Cleanup","text":"<ul> <li>If you delete the <code>additional-pull-secret</code>, the HCCO automatically removes the <code>global-pull-secret</code> secret</li> <li>The system reverts to using only the original pull secret from the HostedControlPlane</li> <li>The DaemonSet continues running but now syncs only the original pull secret to nodes</li> </ul>"},{"location":"how-to/none/global-pull-secret/#registry-precedence-and-conflict-resolution","title":"Registry Precedence and Conflict Resolution","text":"<p>The Global Pull Secret system uses a specific precedence model when merging your additional pull secret with the original one:</p>"},{"location":"how-to/none/global-pull-secret/#merge-behavior","title":"Merge Behavior","text":"<ul> <li>Original pull secret entries always take precedence over additional pull secret entries for the same registry</li> <li>If both secrets contain an entry for <code>quay.io</code>, the original pull secret's credentials will be used</li> <li>Your additional pull secret entries are only added if they don't conflict with existing entries</li> <li>Warnings are logged when conflicts are detected</li> </ul>"},{"location":"how-to/none/global-pull-secret/#recommended-approach","title":"Recommended Approach","text":"<p>To avoid conflicts and ensure your credentials are used, consider these strategies:</p> <ol> <li>Use namespace-specific entries: Instead of <code>quay.io</code>, use <code>quay.io/your-namespace</code></li> <li>Target specific registries: Add entries only for registries not already in the original pull secret</li> <li>Check existing entries: Review what registries are already configured in the HostedControlPlane</li> </ol>"},{"location":"how-to/none/global-pull-secret/#example-merge-scenario","title":"Example Merge Scenario","text":"<p>Original Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"original-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Your Additional Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"your-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"your-namespace-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Resulting Merged Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"original-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"your-namespace-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Note how the <code>quay.io</code> entry keeps the original credentials, but <code>quay.io/mycompany</code> is added from your additional secret.</p>"},{"location":"how-to/none/global-pull-secret/#implementation-details","title":"Implementation details","text":"<p>The implementation consists of several key components working together:</p>"},{"location":"how-to/none/global-pull-secret/#core-components","title":"Core Components","text":"<ol> <li>Global Pull Secret Controller (<code>globalps</code> package)</li> <li>Handles validation of user-provided pull secrets</li> <li>Manages the merging logic between original and additional pull secrets</li> <li>Creates and manages RBAC resources</li> <li>Deploys and manages the DaemonSet</li> <li> <p>Node eligibility assessment: Labels nodes from InPlace NodePools and configures DaemonSet scheduling restrictions</p> </li> <li> <p>Sync Global Pull Secret Command (<code>sync-global-pullsecret</code> package)</p> </li> <li>Runs as a DaemonSet on each node</li> <li>Watches for changes to the <code>global-pull-secret</code> in <code>kube-system</code> namespace</li> <li>Accesses the original <code>pull-secret</code> in <code>openshift-config</code> namespace</li> <li>Updates the kubelet configuration file</li> <li> <p>Manages kubelet service restarts via DBus</p> </li> <li> <p>Hosted Cluster Config Operator Integration</p> </li> <li>Monitors for the presence of <code>additional-pull-secret</code></li> <li>Orchestrates the entire process</li> <li>Handles cleanup when the secret is removed</li> </ol>"},{"location":"how-to/none/global-pull-secret/#architecture-diagram","title":"Architecture Diagram","text":"graph TB     %% User Input     User[User creates additional-pull-secret] --&gt; |kube-system namespace| AdditionalPS[additional-pull-secret Secret]      %% HCCO Controller     HCCO[Hosted Cluster Config Operator] --&gt; |Watches kube-system secrets| GlobalPSController[Global Pull Secret Controller]     GlobalPSController --&gt; |Validates| AdditionalPS     GlobalPSController --&gt; |Gets original| OriginalPS[Original pull-secret from HCP]      %% Secret Processing     AdditionalPS --&gt; |Validates format| ValidatePS[Validate Additional Pull Secret]     OriginalPS --&gt; |Extracts data| OriginalPSData[Original Pull Secret Data]     ValidatePS --&gt; |Extracts data| AdditionalPSData[Additional Pull Secret Data]      %% Merge Process     OriginalPSData --&gt; MergeSecrets[Merge Pull Secrets]     AdditionalPSData --&gt; MergeSecrets     MergeSecrets --&gt; |Creates merged JSON| GlobalPSData[Global Pull Secret Data]      %% Secret Creation     GlobalPSData --&gt; |Creates in kube-system| GlobalPSSecret[global-pull-secret Secret]      %% RBAC Setup     GlobalPSController --&gt; |Creates RBAC| RBACSetup[Setup RBAC Resources]     RBACSetup --&gt; ServiceAccount[global-pull-secret-syncer ServiceAccount]     RBACSetup --&gt; KubeSystemRole[global-pull-secret-syncer Role in kube-system]     RBACSetup --&gt; KubeSystemRoleBinding[global-pull-secret-syncer RoleBinding in kube-system]     RBACSetup --&gt; OpenshiftConfigRole[global-pull-secret-syncer Role in openshift-config]     RBACSetup --&gt; OpenshiftConfigRoleBinding[global-pull-secret-syncer RoleBinding in openshift-config]      %% DaemonSet Deployment     GlobalPSController --&gt; |Deploys DaemonSet| DaemonSet[global-pull-secret-syncer DaemonSet]     DaemonSet --&gt; |Runs on each node| DaemonSetPod[DaemonSet Pod]      %% DaemonSet Pod Details     DaemonSetPod --&gt; |Mounts host paths| HostMounts[Host Path Mounts]     HostMounts --&gt; KubeletPath[\"/var/lib/kubelet\"]     HostMounts --&gt; DbusPath[\"/var/run/dbus\"]      %% Container Execution     DaemonSetPod --&gt; |Runs command| Container[control-plane-operator Container]     Container --&gt; |Executes| SyncCommand[sync-global-pullsecret command]      %% Sync Process     SyncCommand --&gt; |Watches global-pull-secret| SyncController[Global Pull Secret Reconciler]     SyncController --&gt; |Reads secret| ReadGlobalPS[Read global-pull-secret]     SyncController --&gt; |Reads original| ReadOriginalPS[Read original pull-secret]      %% File Update Process     ReadGlobalPS --&gt; |Gets data| GlobalPSBytes[Global Pull Secret Bytes]     ReadOriginalPS --&gt; |Gets data| OriginalPSBytes[Original Pull Secret Bytes]      %% Decision Logic     GlobalPSBytes --&gt; |If exists| UseGlobalPS[Use Global Pull Secret]     OriginalPSBytes --&gt; |If not exists| UseOriginalPS[Use Original Pull Secret]      %% File Update     UseGlobalPS --&gt; |Updates file| UpdateKubeletConfig[\"Update /var/lib/kubelet/config.json\"]     UseOriginalPS --&gt; |Updates file| UpdateKubeletConfig      %% Kubelet Restart     UpdateKubeletConfig --&gt; |Restarts kubelet| RestartKubelet[Restart kubelet.service via systemd]     RestartKubelet --&gt; |Via dbus| DbusConnection[DBus Connection]      %% Error Handling     UpdateKubeletConfig --&gt; |If restart fails| RollbackProcess[Rollback Process]     RollbackProcess --&gt; |Restore original| RestoreOriginal[Restore Original File Content]      %% Cleanup Process     GlobalPSController --&gt; |If additional PS deleted| CleanupProcess[Cleanup Process]     CleanupProcess --&gt; |Deletes global PS| DeleteGlobalPS[Delete global-pull-secret]     CleanupProcess --&gt; |Removes DaemonSet| RemoveDaemonSet[Remove DaemonSet]      %% Styling     classDef userInput fill:#e1f5fe     classDef controller fill:#f3e5f5     classDef secret fill:#e8f5e8     classDef process fill:#fff3e0     classDef daemonSet fill:#fce4ec     classDef fileSystem fill:#f1f8e9      class User,AdditionalPS userInput     class HCCO,GlobalPSController,SyncController controller     class OriginalPS,GlobalPSSecret,ServiceAccount,KubeSystemRole,KubeSystemRoleBinding,OpenshiftConfigRole,OpenshiftConfigRoleBinding secret     class ValidatePS,MergeSecrets,RBACSetup,UpdateKubeletConfig,RestartKubelet process     class DaemonSet,DaemonSetPod,Container daemonSet     class KubeletPath,DbusPath fileSystem"},{"location":"how-to/none/global-pull-secret/#key-features","title":"Key Features","text":"<ul> <li>Security: Only watches specific secrets in <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>Robustness: Includes automatic rollback in case of failures</li> <li>Efficiency</li> <li>Only updates when there are actual changes</li> <li>The globalPullSecret implementation has their own controller so it cannot interfere with the HCCO reonciliation</li> <li>Security considerations: Uses specific RBAC for only the required resources in each namespace. The DaemonSet containers run in privileged mode due to the need to:</li> <li>Write to <code>/var/lib/kubelet/config.json</code> (kubelet configuration file)</li> <li>Connect to systemd via DBus for service management</li> <li>Restart kubelet.service, which requires root privileges</li> <li>Smart node targeting: Automatically excludes nodes from InPlace NodePools to prevent MCD conflicts</li> </ul>"},{"location":"how-to/none/global-pull-secret/#inplace-nodepool-handling","title":"InPlace NodePool Handling","text":"<p>To prevent conflicts with Machine Config Daemon operations, the implementation includes intelligent node targeting:</p>"},{"location":"how-to/none/global-pull-secret/#node-labeling-process","title":"Node Labeling Process","text":"<ol> <li>MachineSets Discovery: The controller queries the management cluster for MachineSets with InPlace-specific annotations (<code>hypershift.openshift.io/nodePoolTargetConfigVersion</code>)</li> <li>Machine Enumeration: For each InPlace MachineSets, it lists all associated Machines</li> <li>Node Identification: Maps Machine objects to their corresponding nodes via <code>machine.Status.NodeRef.Name</code></li> <li>Labeling: Applies <code>hypershift.openshift.io/nodepool-inplace-strategy=true</code> label to identified nodes</li> </ol>"},{"location":"how-to/none/global-pull-secret/#daemonset-scheduling-configuration","title":"DaemonSet Scheduling Configuration","text":"<p>The DaemonSet uses NodeAffinity to exclude InPlace nodes:</p> <pre><code>spec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: hypershift.openshift.io/nodepool-inplace-strategy\n                operator: DoesNotExist\n</code></pre> <p>This ensures that: - Nodes without the label: \u2705 Are eligible for DaemonSet scheduling - Nodes with the label (any value): \u274c Are excluded from DaemonSet scheduling</p>"},{"location":"how-to/none/global-pull-secret/#conflict-prevention-benefits","title":"Conflict Prevention Benefits","text":"<ul> <li>Prevents MCD failures: Avoids conflicts when MCD expects specific kubelet configuration during InPlace upgrades</li> <li>Maintains upgrade reliability: InPlace upgrade processes are not interrupted by Global Pull Secret modifications</li> <li>Automatic detection: No manual intervention required - the system automatically identifies and handles InPlace nodes</li> </ul>"},{"location":"how-to/none/global-pull-secret/#error-handling","title":"Error Handling","text":"<p>The system includes comprehensive error handling:</p> <ul> <li>Validation errors: Invalid DockerConfigJSON format is caught early</li> <li>Restart failures: If kubelet restart fails after 3 attempts, the file is rolled back</li> <li>Resource cleanup: If the additional pull secret is deleted, the HCCO automatically removes the globalPullSecret</li> </ul> <p>This implementation provides a secure, autonomous solution that allows HostedCluster administrators to add private registry credentials without requiring Management Cluster administrator intervention.</p>"},{"location":"how-to/openstack/","title":"Hosted Clusters on OpenStack","text":"<p>This section of the Hypershift documentation contains pages that explain how Hosted Clusters and Nodepools can be deployed on the OpenStack platform.</p> <p>Warning</p> <p>OpenStack support within Hypershift is currently \"dev-preview\" and therefore is not yet intended for production use. However, it is possible to create and manage Hosted Clusters for development and testing purposes. It is expected to work as described in this documentation.</p> <p>Note</p> <p>When you create a HostedCluster with the OpenStack platform, HyperShift will install the CAPI Provider for OpenStack (CAPO). in the Hosted Control Plane (HCP) namespace. It will also install OpenStack Resource Controller (ORC) in the HCP namespace to manage the OpenStack resources such as the Image. Upon scaling up a NodePool, a Machine will be created, and the CAPI provider will create the necessary resources in OpenStack. CAPO created OpenStack resources by leveraging Gophercloud, the OpenStack SDK for Go.</p> <p>Table of content:</p> <ul> <li>Prerequisites</li> <li>Create a Hosted Cluster and a default Nodepool</li> <li>Destroy a Hosted Cluster</li> </ul> <p>Advanced scenarios: additional Nodepools can be created with specific configurations:</p> <ul> <li>Nodepools distributed over Nova availability zones</li> <li>Additional ports for Nodepools</li> <li>Node tuning, SR-IOV Network Operator</li> </ul>"},{"location":"how-to/openstack/additional-ports/","title":"Additional ports for Nodepools","text":"<p>This document describes how to attach additional ports to nodes in a HostedCluster on OpenStack.</p>"},{"location":"how-to/openstack/additional-ports/#use-cases","title":"Use-cases","text":"<ul> <li>SR-IOV: Single Root I/O Virtualization (SR-IOV) is a specification that allows a single Peripheral Component Interconnect Express (PCIe) physical device to appear as multiple separate physical devices. This can be useful for high-performance networking scenarios. The Nodepool can be configured to use SR-IOV by attaching additional ports to the nodes. Workloads that require high-performance networking can use the SR-IOV devices.</li> <li>DPDK: Data Plane Development Kit (DPDK) is a set of libraries and drivers for fast packet processing. DPDK can be used to improve the performance of network applications. The Nodepool can be configured to use DPDK by attaching additional ports to the nodes. Workloads that require high-performance networking can use the DPDK devices.</li> <li>Manila RWX on NFS: Manila ReadWriteMany (RWX) volumes can be used by multiple nodes in a HostedCluster. The Nodepool can be configured to use Manila RWX volumes by attaching additional ports to the nodes. Workloads that require shared storage will be able to reach the NFS network configured for the Manila RWX volumes.</li> <li>Multus CNI: Multus is a meta-plugin for Kubernetes CNI that enables attaching multiple network interfaces to pods. The Nodepool can be configured to use Multus by attaching additional ports to the nodes. Workloads that require multiple network interfaces will be able to use the Multus CNI. For example it's useful if the workload needs to connect to IPv6 networks (dual-stack or single-stack).</li> </ul>"},{"location":"how-to/openstack/additional-ports/#prerequisites","title":"Prerequisites","text":"<p>Additional networks must be created in OpenStack and the project used by the HostedCluster must have access to these networks.</p>"},{"location":"how-to/openstack/additional-ports/#available-options","title":"Available options","text":"<p>The <code>--openstack-node-additional-port</code> flag can be used to attach additional ports to nodes in a HostedCluster on OpenStack. The flag takes a list of parameters separated by commas. The parameter can be used multiple times to attach multiple additional ports to the nodes.</p> <p>The parameters are:</p> Parameter Description Required Default <code>network-id</code> The ID of the network to attach to the node. Yes N/A <code>vnic-type</code> The VNIC type to use for the port. When not specified, Neutron uses the default <code>normal</code>. No N/A <code>disable-port-security</code> Whether to enable port security for the port. When not specified, Neutron will enable it unless explicitly disabled in the network No N/A <code>address-pairs</code> A list of IP address pairs to be assigned to the port. The format is <code>ip_address=mac_address</code>. Multiple pairs can be provided, separated by a \"-\". The <code>mac_address</code> portion is optional and in most cases won't be used. No N/A"},{"location":"how-to/openstack/additional-ports/#example","title":"Example","text":"<p>The following example demonstrates how to create a HostedCluster with additional ports attached to the nodes.</p> <pre><code>export NODEPOOL_NAME=$CLUSTER_NAME-ports\nexport WORKER_COUNT=\"2\"\nexport FLAVOR=\"m1.xlarge\"\n\nhcp create nodepool openstack \\\n  --cluster-name $CLUSTER_NAME \\\n  --name $NODEPOOL_NAME \\\n  --replicas $WORKER_COUNT \\\n  --openstack-node-flavor $FLAVOR \\\n  --openstack-node-additional-port \"network-id=&lt;SRIOV_NET_ID&gt;,vnic-type=direct,disable-port-security=true\" \\\n  --openstack-node-additional-port \"network-id=&lt;LB_NET_ID&gt;,address-pairs:192.168.0.1-192.168.0.2\"\n</code></pre> <p>In this example, two additional ports are attached to the nodes in the Nodepool in addition to the network created by the Cluster API provider using for the control-plane. The first additional port is attached to the network with the ID <code>&lt;SRIOV_NET_ID&gt;</code>. The port uses the <code>direct</code> VNIC type and has port security disabled. The second port is attached to the network with the ID <code>&lt;LB_NET_ID&gt;</code>. The port will allow address pairs so services like MetalLB will be able to handle the traffic on these IPs.</p>"},{"location":"how-to/openstack/az/","title":"Availability Zones","text":"<p>This example will create a Nodepool that will spawn 2 additional machines in a given Nova availability zone. Availability Zones do not necessarily correspond to fault domains and do not inherently provide high availability benefits.</p> <pre><code>export NODEPOOL_NAME=$CLUSTER_NAME-az1\nexport WORKER_COUNT=\"2\"\nexport FLAVOR=\"m1.xlarge\"\nexport AZ=\"az1\"\n\nhcp create nodepool openstack \\\n  --cluster-name $CLUSTER_NAME \\\n  --name $NODEPOOL_NAME \\\n  --replicas $WORKER_COUNT \\\n  --openstack-node-flavor $FLAVOR \\\n  --openstack-node-availability-zone $AZ \\\n</code></pre> <p>Check the status of the NodePool by listing <code>nodepool</code> resources in the <code>clusters</code> namespace:</p> <pre><code>oc get nodepools --namespace clusters\n\nNAME                      CLUSTER         DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE\nexample                   example         5               5               False         False        4.17.0\nexample-az1               example         2                               False         False                  True              True             Minimum availability requires 2 replicas, current 0 available\n</code></pre> <p>After a while, in our hosted cluster this is what we will see:</p> <pre><code>oc --kubeconfig $CLUSTER_NAME-kubeconfig get nodes\n\nNAME                      STATUS   ROLES    AGE     VERSION\n(..)\nexample-extra-az-zh9l5    Ready    worker   2m6s    v1.27.4+18eadca\nexample-extra-az-zr8mj    Ready    worker   102s    v1.27.4+18eadca\n...\n</code></pre> <p>And the nodepool will be in the desired state:</p> <pre><code>oc get nodepools --namespace clusters\n\nNAME                      CLUSTER         DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE\nexample                   example         5               5               False         False        4.17.0\nexample-az1               example         2               2               False         False        4.17.0\n</code></pre>"},{"location":"how-to/openstack/destroy/","title":"Delete a HostedCluster on OpenStack","text":"<p>To delete a HostedCluster on OpenStack:</p> <pre><code>hcp destroy cluster openstack --name $CLUSTER_NAME\n</code></pre> <p>The process will take a few minutes to complete and will destroy all resources associated with the HostedCluster including OpenStack resources such as servers, the RHCOS image, networks, etc.</p>"},{"location":"how-to/openstack/etcd-local-storage/","title":"Etcd local storage","text":"<p>In a Hosted Control Plane (HCP) architecture, the etcd database plays a critical role as the core data store for the Hosted Kubernetes control plane components. By default, Hypershift provisions etcd data on a Persistent Volume Claim (PVC), which relies on the default StorageClass defined in the Management Cluster.</p> <p>However, HyperShift allows you to easily choose another storage class when desired.  On OpenStack, the default RWO StorageClass is generally Cinder via its CSI driver to provision storage. While this driver is suitable for general workloads, it is not ideal for etcd due to the latency and performance characteristics of network-attached storage. Instead, a more optimal approach is to create a StorageClass backed by high performance local disk(s) and use it explicitly for etcd.</p> <p>This document provides detailed instructions on how to configure and leverage Logical Volume Manager Storage through the TopoLVM CSI driver to dynamically provision storage for etcd.</p> <p>Logical Volume Manager (LVM) Storage uses LVM2 through the TopoLVM CSI driver to dynamically provision local storage on a cluster with limited resources. With this method, we can create volume groups, persistent volume claims (PVCs), volume snapshots, and volume clones for etcd.</p> <ol> <li> <p>Follow the official procedure to Install LVM Storage by using the CLI on the Management Cluster.</p> </li> <li> <p>You will need worker nodes with additional ephemeral disk(s) on the Management cluster.    The tested solution is to create an OpenStack Nova flavor that will create an additional ephemeral local disk attached to the instance.</p> </li> </ol> <pre><code>openstack flavor create (...) --ephemeral 100 (...)\n</code></pre> <p>Note</p> <p>When a server will be created with that flavor, Nova will automatically create and attach a local ephemeral storage device to the VM (and format it in vfat).</p> <ol> <li> <p>Now we need to Create a compute machine set that would use this flavor.</p> </li> <li> <p>Scale the MachineSet to the desired replica. Note that if HostedClusters are deployed in High Availability, a minimum of 3 workers has to be deployed so the pods can be distributed accordingly.</p> </li> <li> <p>Add a label to identify the Nodes with the ephemeral disk for etcd so we can identify them as \u201cHypershift ready\u201d:</p> </li> </ol> <p>Note</p> <p>This label is arbitrary and can be changed.</p> <ol> <li>Now that we have the workers that will be used for Hypershift, we can create the LVMCluster object that will describe the configuration of the local storage used for etcd in Hypershift. Create a file named \u201clvmcluster.yaml\u201d:</li> </ol> <pre><code>---\napiVersion: lvm.topolvm.io/v1alpha1\nkind: LVMCluster\nmetadata:\n  name: etcd-hcp\n  namespace: openshift-storage\nspec:\n  storage:\n    deviceClasses:\n    - name: etcd-class\n      default: true\n      nodeSelector:\n         nodeSelectorTerms:\n         - matchExpressions:\n           - key: hypershift-capable\n            operator: In\n            values:\n            - \"true\"\n      deviceSelector:\n        forceWipeDevicesAndDestroyAllData: true\n        paths:\n        - /dev/vdb\n</code></pre> <p>Note</p> <ul> <li>We assume that the ephemeral disk is located on /dev/vdb which is the case in most situations. However this needs to be verified when following this procedure. Using symlinks isn't supported.</li> <li><code>forceWipeDevicesAndDestroyAllData</code> is set to True because the default nova ephemeral disk comes formatted in vfat.</li> <li><code>thinPoolConfig</code> can be used but it will affect the performance therefore we don't recommend it.</li> </ul> <p>Now we create the resource: </p> <pre><code>oc apply -f lvmcluster.yaml\n</code></pre> <p>We can verify that the resource has been created:</p> <pre><code>oc get lvmcluster -A\nNAMESPACE           NAME    STATUS\nopenshift-storage   etcd-hcp   Ready\n</code></pre> <p>Also we can verify that we have a StorageClass for LVM:</p> <pre><code>oc get storageclass -A\nNAME                    PROVISIONER               RECLAIMPOLICY   VOLUMEBINDINGMODE     ALLOWVOLUMEEXPANSION   AGE\nlvms-etcd-class         topolvm.io                Delete          WaitForFirstConsumer  true                   23m\nstandard-csi (default)  cinder.csi.openstack.org  Delete          WaitForFirstConsumer  true                   56m\n</code></pre> <ol> <li>Now that local storage is available through CSI in the Management Cluster, the HostedCluster can be deployed this way:</li> </ol> <pre><code>hcp create cluster openstack \\\n    (...)\n    --etcd-storage-class lvms-etcd-class\n</code></pre> <ol> <li>You can now verify that etcd is placed on local ephemeral storage:</li> </ol> <pre><code>oc get pvc -A\nNAMESPACE               NAME                    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      VOLUMEATTRIBUTESCLASS   AGE\nclusters-emacchi-hcp    data-etcd-0             Bound   pvc-f8b4070f-0d11-48b7-93d3-cc2a56ada7e9   8Gi      RWO         lvms-etcd-class   &lt;unset&gt;               8s\n</code></pre> <p>In the Hosted Control Plane etcd pod:</p> <pre><code>/var/lib/data\nbash-5.1$ df -h\nFilesystem                                      Size  Used Avail Use% Mounted on\n(...)                                           3.2G   86M  3.1G   3% /etc/passwd\n/dev/topolvm/0b1114b3-f084-4c1c-bda5-85ef197459aa  8.0G  215M  7.8G   3% /var/lib\n(...)\n</code></pre> <p>We can see the 8GB device for etcd.</p>"},{"location":"how-to/openstack/global-pull-secret/","title":"Global Pull Secret for Hosted Control Planes","text":""},{"location":"how-to/openstack/global-pull-secret/#overview","title":"Overview","text":"<p>The Global Pull Secret functionality enables Hosted Cluster administrators to include additional pull secrets for accessing container images from private registries without requiring assistance from the Management Cluster administrator. This feature allows you to merge your custom pull secret with the original HostedCluster pull secret, making it available to all nodes in the cluster.</p> <p>The implementation uses a DaemonSet approach that automatically detects when you create an <code>additional-pull-secret</code> in the <code>kube-system</code> namespace of your DataPlane (Hosted Cluster). The system then merges this secret with the original pull secret and deploys the merged result to all nodes via a DaemonSet that updates the kubelet configuration.</p> <p>Note</p> <p>This feature is designed to work autonomously - once you create the additional pull secret, the system automatically handles the rest without requiring Management Cluster administrator intervention.</p>"},{"location":"how-to/openstack/global-pull-secret/#adding-your-pull-secret","title":"Adding your Pull Secret","text":"<p>Important</p> <p>All actions described in this section must be performed on the HostedCluster's workers (DataPlane), not on the Management Cluster.</p> <p>To use this functionality, follow these steps:</p>"},{"location":"how-to/openstack/global-pull-secret/#1-create-your-additional-pull-secret","title":"1. Create your additional pull secret","text":"<p>Create a secret named <code>additional-pull-secret</code> in the <code>kube-system</code> namespace of your Hosted Cluster (DataPlane). The secret must contain a valid DockerConfigJSON format:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: additional-pull-secret\n  namespace: kube-system\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: &lt;base64-encoded-docker-config-json&gt;\n</code></pre>"},{"location":"how-to/openstack/global-pull-secret/#2-example-dockerconfigjson-format","title":"2. Example DockerConfigJSON format","text":"<p>Your <code>.dockerconfigjson</code> should follow this structure:</p> <pre><code>{\n  \"auths\": {\n    \"registry.example.com\": {\n      \"auth\": \"base64-encoded-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"base64-encoded-credentials\"\n    }\n  }\n}\n</code></pre> <p>Using Namespace-Specific Registry Entries</p> <p>For registries like Quay.io that support organization/namespace-specific authentication, you can specify the full path in your registry entry (e.g., <code>quay.io/mycompany</code> instead of just <code>quay.io</code>). This allows you to provide different credentials for different namespaces within the same registry, and helps avoid conflicts with existing registry entries in the original pull secret.</p>"},{"location":"how-to/openstack/global-pull-secret/#3-apply-the-secret","title":"3. Apply the secret","text":"<pre><code>kubectl apply -f additional-pull-secret.yaml\n</code></pre>"},{"location":"how-to/openstack/global-pull-secret/#4-verification","title":"4. Verification","text":"<p>After creating the secret, the system will automatically:</p> <ol> <li>Validate the secret format</li> <li>Merge it with the original pull secret</li> <li>Deploy a DaemonSet to all nodes</li> <li>Update the kubelet configuration on each node</li> </ol> <p>You can verify the deployment by checking:</p> <pre><code># Check if the DaemonSet is running\nkubectl get daemonset global-pull-secret-syncer -n kube-system\n\n# Check the merged pull secret\nkubectl get secret global-pull-secret -n kube-system\n\n# Check DaemonSet pods\nkubectl get pods -n kube-system -l name=global-pull-secret-syncer\n</code></pre>"},{"location":"how-to/openstack/global-pull-secret/#how-it-works","title":"How it works","text":"<p>The Global Pull Secret functionality operates through a multi-component system:</p>"},{"location":"how-to/openstack/global-pull-secret/#automatic-detection","title":"Automatic Detection","text":"<ul> <li>The Hosted Cluster Config Operator (HCCO) continuously monitors the <code>kube-system</code> namespace</li> <li>When it detects the creation of <code>additional-pull-secret</code>, it triggers the reconciliation process</li> </ul>"},{"location":"how-to/openstack/global-pull-secret/#validation-and-merging","title":"Validation and Merging","text":"<ul> <li>The system validates that your secret contains a proper DockerConfigJSON format</li> <li>It retrieves the original pull secret from the HostedControlPlane</li> <li>Your additional pull secret is merged with the original one</li> <li>If there are conflicting registry entries, the original pull secret takes precedence (the additional pull secret entry is ignored for conflicting registries)</li> <li>The system supports namespace-specific registry entries (e.g., <code>quay.io/namespace</code>) for better credential specificity</li> </ul>"},{"location":"how-to/openstack/global-pull-secret/#deployment-process","title":"Deployment Process","text":"<ul> <li>A <code>global-pull-secret</code> is created in the <code>kube-system</code> namespace containing the merged result</li> <li>RBAC resources (ServiceAccount, Role, RoleBinding) are created for the DaemonSet in both <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>We use Role and RoleBinding in both namespaces to access secrets in <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>A DaemonSet named <code>global-pull-secret-syncer</code> is deployed to eligible nodes</li> </ul> <p>NodePool InPlace Strategy Restriction</p> <p>The Global Pull Secret DaemonSet is not deployed to nodes that belong to NodePools using the InPlace upgrade strategy. This restriction prevents conflicts between the DaemonSet's modifications to <code>/var/lib/kubelet/config.json</code> and the Machine Config Daemon (MCD) during InPlace upgrades.</p> <ul> <li>Nodes with Replace strategy: \u2705 Receive Global Pull Secret DaemonSet</li> <li>Nodes with InPlace strategy: \u274c Do not receive Global Pull Secret DaemonSet</li> </ul> <p>This ensures that MCD operations during InPlace upgrades do not fail due to unexpected changes in kubelet configuration files.</p>"},{"location":"how-to/openstack/global-pull-secret/#node-level-synchronization","title":"Node-Level Synchronization","text":"<ul> <li>Each DaemonSet pod runs a controller that watches the secrets under kube-system namespace</li> <li>When changes are detected, it updates <code>/var/lib/kubelet/config.json</code> on the node</li> <li>The kubelet service is restarted via DBus to apply the new configuration</li> <li>If the restart fails after 3 attempts, the system rolls back the file changes</li> </ul>"},{"location":"how-to/openstack/global-pull-secret/#automatic-cleanup","title":"Automatic Cleanup","text":"<ul> <li>If you delete the <code>additional-pull-secret</code>, the HCCO automatically removes the <code>global-pull-secret</code> secret</li> <li>The system reverts to using only the original pull secret from the HostedControlPlane</li> <li>The DaemonSet continues running but now syncs only the original pull secret to nodes</li> </ul>"},{"location":"how-to/openstack/global-pull-secret/#registry-precedence-and-conflict-resolution","title":"Registry Precedence and Conflict Resolution","text":"<p>The Global Pull Secret system uses a specific precedence model when merging your additional pull secret with the original one:</p>"},{"location":"how-to/openstack/global-pull-secret/#merge-behavior","title":"Merge Behavior","text":"<ul> <li>Original pull secret entries always take precedence over additional pull secret entries for the same registry</li> <li>If both secrets contain an entry for <code>quay.io</code>, the original pull secret's credentials will be used</li> <li>Your additional pull secret entries are only added if they don't conflict with existing entries</li> <li>Warnings are logged when conflicts are detected</li> </ul>"},{"location":"how-to/openstack/global-pull-secret/#recommended-approach","title":"Recommended Approach","text":"<p>To avoid conflicts and ensure your credentials are used, consider these strategies:</p> <ol> <li>Use namespace-specific entries: Instead of <code>quay.io</code>, use <code>quay.io/your-namespace</code></li> <li>Target specific registries: Add entries only for registries not already in the original pull secret</li> <li>Check existing entries: Review what registries are already configured in the HostedControlPlane</li> </ol>"},{"location":"how-to/openstack/global-pull-secret/#example-merge-scenario","title":"Example Merge Scenario","text":"<p>Original Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"original-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Your Additional Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"your-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"your-namespace-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Resulting Merged Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"original-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"your-namespace-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Note how the <code>quay.io</code> entry keeps the original credentials, but <code>quay.io/mycompany</code> is added from your additional secret.</p>"},{"location":"how-to/openstack/global-pull-secret/#implementation-details","title":"Implementation details","text":"<p>The implementation consists of several key components working together:</p>"},{"location":"how-to/openstack/global-pull-secret/#core-components","title":"Core Components","text":"<ol> <li>Global Pull Secret Controller (<code>globalps</code> package)</li> <li>Handles validation of user-provided pull secrets</li> <li>Manages the merging logic between original and additional pull secrets</li> <li>Creates and manages RBAC resources</li> <li>Deploys and manages the DaemonSet</li> <li> <p>Node eligibility assessment: Labels nodes from InPlace NodePools and configures DaemonSet scheduling restrictions</p> </li> <li> <p>Sync Global Pull Secret Command (<code>sync-global-pullsecret</code> package)</p> </li> <li>Runs as a DaemonSet on each node</li> <li>Watches for changes to the <code>global-pull-secret</code> in <code>kube-system</code> namespace</li> <li>Accesses the original <code>pull-secret</code> in <code>openshift-config</code> namespace</li> <li>Updates the kubelet configuration file</li> <li> <p>Manages kubelet service restarts via DBus</p> </li> <li> <p>Hosted Cluster Config Operator Integration</p> </li> <li>Monitors for the presence of <code>additional-pull-secret</code></li> <li>Orchestrates the entire process</li> <li>Handles cleanup when the secret is removed</li> </ol>"},{"location":"how-to/openstack/global-pull-secret/#architecture-diagram","title":"Architecture Diagram","text":"graph TB     %% User Input     User[User creates additional-pull-secret] --&gt; |kube-system namespace| AdditionalPS[additional-pull-secret Secret]      %% HCCO Controller     HCCO[Hosted Cluster Config Operator] --&gt; |Watches kube-system secrets| GlobalPSController[Global Pull Secret Controller]     GlobalPSController --&gt; |Validates| AdditionalPS     GlobalPSController --&gt; |Gets original| OriginalPS[Original pull-secret from HCP]      %% Secret Processing     AdditionalPS --&gt; |Validates format| ValidatePS[Validate Additional Pull Secret]     OriginalPS --&gt; |Extracts data| OriginalPSData[Original Pull Secret Data]     ValidatePS --&gt; |Extracts data| AdditionalPSData[Additional Pull Secret Data]      %% Merge Process     OriginalPSData --&gt; MergeSecrets[Merge Pull Secrets]     AdditionalPSData --&gt; MergeSecrets     MergeSecrets --&gt; |Creates merged JSON| GlobalPSData[Global Pull Secret Data]      %% Secret Creation     GlobalPSData --&gt; |Creates in kube-system| GlobalPSSecret[global-pull-secret Secret]      %% RBAC Setup     GlobalPSController --&gt; |Creates RBAC| RBACSetup[Setup RBAC Resources]     RBACSetup --&gt; ServiceAccount[global-pull-secret-syncer ServiceAccount]     RBACSetup --&gt; KubeSystemRole[global-pull-secret-syncer Role in kube-system]     RBACSetup --&gt; KubeSystemRoleBinding[global-pull-secret-syncer RoleBinding in kube-system]     RBACSetup --&gt; OpenshiftConfigRole[global-pull-secret-syncer Role in openshift-config]     RBACSetup --&gt; OpenshiftConfigRoleBinding[global-pull-secret-syncer RoleBinding in openshift-config]      %% DaemonSet Deployment     GlobalPSController --&gt; |Deploys DaemonSet| DaemonSet[global-pull-secret-syncer DaemonSet]     DaemonSet --&gt; |Runs on each node| DaemonSetPod[DaemonSet Pod]      %% DaemonSet Pod Details     DaemonSetPod --&gt; |Mounts host paths| HostMounts[Host Path Mounts]     HostMounts --&gt; KubeletPath[\"/var/lib/kubelet\"]     HostMounts --&gt; DbusPath[\"/var/run/dbus\"]      %% Container Execution     DaemonSetPod --&gt; |Runs command| Container[control-plane-operator Container]     Container --&gt; |Executes| SyncCommand[sync-global-pullsecret command]      %% Sync Process     SyncCommand --&gt; |Watches global-pull-secret| SyncController[Global Pull Secret Reconciler]     SyncController --&gt; |Reads secret| ReadGlobalPS[Read global-pull-secret]     SyncController --&gt; |Reads original| ReadOriginalPS[Read original pull-secret]      %% File Update Process     ReadGlobalPS --&gt; |Gets data| GlobalPSBytes[Global Pull Secret Bytes]     ReadOriginalPS --&gt; |Gets data| OriginalPSBytes[Original Pull Secret Bytes]      %% Decision Logic     GlobalPSBytes --&gt; |If exists| UseGlobalPS[Use Global Pull Secret]     OriginalPSBytes --&gt; |If not exists| UseOriginalPS[Use Original Pull Secret]      %% File Update     UseGlobalPS --&gt; |Updates file| UpdateKubeletConfig[\"Update /var/lib/kubelet/config.json\"]     UseOriginalPS --&gt; |Updates file| UpdateKubeletConfig      %% Kubelet Restart     UpdateKubeletConfig --&gt; |Restarts kubelet| RestartKubelet[Restart kubelet.service via systemd]     RestartKubelet --&gt; |Via dbus| DbusConnection[DBus Connection]      %% Error Handling     UpdateKubeletConfig --&gt; |If restart fails| RollbackProcess[Rollback Process]     RollbackProcess --&gt; |Restore original| RestoreOriginal[Restore Original File Content]      %% Cleanup Process     GlobalPSController --&gt; |If additional PS deleted| CleanupProcess[Cleanup Process]     CleanupProcess --&gt; |Deletes global PS| DeleteGlobalPS[Delete global-pull-secret]     CleanupProcess --&gt; |Removes DaemonSet| RemoveDaemonSet[Remove DaemonSet]      %% Styling     classDef userInput fill:#e1f5fe     classDef controller fill:#f3e5f5     classDef secret fill:#e8f5e8     classDef process fill:#fff3e0     classDef daemonSet fill:#fce4ec     classDef fileSystem fill:#f1f8e9      class User,AdditionalPS userInput     class HCCO,GlobalPSController,SyncController controller     class OriginalPS,GlobalPSSecret,ServiceAccount,KubeSystemRole,KubeSystemRoleBinding,OpenshiftConfigRole,OpenshiftConfigRoleBinding secret     class ValidatePS,MergeSecrets,RBACSetup,UpdateKubeletConfig,RestartKubelet process     class DaemonSet,DaemonSetPod,Container daemonSet     class KubeletPath,DbusPath fileSystem"},{"location":"how-to/openstack/global-pull-secret/#key-features","title":"Key Features","text":"<ul> <li>Security: Only watches specific secrets in <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>Robustness: Includes automatic rollback in case of failures</li> <li>Efficiency</li> <li>Only updates when there are actual changes</li> <li>The globalPullSecret implementation has their own controller so it cannot interfere with the HCCO reonciliation</li> <li>Security considerations: Uses specific RBAC for only the required resources in each namespace. The DaemonSet containers run in privileged mode due to the need to:</li> <li>Write to <code>/var/lib/kubelet/config.json</code> (kubelet configuration file)</li> <li>Connect to systemd via DBus for service management</li> <li>Restart kubelet.service, which requires root privileges</li> <li>Smart node targeting: Automatically excludes nodes from InPlace NodePools to prevent MCD conflicts</li> </ul>"},{"location":"how-to/openstack/global-pull-secret/#inplace-nodepool-handling","title":"InPlace NodePool Handling","text":"<p>To prevent conflicts with Machine Config Daemon operations, the implementation includes intelligent node targeting:</p>"},{"location":"how-to/openstack/global-pull-secret/#node-labeling-process","title":"Node Labeling Process","text":"<ol> <li>MachineSets Discovery: The controller queries the management cluster for MachineSets with InPlace-specific annotations (<code>hypershift.openshift.io/nodePoolTargetConfigVersion</code>)</li> <li>Machine Enumeration: For each InPlace MachineSets, it lists all associated Machines</li> <li>Node Identification: Maps Machine objects to their corresponding nodes via <code>machine.Status.NodeRef.Name</code></li> <li>Labeling: Applies <code>hypershift.openshift.io/nodepool-inplace-strategy=true</code> label to identified nodes</li> </ol>"},{"location":"how-to/openstack/global-pull-secret/#daemonset-scheduling-configuration","title":"DaemonSet Scheduling Configuration","text":"<p>The DaemonSet uses NodeAffinity to exclude InPlace nodes:</p> <pre><code>spec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: hypershift.openshift.io/nodepool-inplace-strategy\n                operator: DoesNotExist\n</code></pre> <p>This ensures that: - Nodes without the label: \u2705 Are eligible for DaemonSet scheduling - Nodes with the label (any value): \u274c Are excluded from DaemonSet scheduling</p>"},{"location":"how-to/openstack/global-pull-secret/#conflict-prevention-benefits","title":"Conflict Prevention Benefits","text":"<ul> <li>Prevents MCD failures: Avoids conflicts when MCD expects specific kubelet configuration during InPlace upgrades</li> <li>Maintains upgrade reliability: InPlace upgrade processes are not interrupted by Global Pull Secret modifications</li> <li>Automatic detection: No manual intervention required - the system automatically identifies and handles InPlace nodes</li> </ul>"},{"location":"how-to/openstack/global-pull-secret/#error-handling","title":"Error Handling","text":"<p>The system includes comprehensive error handling:</p> <ul> <li>Validation errors: Invalid DockerConfigJSON format is caught early</li> <li>Restart failures: If kubelet restart fails after 3 attempts, the file is rolled back</li> <li>Resource cleanup: If the additional pull secret is deleted, the HCCO automatically removes the globalPullSecret</li> </ul> <p>This implementation provides a secure, autonomous solution that allows HostedCluster administrators to add private registry credentials without requiring Management Cluster administrator intervention.</p>"},{"location":"how-to/openstack/hostedcluster/","title":"Create a HostedCluster on OpenStack","text":"<p>Once all the prerequisites are met, it is now possible to create a Hosted Cluster on OpenStack.</p> <p>Here are the available options specific to the OpenStack platform:</p> Option Description Required Default <code>--openstack-ca-cert-file</code> Path to the OpenStack CA certificate file No <code>--openstack-cloud</code> Name of the cloud in <code>clouds.yaml</code> No <code>'openstack'</code> <code>--openstack-credentials-file</code> Path to the OpenStack credentials file No <code>--openstack-external-network-id</code> ID of the OpenStack external network No <code>--openstack-ingress-floating-ip</code> A floating IP for OpenShift ingress No <code>--openstack-node-additional-port</code> Attach additional ports to nodes. Params: <code>network-id</code>, <code>vnic-type</code>, <code>disable-port-security</code>, <code>address-pairs</code>. No <code>--openstack-node-availability-zone</code> Availability zone for the nodepool No <code>--openstack-node-flavor</code> Flavor for the nodepool Yes <code>--openstack-node-image-name</code> Image name for the nodepool No <code>--openstack-dns-nameservers</code> List of DNS server addresses that will be provided when creating the subnet No <p>Below is an example of how to create a cluster using environment variables and the <code>hcp</code> cli tool.</p> <p>Note</p> <p>The <code>--release-image</code> flag could be used to provision the HostedCluster with a specific OpenShift Release (the hypershift operator has a support matrix of releases supported by a given version of the operator).</p> <pre><code>export CLUSTER_NAME=example\nexport BASE_DOMAIN=hypershift.lab\nexport PULL_SECRET=\"$HOME/pull-secret\"\nexport WORKER_COUNT=\"2\"\n\n# OpenStack resources for the HostedCluster will be created\n# in that project.\nexport OS_CLOUD=\"openstack\"\n\n# Flavor for the nodepool\nexport FLAVOR=\"m1.large\"\n\n# Pre-defined floating IP for Ingress\nexport INGRESS_FLOATING_IP=\"&lt;ingress-floating-ip&gt;\"\n\n# Optional flags:\n# External network to use for the Ingress endpoint.\nexport EXTERNAL_NETWORK_ID=\"5387f86a-a10e-47fe-91c6-41ac131f9f30\"\n\n# CA certificate path to use for the OpenStack API if using self-signed certificates.\n# In 4.18, this is not required as the CA cert found in clouds.yaml will be used.\nexport CA_CERT_PATH=\"$HOME/ca.crt\"\n\n# In 4.18, this is not required as the file will be discovered.\nexport CLOUDS_YAML=\"$HOME/clouds.yaml\"\n\n# SSH Key for the nodepool VMs\nexport SSH_KEY=\"$HOME/.ssh/id_rsa.pub\"\n\n# DNS nameserver for the subnet\nexport DNS_NAMESERVERS=\"1.1.1.1\"\n\nhcp create cluster openstack \\\n--name $CLUSTER_NAME \\\n--base-domain $BASE_DOMAIN \\\n--node-pool-replicas $WORKER_COUNT \\\n--pull-secret $PULL_SECRET \\\n--ssh-key $SSH_KEY \\\n--openstack-credentials-file $CLOUDS_YAML \\\n--openstack-ca-cert-file $CA_CERT_PATH \\\n--openstack-external-network-id $EXTERNAL_NETWORK_ID \\\n--openstack-node-flavor $FLAVOR \\\n--openstack-ingress-floating-ip $INGRESS_FLOATING_IP \\\n--openstack-dns-nameservers $DNS_NAMESERVERS\n</code></pre> <p>Note</p> <p>A default NodePool will be created for the cluster with 2 VM worker replicas per the <code>--node-pool-replicas</code> flag.</p> <p>Note</p> <p>When using <code>hcp</code> CLI, High Availability will be enabled by default. Pods will be scheduled across different nodes to ensure that the control plane is highly available. When the management cluster worker nodes are spread across different availability zones, the hosted control plane will be spread across different availability zones as well in <code>PreferredDuringSchedulingIgnoredDuringExecution</code> mode for <code>PodAntiAffinity</code>. If your management cluster doesn't have enough workers (less than 3), which is not recommended nor supported, you'll need to specify the <code>--control-plane-availability-policy</code> flag to <code>SingleReplica</code>.</p> <p>Note</p> <p>When not providing the <code>--openstack-node-image-name</code> flag, the latest RHCOS image will be used. ORC will handle the RHCOS image lifecycle by downloading the image from the OpenShift mirror and deleting it when the HostedCluster is deleted. The OpenStack Glance Image will be named like this: <code>&lt;hosted-cluster-name&gt;-rhcos-&lt;rhcos-version&gt;</code>.</p> <p>After a few moments we should see our hosted control plane pods up and running:</p> <pre><code>oc -n clusters-$CLUSTER_NAME get pods\n\nNAME                                                  READY   STATUS    RESTARTS   AGE\ncapi-provider-5cc7b74f47-n5gkr                        1/1     Running   0          3m\ncatalog-operator-5f799567b7-fd6jw                     2/2     Running   0          69s\ncertified-operators-catalog-784b9899f9-mrp6p          1/1     Running   0          66s\ncluster-api-6bbc867966-l4dwl                          1/1     Running   0          66s\n.\n.\n.\nredhat-operators-catalog-9d5fd4d44-z8qqk              1/1     Running   0          66s\n</code></pre> <p>A guest cluster backed by OpenStack virtual machines typically takes around 10-15 minutes to fully provision.</p>"},{"location":"how-to/openstack/hostedcluster/#accessing-the-hostedcluster","title":"Accessing the HostedCluster","text":"<p>CLI access to the guest cluster is gained by retrieving the guest cluster's kubeconfig. Below is an example of how to retrieve the guest cluster's kubeconfig using the hcp cli.</p> <pre><code>hcp create kubeconfig --name $CLUSTER_NAME &gt; $CLUSTER_NAME-kubeconfig\n</code></pre> <p>If we access the cluster, we will see we have two nodes.</p> <pre><code>oc --kubeconfig $CLUSTER_NAME-kubeconfig get nodes\n\nNAME                  STATUS   ROLES    AGE   VERSION\nexample-n6prw         Ready    worker   32m   v1.27.4+18eadca\nexample-nc6g4         Ready    worker   32m   v1.27.4+18eadca\n</code></pre> <p>We can also check the ClusterVersion:</p> <pre><code>oc --kubeconfig $CLUSTER_NAME-kubeconfig get clusterversion\n\nNAME      VERSION       AVAILABLE   PROGRESSING   SINCE   STATUS\nversion   4.17.0        True        False         5m39s   Cluster version is 4.17.0\n</code></pre>"},{"location":"how-to/openstack/hostedcluster/#ingress-and-dns-optional","title":"Ingress and DNS (optional)","text":"<p>If you haven't created the HostedCluster with <code>--openstack-ingress-floating-ip</code>, you'll need to update the DNS record with the floating IP address that was assigned to the <code>router-default</code> Service.</p> <p>Once the workload cluster is deploying, the Ingress controller will be installed and a router named <code>router-default</code> will be created in the <code>openshift-ingress</code> namespace.</p> <p>You'll need to update your DNS with the external IP of that router so Ingress (and dependent operators like console) can work.</p> <p>Once the HostedCluster is created, you need to wait for the <code>router-default</code> service to get an external IP:</p> <pre><code>oc -w --kubeconfig $CLUSTER_NAME-kubeconfig -n openshift-ingress get service/router-default -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n</code></pre> <p>When the external IP exists, you can now create a DNS A record for <code>*.apps.&lt;cluster-name&gt;.&lt;base-domain&gt;</code> that matches the returned IP address. Once this is done, the Ingress operator will become healthy and the console will be accessible shortly after.</p> <p>Note</p> <p>The DNS propagation time can vary so you might need to wait a few minutes before your HostedCluster becomes healthy.</p>"},{"location":"how-to/openstack/hostedcluster/#access-to-the-guest-cluster","title":"Access to the guest cluster","text":"<p>Once the HostedCluster is healthy, you should be able to access the OpenShift console by navigating to <code>https://console-openshift-console.apps.&lt;cluster-name&gt;.&lt;base-domain&gt;</code> in your browser.</p> <p>To get the <code>kubeadmin</code> password, you can run this command: <pre><code>oc get --namespace clusters Secret/${CLUSTER_NAME}-kubeadmin-password -o jsonpath='{.data.password}' | base64 --decode\n</code></pre></p> <p>To know whether the HostedCluster is healthy, you can verify with this command: <pre><code>oc get --namespace clusters hostedclusters\n\nNAME            VERSION   KUBECONFIG                       PROGRESS   AVAILABLE   PROGRESSING   MESSAGE\nexample         4.17.0    example-admin-kubeconfig         Completed  True        False         The hosted control plane is available\n</code></pre></p>"},{"location":"how-to/openstack/hostedcluster/#scaling-an-existing-nodepool","title":"Scaling an existing NodePool","text":"<p>Manually scale a NodePool using the <code>oc scale</code> command:</p> <pre><code>NODEPOOL_NAME=$CLUSTER_NAME\nNODEPOOL_REPLICAS=5\n\noc scale nodepool/$NODEPOOL_NAME --namespace clusters --replicas=$NODEPOOL_REPLICAS\n</code></pre> <p>After a while, in our hosted cluster this is what we will see:</p> <pre><code>oc --kubeconfig $CLUSTER_NAME-kubeconfig get nodes\n\nNAME                  STATUS   ROLES    AGE     VERSION\nexample-9jvnf         Ready    worker   97s     v1.27.4+18eadca\nexample-n6prw         Ready    worker   116m    v1.27.4+18eadca\nexample-nc6g4         Ready    worker   117m    v1.27.4+18eadca\nexample-thp29         Ready    worker   4m17s   v1.27.4+18eadca\nexample-twxns         Ready    worker   88s     v1.27.4+18eadca\n</code></pre>"},{"location":"how-to/openstack/hostedcluster/#openstack-resources-tagging","title":"OpenStack resources tagging","text":"<p>The OpenStack resources created by the CAPI provider are tagged with <code>openshiftClusterID=&lt;infraID&gt;</code> but additional tags can be added to the resources via the <code>HostedCluster.Spec.Platform.OpenStack.Tags</code> field when creating the HostedCluster with a given YAML file.</p>"},{"location":"how-to/openstack/performance-tuning/","title":"Performance guide","text":"<p>This document describes how to tune the performance of nodes in a HostedCluster on OpenStack</p>"},{"location":"how-to/openstack/performance-tuning/#use-cases","title":"Use-cases","text":"<ul> <li>CNF: Cloud-native network functions (CNFs) are network functions that are designed to run in cloud-native environments. CNFs can be used to provide network services such as routing, firewalling, and load balancing. The Nodepool can be configured to use high-performance computing and networking devices to run CNFs.</li> </ul>"},{"location":"how-to/openstack/performance-tuning/#prerequisite","title":"Prerequisite","text":"<ul> <li>An OpenStack flavor with the necessary resources to run the workload: dedicated CPU, memory, and host aggregate   information so the instance will be scheduled on a host with capable hardware (e.g. Intel or Mellanox NICs capable   of SR-IOV, DPDK, etc).</li> <li>An OpenStack network attached to the SR-IOV or DPDK capable NICs; this network must be usable by the project   used by the HostedCluster.</li> </ul>"},{"location":"how-to/openstack/performance-tuning/#node-tuning","title":"Node tuning","text":"<p>Before creating an additional Nodepool used for performance workloads, we'll create a PerformanceProfile that will be used for the nodes.</p> <p>The PerformanceProfile is a custom resource that allows you to define the performance characteristics of a node. The PerformanceProfile can be used to configure the CPU, memory, hugepages, real-time kernel, and other performance-related settings of a node.</p> <p>Here is an example of a PerformanceProfile that can be used to configure a node for high-performance workloads:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: perfprof-1\n  namespace: clusters\ndata:\n  tuning: |\n    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: cnf-performanceprofile\n      namespace: \"${HYPERSHIFT_NAMESPACE}\"\n    data:\n      tuning: |\n        apiVersion: performance.openshift.io/v2\n        kind: PerformanceProfile\n        metadata:\n          name: cnf-performanceprofile\n        spec:\n          additionalKernelArgs:\n            - nmi_watchdog=0\n            - audit=0\n            - mce=off\n            - processor.max_cstate=1\n            - idle=poll\n            - intel_idle.max_cstate=0\n            - amd_iommu=on\n          cpu:\n            isolated: \"${CPU_ISOLATED}\"\n            reserved: \"${CPU_RESERVED}\"\n          hugepages:\n            defaultHugepagesSize: \"1G\"\n            pages:\n              - count: ${HUGEPAGES}\n                node: 0\n                size: 1G\n          nodeSelector:\n            node-role.kubernetes.io/worker: ''\n          realTimeKernel:\n            enabled: false\n          globallyDisableIrqLoadBalancing: true\n</code></pre> <p>Create this PerformanceProfile (change the namespace field in the example above to specify the clusters namespace) by running:</p> <pre><code>oc apply -f perfprof.yaml\n</code></pre>"},{"location":"how-to/openstack/performance-tuning/#nodepool-creation","title":"Nodepool creation","text":"<p>After creating the PerformanceProfile, we can create a Nodepool that will use the PerformanceProfile for the nodes.</p> <pre><code>export NODEPOOL_NAME=$CLUSTER_NAME-cnf\nexport FLAVOR=\"m1.xlarge.nfv\"\n\nhcp create nodepool openstack \\\n  --cluster-name $CLUSTER_NAME \\\n  --name $NODEPOOL_NAME \\\n  --replicas 0 \\\n  --openstack-node-flavor $FLAVOR\n</code></pre> <p>Now let's patch the Nodepool to use the PerformanceProfile:</p> <pre><code>oc patch nodepool -n ${HYPERSHIFT_NAMESPACE} ${CLUSTER_NAME} -p '{\"spec\":{\"tuningConfig\":[{\"name\":\"cnf-performanceprofile\"}]}}' --type=merge\n</code></pre> <p>We can scale the Nodepool to the desired number of nodes:</p> <pre><code>oc scale NodePool/$CLUSTER_NAME --namespace ${HYPERSHIFT_NAMESPACE} --replicas=1\n</code></pre> <p>To wait for the node to be ready:</p> <pre><code>oc wait --for=condition=UpdatingConfig=True nodepool -n ${HYPERSHIFT_NAMESPACE} ${CLUSTER_NAME} --timeout=5m\noc wait --for=condition=UpdatingConfig=False nodepool -n ${HYPERSHIFT_NAMESPACE} ${CLUSTER_NAME} --timeout=30m\noc wait --for=condition=AllNodesHealthy nodepool -n ${HYPERSHIFT_NAMESPACE} ${CLUSTER_NAME} --timeout=5m\n</code></pre> <p>Once the nodes have been deployed, you can SSH into the nodes and check the performance settings:</p> <p><pre><code>ssh core@&lt;node-ip&gt;\n</code></pre> chroot /host cat /proc/cmdline <pre><code>## SR-IOV Network Operator\n\nThe SR-IOV Network Operator is a Kubernetes operator that manages SR-IOV devices on the nodes where workloads will be running. In the context of OpenShift on OpenStack, the operator can be used to create SR-IOV network attachments that can be used by workload pods.\n\nThe nodes deployed by the Nodepool must be labeled so the SR-IOV Network Operator can be deployed on these nodes.\n\nIf you haven't done it already, we need to create a kubeconfig file for the cluster and export it:\n```shell\nhcp create kubeconfig --name $CLUSTER_NAME &gt; $CLUSTER_NAME-kubeconfig\nexport KUBECONFIG=$CLUSTER_NAME-kubeconfig\n</code></pre></p> <p>For every node that was deployed by the Nodepool, label it with the following command:</p> <pre><code>oc label node \"${WORKER_NODE}\" feature.node.kubernetes.io/network-sriov.capable=\"true\"\n</code></pre> <p>The operator can now be installed in the guest cluster by following the SR-IOV Network Operator documentation. The rest of the operations in order to get the workload running with high performances are the same as on a Standalone OCP cluster.</p>"},{"location":"how-to/openstack/prerequisites/","title":"Prerequisites for OpenStack","text":"<ul> <li>Admin access to an OpenShift cluster (version 4.17+) specified by the <code>KUBECONFIG</code> environment variable.   This cluster is referred to as the Management OCP cluster.</li> <li>The Management OCP cluster can be running on OpenStack (ShiftOnStack), but it could also be running on Baremetal or   a public cloud such as AWS.</li> <li>The Management OCP cluster must be configured with OVNKubernetes as the default pod network CNI.</li> <li>The OpenShift CLI (<code>oc</code>) or Kubernetes CLI (<code>kubectl</code>) must be installed.</li> <li>The <code>hcp</code> CLI must be installed and is the production tool to manage the hosted clusters.</li> <li>The <code>hypershift</code> CLI must be installed to deploy the HyperShift Operator. In production, it is not recommended to use that CLI to   manage the hosted clusters.</li> <li>The HyperShift Operator must be installed in the Management OCP cluster.</li> <li>A load-balancer backend must be installed in the Management OCP cluster (e.g. Octavia) so the kube-api Service can be created for each Hosted Cluster.</li> <li>A valid pull secret file for the <code>quay.io/openshift-release-dev</code> repository.</li> <li>OpenStack Octavia service must be running in the cloud hosting the guest cluster when ingress is configured with an Octavia load balancer.   In the future, we'll explore other Ingress options like MetalLB.</li> <li>The default external network (on which the kube-apiserver LoadBalancer type service is created) of the Management OCP cluster must be reachable from the guest cluster.</li> </ul>"},{"location":"how-to/openstack/prerequisites/#install-the-hypershift-and-hcp-cli","title":"Install the HyperShift and HCP CLI","text":"<p>The <code>hcp</code> CLI tool is used to manage the creation and destruction of guest clusters.</p> <p>The <code>hypershift</code> CLI tool is a development tool that is used to install developer builds of the HyperShift Operator. The command below builds latest hypershift and hcp cli tools from source and places the CLI tool within the <code>/usr/local/bin</code> directory.</p> <p>Note</p> <p>The command below is the same if you use docker.</p> <pre><code>podman run --rm --privileged -it -v \\\n$PWD:/output docker.io/library/golang:1.23 /bin/bash -c \\\n'git clone https://github.com/openshift/hypershift.git &amp;&amp; \\\ncd hypershift/ &amp;&amp; \\\nmake hypershift product-cli &amp;&amp; \\\nmv bin/hypershift /output/hypershift &amp;&amp; \\\nmv bin/hcp /output/hcp'\n\nsudo install -m 0755 -o root -g root $PWD/hypershift /usr/local/bin/hypershift\nsudo install -m 0755 -o root -g root $PWD/hcp /usr/local/bin/hcp\nrm $PWD/hypershift\nrm $PWD/hcp\n</code></pre>"},{"location":"how-to/openstack/prerequisites/#deploy-the-hypershift-operator","title":"Deploy the HyperShift Operator","text":"<p>Use the hypershift cli tool to install the HyperShift operator into the management cluster.</p> <pre><code>hypershift install --tech-preview-no-upgrade\n</code></pre> <p>Note</p> <p>HyperShift on OpenStack is possible behind a feature gate, which is why we have to install the operator with <code>--tech-preview-no-upgrade</code>. Once the platform is GA, the operator will be able to be installed without that flag.</p> <p>Once installed, you should see the operator running in the <code>hypershift</code> namespace:</p> <pre><code>oc -n hypershift get pods\n\nNAME                        READY   STATUS    RESTARTS   AGE\noperator-755d587f44-lrtrq   1/1     Running   0          114s\noperator-755d587f44-qj6pz   1/1     Running   0          114s\n</code></pre>"},{"location":"how-to/openstack/prerequisites/#prepare-the-management-cluster-to-store-etcd-locally","title":"Prepare the management cluster to store etcd locally","text":"<p>HostedClusters will have pod(s) for etcd and its performance is essential for the cluster health. In production environments, it's required to put etcd data on fast storage and in the case of OpenStack it'll be local storage. Follow this procedure to leverage a well-known and tested solution.</p>"},{"location":"how-to/openstack/prerequisites/#upload-rhcos-image-in-openstack-optional","title":"Upload RHCOS image in OpenStack (optional)","text":"<p>The user can specify which RHCOS image to use when deploying the node pools on OpenStack by uploading the image to the OpenStack cloud. If the image is not uploaded to OpenStack, OpenStack Resource Controller (ORC) will manage the RHCOS image lifecycle by downloading the image from the OpenShift mirror and deleting it when the HostedCluster is deleted.</p> <p>Here is an example of how to upload an RHCOS image to OpenStack:</p> <pre><code>openstack image create --disk-format qcow2 --file rhcos-openstack.x86_64.qcow2 rhcos\n</code></pre> <p>Note</p> <p>The <code>rhcos-openstack.x86_64.qcow2</code> file is the RHCOS image that was downloaded from the OpenShift mirror. You can download the latest RHCOS image from the Red Hat OpenShift Container Platform mirror.</p>"},{"location":"how-to/openstack/prerequisites/#create-a-floating-ip-for-the-ingress-optional","title":"Create a floating IP for the Ingress (optional)","text":"<p>To get Ingress healthy in a HostedCluster without manual intervention, you need to create a floating IP that will be used by the Ingress service.</p> <pre><code>openstack floating ip create &lt;external-network-id&gt;\n</code></pre> <p>If you provide the floating IP to the <code>--openstack-ingress-floating-ip</code> flag without pre-creating it, cloud-provider-openstack will create it for you only if the Neutron API policy allows a user to create floating IP with a specific IP address.</p>"},{"location":"how-to/openstack/prerequisites/#update-the-dns-record-for-the-ingress-optional","title":"Update the DNS record for the Ingress (optional)","text":"<p>If you use a pre-defined floating IP for ingress, you need to create a DNS record for the following wildcard domain that needs to point to the Ingress floating IP: <code>*.apps.&lt;cluster-name&gt;.&lt;base-domain&gt;</code></p>"},{"location":"how-to/powervs/create-cluster-powervs/","title":"Create PowerVS Hosted Cluster","text":"<p>Create Hosted cluster in IBM Cloud PowerVS service.</p>"},{"location":"how-to/powervs/create-cluster-powervs/#prerequisites","title":"Prerequisites","text":"<p>Please see prerequisites before setting up the cluster</p>"},{"location":"how-to/powervs/create-cluster-powervs/#creating-the-cluster","title":"Creating the Cluster","text":"<p>Use the <code>hypershift create cluster powervs</code> command:</p> <pre><code>CLUSTER_NAME=example\nREGION=tok\nZONE=tok04\nVPC_REGION=jp-tok\nBASEDOMAIN=hypershift-on-power.com\nRESOURCE_GROUP=ibm-hypershift-dev\nRELEASE_IMAGE=quay.io/openshift-release-dev/ocp-release:4.12.0-0.nightly-multi-2022-09-08-131900\nPULL_SECRET=\"$HOME/pull-secret\"\n\n./bin/hypershift create cluster powervs \\\n    --name $CLUSTER_NAME \\\n    --region $REGION \\\n    --zone $ZONE \\\n    --vpc-region $VPC_REGION \\\n    --base-domain $BASEDOMAIN \\\n    --resource-group $RESOURCE_GROUP \\\n    --release-image $RELEASE_IMAGE \\\n    --pull-secret $PULL_SECRET \\\n    --node-pool-replicas=2 \\\n    --transit-gateway-location $TRANSIT_GATEWAY_LOCATION\n</code></pre> <p>where</p> <ul> <li>CLUSTER_NAME is a name for the cluster.</li> <li>REGION is the region where you want to create the PowerVS resources.</li> <li>ZONE is the zone under REGION where you want to create the PowerVS resources.</li> <li>VPC_REGION is the region where you want to create the VPC resources.</li> <li>BASEDOMAIN is the CIS base domain that will be used for your hosted cluster's ingress. It should be an existing CIS domain name.</li> <li>RESOURCE_GROUP is the resource group in IBM Cloud where your infrastructure resources will be created.</li> <li>RELEASE_IMAGE is the latest multi arch release image.</li> <li>PULL_SECRET is a file that contains a valid OpenShift pull secret.</li> <li>node-pool-replicas is worker node count. </li> <li>TRANSIT_GATEWAY_LOCATION is the location where you want to create the transit gateway. </li> </ul> <p>Running this command will create infra and manifests for the hosted cluster and deploys it.</p> <p>Important</p> <pre><code>Need to understand --recreate-secrets flag usage before using it. Enabling this flag will result in recreating the creds mentioned here [PowerVSPlatformSpec](https://hypershift-docs.netlify.app/reference/api/#hypershift.openshift.io/v1alpha1.PowerVSPlatformSpec)\n\nThis is required when rerunning `hypershift create cluster powervs` command, since API Key once created cannot be retrieved again.\n\nPlease make sure cluster name used is unique across different management cluster before using this flag since this will result in removing the existing cred's service ID and recreate them.\n</code></pre>"},{"location":"how-to/powervs/create-infra-separately/","title":"Create IBMCloud PowerVS Infra resources separately","text":"<p>The default behavior of the <code>hypershift create cluster powervs</code> command is to create cloud infrastructure along with the manifests for hosted cluster and apply it.</p> <p>It is possible to create the cloud infrastructure separately so that the <code>hypershift create cluster powervs</code> command can just be used to create the manifests and apply it.</p> <p>In order to do this, you need to: 1. Create PowerVS infrastructure 2. Create cluster</p>"},{"location":"how-to/powervs/create-infra-separately/#creating-the-powervs-infra","title":"Creating the PowerVS infra","text":"<p>Please see prerequisites before setting up the infra</p> <p>Use the <code>hypershift create infra powervs</code> command:</p> <pre><code>CLUSTER_NAME=example\nINFRA_ID=example-infra\nREGION=tok\nZONE=tok04\nVPC_REGION=jp-tok\nBASEDOMAIN=hypershift-on-power.com\nRESOURCE_GROUP=ibm-hypershift-dev\nOUTPUT_INFRA_FILE=infra.json\n\n./bin/hypershift create infra powervs \\\n    --name $CLUSTER_NAME \\\n    --infra-id $INFRA_ID \\\n    --region $REGION \\\n    --zone $ZONE \\\n    --region $VPC_REGION \\\n    --base-domain $BASEDOMAIN \\\n    --resource-group $RESOURCE_GROUP \\\n    --output-file $OUTPUT_INFRA_FILE\n</code></pre> <p>where</p> <ul> <li>CLUSTER_NAME is a name for the cluster.</li> <li>INFRA_ID is a unique name that will be used to name the infrastructure resources.</li> <li>REGION is the region where you want to create the PowerVS resources.</li> <li>ZONE is the zone under POWERVS_REGION where you want to create the PowerVS resources.</li> <li>VPC_REGION is the region where you want to create the VPC resources.</li> <li>BASEDOMAIN is the CIS base domain that will be used for your hosted cluster's ingress. It should be an existing CIS domain name.</li> <li>RESOURCE_GROUP is the resource group in IBM Cloud where your infrastructure resources will be created.</li> <li>OUTPUT_INFRA_FILE is the file where IDs of the infrastructure that has been created will be stored in JSON format.   This file can then be used as input to the <code>hypershift create cluster powervs</code> command to populate   the appropriate fields in the HostedCluster and NodePool resources.</li> </ul> <p>Running this command should result in the following resources getting created in IBM Cloud:</p>"},{"location":"how-to/powervs/create-infra-separately/#powervs-cluster-infra-resources","title":"PowerVS Cluster Infra Resources","text":"<ul> <li>1 VPC with Subnet</li> <li>1 PowerVS Cloud Instance</li> <li>1 DHCP Service</li> <li>1 DHCP Private Network</li> <li>1 DHCP Public Network</li> <li>1 Cloud Connection</li> </ul>"},{"location":"how-to/powervs/create-infra-separately/#creating-the-cluster","title":"Creating the Cluster","text":"<p>Once you have the <code>OUTPUT_INFRA_FILE</code> generated, can pass this file <code>hypershift create cluster powervs</code> to this command with <code>--infra-json</code> flag Running the below command set up the cluster on infra created separately</p> <p>E.g.:</p> <pre><code>CLUSTER_NAME=example\nREGION=tok\nZONE=tok04\nVPC_REGION=jp-tok\nBASEDOMAIN=hypershift-on-power.com\nRESOURCE_GROUP=ibm-hypershift-dev\nRELEASE_IMAGE=quay.io/openshift-release-dev/ocp-release:4.12.0-0.nightly-multi-2022-09-08-131900\nPULL_SECRET=\"$HOME/pull-secret\"\nINFRA_JSON=infra.json\n\n./bin/hypershift create cluster powervs \\\n    --name $CLUSTER_NAME \\\n    --region $REGION \\\n    --zone $ZONE \\\n    --vpc-region $VPC_REGION \\\n    --base-domain $BASEDOMAIN \\\n    --resource-group $RESOURCE_GROUP \\\n    --release-image $RELEASE_IMAGE\n    --pull-secret $PULL_SECRET \\\n    --infra-json $INFRA_JSON \\\n    --node-pool-replicas=2\n</code></pre> <p>Important</p> <pre><code>Need to understand --recreate-secrets flag usage before using it. Enabling this flag will result in recreating the creds mentioned here [PowerVSPlatformSpec](https://hypershift-docs.netlify.app/reference/api/#hypershift.openshift.io/v1alpha1.PowerVSPlatformSpec)\n\nThis is required when rerunning `hypershift create cluster powervs` command, since API Key once created cannot be retrieved again.\n\nPlease make sure cluster name used is unique across different management cluster before using this flag since this will result in removing the existing cred's service ID and recreate them.\n</code></pre>"},{"location":"how-to/powervs/global-pull-secret/","title":"Global Pull Secret for Hosted Control Planes","text":""},{"location":"how-to/powervs/global-pull-secret/#overview","title":"Overview","text":"<p>The Global Pull Secret functionality enables Hosted Cluster administrators to include additional pull secrets for accessing container images from private registries without requiring assistance from the Management Cluster administrator. This feature allows you to merge your custom pull secret with the original HostedCluster pull secret, making it available to all nodes in the cluster.</p> <p>The implementation uses a DaemonSet approach that automatically detects when you create an <code>additional-pull-secret</code> in the <code>kube-system</code> namespace of your DataPlane (Hosted Cluster). The system then merges this secret with the original pull secret and deploys the merged result to all nodes via a DaemonSet that updates the kubelet configuration.</p> <p>Note</p> <p>This feature is designed to work autonomously - once you create the additional pull secret, the system automatically handles the rest without requiring Management Cluster administrator intervention.</p>"},{"location":"how-to/powervs/global-pull-secret/#adding-your-pull-secret","title":"Adding your Pull Secret","text":"<p>Important</p> <p>All actions described in this section must be performed on the HostedCluster's workers (DataPlane), not on the Management Cluster.</p> <p>To use this functionality, follow these steps:</p>"},{"location":"how-to/powervs/global-pull-secret/#1-create-your-additional-pull-secret","title":"1. Create your additional pull secret","text":"<p>Create a secret named <code>additional-pull-secret</code> in the <code>kube-system</code> namespace of your Hosted Cluster (DataPlane). The secret must contain a valid DockerConfigJSON format:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: additional-pull-secret\n  namespace: kube-system\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: &lt;base64-encoded-docker-config-json&gt;\n</code></pre>"},{"location":"how-to/powervs/global-pull-secret/#2-example-dockerconfigjson-format","title":"2. Example DockerConfigJSON format","text":"<p>Your <code>.dockerconfigjson</code> should follow this structure:</p> <pre><code>{\n  \"auths\": {\n    \"registry.example.com\": {\n      \"auth\": \"base64-encoded-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"base64-encoded-credentials\"\n    }\n  }\n}\n</code></pre> <p>Using Namespace-Specific Registry Entries</p> <p>For registries like Quay.io that support organization/namespace-specific authentication, you can specify the full path in your registry entry (e.g., <code>quay.io/mycompany</code> instead of just <code>quay.io</code>). This allows you to provide different credentials for different namespaces within the same registry, and helps avoid conflicts with existing registry entries in the original pull secret.</p>"},{"location":"how-to/powervs/global-pull-secret/#3-apply-the-secret","title":"3. Apply the secret","text":"<pre><code>kubectl apply -f additional-pull-secret.yaml\n</code></pre>"},{"location":"how-to/powervs/global-pull-secret/#4-verification","title":"4. Verification","text":"<p>After creating the secret, the system will automatically:</p> <ol> <li>Validate the secret format</li> <li>Merge it with the original pull secret</li> <li>Deploy a DaemonSet to all nodes</li> <li>Update the kubelet configuration on each node</li> </ol> <p>You can verify the deployment by checking:</p> <pre><code># Check if the DaemonSet is running\nkubectl get daemonset global-pull-secret-syncer -n kube-system\n\n# Check the merged pull secret\nkubectl get secret global-pull-secret -n kube-system\n\n# Check DaemonSet pods\nkubectl get pods -n kube-system -l name=global-pull-secret-syncer\n</code></pre>"},{"location":"how-to/powervs/global-pull-secret/#how-it-works","title":"How it works","text":"<p>The Global Pull Secret functionality operates through a multi-component system:</p>"},{"location":"how-to/powervs/global-pull-secret/#automatic-detection","title":"Automatic Detection","text":"<ul> <li>The Hosted Cluster Config Operator (HCCO) continuously monitors the <code>kube-system</code> namespace</li> <li>When it detects the creation of <code>additional-pull-secret</code>, it triggers the reconciliation process</li> </ul>"},{"location":"how-to/powervs/global-pull-secret/#validation-and-merging","title":"Validation and Merging","text":"<ul> <li>The system validates that your secret contains a proper DockerConfigJSON format</li> <li>It retrieves the original pull secret from the HostedControlPlane</li> <li>Your additional pull secret is merged with the original one</li> <li>If there are conflicting registry entries, the original pull secret takes precedence (the additional pull secret entry is ignored for conflicting registries)</li> <li>The system supports namespace-specific registry entries (e.g., <code>quay.io/namespace</code>) for better credential specificity</li> </ul>"},{"location":"how-to/powervs/global-pull-secret/#deployment-process","title":"Deployment Process","text":"<ul> <li>A <code>global-pull-secret</code> is created in the <code>kube-system</code> namespace containing the merged result</li> <li>RBAC resources (ServiceAccount, Role, RoleBinding) are created for the DaemonSet in both <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>We use Role and RoleBinding in both namespaces to access secrets in <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>A DaemonSet named <code>global-pull-secret-syncer</code> is deployed to eligible nodes</li> </ul> <p>NodePool InPlace Strategy Restriction</p> <p>The Global Pull Secret DaemonSet is not deployed to nodes that belong to NodePools using the InPlace upgrade strategy. This restriction prevents conflicts between the DaemonSet's modifications to <code>/var/lib/kubelet/config.json</code> and the Machine Config Daemon (MCD) during InPlace upgrades.</p> <ul> <li>Nodes with Replace strategy: \u2705 Receive Global Pull Secret DaemonSet</li> <li>Nodes with InPlace strategy: \u274c Do not receive Global Pull Secret DaemonSet</li> </ul> <p>This ensures that MCD operations during InPlace upgrades do not fail due to unexpected changes in kubelet configuration files.</p>"},{"location":"how-to/powervs/global-pull-secret/#node-level-synchronization","title":"Node-Level Synchronization","text":"<ul> <li>Each DaemonSet pod runs a controller that watches the secrets under kube-system namespace</li> <li>When changes are detected, it updates <code>/var/lib/kubelet/config.json</code> on the node</li> <li>The kubelet service is restarted via DBus to apply the new configuration</li> <li>If the restart fails after 3 attempts, the system rolls back the file changes</li> </ul>"},{"location":"how-to/powervs/global-pull-secret/#automatic-cleanup","title":"Automatic Cleanup","text":"<ul> <li>If you delete the <code>additional-pull-secret</code>, the HCCO automatically removes the <code>global-pull-secret</code> secret</li> <li>The system reverts to using only the original pull secret from the HostedControlPlane</li> <li>The DaemonSet continues running but now syncs only the original pull secret to nodes</li> </ul>"},{"location":"how-to/powervs/global-pull-secret/#registry-precedence-and-conflict-resolution","title":"Registry Precedence and Conflict Resolution","text":"<p>The Global Pull Secret system uses a specific precedence model when merging your additional pull secret with the original one:</p>"},{"location":"how-to/powervs/global-pull-secret/#merge-behavior","title":"Merge Behavior","text":"<ul> <li>Original pull secret entries always take precedence over additional pull secret entries for the same registry</li> <li>If both secrets contain an entry for <code>quay.io</code>, the original pull secret's credentials will be used</li> <li>Your additional pull secret entries are only added if they don't conflict with existing entries</li> <li>Warnings are logged when conflicts are detected</li> </ul>"},{"location":"how-to/powervs/global-pull-secret/#recommended-approach","title":"Recommended Approach","text":"<p>To avoid conflicts and ensure your credentials are used, consider these strategies:</p> <ol> <li>Use namespace-specific entries: Instead of <code>quay.io</code>, use <code>quay.io/your-namespace</code></li> <li>Target specific registries: Add entries only for registries not already in the original pull secret</li> <li>Check existing entries: Review what registries are already configured in the HostedControlPlane</li> </ol>"},{"location":"how-to/powervs/global-pull-secret/#example-merge-scenario","title":"Example Merge Scenario","text":"<p>Original Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"original-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Your Additional Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"your-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"your-namespace-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Resulting Merged Pull Secret: <pre><code>{\n  \"auths\": {\n    \"quay.io\": {\n      \"auth\": \"original-credentials\"\n    },\n    \"quay.io/mycompany\": {\n      \"auth\": \"your-namespace-credentials\"\n    }\n  }\n}\n</code></pre></p> <p>Note how the <code>quay.io</code> entry keeps the original credentials, but <code>quay.io/mycompany</code> is added from your additional secret.</p>"},{"location":"how-to/powervs/global-pull-secret/#implementation-details","title":"Implementation details","text":"<p>The implementation consists of several key components working together:</p>"},{"location":"how-to/powervs/global-pull-secret/#core-components","title":"Core Components","text":"<ol> <li>Global Pull Secret Controller (<code>globalps</code> package)</li> <li>Handles validation of user-provided pull secrets</li> <li>Manages the merging logic between original and additional pull secrets</li> <li>Creates and manages RBAC resources</li> <li>Deploys and manages the DaemonSet</li> <li> <p>Node eligibility assessment: Labels nodes from InPlace NodePools and configures DaemonSet scheduling restrictions</p> </li> <li> <p>Sync Global Pull Secret Command (<code>sync-global-pullsecret</code> package)</p> </li> <li>Runs as a DaemonSet on each node</li> <li>Watches for changes to the <code>global-pull-secret</code> in <code>kube-system</code> namespace</li> <li>Accesses the original <code>pull-secret</code> in <code>openshift-config</code> namespace</li> <li>Updates the kubelet configuration file</li> <li> <p>Manages kubelet service restarts via DBus</p> </li> <li> <p>Hosted Cluster Config Operator Integration</p> </li> <li>Monitors for the presence of <code>additional-pull-secret</code></li> <li>Orchestrates the entire process</li> <li>Handles cleanup when the secret is removed</li> </ol>"},{"location":"how-to/powervs/global-pull-secret/#architecture-diagram","title":"Architecture Diagram","text":"graph TB     %% User Input     User[User creates additional-pull-secret] --&gt; |kube-system namespace| AdditionalPS[additional-pull-secret Secret]      %% HCCO Controller     HCCO[Hosted Cluster Config Operator] --&gt; |Watches kube-system secrets| GlobalPSController[Global Pull Secret Controller]     GlobalPSController --&gt; |Validates| AdditionalPS     GlobalPSController --&gt; |Gets original| OriginalPS[Original pull-secret from HCP]      %% Secret Processing     AdditionalPS --&gt; |Validates format| ValidatePS[Validate Additional Pull Secret]     OriginalPS --&gt; |Extracts data| OriginalPSData[Original Pull Secret Data]     ValidatePS --&gt; |Extracts data| AdditionalPSData[Additional Pull Secret Data]      %% Merge Process     OriginalPSData --&gt; MergeSecrets[Merge Pull Secrets]     AdditionalPSData --&gt; MergeSecrets     MergeSecrets --&gt; |Creates merged JSON| GlobalPSData[Global Pull Secret Data]      %% Secret Creation     GlobalPSData --&gt; |Creates in kube-system| GlobalPSSecret[global-pull-secret Secret]      %% RBAC Setup     GlobalPSController --&gt; |Creates RBAC| RBACSetup[Setup RBAC Resources]     RBACSetup --&gt; ServiceAccount[global-pull-secret-syncer ServiceAccount]     RBACSetup --&gt; KubeSystemRole[global-pull-secret-syncer Role in kube-system]     RBACSetup --&gt; KubeSystemRoleBinding[global-pull-secret-syncer RoleBinding in kube-system]     RBACSetup --&gt; OpenshiftConfigRole[global-pull-secret-syncer Role in openshift-config]     RBACSetup --&gt; OpenshiftConfigRoleBinding[global-pull-secret-syncer RoleBinding in openshift-config]      %% DaemonSet Deployment     GlobalPSController --&gt; |Deploys DaemonSet| DaemonSet[global-pull-secret-syncer DaemonSet]     DaemonSet --&gt; |Runs on each node| DaemonSetPod[DaemonSet Pod]      %% DaemonSet Pod Details     DaemonSetPod --&gt; |Mounts host paths| HostMounts[Host Path Mounts]     HostMounts --&gt; KubeletPath[\"/var/lib/kubelet\"]     HostMounts --&gt; DbusPath[\"/var/run/dbus\"]      %% Container Execution     DaemonSetPod --&gt; |Runs command| Container[control-plane-operator Container]     Container --&gt; |Executes| SyncCommand[sync-global-pullsecret command]      %% Sync Process     SyncCommand --&gt; |Watches global-pull-secret| SyncController[Global Pull Secret Reconciler]     SyncController --&gt; |Reads secret| ReadGlobalPS[Read global-pull-secret]     SyncController --&gt; |Reads original| ReadOriginalPS[Read original pull-secret]      %% File Update Process     ReadGlobalPS --&gt; |Gets data| GlobalPSBytes[Global Pull Secret Bytes]     ReadOriginalPS --&gt; |Gets data| OriginalPSBytes[Original Pull Secret Bytes]      %% Decision Logic     GlobalPSBytes --&gt; |If exists| UseGlobalPS[Use Global Pull Secret]     OriginalPSBytes --&gt; |If not exists| UseOriginalPS[Use Original Pull Secret]      %% File Update     UseGlobalPS --&gt; |Updates file| UpdateKubeletConfig[\"Update /var/lib/kubelet/config.json\"]     UseOriginalPS --&gt; |Updates file| UpdateKubeletConfig      %% Kubelet Restart     UpdateKubeletConfig --&gt; |Restarts kubelet| RestartKubelet[Restart kubelet.service via systemd]     RestartKubelet --&gt; |Via dbus| DbusConnection[DBus Connection]      %% Error Handling     UpdateKubeletConfig --&gt; |If restart fails| RollbackProcess[Rollback Process]     RollbackProcess --&gt; |Restore original| RestoreOriginal[Restore Original File Content]      %% Cleanup Process     GlobalPSController --&gt; |If additional PS deleted| CleanupProcess[Cleanup Process]     CleanupProcess --&gt; |Deletes global PS| DeleteGlobalPS[Delete global-pull-secret]     CleanupProcess --&gt; |Removes DaemonSet| RemoveDaemonSet[Remove DaemonSet]      %% Styling     classDef userInput fill:#e1f5fe     classDef controller fill:#f3e5f5     classDef secret fill:#e8f5e8     classDef process fill:#fff3e0     classDef daemonSet fill:#fce4ec     classDef fileSystem fill:#f1f8e9      class User,AdditionalPS userInput     class HCCO,GlobalPSController,SyncController controller     class OriginalPS,GlobalPSSecret,ServiceAccount,KubeSystemRole,KubeSystemRoleBinding,OpenshiftConfigRole,OpenshiftConfigRoleBinding secret     class ValidatePS,MergeSecrets,RBACSetup,UpdateKubeletConfig,RestartKubelet process     class DaemonSet,DaemonSetPod,Container daemonSet     class KubeletPath,DbusPath fileSystem"},{"location":"how-to/powervs/global-pull-secret/#key-features","title":"Key Features","text":"<ul> <li>Security: Only watches specific secrets in <code>kube-system</code> and <code>openshift-config</code> namespaces</li> <li>Robustness: Includes automatic rollback in case of failures</li> <li>Efficiency</li> <li>Only updates when there are actual changes</li> <li>The globalPullSecret implementation has their own controller so it cannot interfere with the HCCO reonciliation</li> <li>Security considerations: Uses specific RBAC for only the required resources in each namespace. The DaemonSet containers run in privileged mode due to the need to:</li> <li>Write to <code>/var/lib/kubelet/config.json</code> (kubelet configuration file)</li> <li>Connect to systemd via DBus for service management</li> <li>Restart kubelet.service, which requires root privileges</li> <li>Smart node targeting: Automatically excludes nodes from InPlace NodePools to prevent MCD conflicts</li> </ul>"},{"location":"how-to/powervs/global-pull-secret/#inplace-nodepool-handling","title":"InPlace NodePool Handling","text":"<p>To prevent conflicts with Machine Config Daemon operations, the implementation includes intelligent node targeting:</p>"},{"location":"how-to/powervs/global-pull-secret/#node-labeling-process","title":"Node Labeling Process","text":"<ol> <li>MachineSets Discovery: The controller queries the management cluster for MachineSets with InPlace-specific annotations (<code>hypershift.openshift.io/nodePoolTargetConfigVersion</code>)</li> <li>Machine Enumeration: For each InPlace MachineSets, it lists all associated Machines</li> <li>Node Identification: Maps Machine objects to their corresponding nodes via <code>machine.Status.NodeRef.Name</code></li> <li>Labeling: Applies <code>hypershift.openshift.io/nodepool-inplace-strategy=true</code> label to identified nodes</li> </ol>"},{"location":"how-to/powervs/global-pull-secret/#daemonset-scheduling-configuration","title":"DaemonSet Scheduling Configuration","text":"<p>The DaemonSet uses NodeAffinity to exclude InPlace nodes:</p> <pre><code>spec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: hypershift.openshift.io/nodepool-inplace-strategy\n                operator: DoesNotExist\n</code></pre> <p>This ensures that: - Nodes without the label: \u2705 Are eligible for DaemonSet scheduling - Nodes with the label (any value): \u274c Are excluded from DaemonSet scheduling</p>"},{"location":"how-to/powervs/global-pull-secret/#conflict-prevention-benefits","title":"Conflict Prevention Benefits","text":"<ul> <li>Prevents MCD failures: Avoids conflicts when MCD expects specific kubelet configuration during InPlace upgrades</li> <li>Maintains upgrade reliability: InPlace upgrade processes are not interrupted by Global Pull Secret modifications</li> <li>Automatic detection: No manual intervention required - the system automatically identifies and handles InPlace nodes</li> </ul>"},{"location":"how-to/powervs/global-pull-secret/#error-handling","title":"Error Handling","text":"<p>The system includes comprehensive error handling:</p> <ul> <li>Validation errors: Invalid DockerConfigJSON format is caught early</li> <li>Restart failures: If kubelet restart fails after 3 attempts, the file is rolled back</li> <li>Resource cleanup: If the additional pull secret is deleted, the HCCO automatically removes the globalPullSecret</li> </ul> <p>This implementation provides a secure, autonomous solution that allows HostedCluster administrators to add private registry credentials without requiring Management Cluster administrator intervention.</p>"},{"location":"how-to/powervs/prerequisites-and-env-guide/","title":"Prerequisites","text":""},{"location":"how-to/powervs/prerequisites-and-env-guide/#generic","title":"Generic","text":"<ul> <li> <p>The HyperShift CLI (<code>hypershift</code>).</p> <p>Install it using Go 1.18:     <pre><code>git clone https://github.com/openshift/hypershift.git\ncd hypershift\nmake build\nsudo install -m 0755 bin/hypershift /usr/local/bin/hypershift\n</code></pre></p> </li> <li> <p>Admin access to an OpenShift cluster (version 4.8+) specified by the <code>KUBECONFIG</code> environment variable.</p> </li> <li>The OpenShift CLI (<code>oc</code>) or Kubernetes CLI (<code>kubectl</code>).</li> <li>A valid pull secret file for the <code>quay.io/openshift-release-dev</code> repository.</li> </ul>"},{"location":"how-to/powervs/prerequisites-and-env-guide/#install-hypershift","title":"Install Hypershift","text":"<p>Install HyperShift into the management cluster. Once Hypershift CLI and management cluster is ready, run below command to install Hypershift operator and CRDs which are required to setup the cluster.</p> <pre><code>hypershift install\n</code></pre>"},{"location":"how-to/powervs/prerequisites-and-env-guide/#authentication","title":"Authentication","text":"<p>There are two ways to set up authentication</p> <ul> <li>Authenticate IBM Cloud Clients by setting the <code>IBMCLOUD_API_KEY</code> environment var to your API Key.</li> <li>Authenticate IBM Cloud Clients by setting the <code>IBMCLOUD_CREDENTIALS</code> environment var pointing to a file containing your API Key.</li> </ul>"},{"location":"how-to/powervs/prerequisites-and-env-guide/#authorization","title":"Authorization:","text":"<p>API Key used should have below services with respective roles for hypershift cluster to get created in IBM Cloud.</p> Service Roles Workspace for Power Systems Virtual Server Manager, Administrator VPC Infrastructure Services Manager, Administrator Internet Services Manager, Administrator Direct Link Viewer IAM Identity Service User API key creator, Service ID creator, Administrator All account management services Administrator All Identity and Access enabled services Manager, Editor Cloud Object Storage Manager, Administrator Transit Gateway Manager, Editor"},{"location":"how-to/powervs/prerequisites-and-env-guide/#base-domain","title":"Base Domain","text":"<p>Need to have existing CIS Domain in IBM Cloud Internet Services which can be used as a <code>BASEDOMAIN</code> while creating the cluster.</p>"},{"location":"how-to/powervs/prerequisites-and-env-guide/#region-and-zones","title":"Region and Zones","text":"<p>Refer this to get possible region and zone values. Substitute those with <code>REGION</code> <code>ZONE</code> <code>VPC_REGION</code> and <code>TRANSIT_GATEWAY_LOCATION</code> while creating the cluster.</p>"},{"location":"how-to/powervs/prerequisites-and-env-guide/#release-image","title":"Release Image","text":"<p>Use this to get latest multi arch nightly build as release image. Substitute it with <code>RELEASE_IMAGE</code> while creating the cluster.</p>"},{"location":"how-to/powervs/prerequisites-and-env-guide/#custom-endpoints","title":"Custom Endpoints","text":"<p>Use following environment variables to set custom endpoint. <pre><code>IBMCLOUD_POWER_API_ENDPOINT    - to setup PowerVS custom endpoint\nIBMCLOUD_VPC_API_ENDPOINT      - to setup VPC custom endpoint\nIBMCLOUD_PLATFORM_API_ENDPOINT - to setup platform services custom endpoint\nIBMCLOUD_COS_API_ENDPOINT      - to setup COS custom endpoint, can use this to set up custom endpoints mentioned here https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-endpoints#endpoints-region \n</code></pre></p>"},{"location":"how-to/sdn/other-sdn-providers/","title":"Other sdn providers","text":"<p>This document explains how to create a HostedCluster that runs an SDN provider different from OVNKubernetes. The document assumes that you already have the required infrastructure in place to create HostedClusters.</p> <p>Important</p> <p>The work described here is not supported. SDN providers must certify their software on HyperShift before it becomes a supported solution. The steps described here are just a technical reference for people who wants to try different SDN providers in HyperShift.</p> <p>Versions used while writing this doc:</p> <ul> <li>Management cluster running OpenShift <code>v4.14.5</code> and HyperShift Operator version <code>e87182ca75da37c74b371aa0f17aeaa41437561a</code>.</li> <li>HostedCluster release set to OpenShift <code>v4.14.10</code>.</li> </ul> <p>Important</p> <p>To configure a different CNI provider for the Hosted Cluster, you must adjust the <code>hostedcluster.spec.networking.networkType</code> to <code>Other</code>. By doing so, the Control Plane Operator will skip the deployment of the default CNI provider.</p>"},{"location":"how-to/sdn/other-sdn-providers/#calico","title":"Calico","text":""},{"location":"how-to/sdn/other-sdn-providers/#deployment","title":"Deployment","text":"<p>In this scenario we are using the Calico version v3.27.0 which is the last one at the time of this writing. The steps followed rely on the docs by Tigera to deploy Calico on OpenShift.</p> <ol> <li> <p>Create a <code>HostedCluster</code> and set its <code>HostedCluster.spec.networking.networkType</code> to <code>Other</code>.</p> </li> <li> <p>Wait for the HostedCluster's API to be ready. Once it's ready, get the admin kubeconfig.</p> </li> <li> <p>Eventually the compute nodes will show up in the cluster. Keep in mind since the SDN is not deployed yet, they will remain in <code>NotReady</code> state.</p> <pre><code>export KUBECONFIG=/path/to/hostedcluster/admin/kubeconfig\noc get nodes\n</code></pre> <pre><code>NAME             STATUS     ROLES    AGE     VERSION\nhosted-worker1   NotReady   worker   2m51s   v1.27.8+4fab27b\nhosted-worker2   NotReady   worker   2m52s   v1.27.8+4fab27b\n</code></pre> </li> <li> <p>Apply the yaml manifests provided by <code>Tigera</code> in the HostedCluster:</p> <pre><code>mkdir calico\nwget -qO- https://github.com/projectcalico/calico/releases/download/v3.27.0/ocp.tgz | tar xvz --strip-components=1 -C calico\ncd calico/\nls *crd*.yaml | xargs -n1 oc apply -f\nls 00* | xargs -n1 oc apply -f\nls 01* | xargs -n1 oc apply -f\nls 02* | xargs -n1 oc apply -f\n</code></pre> <pre><code>customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/apiservers.operator.tigera.io created\ncustomresourcedefinition.apiextensions.k8s.io/imagesets.operator.tigera.io created\ncustomresourcedefinition.apiextensions.k8s.io/installations.operator.tigera.io created\ncustomresourcedefinition.apiextensions.k8s.io/tigerastatuses.operator.tigera.io created\nnamespace/calico-apiserver created\nnamespace/calico-system created\nnamespace/tigera-operator created\napiserver.operator.tigera.io/default created\ninstallation.operator.tigera.io/default created\nconfigmap/calico-resources created\nclusterrolebinding.rbac.authorization.k8s.io/tigera-operator created\nclusterrole.rbac.authorization.k8s.io/tigera-operator created\nserviceaccount/tigera-operator created\ndeployment.apps/tigera-operator created\n</code></pre> </li> </ol>"},{"location":"how-to/sdn/other-sdn-providers/#checks","title":"Checks","text":"<p>We should see the following pods running in the <code>tigera-operator</code> namespace:</p> <pre><code>oc -n tigera-operator get pods\n</code></pre> <pre><code>NAME                              READY   STATUS    RESTARTS   AGE\ntigera-operator-dc7c9647f-fvcvd   1/1     Running   0          2m1s\n</code></pre> <p>We should see the following pods running in the <code>calico-system</code> namespace:</p> <pre><code>oc -n calico-system get pods\n</code></pre> <pre><code>NAME                                       READY   STATUS    RESTARTS   AGE\ncalico-kube-controllers-69d6d5ff89-5ftcn   1/1     Running   0          2m1s\ncalico-node-6bzth                          1/1     Running   0          2m2s\ncalico-node-bl4b6                          1/1     Running   0          2m2s\ncalico-typha-6558c4c89d-mq2hw              1/1     Running   0          2m2s\ncsi-node-driver-l948w                      2/2     Running   0          2m1s\ncsi-node-driver-r6rgw                      2/2     Running   0          2m2s\n</code></pre> <p>We should see the following pods running in the <code>calico-apiserver</code> namespace:</p> <pre><code>oc -n calico-apiserver get pods\n</code></pre> <pre><code>NAME                                READY   STATUS    RESTARTS   AGE\ncalico-apiserver-7bfbf8fd7c-d75fw   1/1     Running   0          84s\ncalico-apiserver-7bfbf8fd7c-fqxjm   1/1     Running   0          84s\n</code></pre> <p>The nodes should've moved to <code>Ready</code> state:</p> <pre><code>oc get nodes\n</code></pre> <pre><code>NAME             STATUS   ROLES    AGE   VERSION\nhosted-worker1   Ready    worker   10m   v1.27.8+4fab27b\nhosted-worker2   Ready    worker   10m   v1.27.8+4fab27b\n</code></pre> <p>The HostedCluster deployment will continue, at this point the SDN is running.</p>"},{"location":"how-to/sdn/other-sdn-providers/#cilium","title":"Cilium","text":""},{"location":"how-to/sdn/other-sdn-providers/#deployment_1","title":"Deployment","text":"<p>In this scenario we are using the Cilium version v1.14.5 which is the last one at the time of this writing. The steps followed rely on the docs by Cilium project to deploy Cilium on OpenShift.</p> <ol> <li> <p>Create a <code>HostedCluster</code> and set its <code>HostedCluster.spec.networking.networkType</code> to <code>Other</code>.</p> </li> <li> <p>Wait for the HostedCluster's API to be ready. Once it's ready, get the admin kubeconfig.</p> </li> <li> <p>Eventually the compute nodes will show up in the cluster. Keep in mind since the SDN is not deployed yet, they will remain in <code>NotReady</code> state.</p> <pre><code>export KUBECONFIG=/path/to/hostedcluster/admin/kubeconfig\noc get nodes\n</code></pre> <pre><code>NAME             STATUS     ROLES    AGE     VERSION\nhosted-worker1   NotReady   worker   2m30s   v1.27.8+4fab27b\nhosted-worker2   NotReady   worker   2m33s   v1.27.8+4fab27b\n</code></pre> </li> <li> <p>Apply the yaml manifests provided by <code>Isovalent</code> in the HostedCluster:</p> <pre><code>#!/bin/bash\n\nversion=\"1.14.5\"\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-03-cilium-ciliumconfigs-crd.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00000-cilium-namespace.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00001-cilium-olm-serviceaccount.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00002-cilium-olm-deployment.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00003-cilium-olm-service.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00004-cilium-olm-leader-election-role.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00005-cilium-olm-role.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00006-leader-election-rolebinding.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00007-cilium-olm-rolebinding.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00008-cilium-cilium-olm-clusterrole.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00009-cilium-cilium-clusterrole.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00010-cilium-cilium-olm-clusterrolebinding.yaml\noc apply -f https://raw.githubusercontent.com/isovalent/olm-for-cilium/main/manifests/cilium.v${version}/cluster-network-06-cilium-00011-cilium-cilium-clusterrolebinding.yaml\n</code></pre> </li> <li> <p>Use the right configuration for each network stack</p> </li> </ol> IPv4IPv6Dual stack <pre><code>apiVersion: cilium.io/v1alpha1\nkind: CiliumConfig\nmetadata:\n  name: cilium\n  namespace: cilium\nspec:\n  debug:\n    enabled: true\n  k8s:\n    requireIPv4PodCIDR: true\n  logSystemLoad: true\n  bpf:\n    preallocateMaps: true\n  etcd:\n    leaseTTL: 30s\n  ipv4:\n    enabled: true\n  ipv6:\n    enabled: false\n  identityChangeGracePeriod: 0s\n  ipam:\n    mode: \"cluster-pool\"\n    operator:\n      clusterPoolIPv4PodCIDRList:\n        - \"10.128.0.0/14\"\n      clusterPoolIPv4MaskSize: \"23\"\n  nativeRoutingCIDR: \"10.128.0.0/14\"\n  endpointRoutes: {enabled: true}\n  clusterHealthPort: 9940\n  tunnelPort: 4789\n  cni:\n    binPath: \"/var/lib/cni/bin\"\n    confPath: \"/var/run/multus/cni/net.d\"\n    chainingMode: portmap\n  prometheus:\n    serviceMonitor: {enabled: false}\n  hubble:\n    tls: {enabled: false}\n  sessionAffinity: true\n</code></pre> <pre><code>oc apply -f ciliumconfig.yaml\n</code></pre> <pre><code>apiVersion: cilium.io/v1alpha1\nkind: CiliumConfig\nmetadata:\n  name: cilium\n  namespace: cilium\nspec:\n  debug:\n    enabled: true\n  k8s:\n    requireIPv6PodCIDR: true\n  logSystemLoad: true\n  bpf:\n    preallocateMaps: true\n  etcd:\n    leaseTTL: 30s\n  ipv4:\n    enabled: false\n  ipv6:\n    enabled: true\n  identityChangeGracePeriod: 0s\n  ipam:\n    mode: \"cluster-pool\"\n    operator:\n      clusterPoolIPv6PodCIDRList:\n        - \"fd01::/48\"\n      clusterPoolIPv6MaskSize: \"48\"\n  nativeRoutingCIDR: \"fd01::/48\"\n  endpointRoutes: {enabled: true}\n  clusterHealthPort: 9940\n  tunnelPort: 4789\n  cni:\n    binPath: \"/var/lib/cni/bin\"\n    confPath: \"/var/run/multus/cni/net.d\"\n    chainingMode: portmap\n  prometheus:\n    serviceMonitor: {enabled: false}\n  hubble:\n    tls: {enabled: false}\n  sessionAffinity: true\n</code></pre> <pre><code>oc apply -f ciliumconfig.yaml\n</code></pre> <pre><code>apiVersion: cilium.io/v1alpha1\nkind: CiliumConfig\nmetadata:\n  name: cilium\n  namespace: cilium\nspec:\n  debug:\n    enabled: true\n  k8s:\n    requireIPv4PodCIDR: true\n  logSystemLoad: true\n  bpf:\n    preallocateMaps: true\n  etcd:\n    leaseTTL: 30s\n  ipv4:\n    enabled: true\n  ipv6:\n    enabled: true\n  identityChangeGracePeriod: 0s\n  ipam:\n    mode: \"cluster-pool\"\n    operator:\n      clusterPoolIPv4PodCIDRList:\n        - \"10.128.0.0/14\"\n      clusterPoolIPv4MaskSize: \"23\"\n  nativeRoutingCIDR: \"10.128.0.0/14\"\n  endpointRoutes: {enabled: true}\n  clusterHealthPort: 9940\n  tunnelPort: 4789\n  cni:\n    binPath: \"/var/lib/cni/bin\"\n    confPath: \"/var/run/multus/cni/net.d\"\n    chainingMode: portmap\n  prometheus:\n    serviceMonitor: {enabled: false}\n  hubble:\n    tls: {enabled: false}\n  sessionAffinity: true\n</code></pre> <pre><code>oc apply -f ciliumconfig.yaml\n</code></pre> <p>Important</p> <p>Make sure you've changed the networking values according to your platform details <code>spec.ipam.operator.clusterPoolIPv4PodCIDRList</code>, <code>spec.ipam.operator.clusterPoolIPv4MaskSize</code> and <code>nativeRoutingCIDR</code> in IPv4 and <code>spec.ipam.operator.clusterPoolIPv6PodCIDRList</code>, <code>spec.ipam.operator.clusterPoolIPv6MaskSize</code> and <code>nativeRoutingCIDR</code> in IPv6 case.</p>"},{"location":"how-to/sdn/other-sdn-providers/#checks_1","title":"Checks","text":"<p>This will be the output:</p> <pre><code>customresourcedefinition.apiextensions.k8s.io/ciliumconfigs.cilium.io created\nnamespace/cilium created\nserviceaccount/cilium-olm created\nWarning: would violate PodSecurity \"restricted:v1.24\": host namespaces (hostNetwork=true), hostPort (container \"operator\" uses hostPort 9443), allowPrivilegeEscalation != false (container \"operator\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"operator\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"operator\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"operator\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\ndeployment.apps/cilium-olm created\nservice/cilium-olm created\nrole.rbac.authorization.k8s.io/cilium-olm-leader-election created\nrole.rbac.authorization.k8s.io/cilium-olm created\nrolebinding.rbac.authorization.k8s.io/leader-election created\nrolebinding.rbac.authorization.k8s.io/cilium-olm created\nclusterrole.rbac.authorization.k8s.io/cilium-cilium-olm created\nclusterrole.rbac.authorization.k8s.io/cilium-cilium created\nclusterrolebinding.rbac.authorization.k8s.io/cilium-cilium-olm created\nclusterrolebinding.rbac.authorization.k8s.io/cilium-cilium created\nciliumconfig.cilium.io/cilium created\n</code></pre> <p>We should see the following pods running in the <code>cilium</code> namespace:</p> <pre><code>oc -n cilium get pods\n</code></pre> <pre><code>NAME                               READY   STATUS    RESTARTS   AGE\ncilium-ds5tr                       1/1     Running   0          106s\ncilium-olm-7c9cf7c948-txkvt        1/1     Running   0          2m36s\ncilium-operator-595594bf7d-gbnns   1/1     Running   0          106s\ncilium-operator-595594bf7d-mn5wc   1/1     Running   0          106s\ncilium-wzhdk                       1/1     Running   0          106s\n</code></pre> <p>The nodes should've moved to <code>Ready</code> state:</p> <pre><code>oc get nodes\n</code></pre> <pre><code>NAME             STATUS   ROLES    AGE   VERSION\nhosted-worker1   Ready    worker   8m    v1.27.8+4fab27b\nhosted-worker2   Ready    worker   8m    v1.27.8+4fab27b\n</code></pre> <p>The HostedCluster deployment will continue, at this point the SDN is running.</p>"},{"location":"how-to/sdn/other-sdn-providers/#validation","title":"Validation","text":"<p>Additionally you have some conformance tests that could be deployed into the HostedCluster in order to validate if the Cilium SDN was deployed and working properly.</p> <p>In order for Cilium connectivity test pods to run on OpenShift, a simple custom SecurityContextConstraints object is required to allow hostPort/hostNetwork which the connectivity test pods relies on. it should only set allowHostPorts and allowHostNetwork without any other privileges.</p> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: security.openshift.io/v1\nkind: SecurityContextConstraints\nmetadata:\n  name: cilium-test\nallowHostPorts: true\nallowHostNetwork: true\nusers:\n  - system:serviceaccount:cilium-test:default\npriority: null\nreadOnlyRootFilesystem: false\nrunAsUser:\n  type: MustRunAsRange\nseLinuxContext:\n  type: MustRunAs\nvolumes: null\nallowHostDirVolumePlugin: false\nallowHostIPC: false\nallowHostPID: false\nallowPrivilegeEscalation: false\nallowPrivilegedContainer: false\nallowedCapabilities: null\ndefaultAddCapabilities: null\nrequiredDropCapabilities: null\ngroups: null\nEOF\n</code></pre> <pre><code>version=\"1.14.5\"\noc apply -n cilium-test -f https://raw.githubusercontent.com/cilium/cilium/${version}/examples/kubernetes/connectivity-check/connectivity-check.yaml\n</code></pre> <pre><code>oc get pod -n cilium-test\n</code></pre> <pre><code>NAME                                                     READY   STATUS    RESTARTS   AGE\necho-a-846dcb4-kq7zh                                     1/1     Running   0          23h\necho-b-58f67d5b86-5mrtx                                  1/1     Running   0          23h\necho-b-host-84d7468c8d-nf4vk                             1/1     Running   0          23h\nhost-to-b-multi-node-clusterip-b98ff785c-b9vgf           1/1     Running   0          23h\nhost-to-b-multi-node-headless-5c55d85dfc-5xjbc           1/1     Running   0          23h\npod-to-a-6b996b7675-46kkf                                1/1     Running   0          23h\npod-to-a-allowed-cnp-c958b55bf-6vskb                     1/1     Running   0          23h\npod-to-a-denied-cnp-6d9b8cbff5-lbrgp                     1/1     Running   0          23h\npod-to-b-intra-node-nodeport-5f9c4c866f-mhfs4            1/1     Running   0          23h\npod-to-b-multi-node-clusterip-7cb4bf5495-hmmtg           1/1     Running   0          23h\npod-to-b-multi-node-headless-68975fc557-sqbgq            1/1     Running   0          23h\npod-to-b-multi-node-nodeport-559c54c6fc-2rhvv            1/1     Running   0          23h\npod-to-external-1111-5c4cfd9497-6slss                    1/1     Running   0          23h\npod-to-external-fqdn-allow-google-cnp-7d65d9b747-w4cx5   1/1     Running   0          23h\n</code></pre>"},{"location":"labs/","title":"Multi-Cluster Engine for Bare Metal","text":"<p>Within this section, our primary focus revolves around the various deployment methods available for Hosted Clusters, leveraging MCE as a foundational framework. The inclusion of the Hypershift release is planned in MCE's payload, starting from version 2.4, and will be fully integrated by version 2.9 in ACM.</p> <p>In this segment of the documentation, we will delve into both Connected and Disconnected deployment modes, exploring their respective configurations related to Network stacks:</p> <ul> <li>IPv4</li> <li>IPv6</li> <li>Dual Stack</li> </ul> <p>Each of these modes possesses its unique characteristics and intricacies, which will be thoroughly examined\u2014from the Node/Hypervisor configuration to the artifacts entailed in the deployment process.</p> <p>For more information on specific topics, please refer to the following links:</p> <ul> <li>IPv4</li> <li>IPv6</li> <li>Dual Stack</li> </ul> <p>While our primary focus in this documentation is Virtual Machines, it is important to note that the principles discussed herein are equally applicable to bare metal nodes. We will duly emphasize the distinct considerations associated with each type of deployment.</p>"},{"location":"labs/Dual/","title":"Dual Stack","text":"<p>This network configuration is currently designated as disconnected. The primary reason for this designation is because remote registries do not function with IPv6. Consequently, this aspect has been incorporated into the documentation.</p> <p>All the scripts provided contain partial or complete automation to replicate the environment. For this purpose, you can refer to the repository containing all the scripts for Dual Stack environments.</p> <p>Please note that this documentation is designed to be followed in a specific sequence:</p> <ul> <li>Hypervisor</li> <li>DNS</li> <li>Registry</li> <li>Management Cluster</li> <li>Webserver</li> <li>Mirroring</li> <li>Multicluster Engine</li> <li>TLS Certificates</li> <li>HostedCluster</li> <li>Watching Deployment progress</li> </ul>"},{"location":"labs/Dual/dns/","title":"Dns","text":"<p>The DNS configuration is a critical aspect of our setup. To enable name resolution in our virtualized environment, follow these steps:</p> <ol> <li> <p>Create the primary DNS configuration file for the dnsmasq server:</p> </li> <li> <p><code>/opt/dnsmasq/dnsmasq.conf</code> <pre><code>strict-order\nbind-dynamic\n#log-queries\nbogus-priv\ndhcp-authoritative\n\n# BM Network IPv4\ndhcp-range=dual,192.168.126.120,192.168.126.250,255.255.255.0,24h\ndhcp-option=dual,option:dns-server,192.168.126.1\ndhcp-option=dual,option:router,192.168.126.1\n\n# BM Network dual\ndhcp-range=dual,2620:52:0:1306::11,2620:52:0:1306::20,64\ndhcp-option=dual,option6:dns-server,2620:52:0:1306::1\n\nresolv-file=/opt/dnsmasq/upstream-resolv.conf\nexcept-interface=lo\ndhcp-lease-max=81\nlog-dhcp\nno-hosts\n\n# DHCP Reservations\ndhcp-leasefile=/opt/dnsmasq/hosts.leases\n\n# Include all files in a directory depending on the suffix\nconf-dir=/opt/dnsmasq/include.d/*.dual\n</code></pre></p> </li> </ol> <p>Create the upstream resolver to delegate the non-local environments queries</p> <ul> <li><code>/opt/dnsmasq/upstream-resolv.conf</code> <pre><code>nameserver 8.8.8.8\nnameserver 8.8.4.4\n</code></pre></li> </ul> <p>Create the different component DNS configurations</p> <ul> <li> <p><code>/opt/dnsmasq/include.d/hosted-nodeport.dual</code> <pre><code>host-record=api-int.hosted-dual.hypershiftbm.lab,192.168.126.20\nhost-record=api-int.hosted-dual.hypershiftbm.lab,192.168.126.21\nhost-record=api-int.hosted-dual.hypershiftbm.lab,192.168.126.22\nhost-record=api.hosted-dual.hypershiftbm.lab,192.168.126.20\nhost-record=api.hosted-dual.hypershiftbm.lab,192.168.126.21\nhost-record=api.hosted-dual.hypershiftbm.lab,192.168.126.22\n\n## IMPORTANT!: You should point to the node which is exposing the router.\n## You can also use MetalLB to expose the Apps wildcard.\naddress=/apps.hosted-dual.hypershiftbm.lab/192.168.126.30\n\ndhcp-host=aa:aa:aa:aa:04:11,hosted-worker0,192.168.126.30\ndhcp-host=aa:aa:aa:aa:04:12,hosted-worker1,192.168.126.31\ndhcp-host=aa:aa:aa:aa:04:13,hosted-worker2,192.168.126.32\ndhcp-host=aa:aa:aa:aa:11:01,hosted-worker0,[2620:52:0:1306::30]\ndhcp-host=aa:aa:aa:aa:11:02,hosted-worker1,[2620:52:0:1306::31]\ndhcp-host=aa:aa:aa:aa:11:03,hosted-worker2,[2620:52:0:1306::32]\n</code></pre></p> </li> <li> <p><code>/opt/dnsmasq/include.d/hub.dual</code> <pre><code>host-record=api-int.hub-dual.hypershiftbm.lab,192.168.126.10\nhost-record=api.hub-dual.hypershiftbm.lab,192.168.126.10\naddress=/apps.hub-dual.hypershiftbm.lab/192.168.126.11\ndhcp-host=aa:aa:aa:aa:10:01,ocp-master-0,192.168.126.20\ndhcp-host=aa:aa:aa:aa:10:02,ocp-master-1,192.168.126.21\ndhcp-host=aa:aa:aa:aa:10:03,ocp-master-2,192.168.126.22\ndhcp-host=aa:aa:aa:aa:10:06,ocp-installer,192.168.126.25\ndhcp-host=aa:aa:aa:aa:10:07,ocp-bootstrap,192.168.126.26\n\nhost-record=api-int.hub-dual.hypershiftbm.lab,2620:52:0:1306::2\nhost-record=api.hub-dual.hypershiftbm.lab,2620:52:0:1306::2\naddress=/apps.hub-dual.hypershiftbm.lab/2620:52:0:1306::3\ndhcp-host=aa:aa:aa:aa:10:01,ocp-master-0,[2620:52:0:1306::5]\ndhcp-host=aa:aa:aa:aa:10:02,ocp-master-1,[2620:52:0:1306::6]\ndhcp-host=aa:aa:aa:aa:10:03,ocp-master-2,[2620:52:0:1306::7]\ndhcp-host=aa:aa:aa:aa:10:06,ocp-installer,[2620:52:0:1306::8]\ndhcp-host=aa:aa:aa:aa:10:07,ocp-bootstrap,[2620:52:0:1306::9]\n</code></pre></p> </li> <li> <p><code>/opt/dnsmasq/include.d/infra.dual</code> <pre><code>host-record=registry.hypershiftbm.lab,2620:52:0:1306::1\nhost-record=registry.hypershiftbm.lab,192.168.126.1\n</code></pre></p> </li> </ul> <p>To proceed, we must create a systemd service for the management of the dnsmasq service and disable the system's default dnsmasq service:</p> <ul> <li><code>/etc/systemd/system/dnsmasq-virt.service</code> <pre><code>[Unit]\nDescription=DNS server for Openshift 4 Clusters.\nAfter=network.target\n\n[Service]\nUser=root\nGroup=root\nExecStart=/usr/sbin/dnsmasq -k --conf-file=/opt/dnsmasq/dnsmasq.conf\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></li> </ul> <p>The commands to do so:</p> <pre><code>systemctl daemon-reload\nsystemctl disable --now dnsmasq\nsystemctl enable --now dnsmasq-virt\n</code></pre> <p>Note</p> <p>This step is mandatory for both Disconnected and Connected environments. Additionally, it holds significance for both Virtualized and Bare Metal environments. The key distinction lies in the location where the resources will be configured. In a non-virtualized environment, a more robust solution like Bind is recommended instead of a lightweight dnsmasq.</p>"},{"location":"labs/Dual/registry/","title":"Registry","text":"<p>Important</p> <p>This section is exclusively applicable to disconnected scenarios. If this does not apply to your situation, please proceed to the next section.</p> <p>In this section, we will explain how to deploy a small, self-hosted registry using a Podman container. For production environments, we strongly recommend utilizing a more reliable solution such as Quay, Nexus, or Artifactory.</p> <ol> <li> <p>As a privileged user, please access the ${HOME} directory and proceed to create the following script provided below. The script will make certain assumptions, including the registry name based on the hypervisor hostname, as well as the necessary credentials and user access:</p> </li> <li> <p><code>registry.sh</code> <pre><code>#!/usr/bin/env bash\n\nset -euo pipefail\n\nPRIMARY_NIC=$(ls -1 /sys/class/net | grep -v podman | head -1)\nexport PATH=/root/bin:$PATH\nexport PULL_SECRET=\"/root/baremetal/hub/openshift_pull.json\"\n\nif [[ ! -f $PULL_SECRET ]];then\n  echo \"Pull Secret not found, exiting...\"\n  exit 1\nfi\n\ndnf -y install podman httpd httpd-tools jq skopeo libseccomp-devel\nexport IP=$(ip -o addr show $PRIMARY_NIC | head -1 | awk '{print $4}' | cut -d'/' -f1)\nREGISTRY_NAME=registry.$(hostname --long)\nREGISTRY_USER=dummy\nREGISTRY_PASSWORD=dummy\nKEY=$(echo -n $REGISTRY_USER:$REGISTRY_PASSWORD | base64)\necho \"{\\\"auths\\\": {\\\"$REGISTRY_NAME:5000\\\": {\\\"auth\\\": \\\"$KEY\\\", \\\"email\\\": \\\"sample-email@domain.ltd\\\"}}}\" &gt; /root/disconnected_pull.json\nmv ${PULL_SECRET} /root/openshift_pull.json.old\njq \".auths += {\\\"$REGISTRY_NAME:5000\\\": {\\\"auth\\\": \\\"$KEY\\\",\\\"email\\\": \\\"sample-email@domain.ltd\\\"}}\" &lt; /root/openshift_pull.json.old &gt; $PULL_SECRET\nmkdir -p /opt/registry/{auth,certs,data,conf}\ncat &lt;&lt;EOF &gt; /opt/registry/conf/config.yml\nversion: 0.1\nlog:\n  fields:\n    service: registry\nstorage:\n  cache:\n    blobdescriptor: inmemory\n  filesystem:\n    rootdirectory: /var/lib/registry\n  delete:\n    enabled: true\nhttp:\n  addr: :5000\n  headers:\n    X-Content-Type-Options: [nosniff]\nhealth:\n  storagedriver:\n    enabled: true\n    interval: 10s\n    threshold: 3\ncompatibility:\n  schema1:\n    enabled: true\nEOF\nopenssl req -newkey rsa:4096 -nodes -sha256 -keyout /opt/registry/certs/domain.key -x509 -days 3650 -out /opt/registry/certs/domain.crt -subj \"/C=US/ST=Madrid/L=San Bernardo/O=Karmalabs/OU=Guitar/CN=$REGISTRY_NAME\" -addext \"subjectAltName=DNS:$REGISTRY_NAME\"\ncp /opt/registry/certs/domain.crt /etc/pki/ca-trust/source/anchors/\nupdate-ca-trust extract\nhtpasswd -bBc /opt/registry/auth/htpasswd $REGISTRY_USER $REGISTRY_PASSWORD\npodman create --name registry --net host --security-opt label=disable --replace -v /opt/registry/data:/var/lib/registry:z -v /opt/registry/auth:/auth:z -v /opt/registry/conf/config.yml:/etc/docker/registry/config.yml -e \"REGISTRY_AUTH=htpasswd\" -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry\" -e \"REGISTRY_HTTP_SECRET=ALongRandomSecretForRegistry\" -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd -v /opt/registry/certs:/certs:z -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key docker.io/library/registry:latest\n[ \"$?\" == \"0\" ] || !!\nsystemctl enable --now registry\n</code></pre></p> </li> <li> <p>Kindly modify the location of the <code>PULL_SECRET</code> to point to the correct location.</p> </li> <li>Please adjust the permissions by adding the execution flag as follows: <code>chmod u+x ${HOME}/registry.sh</code></li> <li>You can execute the script without any parameters using the following command <code>${HOME}/registry.sh</code></li> </ol> <p>After executing the script <code>${HOME}/registry.sh</code>, the server should now be up and running. If the script has been properly configured and executed without errors, it should have started the server as intended.</p> <p>It will utilize a systemd service for management purposes. So, if you ever need to manage it, you can employ systemctl status/start/stop registry.</p> <p>The root folder for the registry is situated at /opt/registry, and it's structured as follows:</p> <ul> <li><code>certs</code> holds the TLS certificates.</li> <li><code>auth</code> keeps the credentials.</li> <li><code>data</code> contains the registry images.</li> <li><code>conf</code> will hold the registry configuration.</li> </ul>"},{"location":"labs/Dual/tls-certificates/","title":"Tls certificates","text":"<p>Important</p> <p>This section is only relevant in disconnected scenarios. If this doesn't apply to your situation, please proceed to the next section.</p> <p>In this section, we'll cover the TLS certificates involved in the process, primarily focusing on the private registries from which the images will be pulled. While there may be additional certificates, we'll concentrate on these particular ones.</p> <p>It's important to distinguish between the various methods and their impact on the associated cluster. All of these methods essentially modify the content of the following files on the OCP (OpenShift Container Platform) control plane (Master nodes) and data plane (worker nodes):</p> <ul> <li><code>/etc/pki/ca-trust/extracted/pem/</code></li> <li><code>/etc/pki/ca-trust/source/anchors/</code></li> <li><code>/etc/pki/tls/certs/</code></li> </ul>"},{"location":"labs/Dual/tls-certificates/#adding-a-ca-to-the-management-cluster","title":"Adding a CA to the Management Cluster","text":"<p>There exist numerous methods to accomplish this within the OpenShift environment. However, we have chosen to integrate the less intrusive approach.</p> <ol> <li>Initially, you must create a ConfigMap with a name of your choosing. In our specific case, we will utilize the name <code>registry-config</code> The content should resemble the following:</li> </ol> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: registry-config\n  namespace: openshift-config\ndata:\n  registry.hypershiftbm.lab..5000: |\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\n</code></pre> <p>Note</p> <pre><code>The data field ought to contain the registry name, while the value should encompass the Registry certificate. As is evident, the \":\" character is being replaced by \"..\"; therefore, it is imperative to ensure this correction.\n</code></pre> <ol> <li>Now we need to patch the clusterwide object <code>image.config.openshift.io</code> including this:</li> </ol> <pre><code>spec:\n  additionalTrustedCA:\n    name: registry-config\n</code></pre> <p>This modification will result in two significant consequences:</p> <ul> <li>Granting masters the capability to retrieve images from the private registry.</li> <li>Allowing the Hypershift Operator to extract the Openshift payload for the HostedCluster deployments.</li> </ul> <p>Note</p> <pre><code>The modification required several minutes to be successfully executed.\n</code></pre>"},{"location":"labs/Dual/tls-certificates/#alternative-adding-a-ca-to-the-management-cluster","title":"Alternative: Adding a CA to the Management Cluster","text":"<p>We consider this as an alternative, given that it entails the masters undergoing a reboot facilitated by the Machine Config Operator.</p> <p>It's described here. This method involves utilizing the <code>image-registry-operator</code>, which deploys the CAs to the OCP nodes.</p> <p>Hypershift's operators and controllers automatically handle this process, so if you're using a GA (Generally Available) released version, it should work seamlessly, and you won't need to apply these steps. This Hypershift feature is included in the payload of the 2.4 MCE release.</p> <p>However, if this feature is not working as expected or if it doesn't apply to your situation, you can follow this procedure:</p> <ul> <li>Check if the <code>openshift-config</code> namespace in the Management cluster contains a ConfigMap named <code>user-ca-bundle</code>.</li> <li>If the ConfigMap doesn't exist, execute the following command:</li> </ul> <pre><code>## REGISTRY_CERT_PATH=&lt;PATH/TO/YOUR/CERTIFICATE/FILE&gt;\nexport REGISTRY_CERT_PATH=/opt/registry/certs/domain.crt\n\noc create configmap user-ca-bundle -n openshift-config --from-file=ca-bundle.crt=${REGISTRY_CERT_PATH}\n</code></pre> <ul> <li>Otherwise, if that ConfigMap exists, execute this other command:</li> </ul> <pre><code>## REGISTRY_CERT_PATH=&lt;PATH/TO/YOUR/CERTIFICATE/FILE&gt;\nexport REGISTRY_CERT_PATH=/opt/registry/certs/domain.crt\nexport TMP_FILE=$(mktemp)\n\noc get cm -n openshift-config user-ca-bundle -ojsonpath='{.data.ca-bundle\\.crt}' &gt; ${TMP_FILE}\necho &gt;&gt; ${TMP_FILE}\necho \\#registry.$(hostname --long) &gt;&gt; ${TMP_FILE}\ncat ${REGISTRY_CERT_PATH} &gt;&gt; ${TMP_FILE}\noc create configmap user-ca-bundle -n openshift-config --from-file=ca-bundle.crt=${TMP_FILE} --dry-run=client -o yaml | kubectl apply -f -\n</code></pre> <p>You have a functional script located in the <code>assets/&lt;NetworkStack&gt;/09-tls-certificates/01-config.sh</code>, this is the sample for IPv6.</p>"},{"location":"labs/Dual/webserver/","title":"Webserver","text":"<p>Important</p> <p>This section is only relevant in disconnected scenarios, if this is not your case, you can continue with the next section.</p> <p>This section talks about an additional webserver that you need to configure to host the RHCOS images associated with the Openshift release you are trying to deploy as a HostedCluster.</p> <p>The script refers to this repository folder and it's the same for all three different network stacks.</p> <p>To do this, you can use this script:</p> <pre><code>#!/bin/bash\n\nWEBSRV_FOLDER=/opt/srv\nROOTFS_IMG_URL=\"$(../04-management-cluster/openshift-install coreos print-stream-json | jq -r '.architectures.x86_64.artifacts.metal.formats.pxe.rootfs.location')\"\nLIVE_ISO_URL=\"$(../04-management-cluster/openshift-install coreos print-stream-json | jq -r '.architectures.x86_64.artifacts.metal.formats.iso.disk.location')\"\n\nmkdir -p ${WEBSRV_FOLDER}/images\ncurl -Lk ${ROOTFS_IMG_URL} -o ${WEBSRV_FOLDER}/images/${ROOTFS_IMG_URL##*/}\ncurl -Lk ${LIVE_ISO_URL} -o ${WEBSRV_FOLDER}/images/${LIVE_ISO_URL##*/}\nchmod -R 755 ${WEBSRV_FOLDER}/*\n\n## Run Webserver\npodman ps --noheading | grep -q websrv-ai\nif [[ $? == 0 ]];then\n    echo \"Launching Registry pod...\"\n    /usr/bin/podman run --name websrv-ai --net host -v /opt/srv:/usr/local/apache2/htdocs:z quay.io/alosadag/httpd:p8080\nfi\n</code></pre> <p>The script will create a folder under <code>/opt/srv</code>. This folder will contain the <code>images</code> for RHCOS provision in the worker nodes. To be more concrete, we need the <code>RootFS</code> and <code>LiveISO</code> artifacts found on the Openshift CI Release page.</p> <p>After the download, a container will run to host the images under a webserver. It uses a variation of the official httpd image, which also allows it to work with IPv6.</p>"},{"location":"labs/Dual/hostedcluster/","title":"Index","text":"<p>A Hosted Cluster, as mentioned in the documentation here, is essentially an OCP API endpoint managed by Hypershift. In this context, we will also include the term HostedControlPlane to enhance readability and comprehension. This terminology is further explained in the same link.</p> <p>The Hosted Cluster comprises two main components: - The Control Plane, which runs as pods in the management cluster. - The Data Plane, consisting of external nodes managed by the end user.</p> <p>With this foundational understanding, we can commence our Hosted Cluster deployment. Typically, an ACM/MCE user would utilize the web UI to create a cluster. However, in this scenario, we will leverage manifests, providing us with greater flexibility to modify the artifacts.</p>"},{"location":"labs/Dual/hostedcluster/baremetalhost/","title":"Baremetalhost","text":""},{"location":"labs/Dual/hostedcluster/baremetalhost/#bare-metal-hosts","title":"Bare Metal Hosts","text":"<p>A BareMetalHost is an openshift-machine-api object that encompasses both physical and logical details, allowing it to be identified by the Metal3 operator. Subsequently, these details are associated with other Assisted Service objects known as Agents. The structure of this object is as follows:</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: hosted-dual-worker0-bmc-secret\n  namespace: clusters-hosted-dual\ndata:\n  password: YWRtaW4=\n  username: YWRtaW4=\ntype: Opaque\n---\napiVersion: metal3.io/v1alpha1\nkind: BareMetalHost\nmetadata:\n  name: hosted-dual-worker0\n  namespace: clusters-hosted-dual\n  labels:\n    infraenvs.agent-install.openshift.io: hosted-dual\n  annotations:\n    inspect.metal3.io: disabled\n    bmac.agent-install.openshift.io/hostname: hosted-dual-worker0\nspec:\n  automatedCleaningMode: disabled\n  bmc:\n    disableCertificateVerification: true\n    address: redfish-virtualmedia://[192.168.126.1]:9000/redfish/v1/Systems/local/hosted-dual-worker0\n    credentialsName: hosted-dual-worker0-bmc-secret\n  bootMACAddress: aa:aa:aa:aa:02:11\n  online: true\n</code></pre> <p>Details:</p> <ul> <li>We will have at least 1 secret that holds the BMH credentials, so we will need to create at least 2 objects per worker node.</li> <li><code>spec.metadata.labels[\"infraenvs.agent-install.openshift.io\"]</code> serves as the link between the Assisted Installer and the BareMetalHost objects.</li> <li><code>spec.metadata.annotations[\"bmac.agent-install.openshift.io/hostname\"]</code> represents the node name it will adopt during deployment.</li> <li><code>spec.automatedCleaningMode</code> prevents the node from being erased by the Metal3 operator.</li> <li><code>spec.bmc.disableCertificateVerification</code> is set to <code>true</code> to bypass certificate validation from the client.</li> <li><code>spec.bmc.address</code> denotes the BMC address of the worker node.</li> <li><code>spec.bmc.credentialsName</code> points to the Secret where User/Password credentials are stored.</li> <li><code>spec.bootMACAddress</code> indicates the interface MACAddress from which the node will boot.</li> <li><code>spec.online</code> defines the desired state of the node once the BMH object is created.</li> </ul> <p>To deploy this object, simply follow the same procedure as before:</p> <p>Important</p> <p>Please create the virtual machines before you create the BareMetalHost and the destination Nodes.</p> <p>To deploy the BareMetalHost object, execute the following command:</p> <pre><code>oc apply -f 04-bmh.yaml\n</code></pre> <p>This will be the process:</p> <ul> <li> <p>Preparing (Trying to reach the nodes): <pre><code>NAMESPACE         NAME             STATE         CONSUMER   ONLINE   ERROR   AGE\nclusters-hosted   hosted-worker0   registering              true             2s\nclusters-hosted   hosted-worker1   registering              true             2s\nclusters-hosted   hosted-worker2   registering              true             2s\n</code></pre></p> </li> <li> <p>Provisioning (Nodes Booting up) <pre><code>NAMESPACE         NAME             STATE          CONSUMER   ONLINE   ERROR   AGE\nclusters-hosted   hosted-worker0   provisioning              true             16s\nclusters-hosted   hosted-worker1   provisioning              true             16s\nclusters-hosted   hosted-worker2   provisioning              true             16s\n</code></pre></p> </li> <li> <p>Provisioned (Nodes Booted up successfully) <pre><code>NAMESPACE         NAME             STATE         CONSUMER   ONLINE   ERROR   AGE\nclusters-hosted   hosted-worker0   provisioned              true             67s\nclusters-hosted   hosted-worker1   provisioned              true             67s\nclusters-hosted   hosted-worker2   provisioned              true             67s\n</code></pre></p> </li> </ul>"},{"location":"labs/Dual/hostedcluster/baremetalhost/#agents-registration","title":"Agents registration","text":"<p>After the nodes have booted up, you will observe the appearance of agents within the namespace.</p> <pre><code>NAMESPACE         NAME                                   CLUSTER   APPROVED   ROLE          STAGE\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0411             true       auto-assign\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0412             true       auto-assign\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0413             true       auto-assign\n</code></pre> <p>These agents represent the nodes available for installation. To assign them to a HostedCluster, scale up the NodePool.</p>"},{"location":"labs/Dual/hostedcluster/baremetalhost/#scaling-up-the-nodepool","title":"Scaling Up the Nodepool","text":"<p>Once we have the BareMetalHosts created, the statuses of these BareMetalHosts will transition from <code>Registering</code> (Attempting to reach the Node's BMC) to <code>Provisioning</code> (Node Booting Up), and finally to <code>Provisioned</code> (Successful node boot-up).</p> <p>The nodes will boot with the Agent's RHCOS LiveISO and a default pod named \"agent.\" This agent is responsible for receiving instructions from the Assisted Service Operator to install the Openshift payload.</p> <p>To accomplish this, execute the following command:</p> <pre><code>oc -n clusters scale nodepool hosted-dual --replicas 3\n</code></pre> <p>After the NodePool scaling, you will notice that the agents are assigned to a Hosted Cluster.</p> <pre><code>NAMESPACE         NAME                                   CLUSTER   APPROVED   ROLE          STAGE\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0411   hosted    true       auto-assign\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0412   hosted    true       auto-assign\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0413   hosted    true       auto-assign\n</code></pre> <p>And the NodePool replicas set</p> <pre><code>NAMESPACE   NAME     CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION                              UPDATINGVERSION   UPDATINGCONFIG   MESSAGE\nclusters    hosted   hosted    3                               False         False        4.14.0-0.nightly-2023-08-29-102237                                      Minimum availability requires 3 replicas, current 0 available\n</code></pre> <p>So now, we need to wait until the nodes join the cluster. The Agents will provide updates on their current stage and status. Initially, they may not post any status, but eventually, they will.</p>"},{"location":"labs/Dual/hostedcluster/hostedcluster/","title":"Hostedcluster","text":"<p>In this section, we will focus on all the related objects necessary to achieve a Disconnected Hosted Cluster deployment.</p> <p>Premises:</p> <ul> <li>HostedCluster Name: <code>hosted-dual</code></li> <li>HostedCluster Namespace: <code>clusters</code></li> <li>Disconnected: <code>true</code></li> <li>Network Stack: <code>Dual</code></li> </ul>"},{"location":"labs/Dual/hostedcluster/hostedcluster/#openshift-objects","title":"Openshift Objects","text":""},{"location":"labs/Dual/hostedcluster/hostedcluster/#namespaces","title":"Namespaces","text":"<p>In a typical situation, the operator would be responsible for creating the HCP (HostedControlPlane) namespace. However, in this case, we want to include all the objects before the operator begins reconciliation over the HostedCluster object. This way, when the operator commences the reconciliation process, it will find all the objects in place.</p> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: null\n  name: clusters-hosted-dual\nspec: {}\nstatus: {}\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: null\n  name: clusters\nspec: {}\nstatus: {}\n</code></pre> <p>Note</p> <p>We will not create objects one by one but will concatenate all of them in the same file and apply them with just one command.</p>"},{"location":"labs/Dual/hostedcluster/hostedcluster/#configmap-and-secrets","title":"ConfigMap and Secrets","text":"<p>These are the ConfigMaps and Secrets that we will include in the HostedCluster deployment.</p> <pre><code>---\napiVersion: v1\ndata:\n  ca-bundle.crt: |\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\nkind: ConfigMap\nmetadata:\n  name: user-ca-bundle\n  namespace: clusters\n---\napiVersion: v1\ndata:\n  .dockerconfigjson: xxxxxxxxx\nkind: Secret\nmetadata:\n  creationTimestamp: null\n  name: hosted-dual-pull-secret\n  namespace: clusters\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: sshkey-cluster-hosted-dual\n  namespace: clusters\nstringData:\n  id_rsa.pub: ssh-rsa xxxxxxxxx\n---\napiVersion: v1\ndata:\n  key: nTPtVBEt03owkrKhIdmSW8jrWRxU57KO/fnZa8oaG0Y=\nkind: Secret\nmetadata:\n  creationTimestamp: null\n  name: hosted-dual-etcd-encryption-key\n  namespace: clusters\ntype: Opaque\n</code></pre>"},{"location":"labs/Dual/hostedcluster/hostedcluster/#rbac-roles","title":"RBAC Roles","text":"<p>While not mandatory, it allows us to have the Assisted Service Agents located in the same HostedControlPlane namespace as the HostedControlPlane and still be managed by CAPI.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  creationTimestamp: null\n  name: capi-provider-role\n  namespace: clusters-hosted-dual\nrules:\n- apiGroups:\n  - agent-install.openshift.io\n  resources:\n  - agents\n  verbs:\n  - '*'\n</code></pre>"},{"location":"labs/Dual/hostedcluster/hostedcluster/#hosted-cluster","title":"Hosted Cluster","text":"<p>This is a sample of the HostedCluster Object</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: HostedCluster\nmetadata:\n  name: hosted-dual\n  namespace: clusters\nspec:\n  additionalTrustBundle:\n    name: \"user-ca-bundle\"\n  olmCatalogPlacement: guest\n  imageContentSources:\n  - source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n    mirrors:\n    - registry.hypershiftbm.lab:5000/openshift/release\n  - source: quay.io/openshift-release-dev/ocp-release\n    mirrors:\n    - registry.hypershiftbm.lab:5000/openshift/release-images\n  - mirrors:\n  ...\n  ...\n  autoscaling: {}\n  controllerAvailabilityPolicy: SingleReplica\n  dns:\n    baseDomain: hypershiftbm.lab\n  etcd:\n    managed:\n      storage:\n        persistentVolume:\n          size: 8Gi\n        restoreSnapshotURL: null\n        type: PersistentVolume\n    managementType: Managed\n  fips: false\n  networking:\n    clusterNetwork:\n    - cidr: 10.132.0.0/14\n    - cidr: fd01::/48\n    networkType: OVNKubernetes\n    serviceNetwork:\n    - cidr: 172.31.0.0/16\n    - cidr: fd02::/112\n  platform:\n    agent:\n      agentNamespace: clusters-hosted-dual\n    type: Agent\n  pullSecret:\n    name: hosted-dual-pull-secret\n  release:\n    image: registry.hypershiftbm.lab:5000/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237\n  secretEncryption:\n    aescbc:\n      activeKey:\n        name: hosted-dual-etcd-encryption-key\n    type: aescbc\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: api.hosted-dual.hypershiftbm.lab\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: api.hosted-dual.hypershiftbm.lab\n      type: NodePort\n  - service: OIDC\n    servicePublishingStrategy:\n      nodePort:\n        address: api.hosted-dual.hypershiftbm.lab\n      type: NodePort\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: api.hosted-dual.hypershiftbm.lab\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: api.hosted-dual.hypershiftbm.lab\n      type: NodePort\n  sshKey:\n    name: sshkey-cluster-hosted-dual\nstatus:\n  controlPlaneEndpoint:\n    host: \"\"\n    port: 0\n</code></pre> <p>Note</p> <p>The <code>imageContentSources</code> section within the <code>spec</code> field contains mirror references for user workloads within the HostedCluster.</p> <p>As you can see, all the objects created before are referenced here. You can also refer to the documentation where all the fields are described.</p>"},{"location":"labs/Dual/hostedcluster/hostedcluster/#deployment","title":"Deployment","text":"<p>To deploy these objects, simply concatenate them into the same file and apply them against the management cluster:</p> <pre><code>oc apply -f 01-4.14-hosted_cluster-nodeport.yaml\n</code></pre> <p>This will raise up a functional Hosted Control Plane.</p> <pre><code>NAME                                                  READY   STATUS    RESTARTS   AGE\ncapi-provider-5b57dbd6d5-pxlqc                        1/1     Running   0          3m57s\ncatalog-operator-9694884dd-m7zzv                      2/2     Running   0          93s\ncluster-api-f98b9467c-9hfrq                           1/1     Running   0          3m57s\ncluster-autoscaler-d7f95dd5-d8m5d                     1/1     Running   0          93s\ncluster-image-registry-operator-5ff5944b4b-648ht      1/2     Running   0          93s\ncluster-network-operator-77b896ddc-wpkq8              1/1     Running   0          94s\ncluster-node-tuning-operator-84956cd484-4hfgf         1/1     Running   0          94s\ncluster-policy-controller-5fd8595d97-rhbwf            1/1     Running   0          95s\ncluster-storage-operator-54dcf584b5-xrnts             1/1     Running   0          93s\ncluster-version-operator-9c554b999-l22s7              1/1     Running   0          95s\ncontrol-plane-operator-6fdc9c569-t7hr4                1/1     Running   0          3m57s\ncsi-snapshot-controller-785c6dc77c-8ljmr              1/1     Running   0          77s\ncsi-snapshot-controller-operator-7c6674bc5b-d9dtp     1/1     Running   0          93s\ndns-operator-6874b577f-9tc6b                          1/1     Running   0          94s\netcd-0                                                3/3     Running   0          3m39s\nhosted-cluster-config-operator-f5cf5c464-4nmbh        1/1     Running   0          93s\nignition-server-6b689748fc-zdqzk                      1/1     Running   0          95s\nignition-server-proxy-54d4bb9b9b-6zkg7                1/1     Running   0          95s\ningress-operator-6548dc758b-f9gtg                     1/2     Running   0          94s\nkonnectivity-agent-7767cdc6f5-tw782                   1/1     Running   0          95s\nkube-apiserver-7b5799b6c8-9f5bp                       4/4     Running   0          3m7s\nkube-controller-manager-5465bc4dd6-zpdlk              1/1     Running   0          44s\nkube-scheduler-5dd5f78b94-bbbck                       1/1     Running   0          2m36s\nmachine-approver-846c69f56-jxvfr                      1/1     Running   0          92s\noauth-openshift-79c7bf44bf-j975g                      2/2     Running   0          62s\nolm-operator-767f9584c-4lcl2                          2/2     Running   0          93s\nopenshift-apiserver-5d469778c6-pl8tj                  3/3     Running   0          2m36s\nopenshift-controller-manager-6475fdff58-hl4f7         1/1     Running   0          95s\nopenshift-oauth-apiserver-dbbc5cc5f-98574             2/2     Running   0          95s\nopenshift-route-controller-manager-5f6997b48f-s9vdc   1/1     Running   0          95s\npackageserver-67c87d4d4f-kl7qh                        2/2     Running   0          93s\n</code></pre> <p>And this is how the HostedCluster looks like:</p> <pre><code>NAMESPACE   NAME         VERSION   KUBECONFIG                PROGRESS   AVAILABLE   PROGRESSING   MESSAGE\nclusters    hosted-dual            hosted-admin-kubeconfig   Partial    True          False         The hosted control plane is available\n</code></pre> <p>After some time, we will have almost all the pieces in place, and the Control Plane operator awaits for the worker nodes to join the cluster. To achieve this, we need to create some more objects. Let's discuss the <code>InfraEnv</code> and the <code>BareMetalHost</code> in the following sections.</p>"},{"location":"labs/Dual/hostedcluster/infraenv/","title":"Infraenv","text":"<p>The <code>InfraEnv</code> is an Assisted Service object that includes essential details such as the <code>pullSecretRef</code> and the <code>sshAuthorizedKey</code>. These details are used to create the RHCOS Boot Image customized specifically for the cluster. Below is the structure of this object:</p> <pre><code>---\napiVersion: agent-install.openshift.io/v1beta1\nkind: InfraEnv\nmetadata:\n  name: hosted-dual\n  namespace: clusters-hosted-dual\nspec:\n  pullSecretRef:\n    name: pull-secret\n  sshAuthorizedKey: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDk7ICaUE+/k4zTpxLk4+xFdHi4ZuDi5qjeF52afsNkw0w/glILHhwpL5gnp5WkRuL8GwJuZ1VqLC9EKrdmegn4MrmUlq7WTsP0VFOZFBfq2XRUxo1wrRdor2z0Bbh93ytR+ZsDbbLlGngXaMa0Vbt+z74FqlcajbHTZ6zBmTpBVq5RHtDPgKITdpE1fongp7+ZXQNBlkaavaqv8bnyrP4BWahLP4iO9/xJF9lQYboYwEEDzmnKLMW1VtCE6nJzEgWCufACTbxpNS7GvKtoHT/OVzw8ArEXhZXQUS1UY8zKsX2iXwmyhw5Sj6YboA8WICs4z+TrFP89LmxXY0j6536TQFyRz1iB4WWvCbH5n6W+ABV2e8ssJB1AmEy8QYNwpJQJNpSxzoKBjI73XxvPYYC/IjPFMySwZqrSZCkJYqQ023ySkaQxWZT7in4KeMu7eS2tC+Kn4deJ7KwwUycx8n6RHMeD8Qg9flTHCv3gmab8JKZJqN3hW1D378JuvmIX4V0=\n</code></pre> <p>Details:</p> <ul> <li><code>pullSecretRef</code> refers to the ConfigMap reference (in the same Namespace as the InfraEnv) where the PullSecret will be utilized.</li> <li><code>sshAuthorizedKey</code> represents the SSH Public key that will be injected into the Boot Image. This SSH key will, by default, allow access to the worker nodes as the <code>core</code> user.</li> </ul> <p>To deploy this object, follow the same procedure as before:</p> <pre><code>oc apply -f 03-infraenv.yaml\n</code></pre> <p>Once deployed, it will appear as follows:</p> <pre><code>NAMESPACE              NAME     ISO CREATED AT\nclusters-hosted-dual   hosted   2023-09-11T15:14:10Z\n</code></pre>"},{"location":"labs/Dual/hostedcluster/nodepool/","title":"Nodepool","text":"<p>A <code>NodePool</code> is a scalable set of worker nodes associated with a HostedCluster. NodePool machine architectures remain consistent within a specific pool and are independent of the underlying machine architecture of the control plane.</p> <p>Note</p> <p>Please ensure you modify the appropriate fields to align with your laboratory environment.</p> <p>Warning</p> <p>Before a day-1 patch, the release image set in the HostedCluster should use the digest rather than the tag. (e.g <code>quay.io/openshift-release-dev/ocp-release@sha256:e3ba11bd1e5e8ea5a0b36a75791c90f29afb0fdbe4125be4e48f69c76a5c47a0</code>)</p> <p>This is how one looks like:</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: NodePool\nmetadata:\n  creationTimestamp: null\n  name: hosted-dual\n  namespace: clusters\nspec:\n  arch: amd64\n  clusterName: hosted-dual\n  management:\n    autoRepair: false\n    upgradeType: InPlace\n  nodeDrainTimeout: 0s\n  nodeVolumeDetachTimeout: 0s\n  platform:\n    type: Agent\n  release:\n    image: registry.hypershiftbm.lab:5000/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237\n  replicas: 0\nstatus:\n  replicas: 0\n</code></pre> <p>Details:</p> <ul> <li>All the nodes included in this NodePool will be based on the Openshift version <code>4.14.0-0.nightly-2023-08-29-102237</code>.</li> <li>The Upgrade type is set to <code>InPlace</code>, indicating that the same bare-metal node will be reused during an upgrade.</li> <li>Autorepair is set to <code>false</code> because the node will not be recreated when it disappears.</li> <li>Replicas are set to <code>0</code> because we intend to scale them when needed.</li> </ul> <p>You can find more information about NodePool in the NodePool documentation.</p> <p>To deploy this object, simply follow the same procedure as before:</p> <pre><code>oc apply -f 02-nodepool.yaml\n</code></pre> <p>And this is how the NodePool looks like at this point:</p> <pre><code>NAMESPACE   NAME          CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION                              UPDATINGVERSION   UPDATINGCONFIG   MESSAGE\nclusters    hosted-dual   hosted    0                               False         False        4.14.0-0.nightly-2023-08-29-102237\n</code></pre> <p>Important</p> <p>Keep the nodepool replicas to 0 until all the steps are in place.</p>"},{"location":"labs/Dual/hostedcluster/worker-nodes/","title":"Worker nodes","text":"<p>Regarding the worker nodes, if you are working on real bare metal, this step is crucial to ensure that the details set in the <code>BareMetalHost</code> are correctly configured. If not, you will need to debug why it's not functioning as expected.</p> <p>However, if you are working with virtual machines, you can follow these steps to create empty ones that will be consumed by the Metal3 operator. To achieve this, we will utilize Kcli.</p>"},{"location":"labs/Dual/hostedcluster/worker-nodes/#creating-virtual-machines","title":"Creating Virtual Machines","text":"<p>If this is not your first attempt, you must first delete the previous setup. To do so, please refer to the Deleting Virtual Machines section.</p> <p>Now, you can execute the following commands for VM creation:</p> <pre><code>kcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-dual -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=[\"{\\\"name\\\": \\\"dual\\\", \\\"mac\\\": \\\"aa:aa:aa:aa:11:01\\\"}\"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa1101 -P name=hosted-dual-worker0\nkcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-dual -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=[\"{\\\"name\\\": \\\"dual\\\", \\\"mac\\\": \\\"aa:aa:aa:aa:11:02\\\"}\"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa1102 -P name=hosted-dual-worker1\nkcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-dual -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=[\"{\\\"name\\\": \\\"dual\\\", \\\"mac\\\": \\\"aa:aa:aa:aa:11:03\\\"}\"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa1103 -P name=hosted-dual-worker2\n\nsleep 2\nsystemctl restart ksushy\n</code></pre> <p>Let's dissect the creation command:</p> <ul> <li><code>start=False</code>: The VM will not boot automatically upon creation.</li> <li><code>uefi_legacy=true</code>: We will use UEFI legacy boot to ensure compatibility with older UEFI implementations.</li> <li><code>plan=hosted-dual</code>: The plan name, which identifies a group of machines as a cluster.</li> <li><code>memory=8192</code> and <code>numcpus=16</code>: These parameters specify the resources for the VM, including RAM and CPU.</li> <li><code>disks=[200,200]</code>: We are creating 2 disks (thin provisioned) in the virtual machine.</li> <li><code>nets=[{\"name\": \"dual\", \"mac\": \"aa:aa:aa:aa:11:13\"}]</code>: Network details, including the network name it will be connected to and the MAC address for the primary interface.</li> <li>The <code>ksushy</code> restart is performed to make our <code>ksushy</code> (VM's BMC) aware of the new VMs added.</li> </ul> <p>This is what the command looks like:</p> <pre><code>+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+\n|         Name        | Status |         Ip        |                       Source                       |     Plan    | Profile |\n+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+\n|    hosted-worker0   |  down  |                   |                                                    | hosted-dual |  kvirt  |\n|    hosted-worker1   |  down  |                   |                                                    | hosted-dual |  kvirt  |\n|    hosted-worker2   |  down  |                   |                                                    | hosted-dual |  kvirt  |\n+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+\n</code></pre>"},{"location":"labs/Dual/hostedcluster/worker-nodes/#deleting-virtual-machines","title":"Deleting Virtual Machines","text":"<p>To delete the VMs, you simply need to delete the plan, which, in our case, is:</p> <pre><code>kcli delete plan hosted-dual\n</code></pre> <pre><code>$ kcli delete plan hosted-dual\nAre you sure? [y/N]: y\nhosted-worker0 deleted on local!\nhosted-worker1 deleted on local!\nhosted-worker2 deleted on local!\nPlan hosted-dual deleted!\n</code></pre>"},{"location":"labs/Dual/hypervisor/","title":"Hypervisor","text":"<p>This section is primarily focused on Virtual Machines. If you are working with real bare metal, you can skip this section and verify whether your laboratory has the necessary components to run the deployment properly. If you find that you've missed something, you can configure it separately to address any gaps.</p>"},{"location":"labs/Dual/hypervisor/hypervisor-system-config/","title":"Hypervisor system config","text":"<p>This process is primarily intended for development environments. In production environments, it's essential to establish proper rules for the <code>firewalld</code> service and configure SELinux policies to maintain a secure environment.</p> <p>With these instructions, we allow various types of connections through the different virtual networks within the environment:</p> <pre><code>## SELinux\nsed -i s/^SELINUX=.*$/SELINUX=permissive/ /etc/selinux/config; setenforce 0\n\n## Firewalld\nsystemctl disable --now firewalld\n\n## Libvirtd\nsystemctl restart libvirtd\nsystemctl enable --now libvirtd\n</code></pre>"},{"location":"labs/Dual/hypervisor/network-manager-dispatcher/","title":"Network manager dispatcher","text":"<p>This script modifies the system DNS resolver to prioritize pointing to the <code>dnsmasq</code> service (configured later). This ensures that virtual machines can resolve the various domains, routes, and registries required for the different steps of the process.</p> <p>To enable this, you need to create a script named <code>forcedns</code> in <code>/etc/NetworkManager/dispatcher.d/</code> with the following content:</p> <p>Note</p> <p>Please ensure you modify the appropriate fields to align with your laboratory environment.</p> IPv4IPv6Dual stack <pre><code>#!/bin/bash\n\nexport IP=\"192.168.125.1\"\nexport BASE_RESOLV_CONF=\"/run/NetworkManager/resolv.conf\"\n\nif ! [[ `grep -q \"$IP\" /etc/resolv.conf` ]]; then\nexport TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)\ncp $BASE_RESOLV_CONF $TMP_FILE\nchmod --reference=$BASE_RESOLV_CONF $TMP_FILE\nsed -i -e \"s/hypershiftbm.lab//\" -e \"s/search /&amp; hypershiftbm.lab /\" -e \"0,/nameserver/s/nameserver/&amp; $IP\\n&amp;/\" $TMP_FILE\nmv $TMP_FILE /etc/resolv.conf\nfi\necho \"ok\"\n</code></pre> <pre><code>#!/bin/bash\n\nexport IP=\"2620:52:0:1306::1\"\nexport BASE_RESOLV_CONF=\"/run/NetworkManager/resolv.conf\"\n\nif ! [[ `grep -q \"$IP\" /etc/resolv.conf` ]]; then\nexport TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)\ncp $BASE_RESOLV_CONF $TMP_FILE\nchmod --reference=$BASE_RESOLV_CONF $TMP_FILE\nsed -i -e \"s/hypershiftbm.lab//\" -e \"s/search /&amp; hypershiftbm.lab /\" -e \"0,/nameserver/s/nameserver/&amp; $IP\\n&amp;/\" $TMP_FILE\nmv $TMP_FILE /etc/resolv.conf\nfi\necho \"ok\"\n</code></pre> <pre><code>#!/bin/bash\n\nexport IP=\"192.168.126.1\"\nexport BASE_RESOLV_CONF=\"/run/NetworkManager/resolv.conf\"\n\nif ! [[ `grep -q \"$IP\" /etc/resolv.conf` ]]; then\nexport TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)\ncp $BASE_RESOLV_CONF $TMP_FILE\nchmod --reference=$BASE_RESOLV_CONF $TMP_FILE\nsed -i -e \"s/hypershiftbm.lab//\" -e \"s/search /&amp; hypershiftbm.lab /\" -e \"0,/nameserver/s/nameserver/&amp; $IP\\n&amp;/\" $TMP_FILE\nmv $TMP_FILE /etc/resolv.conf\nfi\necho \"ok\"\n</code></pre> <p>The <code>IP</code> variable at the beginning of the script must be modified to point to the IP address of the Hypervisor's interface hosting the Openshift management cluster.</p> <p>After creating the file, you need to add execution permissions using the command:</p> <pre><code>chmod 755 /etc/NetworkManager/dispatcher.d/forcedns\n</code></pre> <p>Then, execute it once. The output should indicate <code>ok</code>.</p>"},{"location":"labs/Dual/hypervisor/packaging/","title":"Hypervisor Packaging","text":""},{"location":"labs/Dual/hypervisor/packaging/#system","title":"System","text":"<p>These are the main packages that are needed to deploy a virtualized Openshift Management cluster.</p> <pre><code>sudo dnf install dnsmasq radvd vim golang podman bind-utils net-tools httpd-tools tree htop strace tmux -y\n</code></pre> <p>Additionally, you need to enable and start the Podman service using the following command:</p> <pre><code>systemctl enable --now podman\n</code></pre>"},{"location":"labs/Dual/hypervisor/packaging/#kcli","title":"Kcli","text":"<p>We will utilize Kcli to deploy the Openshift Management cluster and various other virtualized components. To do so, you'll need to install and configure the hypervisor using the following commands:</p> <pre><code>sudo yum -y install libvirt libvirt-daemon-driver-qemu qemu-kvm\nsudo usermod -aG qemu,libvirt $(id -un)\nsudo newgrp libvirt\nsudo systemctl enable --now libvirtd\nsudo dnf -y copr enable karmab/kcli\nsudo dnf -y install kcli\nsudo kcli create pool -p /var/lib/libvirt/images default\nkcli create host kvm -H 127.0.0.1 local\nsudo setfacl -m u:$(id -un):rwx /var/lib/libvirt/images\nkcli create network  -c 192.168.122.0/24 default\n</code></pre> <p>For more info about Kcli please visit the official documentation.</p>"},{"location":"labs/Dual/hypervisor/redfish-for-vms/","title":"Redfish for vms","text":"<p>In a bare metal environment, the preferred approach is to utilize the actual BMC (Baseboard Management Controller) of the nodes used for the management cluster, which can be managed by Metal3 for discovery and provisioning. However, in a virtual environment, this approach is not feasible. As a workaround, we will use <code>ksushy</code>, which is an implementation of <code>sushy-tools</code>, allowing us to simulate BMCs for the virtual machines.</p> <p>To configure <code>ksushy</code>, execute the following commands:</p> <pre><code>sudo dnf install python3-pyOpenSSL.noarch python3-cherrypy -y\nkcli create sushy-service --ssl --ipv6 --port 9000\nsudo systemctl daemon-reload\nsystemctl enable --now ksushy\n</code></pre> <p>To test if this service is functioning correctly, you can check the service status with <code>systemctl status ksushy</code>. Additionally, you can execute a <code>curl</code> command against the exposed interface:</p> <pre><code>curl -Lk https://[2620:52:0:1306::1]:9000/redfish/v1\n</code></pre>"},{"location":"labs/Dual/hypervisor/requisites/","title":"Hypervisor Prerequisites","text":"<ul> <li>CPU: The number of CPUs provided determines how many HostedClusters can run concurrently.</li> <li>Recommended: 16 CPUs per Node for 3 nodes.</li> <li> <p>Minimal Dev: In a development environment, you may manage with 12 CPUs per Node for 3 nodes.</p> </li> <li> <p>Memory: The amount of RAM impacts how many HostedClusters can be hosted.</p> </li> <li>Recommended: 48 GB of RAM per Node.</li> <li> <p>Minimal Dev: For minimal development, 18 GB of RAM per Node may suffice.</p> </li> <li> <p>Storage: Using SSD storage for MCE is crucial.</p> </li> <li>Management Cluster: 250 GB.</li> <li>Registry: Depends on the number of releases, operators, and images hosted. An acceptable number could be 500 GB, preferably separated from the disk where the HostedCluster is hosted.</li> <li> <p>Webserver: The required storage depends on the number of ISOs and images hosted. An acceptable number could be 500 GB.</p> </li> <li> <p>Production: For a production environment, it's advisable to keep these three components separated on different disks. A recommended configuration for production is as follows:</p> </li> <li>Registry: 2 TB.</li> <li>Management Cluster: 500 GB.</li> <li>WebServer: 2 TB.</li> </ul>"},{"location":"labs/Dual/mce/","title":"Index","text":"<p>The Multicluster Engine (MCE) is a component of the ACM bundle. It plays a crucial role in deploying clusters across multiple providers.</p>"},{"location":"labs/Dual/mce/#credentials-and-authorization","title":"Credentials and Authorization","text":"<p>To prepare for ACM/MCE deployment, certain prerequisites must be met. Additionally, you will need to request permissions to access internal builds. For detailed information, please refer to this documentation.</p> <p>ACM/MCE Deployment Agent Service Config</p>"},{"location":"labs/Dual/mce/agentserviceconfig/","title":"Agentserviceconfig","text":"<p>The Agent Service Config object is an essential component of the Assisted Service addon included in MCE/ACM, responsible for Baremetal cluster deployment. When the addon is enabled, you must deploy an operand (CRD) named <code>AgentServiceConfig</code> to configure it.</p>"},{"location":"labs/Dual/mce/agentserviceconfig/#agent-service-config-objects","title":"Agent Service Config Objects","text":"<p>You can find the CRD described here. In this context, we will focus on the main aspects to ensure its functionality in disconnected environments.</p> <p>In addition to configuring the Agent Service Config, to ensure that Multicluster Engine functions properly in a disconnected environment, we need to include some additional ConfigMaps.</p>"},{"location":"labs/Dual/mce/agentserviceconfig/#custom-registries-configuration","title":"Custom Registries Configuration","text":"<p>This ConfigMap contains the disconnected details necessary to customize the deployment.</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: custom-registries\n  namespace: multicluster-engine\n  labels:\n    app: assisted-service\ndata:\n  ca-bundle.crt: |\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\n  registries.conf: |\n    unqualified-search-registries = [\"registry.access.redhat.com\", \"docker.io\"]\n\n    [[registry]]\n    prefix = \"\"\n    location = \"registry.redhat.io/openshift4\"\n    mirror-by-digest-only = true\n\n    [[registry.mirror]]\n      location = \"registry.hypershiftbm.lab:5000/openshift4\"\n\n    [[registry]]\n    prefix = \"\"\n    location = \"registry.redhat.io/rhacm2\"\n    mirror-by-digest-only = true\n    ...\n    ...\n</code></pre> <p>This object includes two fields:</p> <ol> <li>Custom CAs: This field contains the Certificate Authorities (CAs) that will be loaded into the various processes of the deployment.</li> <li>Registries: The <code>Registries.conf</code> field contains information about images and namespaces that need to be consumed from a mirror registry instead of the original source registry.</li> </ol>"},{"location":"labs/Dual/mce/agentserviceconfig/#assisted-service-customization","title":"Assisted Service Customization","text":"<p>The Assisted Service Customization ConfigMap is consumed by the Assisted Service operator and contains variables that modify the behavior of the controllers.</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: assisted-service-config\n  namespace: multicluster-engine\ndata:\n  ALLOW_CONVERGED_FLOW: \"false\"\n</code></pre> <p>You can find documentation on how to customize the operator here.</p>"},{"location":"labs/Dual/mce/agentserviceconfig/#assisted-service-config","title":"Assisted Service Config","text":"<p>The Assisted Service Config object includes the necessary information to ensure the correct functioning of the operator.</p> <pre><code>---\napiVersion: agent-install.openshift.io/v1beta1\nkind: AgentServiceConfig\nmetadata:\n  annotations:\n    unsupported.agent-install.openshift.io/assisted-service-configmap: assisted-service-config\n  name: agent\n  namespace: multicluster-engine\nspec:\n  mirrorRegistryRef:\n    name: custom-registries\n  databaseStorage:\n    storageClassName: lvms-vg1\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 10Gi\n  filesystemStorage:\n    storageClassName: lvms-vg1\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 20Gi\n  osImages:\n  - cpuArchitecture: x86_64\n    openshiftVersion: \"4.14\"\n    rootFSUrl: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202308281054-0-live-rootfs.x86_64.img\n    url: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202308281054-0-live.x86_64.iso\n    version: 414.92.202308281054-0\n</code></pre> <p>If your mirror registry doesn't require authentication in your pull secret, see this document on how to add it to the unauthenticated list in the AgentServiceConfig CR.</p> <p>In this section, we will emphasize the important aspects:</p> <ul> <li>The <code>metadata.annotations[\"unsupported.agent-install.openshift.io/assisted-service-configmap\"]</code> Annotation references the ConfigMap name to be consumed by the operator for customizing behavior.</li> <li>The <code>spec.mirrorRegistryRef.name</code> points to the ConfigMap containing disconnected registry information to be consumed by the Assisted Service Operator. This ConfigMap injects these resources during the deployment process.</li> <li>The <code>spec.osImages</code> field contains different versions available for deployment by this operator. These fields are mandatory.</li> </ul> <p>Let's fill in this section for the 4.14 dev preview ec4 version (sample):</p> <ul> <li>Release URL</li> <li>RHCOS Info</li> </ul> <p>Assuming you've already downloaded the RootFS and LiveIso files:</p> <pre><code>  - cpuArchitecture: x86_64\n    openshiftVersion: \"4.14\"\n    rootFSUrl: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202309101331-0-live-rootfs.x86_64.img\n    url: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202309101331-0-live.x86_64.iso\n    version: 414.92.202307250657-0\n</code></pre>"},{"location":"labs/Dual/mce/agentserviceconfig/#deployment","title":"Deployment","text":"<p>To deploy all these objects, simply concatenate them into a single file and apply them to the Management Cluster.</p> <pre><code>oc apply -f agentServiceConfig.yaml\n</code></pre> <p>This will trigger 2 pods</p> <pre><code>assisted-image-service-0                               1/1     Running   2             11d\nassisted-service-668b49548-9m7xw                       2/2     Running   5             11d\n</code></pre> <p>Note</p> <p>The <code>Assisted Image Service</code> pod is responsible for creating the RHCOS Boot Image template, which will be customized for each cluster you deploy.</p> <p>The <code>Assisted Service</code> refers to the operator.</p>"},{"location":"labs/Dual/mce/multicluster-engine/","title":"Multicluster engine","text":"<p>The deployment of each component will depend on your needs, follow the next links accordingly:</p> <ul> <li>ACM Deployment</li> <li>ACM Downstream builds</li> <li>MCE Deployment</li> <li>MCE Upgrade</li> <li>ACM/MCE Cluster deletion</li> <li>MulticlusterEngine deletion</li> <li>Last resource: nuke.sh</li> </ul>"},{"location":"labs/Dual/mgmt-cluster/","title":"Index","text":""},{"location":"labs/Dual/mgmt-cluster/#openshift-management-cluster","title":"Openshift Management Cluster","text":"<p>This section contains the necessary artifacts to set up an Openshift management cluster based on virtual machines using kcli as the primary tool. Another option is to use dev-scripts, which uses a different approach.</p> <p>Networking Openshift Compact Dual</p>"},{"location":"labs/Dual/mgmt-cluster/compact-dual/","title":"Compact dual","text":"<p>In this section, we will discuss how to deploy the Openshift management cluster. To do that, we need to have the following files in place:</p> <ol> <li>Pull Secret</li> <li>Kcli Plan</li> </ol> <p>Ensure that these files are properly set up for the deployment process.</p>"},{"location":"labs/Dual/mgmt-cluster/compact-dual/#pull-secret","title":"Pull Secret","text":"<p>The Pull Secret should be located in the same folder as the kcli plan and should be named <code>openshift_pull.json</code>.</p>"},{"location":"labs/Dual/mgmt-cluster/compact-dual/#kcli-plan","title":"Kcli plan","text":"<p>The Kcli plan contains the Openshift definition. This is how looks like:</p> <p>Note</p> <p>Please ensure you modify the appropriate fields to align with your laboratory environment.</p> <ul> <li><code>mgmt-compact-hub-dual.yaml</code></li> </ul> <pre><code>plan: hub-dual\nforce: true\nversion: nightly\ntag: \"4.14.0-0.nightly-2023-08-29-102237\"\ncluster: \"hub-dual\"\ndualstack: true\ndomain: hypershiftbm.lab\napi_ip: 192.168.126.10\ningress_ip: 192.168.126.11\nservice_networks:\n- 172.30.0.0/16\n- fd02::/112\ncluster_networks:\n- 10.132.0.0/14\n- fd01::/48\ndisconnected_url: registry.hypershiftbm.lab:5000\ndisconnected_update: true\ndisconnected_user: dummy\ndisconnected_password: dummy\ndisconnected_operators_version: v4.13\ndisconnected_operators:\n- name: metallb-operator\n- name: lvms-operator\n  channels:\n  - name: stable-4.13\ndisconnected_extra_images:\n- quay.io/mavazque/trbsht:latest\n- quay.io/jparrill/hypershift:BMSelfManage-v4.14-rc-v3\n- registry.redhat.io/openshift4/ose-kube-rbac-proxy:v4.10\ndualstack: true\ndisk_size: 200\nextra_disks: [200]\nmemory: 48000\nnumcpus: 16\nctlplanes: 3\nworkers: 0\nmanifests: extra-manifests\nmetal3: true\nnetwork: dual\nusers_dev: developer\nusers_devpassword: developer\nusers_admin: admin\nusers_adminpassword: admin\nmetallb_pool: dual-virtual-network\nmetallb_ranges:\n- 192.168.126.150-192.168.126.190\nmetallb_autoassign: true\napps:\n- users\n- lvms-operator\n- metallb-operator\nvmrules:\n- hub-bootstrap:\n    nets:\n    - name: ipv6\n      mac: aa:aa:aa:aa:10:07\n- hub-ctlplane-0:\n    nets:\n    - name: ipv6\n      mac: aa:aa:aa:aa:10:01\n- hub-ctlplane-1:\n    nets:\n    - name: ipv6\n      mac: aa:aa:aa:aa:10:02\n- hub-ctlplane-2:\n    nets:\n    - name: ipv6\n      mac: aa:aa:aa:aa:10:03\n</code></pre> <p>Note</p> <p>To understand the meaning of each of these parameters, you can refer to the official documentation here.</p>"},{"location":"labs/Dual/mgmt-cluster/compact-dual/#deployment","title":"Deployment","text":"<p>To initiate the provisioning procedure, execute the following:</p> <pre><code>kcli create cluster openshift --pf mgmt-compact-hub-dual.yaml\n</code></pre>"},{"location":"labs/Dual/mgmt-cluster/network/","title":"Network","text":"<p>Firstly, we need to ensure that we have the right networks prepared for use in the Hypervisor. These networks will be used to host both the Management and Hosted clusters.</p> <p>To configure these networks, we will use the following <code>kcli</code> command:</p> <pre><code>kcli create network -c 192.168.126.0/24 -P dhcp=false -P dns=false -d 2620:52:0:1306::0/64 --domain hypershiftbm.lab --nodhcp dual\n</code></pre> <p>Where:</p> <ul> <li><code>-c</code> specifies the CIDR used for that network.</li> <li><code>-P dhcp=false</code> configures the network to disable DHCP, which will be handled by the previously configured dnsmasq.</li> <li><code>-P dns=false</code> configures the network to disable DNS, which will also be handled by the dnsmasq.</li> <li><code>--domain</code> sets the domain to search into.</li> <li><code>dual</code> is the name of the network that will be created.</li> </ul> <p>This is what the network will look like once created:</p> <pre><code>[root@hypershiftbm ~]# kcli list network\nListing Networks...\n+---------+--------+---------------------+-------+------------------+------+\n| Network |  Type  |         Cidr        |  Dhcp |      Domain      | Mode |\n+---------+--------+---------------------+-------+------------------+------+\n| default | routed |   192.168.122.0/24  |  True |     default      | nat  |\n| dual    | routed | 2620:52:0:1306::/64 | False | hypershiftbm.lab | nat  |\n| dual    | routed |   192.168.125.0/24  | False | hypershiftbm.lab | nat  |\n| ipv6    | routed | 2620:52:0:1305::/64 | False | hypershiftbm.lab | nat  |\n+---------+--------+---------------------+-------+------------------+------+\n</code></pre> <pre><code>[root@hypershiftbm ~]# kcli info network ipv6\nProviding information about network ipv6...\ncidr: 2620:52:0:1306::/64\ndhcp: false\ndomain: hypershiftbm.lab\nmode: nat\nplan: kvirt\ntype: routed\n</code></pre>"},{"location":"labs/Dual/mirror/","title":"Index","text":"<p>Important</p> <p>This section is only relevant in disconnected scenarios. If this does not apply to your situation, you can continue with the next section.</p> <p>Image mirroring is the process of fetching images from external registries, such as registry.redhat.com or quay.io, and storing them in your private registry.</p> <p>To learn how to perform image mirroring, follow the next sections:</p> <p>Image Mirroring Process ICSP and IDMS</p>"},{"location":"labs/Dual/mirror/ICSP-IDMS/","title":"ICSP IDMS","text":"<p>Once the mirroring process is complete, you will have two main objects that need to be applied in the Management Cluster:</p> <ol> <li>ICSP (Image Content Source Policies) or IDMS (Image Digest Mirror Set).</li> <li>Catalog Sources.</li> </ol> <p>Using the <code>oc-mirror</code> tool, the output artifacts will be located in a new folder called <code>oc-mirror-workspace/results-XXXXXX/</code>.</p> <p>ICSP/IDMS will trigger a \"special\" MachineConfig change that will not reboot your nodes but will reboot the kubelet on each of them.</p> <p>Once all nodes are schedulable and marked as <code>READY</code>, you will need to apply the new catalog sources generated.</p> <p>The catalog sources will trigger some actions in the <code>openshift-marketplace operator</code>, such as downloading the catalog image and processing it to retrieve all the <code>PackageManifests</code> included in that image. You can check the new sources by executing <code>oc get packagemanifest</code> using the new CatalogSource as a source.</p>"},{"location":"labs/Dual/mirror/ICSP-IDMS/#applying-the-artifacts","title":"Applying the Artifacts","text":"<p>First, we need to create the ICSP/IDMS artifacts:</p> <pre><code>oc apply -f oc-mirror-workspace/results-XXXXXX/imageContentSourcePolicy.yaml\n</code></pre> <p>Now, wait for the nodes to become ready again and execute the following command:</p> <pre><code>oc apply -f catalogSource-XXXXXXXX-index.yaml\n</code></pre>"},{"location":"labs/Dual/mirror/mirroring/","title":"Mirroring","text":"<p>The mirroring step can take some time to complete, so we recommend starting with this part once the Registry server is up and running.</p> <p>For this purpose, we will use the <code>oc-mirror</code> tool, a binary that utilizes an object called <code>ImageSetConfiguration</code>.</p> <p>In this file, you can specify:</p> <ul> <li>The OpenShift versions to mirror (they should be located in quay.io).</li> <li>The additional operators to mirror, selecting packages individually.</li> <li>The extra images you want to add to the repository.</li> </ul> <p>Note</p> <p>Please ensure you modify the appropriate fields to align with your laboratory environment.</p> <p>Here is an example of the <code>ImageSetConfiguration</code> that we will use for our mirroring:</p> <pre><code>apiVersion: mirror.openshift.io/v1alpha2\nkind: ImageSetConfiguration\nstorageConfig:\n  registry:\n    imageURL: registry.hypershiftbm.lab:5000/openshift/release/metadata:latest\nmirror:\n  platform:\n    channels:\n    - name: candidate-4.14\n      minVersion: 4.14.0-ec.1\n      maxVersion: 4.14.0-ec.3\n      type: ocp\n    graph: true\n  additionalImages:\n  - name: quay.io/karmab/origin-keepalived-ipfailover:latest\n  - name: quay.io/karmab/kubectl:latest\n  - name: quay.io/karmab/haproxy:latest\n  - name: quay.io/karmab/mdns-publisher:latest\n  - name: quay.io/karmab/origin-coredns:latest\n  - name: quay.io/karmab/curl:latest\n  - name: quay.io/karmab/kcli:latest\n  - name: quay.io/mavazque/trbsht:latest\n  - name: quay.io/jparrill/hypershift:BMSelfManage-v4.14-rc-v3\n  - name: registry.redhat.io/openshift4/ose-kube-rbac-proxy:v4.10\n  operators:\n  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13\n    packages:\n    - name: lvms-operator\n    - name: local-storage-operator\n    - name: odf-csi-addons-operator\n    - name: odf-operator\n    - name: mcg-operator\n    - name: ocs-operator\n    - name: metallb-operator\n</code></pre> <p>Make sure you have your <code>${HOME}/.docker/config.json</code> file updated with the registries you are trying to mirror from and your private registry to push the images to.</p> <p>After that, we can begin the mirroring process:</p> <pre><code>oc-mirror --source-skip-tls --config imagesetconfig.yaml docker://${REGISTRY}\n</code></pre> <p>Once the mirror finishes, you will have a new folder called <code>oc-mirror-workspace/results-XXXXXX/</code> which contains the ICSP and the CatalogSources to be applied later on the HostedCluster.</p>"},{"location":"labs/Dual/mirror/mirroring/#mirroring-nightly-and-ci-releases","title":"Mirroring Nightly and CI releases","text":"<p>The bad part in all of this, we cannot cover nightly or CI versions of Openshift so we will need to use the <code>oc adm release mirror</code> to mirror those versions.</p> <p>To mirror the nightly versions we need for this deployment, you need to execute this:</p> <pre><code>REGISTRY=registry.$(hostname --long):5000\n\noc adm release mirror \\\n  --from=registry.ci.openshift.org/ocp/release:4.14.0-0.nightly-2023-08-29-102237 \\\n  --to=${REGISTRY}/openshift/release \\\n  --to-release-image=${REGISTRY}/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237\n</code></pre> <p>For more detailed and updated information, you can visit the official Documentation or GitHub repository</p>"},{"location":"labs/Dual/mirror/mirroring/#mirror-mce-internal-releases","title":"Mirror MCE internal releases","text":"<p>In order to mirror all the MCE latest images uploaded to quay.io or if it's internal and you can access the ACM documentation.</p> <ul> <li>Red Hat Official Documentation</li> <li>Red Hat Internal deployment Brew Registry deployment</li> </ul>"},{"location":"labs/Dual/watching/","title":"Index","text":"<p>This section is purely done to let you know how we should monitor the deployment of a HostedCluster from two perspectives:</p> <ul> <li>Control Plane</li> <li>Data Plane</li> </ul> <p>So please follow these links accordingly.</p> <p>Control Plane perspective Data Plane perspective</p>"},{"location":"labs/Dual/watching/watching-cp/","title":"Watching cp","text":"<p>Now it's a matter of waiting for the cluster to finish the deployment, so let's take a look at some useful commands on the Management cluster side:</p> <pre><code>export KUBECONFIG=/root/.kcli/clusters/hub-ipv4/auth/kubeconfig\n\nwatch \"oc get pod -n hypershift;echo;echo;oc get pod -n clusters-hosted-ipv4;echo;echo;oc get bmh -A;echo;echo;oc get agent -A;echo;echo;oc get infraenv -A;echo;echo;oc get hostedcluster -A;echo;echo;oc get nodepool -A;echo;echo;\"\n</code></pre> <p>This command will give you info about:</p> <ul> <li>What is the Hypershift Operator status</li> <li>The HostedControlPlane pod status</li> <li>The BareMetalHosts</li> <li>The Agents</li> <li>The Infraenv</li> <li>The HostedCluster and NodePool</li> </ul> <p>This is how it looks:</p> <p></p>"},{"location":"labs/Dual/watching/watching-dp/","title":"Watching dp","text":"<p>If you check the Hosted cluster side you can check how the Operators are progressing and what is the status. To do that we will use these commands</p> <pre><code>oc get secret -n clusters-hosted-ipv4 admin-kubeconfig -o jsonpath='{.data.kubeconfig}' |base64 -d &gt; /root/hc_admin_kubeconfig.yaml\nexport KUBECONFIG=/root/hc_admin_kubeconfig.yaml\n\nwatch \"oc get clusterversion,nodes,co\"\n</code></pre> <p>This command will give you info about:</p> <ul> <li>Check the clusterversion</li> <li>Check if the Nodes has joined the cluster</li> <li>Check the ClusterOperators</li> </ul> <p>This is how looks like:</p> <p></p>"},{"location":"labs/IPv4/","title":"IPv4","text":"<p>This is one of the simplest network configurations for this type of deployment. We will primarily focus on IPv4 ranges, requiring fewer external components than IPv6 or dual-stack setups.</p> <p>All the scripts provided contain either partial or complete automation to recreate the environment. To follow along, refer to the repository containing all the scripts for IPv4 environments.</p> <p>This documentation is structured to be followed in a specific order:</p> <ul> <li>Hypervisor</li> <li>DNS</li> <li>Registry</li> <li>Management Cluster</li> <li>Webserver</li> <li>Mirroring</li> <li>Multicluster Engine</li> <li>TLS Certificates</li> <li>HostedCluster</li> <li>Watching Deployment progress</li> </ul>"},{"location":"labs/IPv4/dns/","title":"Dns","text":"<p>The DNS configuration is a critical aspect of our setup. To enable name resolution in our virtualized environment, follow these steps:</p> <ol> <li> <p>Create the primary DNS configuration file for the dnsmasq server:</p> </li> <li> <p><code>/opt/dnsmasq/dnsmasq.conf</code> <pre><code>strict-order\nbind-dynamic\n#log-queries\nbogus-priv\ndhcp-authoritative\n\ndhcp-range=ipv4,192.168.125.120,192.168.125.250,255.255.255.0,24h\ndhcp-option=ipv4,option:dns-server,192.168.125.1\ndhcp-option=ipv4,option:router,192.168.125.1\n\nresolv-file=/opt/dnsmasq/upstream-resolv.conf\nexcept-interface=lo\ndhcp-lease-max=81\nlog-dhcp\nno-hosts\n\n# DHCP Reservations\ndhcp-leasefile=/opt/dnsmasq/hosts.leases\n\n# Include all files in a directory depending on the suffix\nconf-dir=/opt/dnsmasq/include.d/*.ipv4\n</code></pre></p> </li> </ol> <p>Create the upstream resolver to delegate the non-local environments queries</p> <ul> <li><code>/opt/dnsmasq/upstream-resolv.conf</code> <pre><code>nameserver 8.8.8.8\nnameserver 8.8.4.4\n</code></pre></li> </ul> <p>Create the different component DNS configurations</p> <ul> <li> <p><code>/opt/dnsmasq/include.d/hosted-nodeport.ipv4</code> <pre><code>## Nodeport\nhost-record=api-int.hosted-ipv4.hypershiftbm.lab,192.168.125.20\nhost-record=api-int.hosted-ipv4.hypershiftbm.lab,192.168.125.21\nhost-record=api-int.hosted-ipv4.hypershiftbm.lab,192.168.125.22\nhost-record=api.hosted-ipv4.hypershiftbm.lab,192.168.125.20\nhost-record=api.hosted-ipv4.hypershiftbm.lab,192.168.125.21\nhost-record=api.hosted-ipv4.hypershiftbm.lab,192.168.125.22\n\n## Nodeport\n## IMPORTANT!: You should point to the node which is exposing the router.\n## You can also use MetalLB to expose the Apps wildcard.\naddress=/apps.hosted-ipv4.hypershiftbm.lab/192.168.125.30\n\n## General\ndhcp-host=aa:aa:aa:aa:02:01,hosted-ipv4-worker0,192.168.125.30\ndhcp-host=aa:aa:aa:aa:02:02,hosted-ipv4-worker1,192.168.125.31\ndhcp-host=aa:aa:aa:aa:02:03,hosted-ipv4-worker2,192.168.125.32\n</code></pre></p> </li> <li> <p><code>/opt/dnsmasq/include.d/hub.ipv4</code> <pre><code>host-record=api-int.hub-ipv4.hypershiftbm.lab,192.168.125.10\nhost-record=api.hub-ipv4.hypershiftbm.lab,192.168.125.10\naddress=/apps.hub-ipv4.hypershiftbm.lab/192.168.125.11\ndhcp-host=aa:aa:aa:aa:02:01,ocp-master-0,192.168.125.20\ndhcp-host=aa:aa:aa:aa:02:02,ocp-master-1,192.168.125.21\ndhcp-host=aa:aa:aa:aa:02:03,ocp-master-2,192.168.125.22\ndhcp-host=aa:aa:aa:aa:02:06,ocp-installer,192.168.125.25\ndhcp-host=aa:aa:aa:aa:02:10,ocp-bootstrap,192.168.125.26\n</code></pre></p> </li> <li> <p><code>/opt/dnsmasq/include.d/infra.ipv4</code> <pre><code>host-record=registry.hypershiftbm.lab,192.168.125.1\n</code></pre></p> </li> </ul> <p>To proceed, we must create a systemd service for the management of the dnsmasq service and disable the system's default dnsmasq service:</p> <ul> <li><code>/etc/systemd/system/dnsmasq-virt.service</code> <pre><code>[Unit]\nDescription=DNS server for Openshift 4 Clusters.\nAfter=network.target\n\n[Service]\nUser=root\nGroup=root\nExecStart=/usr/sbin/dnsmasq -k --conf-file=/opt/dnsmasq/dnsmasq.conf\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></li> </ul> <p>The commands to do so:</p> <pre><code>systemctl daemon-reload\nsystemctl disable --now dnsmasq\nsystemctl enable --now dnsmasq-virt\n</code></pre> <p>Note</p> <p>This step is mandatory for both Disconnected and Connected environments. Additionally, it holds significance for both Virtualized and Bare Metal environments. The key distinction lies in the location where the resources will be configured. In a non-virtualized environment, a more robust solution like Bind is recommended instead of a lightweight dnsmasq.</p>"},{"location":"labs/IPv4/registry/","title":"Registry","text":"<p>Important</p> <p>This section is exclusively applicable to disconnected scenarios. If this does not apply to your situation, please proceed to the next section.</p> <p>In this section, we will explain how to deploy a small, self-hosted registry using a Podman container. For production environments, we strongly recommend utilizing a more reliable solution such as Quay, Nexus, or Artifactory.</p> <ol> <li> <p>As a privileged user, please access the ${HOME} directory and proceed to create the following script provided below. The script will make certain assumptions, including the registry name based on the hypervisor hostname, as well as the necessary credentials and user access:</p> </li> <li> <p><code>registry.sh</code> <pre><code>#!/usr/bin/env bash\n\nset -euo pipefail\n\nPRIMARY_NIC=$(ls -1 /sys/class/net | grep -v podman | head -1)\nexport PATH=/root/bin:$PATH\nexport PULL_SECRET=\"/root/baremetal/hub/openshift_pull.json\"\n\nif [[ ! -f $PULL_SECRET ]];then\n  echo \"Pull Secret not found, exiting...\"\n  exit 1\nfi\n\ndnf -y install podman httpd httpd-tools jq skopeo libseccomp-devel\nexport IP=$(ip -o addr show $PRIMARY_NIC | head -1 | awk '{print $4}' | cut -d'/' -f1)\nREGISTRY_NAME=registry.$(hostname --long)\nREGISTRY_USER=dummy\nREGISTRY_PASSWORD=dummy\nKEY=$(echo -n $REGISTRY_USER:$REGISTRY_PASSWORD | base64)\necho \"{\\\"auths\\\": {\\\"$REGISTRY_NAME:5000\\\": {\\\"auth\\\": \\\"$KEY\\\", \\\"email\\\": \\\"sample-email@domain.ltd\\\"}}}\" &gt; /root/disconnected_pull.json\nmv ${PULL_SECRET} /root/openshift_pull.json.old\njq \".auths += {\\\"$REGISTRY_NAME:5000\\\": {\\\"auth\\\": \\\"$KEY\\\",\\\"email\\\": \\\"sample-email@domain.ltd\\\"}}\" &lt; /root/openshift_pull.json.old &gt; $PULL_SECRET\nmkdir -p /opt/registry/{auth,certs,data,conf}\ncat &lt;&lt;EOF &gt; /opt/registry/conf/config.yml\nversion: 0.1\nlog:\n  fields:\n    service: registry\nstorage:\n  cache:\n    blobdescriptor: inmemory\n  filesystem:\n    rootdirectory: /var/lib/registry\n  delete:\n    enabled: true\nhttp:\n  addr: :5000\n  headers:\n    X-Content-Type-Options: [nosniff]\nhealth:\n  storagedriver:\n    enabled: true\n    interval: 10s\n    threshold: 3\ncompatibility:\n  schema1:\n    enabled: true\nEOF\nopenssl req -newkey rsa:4096 -nodes -sha256 -keyout /opt/registry/certs/domain.key -x509 -days 3650 -out /opt/registry/certs/domain.crt -subj \"/C=US/ST=Madrid/L=San Bernardo/O=Karmalabs/OU=Guitar/CN=$REGISTRY_NAME\" -addext \"subjectAltName=DNS:$REGISTRY_NAME\"\ncp /opt/registry/certs/domain.crt /etc/pki/ca-trust/source/anchors/\nupdate-ca-trust extract\nhtpasswd -bBc /opt/registry/auth/htpasswd $REGISTRY_USER $REGISTRY_PASSWORD\npodman create --name registry --net host --security-opt label=disable --replace -v /opt/registry/data:/var/lib/registry:z -v /opt/registry/auth:/auth:z -v /opt/registry/conf/config.yml:/etc/docker/registry/config.yml -e \"REGISTRY_AUTH=htpasswd\" -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry\" -e \"REGISTRY_HTTP_SECRET=ALongRandomSecretForRegistry\" -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd -v /opt/registry/certs:/certs:z -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key docker.io/library/registry:latest\n[ \"$?\" == \"0\" ] || !!\nsystemctl enable --now registry\n</code></pre></p> </li> <li> <p>Kindly modify the location of the <code>PULL_SECRET</code> to point to the correct location.</p> </li> <li>Please adjust the permissions by adding the execution flag as follows: <code>chmod u+x ${HOME}/registry.sh</code></li> <li>You can execute the script without any parameters using the following command <code>${HOME}/registry.sh</code></li> </ol> <p>After executing the script <code>${HOME}/registry.sh</code>, the server should now be up and running. If the script has been properly configured and executed without errors, it should have started the server as intended.</p> <p>It will utilize a systemd service for management purposes. So, if you ever need to manage it, you can employ systemctl status/start/stop registry.</p> <p>The root folder for the registry is situated at /opt/registry, and it's structured as follows:</p> <ul> <li><code>certs</code> holds the TLS certificates.</li> <li><code>auth</code> keeps the credentials.</li> <li><code>data</code> contains the registry images.</li> <li><code>conf</code> will hold the registry configuration.</li> </ul>"},{"location":"labs/IPv4/tls-certificates/","title":"Tls certificates","text":"<p>Important</p> <p>This section is only relevant in disconnected scenarios. If this doesn't apply to your situation, please proceed to the next section.</p> <p>In this section, we'll cover the TLS certificates involved in the process, primarily focusing on the private registries from which the images will be pulled. While there may be additional certificates, we'll concentrate on these particular ones.</p> <p>It's important to distinguish between the various methods and their impact on the associated cluster. All of these methods essentially modify the content of the following files on the OCP (OpenShift Container Platform) control plane (Master nodes) and data plane (worker nodes):</p> <ul> <li><code>/etc/pki/ca-trust/extracted/pem/</code></li> <li><code>/etc/pki/ca-trust/source/anchors/</code></li> <li><code>/etc/pki/tls/certs/</code></li> </ul>"},{"location":"labs/IPv4/tls-certificates/#adding-a-ca-to-the-management-cluster","title":"Adding a CA to the Management Cluster","text":"<p>There exist numerous methods to accomplish this within the OpenShift environment. However, we have chosen to integrate the less intrusive approach.</p> <ol> <li>Initially, you must create a ConfigMap with a name of your choosing. In our specific case, we will utilize the name <code>registry-config</code> The content should resemble the following:</li> </ol> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: registry-config\n  namespace: openshift-config\ndata:\n  registry.hypershiftbm.lab..5000: |\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\n</code></pre> <p>Note</p> <pre><code>The data field ought to contain the registry name, while the value should encompass the Registry certificate. As is evident, the \":\" character is being replaced by \"..\"; therefore, it is imperative to ensure this correction.\n</code></pre> <ol> <li>Now we need to patch the clusterwide object <code>image.config.openshift.io</code> including this:</li> </ol> <pre><code>spec:\n  additionalTrustedCA:\n    name: registry-config\n</code></pre> <p>This modification will result in two significant consequences:</p> <ul> <li>Granting masters the capability to retrieve images from the private registry.</li> <li>Allowing the Hypershift Operator to extract the Openshift payload for the HostedCluster deployments.</li> </ul> <p>Note</p> <pre><code>The modification required several minutes to be successfully executed.\n</code></pre>"},{"location":"labs/IPv4/tls-certificates/#alternative-adding-a-ca-to-the-management-cluster","title":"Alternative: Adding a CA to the Management Cluster","text":"<p>We consider this as an alternative, given that it entails the masters undergoing a reboot facilitated by the Machine Config Operator.</p> <p>It's described here. This method involves utilizing the <code>image-registry-operator</code>, which deploys the CAs to the OCP nodes.</p> <p>Hypershift's operators and controllers automatically handle this process, so if you're using a GA (Generally Available) released version, it should work seamlessly, and you won't need to apply these steps. This Hypershift feature is included in the payload of the 2.4 MCE release.</p> <p>However, if this feature is not working as expected or if it doesn't apply to your situation, you can follow this procedure:</p> <ul> <li>Check if the <code>openshift-config</code> namespace in the Management cluster contains a ConfigMap named <code>user-ca-bundle</code>.</li> <li>If the ConfigMap doesn't exist, execute the following command:</li> </ul> <pre><code>## REGISTRY_CERT_PATH=&lt;PATH/TO/YOUR/CERTIFICATE/FILE&gt;\nexport REGISTRY_CERT_PATH=/opt/registry/certs/domain.crt\n\noc create configmap user-ca-bundle -n openshift-config --from-file=ca-bundle.crt=${REGISTRY_CERT_PATH}\n</code></pre> <ul> <li>Otherwise, if that ConfigMap exists, execute this other command:</li> </ul> <pre><code>## REGISTRY_CERT_PATH=&lt;PATH/TO/YOUR/CERTIFICATE/FILE&gt;\nexport REGISTRY_CERT_PATH=/opt/registry/certs/domain.crt\nexport TMP_FILE=$(mktemp)\n\noc get cm -n openshift-config user-ca-bundle -ojsonpath='{.data.ca-bundle\\.crt}' &gt; ${TMP_FILE}\necho &gt;&gt; ${TMP_FILE}\necho \\#registry.$(hostname --long) &gt;&gt; ${TMP_FILE}\ncat ${REGISTRY_CERT_PATH} &gt;&gt; ${TMP_FILE}\noc create configmap user-ca-bundle -n openshift-config --from-file=ca-bundle.crt=${TMP_FILE} --dry-run=client -o yaml | kubectl apply -f -\n</code></pre> <p>You have a functional script located in the <code>assets/&lt;NetworkStack&gt;/09-tls-certificates/01-config.sh</code>, this is the sample for IPv6.</p>"},{"location":"labs/IPv4/webserver/","title":"Webserver","text":"<p>Important</p> <p>This section is only relevant in disconnected scenarios, if this is not your case, you can continue with the next section.</p> <p>This section talks about an additional webserver that you need to configure to host the RHCOS images associated with the Openshift release you are trying to deploy as a HostedCluster.</p> <p>The script refers to this repository folder and it's the same for all three different network stacks.</p> <p>To do this, you can use this script:</p> <pre><code>#!/bin/bash\n\nWEBSRV_FOLDER=/opt/srv\nROOTFS_IMG_URL=\"$(../04-management-cluster/openshift-install coreos print-stream-json | jq -r '.architectures.x86_64.artifacts.metal.formats.pxe.rootfs.location')\"\nLIVE_ISO_URL=\"$(../04-management-cluster/openshift-install coreos print-stream-json | jq -r '.architectures.x86_64.artifacts.metal.formats.iso.disk.location')\"\n\nmkdir -p ${WEBSRV_FOLDER}/images\ncurl -Lk ${ROOTFS_IMG_URL} -o ${WEBSRV_FOLDER}/images/${ROOTFS_IMG_URL##*/}\ncurl -Lk ${LIVE_ISO_URL} -o ${WEBSRV_FOLDER}/images/${LIVE_ISO_URL##*/}\nchmod -R 755 ${WEBSRV_FOLDER}/*\n\n## Run Webserver\npodman ps --noheading | grep -q websrv-ai\nif [[ $? == 0 ]];then\n    echo \"Launching Registry pod...\"\n    /usr/bin/podman run --name websrv-ai --net host -v /opt/srv:/usr/local/apache2/htdocs:z quay.io/alosadag/httpd:p8080\nfi\n</code></pre> <p>The script will create a folder under <code>/opt/srv</code>. This folder will contain the <code>images</code> for RHCOS provision in the worker nodes. To be more concrete, we need the <code>RootFS</code> and <code>LiveISO</code> artifacts found on the Openshift CI Release page.</p> <p>After the download, a container will run to host the images under a webserver. It uses a variation of the official httpd image, which also allows it to work with IPv6.</p>"},{"location":"labs/IPv4/hostedcluster/","title":"Index","text":"<p>Hosted Cluster, as mentioned in the documentation here, is essentially an OCP API endpoint managed by Hypershift. In this context, we will also include the term HostedControlPlane to enhance readability and comprehension. This terminology is further explained in the same link.</p> <p>The Hosted Cluster consists of two main components: - The Control Plane, which operates within the management cluster as pods. - The Data Plane, which comprises external nodes managed by the end user.</p> <p>Now, with this foundational understanding, we can proceed with the deployment of our Hosted Cluster. While ACM/MCE users typically employ the web UI for cluster creation, we will take advantage of manifests, which provide greater flexibility for modifying the artifacts.</p>"},{"location":"labs/IPv4/hostedcluster/baremetalhost/","title":"Baremetalhost","text":""},{"location":"labs/IPv4/hostedcluster/baremetalhost/#bare-metal-hosts","title":"Bare Metal Hosts","text":"<p>A BareMetalHost is an openshift-machine-api object that encompasses both physical and logical details, allowing it to be identified by the Metal3 operator. Subsequently, these details are associated with other Assisted Service objects known as Agents. The structure of this object is as follows: <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: hosted-ipv4-worker0-bmc-secret\n  namespace: clusters-hosted-ipv4\ndata:\n  password: YWRtaW4=\n  username: YWRtaW4=\ntype: Opaque\n---\napiVersion: metal3.io/v1alpha1\nkind: BareMetalHost\nmetadata:\n  name: hosted-ipv4-worker0\n  namespace: clusters-hosted-ipv4\n  labels:\n    infraenvs.agent-install.openshift.io: hosted-ipv4\n  annotations:\n    inspect.metal3.io: disabled\n    bmac.agent-install.openshift.io/hostname: hosted-ipv4-worker0\nspec:\n  automatedCleaningMode: disabled\n  bmc:\n    disableCertificateVerification: true\n    address: redfish-virtualmedia://[192.168.125.1]:9000/redfish/v1/Systems/local/hosted-ipv4-worker0\n    credentialsName: hosted-ipv4-worker0-bmc-secret\n  bootMACAddress: aa:aa:aa:aa:02:11\n  online: true\n</code></pre></p> <p>Details:</p> <ul> <li>We will have at least 1 secret that holds the BMH credentials, so we will need to create at least 2 objects per worker node.</li> <li><code>spec.metadata.labels[\"infraenvs.agent-install.openshift.io\"]</code> serves as the link between the Assisted Installer and the BareMetalHost objects.</li> <li><code>spec.metadata.annotations[\"bmac.agent-install.openshift.io/hostname\"]</code> represents the node name it will adopt during deployment.</li> <li><code>spec.automatedCleaningMode</code> prevents the node from being erased by the Metal3 operator.</li> <li><code>spec.bmc.disableCertificateVerification</code> is set to <code>true</code> to bypass certificate validation from the client.</li> <li><code>spec.bmc.address</code> denotes the BMC address of the worker node.</li> <li><code>spec.bmc.credentialsName</code> points to the Secret where User/Password credentials are stored.</li> <li><code>spec.bootMACAddress</code> indicates the interface MACAddress from which the node will boot.</li> <li><code>spec.online</code> defines the desired state of the node once the BMH object is created.</li> </ul> <p>To deploy this object, simply follow the same procedure as before:</p> <p>Important</p> <p>Please create the virtual machines before you create the BareMetalHost and the destination Nodes.</p> <p>To deploy the BareMetalHost object, execute the following command:</p> <pre><code>oc apply -f 04-bmh.yaml\n</code></pre> <p>This will be the process:</p> <ul> <li> <p>Preparing (Trying to reach the nodes): <pre><code>NAMESPACE         NAME             STATE         CONSUMER   ONLINE   ERROR   AGE\nclusters-hosted   hosted-worker0   registering              true             2s\nclusters-hosted   hosted-worker1   registering              true             2s\nclusters-hosted   hosted-worker2   registering              true             2s\n</code></pre></p> </li> <li> <p>Provisioning (Nodes Booting up) <pre><code>NAMESPACE         NAME             STATE          CONSUMER   ONLINE   ERROR   AGE\nclusters-hosted   hosted-worker0   provisioning              true             16s\nclusters-hosted   hosted-worker1   provisioning              true             16s\nclusters-hosted   hosted-worker2   provisioning              true             16s\n</code></pre></p> </li> <li> <p>Provisioned (Nodes Booted up successfully) <pre><code>NAMESPACE         NAME             STATE         CONSUMER   ONLINE   ERROR   AGE\nclusters-hosted   hosted-worker0   provisioned              true             67s\nclusters-hosted   hosted-worker1   provisioned              true             67s\nclusters-hosted   hosted-worker2   provisioned              true             67s\n</code></pre></p> </li> </ul>"},{"location":"labs/IPv4/hostedcluster/baremetalhost/#agents-registration","title":"Agents registration","text":"<p>After the nodes have booted up, you will observe the appearance of agents within the namespace.</p> <pre><code>NAMESPACE         NAME                                   CLUSTER   APPROVED   ROLE          STAGE\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0411             true       auto-assign\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0412             true       auto-assign\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0413             true       auto-assign\n</code></pre> <p>These agents represent the nodes available for installation. To assign them to a HostedCluster, scale up the NodePool.</p>"},{"location":"labs/IPv4/hostedcluster/baremetalhost/#scaling-up-the-nodepool","title":"Scaling Up the Nodepool","text":"<p>Once we have the BareMetalHosts created, the statuses of these BareMetalHosts will transition from <code>Registering</code> (Attempting to reach the Node's BMC) to <code>Provisioning</code> (Node Booting Up), and finally to <code>Provisioned</code> (Successful node boot-up).</p> <p>The nodes will boot with the Agent's RHCOS LiveISO and a default pod named \"agent.\" This agent is responsible for receiving instructions from the Assisted Service Operator to install the Openshift payload.</p> <p>To accomplish this, execute the following command:</p> <pre><code>oc -n clusters scale nodepool hosted-ipv4 --replicas 3\n</code></pre> <p>After the NodePool scaling, you will notice that the agents are assigned to a Hosted Cluster.</p> <pre><code>NAMESPACE         NAME                                   CLUSTER   APPROVED   ROLE          STAGE\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0411   hosted    true       auto-assign\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0412   hosted    true       auto-assign\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0413   hosted    true       auto-assign\n</code></pre> <p>And the NodePool replicas set</p> <pre><code>NAMESPACE   NAME     CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION                              UPDATINGVERSION   UPDATINGCONFIG   MESSAGE\nclusters    hosted   hosted    3                               False         False        4.14.0-0.nightly-2023-08-29-102237                                      Minimum availability requires 3 replicas, current 0 available\n</code></pre> <p>So now, we need to wait until the nodes join the cluster. The Agents will provide updates on their current stage and status. Initially, they may not post any status, but eventually, they will.</p>"},{"location":"labs/IPv4/hostedcluster/hostedcluster/","title":"Hostedcluster","text":"<p>In this section, we will focus on all the related objects necessary to achieve a Disconnected Hosted Cluster deployment. Premises:</p> <ul> <li>HostedCluster Name: <code>hosted-ipv4</code></li> <li>HostedCluster Namespace: <code>clusters</code></li> <li>Disconnected: <code>true</code></li> <li>Network Stack: <code>IPv4</code></li> </ul>"},{"location":"labs/IPv4/hostedcluster/hostedcluster/#openshift-objects","title":"Openshift Objects","text":""},{"location":"labs/IPv4/hostedcluster/hostedcluster/#namespaces","title":"Namespaces","text":"<p>In a typical situation, the operator would be responsible for creating the HCP (HostedControlPlane) namespace. However, in this case, we want to include all the objects before the operator begins reconciliation over the HostedCluster object. This way, when the operator commences the reconciliation process, it will find all the objects in place.</p> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: null\n  name: clusters-hosted-ipv4\nspec: {}\nstatus: {}\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: null\n  name: clusters\nspec: {}\nstatus: {}\n</code></pre> <p>Note</p> <p>We will not create objects one by one but will concatenate all of them in the same file and apply them with just one command.</p>"},{"location":"labs/IPv4/hostedcluster/hostedcluster/#configmap-and-secrets","title":"ConfigMap and Secrets","text":"<p>These are the ConfigMaps and Secrets that we will include in the HostedCluster deployment.</p> <pre><code>---\napiVersion: v1\ndata:\n  ca-bundle.crt: |\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\nkind: ConfigMap\nmetadata:\n  name: user-ca-bundle\n  namespace: clusters\n---\napiVersion: v1\ndata:\n  .dockerconfigjson: xxxxxxxxx\nkind: Secret\nmetadata:\n  creationTimestamp: null\n  name: hosted-ipv4-pull-secret\n  namespace: clusters\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: sshkey-cluster-hosted-ipv4\n  namespace: clusters\nstringData:\n  id_rsa.pub: ssh-rsa xxxxxxxxx\n---\napiVersion: v1\ndata:\n  key: nTPtVBEt03owkrKhIdmSW8jrWRxU57KO/fnZa8oaG0Y=\nkind: Secret\nmetadata:\n  creationTimestamp: null\n  name: hosted-ipv4-etcd-encryption-key\n  namespace: clusters\ntype: Opaque\n</code></pre>"},{"location":"labs/IPv4/hostedcluster/hostedcluster/#rbac-roles","title":"RBAC Roles","text":"<p>While not mandatory, it allows us to have the Assisted Service Agents located in the same HostedControlPlane namespace as the HostedControlPlane and still be managed by CAPI.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  creationTimestamp: null\n  name: capi-provider-role\n  namespace: clusters-hosted-ipv4\nrules:\n- apiGroups:\n  - agent-install.openshift.io\n  resources:\n  - agents\n  verbs:\n  - '*'\n</code></pre>"},{"location":"labs/IPv4/hostedcluster/hostedcluster/#hosted-cluster","title":"Hosted Cluster","text":"<p>This is a sample of the HostedCluster Object</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: HostedCluster\nmetadata:\n  name: hosted-ipv4\n  namespace: clusters\nspec:\n  additionalTrustBundle:\n    name: \"user-ca-bundle\"\n  olmCatalogPlacement: guest\n  imageContentSources:\n  - source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n    mirrors:\n    - registry.hypershiftbm.lab:5000/openshift/release\n  - source: quay.io/openshift-release-dev/ocp-release\n    mirrors:\n    - registry.hypershiftbm.lab:5000/openshift/release-images\n  - mirrors:\n  ...\n  ...\n  autoscaling: {}\n  controllerAvailabilityPolicy: SingleReplica\n  dns:\n    baseDomain: hypershiftbm.lab\n  etcd:\n    managed:\n      storage:\n        persistentVolume:\n          size: 8Gi\n        restoreSnapshotURL: null\n        type: PersistentVolume\n    managementType: Managed\n  fips: false\n  networking:\n    clusterNetwork:\n    - cidr: 10.132.0.0/14\n    networkType: OVNKubernetes\n    serviceNetwork:\n    - cidr: 172.31.0.0/16\n  platform:\n    agent:\n      agentNamespace: clusters-hosted-ipv4\n    type: Agent\n  pullSecret:\n    name: hosted-ipv4-pull-secret\n  release:\n    image: registry.hypershiftbm.lab:5000/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237\n  secretEncryption:\n    aescbc:\n      activeKey:\n        name: hosted-ipv4-etcd-encryption-key\n    type: aescbc\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: api.hosted-ipv4.hypershiftbm.lab\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: api.hosted-ipv4.hypershiftbm.lab\n      type: NodePort\n  - service: OIDC\n    servicePublishingStrategy:\n      nodePort:\n        address: api.hosted-ipv4.hypershiftbm.lab\n      type: NodePort\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: api.hosted-ipv4.hypershiftbm.lab\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: api.hosted-ipv4.hypershiftbm.lab\n      type: NodePort\n  sshKey:\n    name: sshkey-cluster-hosted-ipv4\nstatus:\n  controlPlaneEndpoint:\n    host: \"\"\n    port: 0\n</code></pre> <p>Note</p> <p>The <code>imageContentSources</code> section within the <code>spec</code> field contains mirror references for user workloads within the HostedCluster.</p> <p>As you can see, all the objects created before are referenced here. You can also refer to the documentation where all the fields are described.</p>"},{"location":"labs/IPv4/hostedcluster/hostedcluster/#deployment","title":"Deployment","text":"<p>To deploy these objects, simply concatenate them into the same file and apply them against the management cluster:</p> <pre><code>oc apply -f 01-4.14-hosted_cluster-nodeport.yaml\n</code></pre> <p>This will raise up a functional Hosted Control Plane.</p> <pre><code>NAME                                                  READY   STATUS    RESTARTS   AGE\ncapi-provider-5b57dbd6d5-pxlqc                        1/1     Running   0          3m57s\ncatalog-operator-9694884dd-m7zzv                      2/2     Running   0          93s\ncluster-api-f98b9467c-9hfrq                           1/1     Running   0          3m57s\ncluster-autoscaler-d7f95dd5-d8m5d                     1/1     Running   0          93s\ncluster-image-registry-operator-5ff5944b4b-648ht      1/2     Running   0          93s\ncluster-network-operator-77b896ddc-wpkq8              1/1     Running   0          94s\ncluster-node-tuning-operator-84956cd484-4hfgf         1/1     Running   0          94s\ncluster-policy-controller-5fd8595d97-rhbwf            1/1     Running   0          95s\ncluster-storage-operator-54dcf584b5-xrnts             1/1     Running   0          93s\ncluster-version-operator-9c554b999-l22s7              1/1     Running   0          95s\ncontrol-plane-operator-6fdc9c569-t7hr4                1/1     Running   0          3m57s\ncsi-snapshot-controller-785c6dc77c-8ljmr              1/1     Running   0          77s\ncsi-snapshot-controller-operator-7c6674bc5b-d9dtp     1/1     Running   0          93s\ndns-operator-6874b577f-9tc6b                          1/1     Running   0          94s\netcd-0                                                3/3     Running   0          3m39s\nhosted-cluster-config-operator-f5cf5c464-4nmbh        1/1     Running   0          93s\nignition-server-6b689748fc-zdqzk                      1/1     Running   0          95s\nignition-server-proxy-54d4bb9b9b-6zkg7                1/1     Running   0          95s\ningress-operator-6548dc758b-f9gtg                     1/2     Running   0          94s\nkonnectivity-agent-7767cdc6f5-tw782                   1/1     Running   0          95s\nkube-apiserver-7b5799b6c8-9f5bp                       4/4     Running   0          3m7s\nkube-controller-manager-5465bc4dd6-zpdlk              1/1     Running   0          44s\nkube-scheduler-5dd5f78b94-bbbck                       1/1     Running   0          2m36s\nmachine-approver-846c69f56-jxvfr                      1/1     Running   0          92s\noauth-openshift-79c7bf44bf-j975g                      2/2     Running   0          62s\nolm-operator-767f9584c-4lcl2                          2/2     Running   0          93s\nopenshift-apiserver-5d469778c6-pl8tj                  3/3     Running   0          2m36s\nopenshift-controller-manager-6475fdff58-hl4f7         1/1     Running   0          95s\nopenshift-oauth-apiserver-dbbc5cc5f-98574             2/2     Running   0          95s\nopenshift-route-controller-manager-5f6997b48f-s9vdc   1/1     Running   0          95s\npackageserver-67c87d4d4f-kl7qh                        2/2     Running   0          93s\n</code></pre> <p>And this is how the HostedCluster looks like:</p> <pre><code>NAMESPACE   NAME         VERSION   KUBECONFIG                PROGRESS   AVAILABLE   PROGRESSING   MESSAGE\nclusters    hosted-ipv4            hosted-admin-kubeconfig   Partial    True          False         The hosted control plane is available\n</code></pre> <p>After some time, we will have almost all the pieces in place, and the Control Plane operator awaits for the worker nodes to join the cluster. To achieve this, we need to create some more objects. Let's discuss the <code>InfraEnv</code> and the <code>BareMetalHost</code> in the following sections.</p>"},{"location":"labs/IPv4/hostedcluster/infraenv/","title":"Infraenv","text":"<p>The <code>InfraEnv</code> is an Assisted Service object that includes essential details such as the <code>pullSecretRef</code> and the <code>sshAuthorizedKey</code>. These details are used to create the RHCOS Boot Image customized specifically for the cluster. Below is the structure of this object:</p> <pre><code>---\napiVersion: agent-install.openshift.io/v1beta1\nkind: InfraEnv\nmetadata:\n  name: hosted-ipv4\n  namespace: clusters-hosted-ipv4\nspec:\n  pullSecretRef:\n    name: pull-secret\n  sshAuthorizedKey: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDk7ICaUE+/k4zTpxLk4+xFdHi4ZuDi5qjeF52afsNkw0w/glILHhwpL5gnp5WkRuL8GwJuZ1VqLC9EKrdmegn4MrmUlq7WTsP0VFOZFBfq2XRUxo1wrRdor2z0Bbh93ytR+ZsDbbLlGngXaMa0Vbt+z74FqlcajbHTZ6zBmTpBVq5RHtDPgKITdpE1fongp7+ZXQNBlkaavaqv8bnyrP4BWahLP4iO9/xJF9lQYboYwEEDzmnKLMW1VtCE6nJzEgWCufACTbxpNS7GvKtoHT/OVzw8ArEXhZXQUS1UY8zKsX2iXwmyhw5Sj6YboA8WICs4z+TrFP89LmxXY0j6536TQFyRz1iB4WWvCbH5n6W+ABV2e8ssJB1AmEy8QYNwpJQJNpSxzoKBjI73XxvPYYC/IjPFMySwZqrSZCkJYqQ023ySkaQxWZT7in4KeMu7eS2tC+Kn4deJ7KwwUycx8n6RHMeD8Qg9flTHCv3gmab8JKZJqN3hW1D378JuvmIX4V0=\n</code></pre> <p>Details:</p> <ul> <li><code>pullSecretRef</code> refers to the ConfigMap reference (in the same Namespace as the InfraEnv) where the PullSecret will be utilized.</li> <li><code>sshAuthorizedKey</code> represents the SSH Public key that will be injected into the Boot Image. This SSH key will, by default, allow access to the worker nodes as the <code>core</code> user.</li> </ul> <p>To deploy this object, follow the same procedure as before:</p> <pre><code>oc apply -f 03-infraenv.yaml\n</code></pre> <p>And this is how looks like</p> <pre><code>NAMESPACE              NAME     ISO CREATED AT\nclusters-hosted-ipv4   hosted   2023-09-11T15:14:10Z\n</code></pre>"},{"location":"labs/IPv4/hostedcluster/nodepool/","title":"Nodepool","text":"<p>A <code>NodePool</code> is a scalable set of worker nodes associated with a HostedCluster. NodePool machine architectures remain consistent within a specific pool and are independent of the underlying machine architecture of the control plane.</p> <p>Note</p> <p>Please ensure you modify the appropriate fields to align with your laboratory environment.</p> <p>Warning</p> <p>Before a day-1 patch, the release image set in the HostedCluster should use the digest rather than the tag. (e.g <code>quay.io/openshift-release-dev/ocp-release@sha256:e3ba11bd1e5e8ea5a0b36a75791c90f29afb0fdbe4125be4e48f69c76a5c47a0</code>)</p> <p>This is how one looks like:</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: NodePool\nmetadata:\n  creationTimestamp: null\n  name: hosted-ipv4\n  namespace: clusters\nspec:\n  arch: amd64\n  clusterName: hosted-ipv4\n  management:\n    autoRepair: false\n    upgradeType: InPlace\n  nodeDrainTimeout: 0s\n  nodeVolumeDetachTimeout: 0s\n  platform:\n    type: Agent\n  release:\n    image: registry.hypershiftbm.lab:5000/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237\n  replicas: 0\nstatus:\n  replicas: 0\n</code></pre> <p>Details:</p> <ul> <li>All the nodes included in this NodePool will be based on the Openshift version <code>4.14.0-0.nightly-2023-08-29-102237</code>.</li> <li>The Upgrade type is set to <code>InPlace</code>, indicating that the same bare-metal node will be reused during an upgrade.</li> <li>Autorepair is set to <code>false</code> because the node will not be recreated when it disappears.</li> <li>Replicas are set to <code>0</code> because we intend to scale them when needed.</li> </ul> <p>You can find more information about NodePool in the NodePool documentation.</p> <p>To deploy this object, simply follow the same procedure as before:</p> <pre><code>oc apply -f 02-nodepool.yaml\n</code></pre> <p>And this is how the NodePool looks like (at this point):</p> <pre><code>NAMESPACE   NAME          CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION                              UPDATINGVERSION   UPDATINGCONFIG   MESSAGE\nclusters    hosted-ipv4   hosted    0                               False         False        4.14.0-0.nightly-2023-08-29-102237\n</code></pre> <p>Important</p> <p>Keep the nodepool replicas to 0 until all the steps are in place.</p>"},{"location":"labs/IPv4/hostedcluster/worker-nodes/","title":"Worker nodes","text":"<p>Regarding the worker nodes, if you are working on real bare metal, this step is crucial to ensure that the details set in the <code>BareMetalHost</code> are correctly configured. If not, you will need to debug why it's not functioning as expected.</p> <p>However, if you are working with virtual machines, you can follow these steps to create empty ones that will be consumed by the Metal3 operator. To achieve this, we will utilize Kcli.</p>"},{"location":"labs/IPv4/hostedcluster/worker-nodes/#creating-virtual-machines","title":"Creating Virtual Machines","text":"<p>If this is not your first attempt, you must first delete the previous setup. To do so, please refer to the Deleting Virtual Machines section.</p> <p>Now, you can execute the following commands for VM creation:</p> <pre><code>kcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-ipv4 -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=[\"{\\\"name\\\": \\\"ipv4\\\", \\\"mac\\\": \\\"aa:aa:aa:aa:02:11\\\"}\"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0211 -P name=hosted-ipv4-worker0\nkcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-ipv4 -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=[\"{\\\"name\\\": \\\"ipv4\\\", \\\"mac\\\": \\\"aa:aa:aa:aa:02:12\\\"}\"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0212 -P name=hosted-ipv4-worker1\nkcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-ipv4 -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=[\"{\\\"name\\\": \\\"ipv4\\\", \\\"mac\\\": \\\"aa:aa:aa:aa:02:13\\\"}\"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0213 -P name=hosted-ipv4-worker2\n\nsleep 2\nsystemctl restart ksushy\n</code></pre> <p>Let's dissect the creation command:</p> <ul> <li><code>start=False</code>: The VM will not boot automatically upon creation.</li> <li><code>uefi_legacy=true</code>: We will use UEFI legacy boot to ensure compatibility with older UEFI implementations.</li> <li><code>plan=hosted-dual</code>: The plan name, which identifies a group of machines as a cluster.</li> <li><code>memory=8192</code> and <code>numcpus=16</code>: These parameters specify the resources for the VM, including RAM and CPU.</li> <li><code>disks=[200,200]</code>: We are creating 2 disks (thin provisioned) in the virtual machine.</li> <li><code>nets=[{\"name\": \"dual\", \"mac\": \"aa:aa:aa:aa:02:13\"}]</code>: Network details, including the network name it will be connected to and the MAC address for the primary interface.</li> <li>The <code>ksushy</code> restart is performed to make our <code>ksushy</code> (VM's BMC) aware of the new VMs added.</li> </ul> <p>This is what the command looks like:</p> <pre><code>+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+\n|         Name        | Status |         Ip        |                       Source                       |     Plan    | Profile |\n+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+\n|    hosted-worker0   |  down  |                   |                                                    | hosted-ipv4 |  kvirt  |\n|    hosted-worker1   |  down  |                   |                                                    | hosted-ipv4 |  kvirt  |\n|    hosted-worker2   |  down  |                   |                                                    | hosted-ipv4 |  kvirt  |\n+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+\n</code></pre>"},{"location":"labs/IPv4/hostedcluster/worker-nodes/#deleting-virtual-machines","title":"Deleting Virtual Machines","text":"<p>To delete the VMs, you simply need to delete the plan, which, in our case, is:</p> <pre><code>kcli delete plan hosted-ipv4\n</code></pre> <pre><code>$ kcli delete plan hosted-ipv4\nAre you sure? [y/N]: y\nhosted-worker0 deleted on local!\nhosted-worker1 deleted on local!\nhosted-worker2 deleted on local!\nPlan hosted-ipv4 deleted!\n</code></pre>"},{"location":"labs/IPv4/hypervisor/","title":"Hypervisor","text":"<p>This section is entirely dedicated to virtual machine environments. If you are working with physical bare-metal infrastructure, you can skip this section and ensure that your lab has all the required components in place for a proper deployment. If you find any gaps in your setup, you can configure them separately to address those specific needs.</p>"},{"location":"labs/IPv4/hypervisor/hypervisor-system-config/","title":"Hypervisor system config","text":"<p>This process is primarily intended for development environments. In production environments, it's essential to establish proper rules for the <code>firewalld</code> service and configure SELinux policies to maintain a secure environment.</p> <p>With these instructions, we allow various types of connections through the different virtual networks within the environment:</p> <pre><code>## SELinux\nsed -i s/^SELINUX=.*$/SELINUX=permissive/ /etc/selinux/config; setenforce 0\n\n## Firewalld\nsystemctl disable --now firewalld\n\n## Libvirtd\nsystemctl restart libvirtd\nsystemctl enable --now libvirtd\n</code></pre>"},{"location":"labs/IPv4/hypervisor/network-manager-dispatcher/","title":"Network manager dispatcher","text":"<p>This script modifies the system DNS resolver to prioritize pointing to the <code>dnsmasq</code> service (configured later). This ensures that virtual machines can resolve the various domains, routes, and registries required for the different steps of the process.</p> <p>To enable this, you need to create a script named <code>forcedns</code> in <code>/etc/NetworkManager/dispatcher.d/</code> with the following content:</p> <p>Note</p> <p>Please ensure you modify the appropriate fields to align with your laboratory environment.</p> IPv4IPv6Dual stack <pre><code>#!/bin/bash\n\nexport IP=\"192.168.125.1\"\nexport BASE_RESOLV_CONF=\"/run/NetworkManager/resolv.conf\"\n\nif ! [[ `grep -q \"$IP\" /etc/resolv.conf` ]]; then\nexport TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)\ncp $BASE_RESOLV_CONF $TMP_FILE\nchmod --reference=$BASE_RESOLV_CONF $TMP_FILE\nsed -i -e \"s/hypershiftbm.lab//\" -e \"s/search /&amp; hypershiftbm.lab /\" -e \"0,/nameserver/s/nameserver/&amp; $IP\\n&amp;/\" $TMP_FILE\nmv $TMP_FILE /etc/resolv.conf\nfi\necho \"ok\"\n</code></pre> <pre><code>#!/bin/bash\n\nexport IP=\"2620:52:0:1306::1\"\nexport BASE_RESOLV_CONF=\"/run/NetworkManager/resolv.conf\"\n\nif ! [[ `grep -q \"$IP\" /etc/resolv.conf` ]]; then\nexport TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)\ncp $BASE_RESOLV_CONF $TMP_FILE\nchmod --reference=$BASE_RESOLV_CONF $TMP_FILE\nsed -i -e \"s/hypershiftbm.lab//\" -e \"s/search /&amp; hypershiftbm.lab /\" -e \"0,/nameserver/s/nameserver/&amp; $IP\\n&amp;/\" $TMP_FILE\nmv $TMP_FILE /etc/resolv.conf\nfi\necho \"ok\"\n</code></pre> <pre><code>#!/bin/bash\n\nexport IP=\"192.168.126.1\"\nexport BASE_RESOLV_CONF=\"/run/NetworkManager/resolv.conf\"\n\nif ! [[ `grep -q \"$IP\" /etc/resolv.conf` ]]; then\nexport TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)\ncp $BASE_RESOLV_CONF $TMP_FILE\nchmod --reference=$BASE_RESOLV_CONF $TMP_FILE\nsed -i -e \"s/hypershiftbm.lab//\" -e \"s/search /&amp; hypershiftbm.lab /\" -e \"0,/nameserver/s/nameserver/&amp; $IP\\n&amp;/\" $TMP_FILE\nmv $TMP_FILE /etc/resolv.conf\nfi\necho \"ok\"\n</code></pre> <p>The <code>IP</code> variable at the beginning of the script must be modified to point to the IP address of the Hypervisor's interface hosting the Openshift management cluster.</p> <p>After creating the file, you need to add execution permissions using the command:</p> <pre><code>chmod 755 /etc/NetworkManager/dispatcher.d/forcedns\n</code></pre> <p>Then, execute it once. The output should indicate <code>ok</code>.</p>"},{"location":"labs/IPv4/hypervisor/packaging/","title":"Hypervisor Packaging","text":""},{"location":"labs/IPv4/hypervisor/packaging/#system","title":"System","text":"<p>These are the main packages that are needed to deploy a virtualized Openshift Management cluster.</p> <pre><code>sudo dnf install dnsmasq radvd vim golang podman bind-utils net-tools httpd-tools tree htop strace tmux -y\n</code></pre> <p>Additionally, you need to enable and start the Podman service using the following command:</p> <pre><code>systemctl enable --now podman\n</code></pre>"},{"location":"labs/IPv4/hypervisor/packaging/#kcli","title":"Kcli","text":"<p>We will utilize Kcli to deploy the Openshift Management cluster and various other virtualized components. To do so, you'll need to install and configure the hypervisor using the following commands:</p> <pre><code>sudo yum -y install libvirt libvirt-daemon-driver-qemu qemu-kvm\nsudo usermod -aG qemu,libvirt $(id -un)\nsudo newgrp libvirt\nsudo systemctl enable --now libvirtd\nsudo dnf -y copr enable karmab/kcli\nsudo dnf -y install kcli\nsudo kcli create pool -p /var/lib/libvirt/images default\nkcli create host kvm -H 127.0.0.1 local\nsudo setfacl -m u:$(id -un):rwx /var/lib/libvirt/images\nkcli create network  -c 192.168.122.0/24 default\n</code></pre> <p>For more info about Kcli please visit the official documentation.</p>"},{"location":"labs/IPv4/hypervisor/redfish-for-vms/","title":"Redfish for vms","text":"<p>In a bare metal environment, the preferred approach is to utilize the actual BMC (Baseboard Management Controller) of the nodes used for the management cluster, which can be managed by Metal3 for discovery and provisioning. However, in a virtual environment, this approach is not feasible. As a workaround, we will use <code>ksushy</code>, which is an implementation of <code>sushy-tools</code>, allowing us to simulate BMCs for the virtual machines.</p> <p>To configure <code>ksushy</code> we need to execute these commands:</p> <pre><code>sudo dnf install python3-pyOpenSSL.noarch python3-cherrypy -y\nkcli create sushy-service --ssl --port 9000\nsudo systemctl daemon-reload\nsystemctl enable --now ksushy\n</code></pre> <p>To test if this service is functioning correctly, you can check the service status with <code>systemctl status ksushy</code>. Additionally, you can execute a <code>curl</code> command against the exposed interface: <pre><code>curl -Lk https://192.168.125.1:9000/redfish/v1\n</code></pre></p>"},{"location":"labs/IPv4/hypervisor/requisites/","title":"Hypervisor Prerequisites","text":"<ul> <li>CPU: The number of CPUs provided determines how many HostedClusters can run concurrently.</li> <li>Recommended: 16 CPUs per Node for 3 nodes.</li> <li> <p>Minimal Dev: In a development environment, you may manage with 12 CPUs per Node for 3 nodes.</p> </li> <li> <p>Memory: The amount of RAM impacts how many HostedClusters can be hosted.</p> </li> <li>Recommended: 48 GB of RAM per Node.</li> <li> <p>Minimal Dev: For minimal development, 18 GB of RAM per Node may suffice.</p> </li> <li> <p>Storage: Using SSD storage for MCE is crucial.</p> </li> <li>Management Cluster: 250 GB.</li> <li>Registry: Depends on the number of releases, operators, and images hosted. An acceptable number could be 500 GB, preferably separated from the disk where the HostedCluster is hosted.</li> <li> <p>Webserver: The required storage depends on the number of ISOs and images hosted. An acceptable number could be 500 GB.</p> </li> <li> <p>Production: For a production environment, it's advisable to keep these three components separated on different disks. A recommended configuration for production is as follows:</p> </li> <li>Registry: 2 TB.</li> <li>Management Cluster: 500 GB.</li> <li>WebServer: 2 TB.</li> </ul>"},{"location":"labs/IPv4/mce/","title":"Index","text":"<p>The Multicluster Engine (MCE) is a component of the ACM bundle. It plays a crucial role in deploying clusters across multiple providers.</p>"},{"location":"labs/IPv4/mce/#credentials-and-authorization","title":"Credentials and Authorization","text":"<p>To prepare for ACM/MCE deployment, certain prerequisites must be met. Additionally, you will need to request permissions to access internal builds. For detailed information, please refer to this documentation.</p> <p>ACM/MCE Deployment Agent Service Config</p>"},{"location":"labs/IPv4/mce/agentserviceconfig/","title":"Agentserviceconfig","text":"<p>The Agent Service Config object is an essential component of the Assisted Service addon included in MCE/ACM, responsible for Baremetal cluster deployment. When the addon is enabled, you must deploy an operand (CRD) named <code>AgentServiceConfig</code> to configure it.</p>"},{"location":"labs/IPv4/mce/agentserviceconfig/#agent-service-config-objects","title":"Agent Service Config Objects","text":"<p>You can find the CRD described here. In this context, we will focus on the main aspects to ensure its functionality in disconnected environments.</p> <p>In addition to configuring the Agent Service Config, to ensure that Multicluster Engine functions properly in a disconnected environment, we need to include some additional ConfigMaps.</p>"},{"location":"labs/IPv4/mce/agentserviceconfig/#custom-registries-configuration","title":"Custom Registries Configuration","text":"<p>This ConfigMap contains the disconnected details necessary to customize the deployment.</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: custom-registries\n  namespace: multicluster-engine\n  labels:\n    app: assisted-service\ndata:\n  ca-bundle.crt: |\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\n  registries.conf: |\n    unqualified-search-registries = [\"registry.access.redhat.com\", \"docker.io\"]\n\n    [[registry]]\n    prefix = \"\"\n    location = \"registry.redhat.io/openshift4\"\n    mirror-by-digest-only = true\n\n    [[registry.mirror]]\n      location = \"registry.hypershiftbm.lab:5000/openshift4\"\n\n    [[registry]]\n    prefix = \"\"\n    location = \"registry.redhat.io/rhacm2\"\n    mirror-by-digest-only = true\n    ...\n    ...\n</code></pre> <p>This object includes two fields:</p> <ol> <li>Custom CAs: This field contains the Certificate Authorities (CAs) that will be loaded into the various processes of the deployment.</li> <li>Registries: The <code>Registries.conf</code> field contains information about images and namespaces that need to be consumed from a mirror registry instead of the original source registry.</li> </ol>"},{"location":"labs/IPv4/mce/agentserviceconfig/#assisted-service-customization","title":"Assisted Service Customization","text":"<p>The Assisted Service Customization ConfigMap is consumed by the Assisted Service operator and contains variables that modify the behavior of the controllers.</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: assisted-service-config\n  namespace: multicluster-engine\ndata:\n  ALLOW_CONVERGED_FLOW: \"false\"\n</code></pre> <p>You can find documentation on how to customize the operator here.</p>"},{"location":"labs/IPv4/mce/agentserviceconfig/#assisted-service-config","title":"Assisted Service Config","text":"<p>The Assisted Service Config object includes the necessary information to ensure the correct functioning of the operator.</p> <pre><code>---\napiVersion: agent-install.openshift.io/v1beta1\nkind: AgentServiceConfig\nmetadata:\n  annotations:\n    unsupported.agent-install.openshift.io/assisted-service-configmap: assisted-service-config\n  name: agent\n  namespace: multicluster-engine\nspec:\n  mirrorRegistryRef:\n    name: custom-registries\n  databaseStorage:\n    storageClassName: lvms-vg1\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 10Gi\n  filesystemStorage:\n    storageClassName: lvms-vg1\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 20Gi\n  osImages:\n  - cpuArchitecture: x86_64\n    openshiftVersion: \"4.14\"\n    rootFSUrl: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202308281054-0-live-rootfs.x86_64.img\n    url: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202308281054-0-live.x86_64.iso\n    version: 414.92.202308281054-0\n</code></pre> <p>If your mirror registry doesn't require authentication in your pull secret, see this document on how to add it to the unauthenticated list in the AgentServiceConfig CR.</p> <p>In this section, we will emphasize the important aspects:</p> <ul> <li>The <code>metadata.annotations[\"unsupported.agent-install.openshift.io/assisted-service-configmap\"]</code> Annotation references the ConfigMap name to be consumed by the operator for customizing behavior.</li> <li>The <code>spec.mirrorRegistryRef.name</code> points to the ConfigMap containing disconnected registry information to be consumed by the Assisted Service Operator. This ConfigMap injects these resources during the deployment process.</li> <li>The <code>spec.osImages</code> field contains different versions available for deployment by this operator. These fields are mandatory.</li> </ul> <p>Let's fill in this section for the 4.14 dev preview ec4 version (sample):</p> <ul> <li>Release URL</li> <li>RHCOS Info</li> </ul> <p>Assuming you've already downloaded the RootFS and LiveIso files:</p> <pre><code>  - cpuArchitecture: x86_64\n    openshiftVersion: \"4.14\"\n    rootFSUrl: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202309101331-0-live-rootfs.x86_64.img\n    url: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202309101331-0-live.x86_64.iso\n    version: 414.92.202307250657-0\n</code></pre>"},{"location":"labs/IPv4/mce/agentserviceconfig/#deployment","title":"Deployment","text":"<p>To deploy all these objects, simply concatenate them into a single file and apply them to the Management Cluster.</p> <pre><code>oc apply -f agentServiceConfig.yaml\n</code></pre> <p>This will trigger 2 pods</p> <pre><code>assisted-image-service-0                               1/1     Running   2             11d\nassisted-service-668b49548-9m7xw                       2/2     Running   5             11d\n</code></pre> <p>Note</p> <p>The <code>Assisted Image Service</code> pod is responsible for creating the RHCOS Boot Image template, which will be customized for each cluster you deploy.</p> <p>The <code>Assisted Service</code> refers to the operator.</p>"},{"location":"labs/IPv4/mce/multicluster-engine/","title":"Multicluster engine","text":"<p>The deployment of each component will depend on your needs, follow the next links accordingly:</p> <ul> <li>ACM Deployment</li> <li>ACM Downstream builds</li> <li>MCE Deployment</li> <li>MCE Upgrade</li> <li>ACM/MCE Cluster deletion</li> <li>MulticlusterEngine deletion</li> <li>Last resource: nuke.sh</li> </ul>"},{"location":"labs/IPv4/mgmt-cluster/","title":"Index","text":""},{"location":"labs/IPv4/mgmt-cluster/#openshift-management-cluster","title":"Openshift Management Cluster","text":"<p>This section contains the necessary artifacts to set up an Openshift management cluster based on virtual machines using kcli as the primary tool. Another option is to use dev-scripts, which uses a different approach.</p> <p>Networking Openshift Compact Dual</p>"},{"location":"labs/IPv4/mgmt-cluster/compact-ipv4/","title":"Compact ipv4","text":"<p>In this section, we will discuss how to deploy the Openshift management cluster. To do that, we need to have the following files in place:</p> <ol> <li>Pull Secret</li> <li>Kcli Plan</li> </ol> <p>Ensure that these files are properly set up for the deployment process.</p>"},{"location":"labs/IPv4/mgmt-cluster/compact-ipv4/#pull-secret","title":"Pull Secret","text":"<p>The Pull Secret should be located in the same folder as the kcli plan and should be named <code>openshift_pull.json</code>.</p>"},{"location":"labs/IPv4/mgmt-cluster/compact-ipv4/#kcli-plan","title":"Kcli plan","text":"<p>The Kcli plan contains the Openshift definition. This is how looks like:</p> <ul> <li><code>mgmt-compact-hub-ipv4.yaml</code></li> </ul> <pre><code>plan: hub-ipv4\nforce: true\nversion: nightly\ntag: \"4.14.0-0.nightly-2023-08-29-102237\"\ncluster: \"hub-ipv4\"\ndomain: hypershiftbm.lab\napi_ip: 192.168.125.10\ningress_ip: 192.168.125.11\ndisconnected_url: registry.hypershiftbm.lab:5000\ndisconnected_update: true\ndisconnected_user: dummy\ndisconnected_password: dummy\ndisconnected_operators_version: v4.13\ndisconnected_operators:\n- name: metallb-operator\n- name: lvms-operator\n  channels:\n  - name: stable-4.13\ndisconnected_extra_images:\n- quay.io/mavazque/trbsht:latest\n- quay.io/jparrill/hypershift:BMSelfManage-v4.14-rc-v3\n- registry.redhat.io/openshift4/ose-kube-rbac-proxy:v4.10\ndualstack: false\ndisk_size: 200\nextra_disks: [200]\nmemory: 48000\nnumcpus: 16\nctlplanes: 3\nworkers: 0\nmanifests: extra-manifests\nmetal3: true\nnetwork: ipv4\nusers_dev: developer\nusers_devpassword: developer\nusers_admin: admin\nusers_adminpassword: admin\nmetallb_pool: ipv4-virtual-network\nmetallb_ranges:\n- 192.168.125.150-192.168.125.190\nmetallb_autoassign: true\napps:\n- users\n- lvms-operator\n- metallb-operator\nvmrules:\n- hub-bootstrap:\n    nets:\n    - name: ipv4\n      mac: aa:aa:aa:aa:02:10\n- hub-ctlplane-0:\n    nets:\n    - name: ipv4\n      mac: aa:aa:aa:aa:02:01\n- hub-ctlplane-1:\n    nets:\n    - name: ipv4\n      mac: aa:aa:aa:aa:02:02\n- hub-ctlplane-2:\n    nets:\n    - name: ipv4\n      mac: aa:aa:aa:aa:02:03\n</code></pre> <p>Note</p> <p>To understand the meaning of each of these parameters, you can refer to the official documentation here.</p>"},{"location":"labs/IPv4/mgmt-cluster/compact-ipv4/#deployment","title":"Deployment","text":"<p>To initiate the provisioning procedure, execute the following:</p> <pre><code>kcli create cluster openshift --pf mgmt-compact-hub-ipv4.yaml\n</code></pre>"},{"location":"labs/IPv4/mgmt-cluster/network/","title":"Network","text":"<p>Firstly, we need to ensure that we have the right networks prepared for use in the Hypervisor. These networks will be used to host both the Management and Hosted clusters.</p> <p>To configure these networks, we will use the following <code>kcli</code> command:</p> <pre><code>kcli create network -c 192.168.125.0/24 -P dhcp=false -P dns=false --domain hypershiftbm.lab ipv4\n</code></pre> <p>Where:</p> <ul> <li><code>-c</code> specifies the CIDR used for that network.</li> <li><code>-P dhcp=false</code> configures the network to disable DHCP, which will be handled by the previously configured dnsmasq.</li> <li><code>-P dns=false</code> configures the network to disable DNS, which will also be handled by the dnsmasq.</li> <li><code>--domain</code> sets the domain to search into.</li> <li><code>ipv4</code> is the name of the network that will be created.</li> </ul> <p>This is what the network will look like once created:</p> <pre><code>[root@hypershiftbm ~]# kcli list network\nListing Networks...\n+---------+--------+---------------------+-------+------------------+------+\n| Network |  Type  |         Cidr        |  Dhcp |      Domain      | Mode |\n+---------+--------+---------------------+-------+------------------+------+\n| default | routed |   192.168.122.0/24  |  True |     default      | nat  |\n| ipv4    | routed |   192.168.125.0/24  | False | hypershiftbm.lab | nat  |\n| ipv6    | routed | 2620:52:0:1306::/64 | False | hypershiftbm.lab | nat  |\n+---------+--------+---------------------+-------+------------------+------+\n</code></pre> <pre><code>[root@hypershiftbm ~]# kcli info network ipv4\nProviding information about network ipv4...\ncidr: 192.168.125.0/24\ndhcp: false\ndomain: hypershiftbm.lab\nmode: nat\nplan: kvirt\ntype: routed\n</code></pre>"},{"location":"labs/IPv4/mirror/","title":"Index","text":"<p>Important</p> <p>This section is only relevant in disconnected scenarios. If this does not apply to your situation, you can continue with the next section.</p> <p>Image mirroring is the process of fetching images from external registries, such as registry.redhat.com or quay.io, and storing them in your private registry.</p> <p>To learn how to perform image mirroring, follow the next sections:</p> <p>Image Mirroring Process ICSP and IDMS</p>"},{"location":"labs/IPv4/mirror/ICSP-IDMS/","title":"ICSP IDMS","text":"<p>Once the mirroring process is complete, you will have two main objects that need to be applied in the Management Cluster:</p> <ol> <li>ICSP (Image Content Source Policies) or IDMS (Image Digest Mirror Set).</li> <li>Catalog Sources.</li> </ol> <p>Using the <code>oc-mirror</code> tool, the output artifacts will be located in a new folder called <code>oc-mirror-workspace/results-XXXXXX/</code>.</p> <p>ICSP/IDMS will trigger a \"special\" MachineConfig change that will not reboot your nodes but will reboot the kubelet on each of them.</p> <p>Once all nodes are schedulable and marked as <code>READY</code>, you will need to apply the new catalog sources generated.</p> <p>The catalog sources will trigger some actions in the <code>openshift-marketplace operator</code>, such as downloading the catalog image and processing it to retrieve all the <code>PackageManifests</code> included in that image. You can check the new sources by executing <code>oc get packagemanifest</code> using the new CatalogSource as a source.</p>"},{"location":"labs/IPv4/mirror/ICSP-IDMS/#applying-the-artifacts","title":"Applying the Artifacts","text":"<p>First, we need to create the ICSP/IDMS artifacts:</p> <pre><code>oc apply -f oc-mirror-workspace/results-XXXXXX/imageContentSourcePolicy.yaml\n</code></pre> <p>Now, wait for the nodes to become ready again and execute the following command:</p> <pre><code>oc apply -f catalogSource-XXXXXXXX-index.yaml\n</code></pre>"},{"location":"labs/IPv4/mirror/mirroring/","title":"Mirroring","text":"<p>The mirroring step can take some time to complete, so we recommend starting with this part once the Registry server is up and running.</p> <p>For this purpose, we will use the <code>oc-mirror</code> tool, a binary that utilizes an object called <code>ImageSetConfiguration</code>.</p> <p>In this file, you can specify:</p> <ul> <li>The OpenShift versions to mirror (they should be located in quay.io).</li> <li>The additional operators to mirror, selecting packages individually.</li> <li>The extra images you want to add to the repository.</li> </ul> <p>Note</p> <p>Please ensure you modify the appropriate fields to align with your laboratory environment.</p> <p>Here is an example of the <code>ImageSetConfiguration</code> that we will use for our mirroring:</p> <pre><code>apiVersion: mirror.openshift.io/v1alpha2\nkind: ImageSetConfiguration\nstorageConfig:\n  registry:\n    imageURL: registry.hypershiftbm.lab:5000/openshift/release/metadata:latest\nmirror:\n  platform:\n    channels:\n    - name: candidate-4.14\n      minVersion: 4.14.0-ec.1\n      maxVersion: 4.14.0-ec.3\n      type: ocp\n    graph: true\n  additionalImages:\n  - name: quay.io/karmab/origin-keepalived-ipfailover:latest\n  - name: quay.io/karmab/kubectl:latest\n  - name: quay.io/karmab/haproxy:latest\n  - name: quay.io/karmab/mdns-publisher:latest\n  - name: quay.io/karmab/origin-coredns:latest\n  - name: quay.io/karmab/curl:latest\n  - name: quay.io/karmab/kcli:latest\n  - name: quay.io/mavazque/trbsht:latest\n  - name: quay.io/jparrill/hypershift:BMSelfManage-v4.14-rc-v3\n  - name: registry.redhat.io/openshift4/ose-kube-rbac-proxy:v4.10\n  operators:\n  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13\n    packages:\n    - name: lvms-operator\n    - name: local-storage-operator\n    - name: odf-csi-addons-operator\n    - name: odf-operator\n    - name: mcg-operator\n    - name: ocs-operator\n    - name: metallb-operator\n</code></pre> <p>Make sure you have your <code>${HOME}/.docker/config.json</code> file updated with the registries you are trying to mirror from and your private registry to push the images to.</p> <p>After that, we can begin the mirroring process:</p> <pre><code>oc-mirror --source-skip-tls --config imagesetconfig.yaml docker://${REGISTRY}\n</code></pre> <p>Once the mirror finishes, you will have a new folder called <code>oc-mirror-workspace/results-XXXXXX/</code> which contains the ICSP and the CatalogSources to be applied later on the HostedCluster.</p>"},{"location":"labs/IPv4/mirror/mirroring/#mirroring-nightly-and-ci-releases","title":"Mirroring Nightly and CI releases","text":"<p>The bad part in all of this, we cannot cover nightly or CI versions of Openshift so we will need to use the <code>oc adm release mirror</code> to mirror those versions.</p> <p>To mirror the nightly versions we need for this deployment, you need to execute this:</p> <pre><code>REGISTRY=registry.$(hostname --long):5000\n\noc adm release mirror \\\n  --from=registry.ci.openshift.org/ocp/release:4.14.0-0.nightly-2023-08-29-102237 \\\n  --to=${REGISTRY}/openshift/release \\\n  --to-release-image=${REGISTRY}/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237\n</code></pre> <p>For more detailed and updated information, you can visit the official Documentation or GitHub repository</p>"},{"location":"labs/IPv4/mirror/mirroring/#mirror-mce-internal-releases","title":"Mirror MCE internal releases","text":"<p>In order to mirror all the MCE latest images uploaded to quay.io or if it's internal and you can access the ACM documentation.</p> <ul> <li>Red Hat Official Documentation</li> <li>Red Hat Internal deployment Brew Registry deployment</li> </ul>"},{"location":"labs/IPv4/watching/","title":"Index","text":"<p>This section is purely done to let you know how we should monitor the deployment of a HostedCluster from two perspectives:</p> <ul> <li>Control Plane</li> <li>Data Plane</li> </ul> <p>So please follow these links accordingly.</p> <p>Control Plane perspective Data Plane perspective</p>"},{"location":"labs/IPv4/watching/watching-cp/","title":"Watching cp","text":"<p>Now it's a matter of waiting for the cluster to finish the deployment, so let's take a look at some useful commands on the Management cluster side:</p> <pre><code>export KUBECONFIG=/root/.kcli/clusters/hub-ipv4/auth/kubeconfig\n\nwatch \"oc get pod -n hypershift;echo;echo;oc get pod -n clusters-hosted-ipv4;echo;echo;oc get bmh -A;echo;echo;oc get agent -A;echo;echo;oc get infraenv -A;echo;echo;oc get hostedcluster -A;echo;echo;oc get nodepool -A;echo;echo;\"\n</code></pre> <p>This command will give you info about:</p> <ul> <li>What is the Hypershift Operator status</li> <li>The HostedControlPlane pod status</li> <li>The BareMetalHosts</li> <li>The Agents</li> <li>The Infraenv</li> <li>The HostedCluster and NodePool</li> </ul> <p>This is how it looks:</p> <p></p>"},{"location":"labs/IPv4/watching/watching-dp/","title":"Watching dp","text":"<p>If you check the Hosted cluster side you can check how the Operators are progressing and what is the status. To do that we will use these commands</p> <pre><code>oc get secret -n clusters-hosted-ipv4 admin-kubeconfig -o jsonpath='{.data.kubeconfig}' |base64 -d &gt; /root/hc_admin_kubeconfig.yaml\nexport KUBECONFIG=/root/hc_admin_kubeconfig.yaml\n\nwatch \"oc get clusterversion,nodes,co\"\n</code></pre> <p>This command will give you info about:</p> <ul> <li>Check the clusterversion</li> <li>Check if the Nodes has joined the cluster</li> <li>Check the ClusterOperators</li> </ul> <p>This is how looks like:</p> <p></p>"},{"location":"labs/IPv6/","title":"IPv6","text":"<p>This network configuration is (for now) by definition disconnected. The main reason is because the remote registries cannot work with IPv6 so we will include this part in the documentation.</p> <p>All the scripts provided will hold part or the whole automation to reproduce the environment. To do that this is the repository holding all the scripts for IPv6 environments.</p> <p>This documentation is prepared to be followed in a concrete order:</p> <ul> <li>Hypervisor</li> <li>DNS</li> <li>Registry</li> <li>Management Cluster</li> <li>Webserver</li> <li>Mirroring</li> <li>Multicluster Engine</li> <li>TLS Certificates</li> <li>HostedCluster</li> <li>Watching Deployment progress</li> </ul>"},{"location":"labs/IPv6/dns/","title":"Dns","text":"<p>The DNS configuration is a critical aspect of our setup. To enable name resolution in our virtualized environment, follow these steps:</p> <ol> <li> <p>Create the primary DNS configuration file for the dnsmasq server:</p> </li> <li> <p><code>/opt/dnsmasq/dnsmasq.conf</code> <pre><code>strict-order\nbind-dynamic\n#log-queries\nbogus-priv\ndhcp-authoritative\n\n# BM Network IPv6\ndhcp-range=ipv6,2620:52:0:1305::11,2620:52:0:1305::20,64\ndhcp-option=ipv6,option6:dns-server,2620:52:0:1305::1\n\nresolv-file=/opt/dnsmasq/upstream-resolv.conf\nexcept-interface=lo\ndhcp-lease-max=81\nlog-dhcp\nno-hosts\n\n# DHCP Reservations\ndhcp-leasefile=/opt/dnsmasq/hosts.leases\n\n# Include all files in a directory depending on the suffix\nconf-dir=/opt/dnsmasq/include.d/*.ipv6\n</code></pre></p> </li> </ol> <p>Create the upstream resolver to delegate the non-local environments queries</p> <ul> <li><code>/opt/dnsmasq/upstream-resolv.conf</code> <pre><code>nameserver 8.8.8.8\nnameserver 8.8.4.4\n</code></pre></li> </ul> <p>Create the different component DNS configurations</p> <ul> <li> <p><code>/opt/dnsmasq/include.d/hosted-nodeport.ipv6</code> <pre><code>host-record=api-int.hosted-ipv6.hypershiftbm.lab,2620:52:0:1305::5\nhost-record=api-int.hosted-ipv6.hypershiftbm.lab,2620:52:0:1305::6\nhost-record=api-int.hosted-ipv6.hypershiftbm.lab,2620:52:0:1305::7\nhost-record=api.hosted-ipv6.hypershiftbm.lab,2620:52:0:1305::5\nhost-record=api.hosted-ipv6.hypershiftbm.lab,2620:52:0:1305::6\nhost-record=api.hosted-ipv6.hypershiftbm.lab,2620:52:0:1305::7\naddress=/apps.hosted-ipv6.hypershiftbm.lab/2620:52:0:1305::60\ndhcp-host=aa:aa:aa:aa:04:11,hosted-worker0,[2620:52:0:1305::11]\ndhcp-host=aa:aa:aa:aa:04:12,hosted-worker1,[2620:52:0:1305::12]\ndhcp-host=aa:aa:aa:aa:04:13,hosted-worker2,[2620:52:0:1305::13]\n</code></pre></p> </li> <li> <p><code>/opt/dnsmasq/include.d/hub.ipv6</code> <pre><code>host-record=api-int.hub-ipv6.hypershiftbm.lab,2620:52:0:1305::2\nhost-record=api.hub-ipv6.hypershiftbm.lab,2620:52:0:1305::2\naddress=/apps.hub-ipv6.hypershiftbm.lab/2620:52:0:1305::3\ndhcp-host=aa:aa:aa:aa:03:01,ocp-master-0,[2620:52:0:1305::5]\ndhcp-host=aa:aa:aa:aa:03:02,ocp-master-1,[2620:52:0:1305::6]\ndhcp-host=aa:aa:aa:aa:03:03,ocp-master-2,[2620:52:0:1305::7]\ndhcp-host=aa:aa:aa:aa:03:06,ocp-installer,[2620:52:0:1305::8]\ndhcp-host=aa:aa:aa:aa:03:07,ocp-bootstrap,[2620:52:0:1305::9]\n</code></pre></p> </li> <li> <p><code>/opt/dnsmasq/include.d/infra.ipv6</code> <pre><code>host-record=registry.hypershiftbm.lab,2620:52:0:1305::1\n</code></pre></p> </li> </ul> <p>To proceed, we must create a systemd service for the management of the dnsmasq service and disable the system's default dnsmasq service:</p> <ul> <li><code>/etc/systemd/system/dnsmasq-virt.service</code> <pre><code>[Unit]\nDescription=DNS server for Openshift 4 Clusters.\nAfter=network.target\n\n[Service]\nUser=root\nGroup=root\nExecStart=/usr/sbin/dnsmasq -k --conf-file=/opt/dnsmasq/dnsmasq.conf\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></li> </ul> <p>The commands to do so:</p> <pre><code>systemctl daemon-reload\nsystemctl disable --now dnsmasq\nsystemctl enable --now dnsmasq-virt\n</code></pre> <p>Note</p> <p>This step is mandatory for both Disconnected and Connected environments. Additionally, it holds significance for both Virtualized and Bare Metal environments. The key distinction lies in the location where the resources will be configured. In a non-virtualized environment, a more robust solution like Bind is recommended instead of a lightweight dnsmasq.</p>"},{"location":"labs/IPv6/registry/","title":"Registry","text":"<p>Important</p> <p>This section is exclusively applicable to disconnected scenarios. If this does not apply to your situation, please proceed to the next section.</p> <p>In this section, we will explain how to deploy a small, self-hosted registry using a Podman container. For production environments, we strongly recommend utilizing a more reliable solution such as Quay, Nexus, or Artifactory.</p> <ol> <li> <p>As a privileged user, please access the ${HOME} directory and proceed to create the following script provided below. The script will make certain assumptions, including the registry name based on the hypervisor hostname, as well as the necessary credentials and user access:</p> </li> <li> <p><code>registry.sh</code> <pre><code>#!/usr/bin/env bash\n\nset -euo pipefail\n\nPRIMARY_NIC=$(ls -1 /sys/class/net | grep -v podman | head -1)\nexport PATH=/root/bin:$PATH\nexport PULL_SECRET=\"/root/baremetal/hub/openshift_pull.json\"\n\nif [[ ! -f $PULL_SECRET ]];then\n  echo \"Pull Secret not found, exiting...\"\n  exit 1\nfi\n\ndnf -y install podman httpd httpd-tools jq skopeo libseccomp-devel\nexport IP=$(ip -o addr show $PRIMARY_NIC | head -1 | awk '{print $4}' | cut -d'/' -f1)\nREGISTRY_NAME=registry.$(hostname --long)\nREGISTRY_USER=dummy\nREGISTRY_PASSWORD=dummy\nKEY=$(echo -n $REGISTRY_USER:$REGISTRY_PASSWORD | base64)\necho \"{\\\"auths\\\": {\\\"$REGISTRY_NAME:5000\\\": {\\\"auth\\\": \\\"$KEY\\\", \\\"email\\\": \\\"sample-email@domain.ltd\\\"}}}\" &gt; /root/disconnected_pull.json\nmv ${PULL_SECRET} /root/openshift_pull.json.old\njq \".auths += {\\\"$REGISTRY_NAME:5000\\\": {\\\"auth\\\": \\\"$KEY\\\",\\\"email\\\": \\\"sample-email@domain.ltd\\\"}}\" &lt; /root/openshift_pull.json.old &gt; $PULL_SECRET\nmkdir -p /opt/registry/{auth,certs,data,conf}\ncat &lt;&lt;EOF &gt; /opt/registry/conf/config.yml\nversion: 0.1\nlog:\n  fields:\n    service: registry\nstorage:\n  cache:\n    blobdescriptor: inmemory\n  filesystem:\n    rootdirectory: /var/lib/registry\n  delete:\n    enabled: true\nhttp:\n  addr: :5000\n  headers:\n    X-Content-Type-Options: [nosniff]\nhealth:\n  storagedriver:\n    enabled: true\n    interval: 10s\n    threshold: 3\ncompatibility:\n  schema1:\n    enabled: true\nEOF\nopenssl req -newkey rsa:4096 -nodes -sha256 -keyout /opt/registry/certs/domain.key -x509 -days 3650 -out /opt/registry/certs/domain.crt -subj \"/C=US/ST=Madrid/L=San Bernardo/O=Karmalabs/OU=Guitar/CN=$REGISTRY_NAME\" -addext \"subjectAltName=DNS:$REGISTRY_NAME\"\ncp /opt/registry/certs/domain.crt /etc/pki/ca-trust/source/anchors/\nupdate-ca-trust extract\nhtpasswd -bBc /opt/registry/auth/htpasswd $REGISTRY_USER $REGISTRY_PASSWORD\npodman create --name registry --net host --security-opt label=disable --replace -v /opt/registry/data:/var/lib/registry:z -v /opt/registry/auth:/auth:z -v /opt/registry/conf/config.yml:/etc/docker/registry/config.yml -e \"REGISTRY_AUTH=htpasswd\" -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry\" -e \"REGISTRY_HTTP_SECRET=ALongRandomSecretForRegistry\" -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd -v /opt/registry/certs:/certs:z -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key docker.io/library/registry:latest\n[ \"$?\" == \"0\" ] || !!\nsystemctl enable --now registry\n</code></pre></p> </li> <li> <p>Kindly modify the location of the <code>PULL_SECRET</code> to point to the correct location.</p> </li> <li>Please adjust the permissions by adding the execution flag as follows: <code>chmod u+x ${HOME}/registry.sh</code></li> <li>You can execute the script without any parameters using the following command <code>${HOME}/registry.sh</code></li> </ol> <p>After executing the script <code>${HOME}/registry.sh</code>, the server should now be up and running. If the script has been properly configured and executed without errors, it should have started the server as intended.</p> <p>It will utilize a systemd service for management purposes. So, if you ever need to manage it, you can employ systemctl status/start/stop registry.</p> <p>The root folder for the registry is situated at /opt/registry, and it's structured as follows:</p> <ul> <li><code>certs</code> holds the TLS certificates.</li> <li><code>auth</code> keeps the credentials.</li> <li><code>data</code> contains the registry images.</li> <li><code>conf</code> will hold the registry configuration.</li> </ul>"},{"location":"labs/IPv6/tls-certificates/","title":"Tls certificates","text":"<p>Important</p> <p>This section is only relevant in disconnected scenarios. If this doesn't apply to your situation, please proceed to the next section.</p> <p>In this section, we'll cover the TLS certificates involved in the process, primarily focusing on the private registries from which the images will be pulled. While there may be additional certificates, we'll concentrate on these particular ones.</p> <p>It's important to distinguish between the various methods and their impact on the associated cluster. All of these methods essentially modify the content of the following files on the OCP (OpenShift Container Platform) control plane (Master nodes) and data plane (worker nodes):</p> <ul> <li><code>/etc/pki/ca-trust/extracted/pem/</code></li> <li><code>/etc/pki/ca-trust/source/anchors/</code></li> <li><code>/etc/pki/tls/certs/</code></li> </ul>"},{"location":"labs/IPv6/tls-certificates/#adding-a-ca-to-the-management-cluster","title":"Adding a CA to the Management Cluster","text":"<p>There exist numerous methods to accomplish this within the OpenShift environment. However, we have chosen to integrate the less intrusive approach.</p> <ol> <li>Initially, you must create a ConfigMap with a name of your choosing. In our specific case, we will utilize the name <code>registry-config</code> The content should resemble the following:</li> </ol> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: registry-config\n  namespace: openshift-config\ndata:\n  registry.hypershiftbm.lab..5000: |\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\n</code></pre> <p>Note</p> <pre><code>The data field ought to contain the registry name, while the value should encompass the Registry certificate. As is evident, the \":\" character is being replaced by \"..\"; therefore, it is imperative to ensure this correction.\n</code></pre> <ol> <li>Now we need to patch the clusterwide object <code>image.config.openshift.io</code> including this:</li> </ol> <pre><code>spec:\n  additionalTrustedCA:\n    name: registry-config\n</code></pre> <p>This modification will result in two significant consequences:</p> <ul> <li>Granting masters the capability to retrieve images from the private registry.</li> <li>Allowing the Hypershift Operator to extract the Openshift payload for the HostedCluster deployments.</li> </ul> <p>Note</p> <pre><code>The modification required several minutes to be successfully executed.\n</code></pre>"},{"location":"labs/IPv6/tls-certificates/#alternative-adding-a-ca-to-the-management-cluster","title":"Alternative: Adding a CA to the Management Cluster","text":"<p>We consider this as an alternative, given that it entails the masters undergoing a reboot facilitated by the Machine Config Operator.</p> <p>It's described here. This method involves utilizing the <code>image-registry-operator</code>, which deploys the CAs to the OCP nodes.</p> <p>Hypershift's operators and controllers automatically handle this process, so if you're using a GA (Generally Available) released version, it should work seamlessly, and you won't need to apply these steps. This Hypershift feature is included in the payload of the 2.4 MCE release.</p> <p>However, if this feature is not working as expected or if it doesn't apply to your situation, you can follow this procedure:</p> <ul> <li>Check if the <code>openshift-config</code> namespace in the Management cluster contains a ConfigMap named <code>user-ca-bundle</code>.</li> <li>If the ConfigMap doesn't exist, execute the following command:</li> </ul> <pre><code>## REGISTRY_CERT_PATH=&lt;PATH/TO/YOUR/CERTIFICATE/FILE&gt;\nexport REGISTRY_CERT_PATH=/opt/registry/certs/domain.crt\n\noc create configmap user-ca-bundle -n openshift-config --from-file=ca-bundle.crt=${REGISTRY_CERT_PATH}\n</code></pre> <ul> <li>Otherwise, if that ConfigMap exists, execute this other command:</li> </ul> <pre><code>## REGISTRY_CERT_PATH=&lt;PATH/TO/YOUR/CERTIFICATE/FILE&gt;\nexport REGISTRY_CERT_PATH=/opt/registry/certs/domain.crt\nexport TMP_FILE=$(mktemp)\n\noc get cm -n openshift-config user-ca-bundle -ojsonpath='{.data.ca-bundle\\.crt}' &gt; ${TMP_FILE}\necho &gt;&gt; ${TMP_FILE}\necho \\#registry.$(hostname --long) &gt;&gt; ${TMP_FILE}\ncat ${REGISTRY_CERT_PATH} &gt;&gt; ${TMP_FILE}\noc create configmap user-ca-bundle -n openshift-config --from-file=ca-bundle.crt=${TMP_FILE} --dry-run=client -o yaml | kubectl apply -f -\n</code></pre> <p>You have a functional script located in the <code>assets/&lt;NetworkStack&gt;/09-tls-certificates/01-config.sh</code>, this is the sample for IPv6.</p>"},{"location":"labs/IPv6/webserver/","title":"Webserver","text":"<p>Important</p> <p>This section is only relevant in disconnected scenarios, if this is not your case, you can continue with the next section.</p> <p>This section talks about an additional webserver that you need to configure to host the RHCOS images associated with the Openshift release you are trying to deploy as a HostedCluster.</p> <p>The script refers to this repository folder and it's the same for all three different network stacks.</p> <p>To do this, you can use this script:</p> <pre><code>#!/bin/bash\n\nWEBSRV_FOLDER=/opt/srv\nROOTFS_IMG_URL=\"$(../04-management-cluster/openshift-install coreos print-stream-json | jq -r '.architectures.x86_64.artifacts.metal.formats.pxe.rootfs.location')\"\nLIVE_ISO_URL=\"$(../04-management-cluster/openshift-install coreos print-stream-json | jq -r '.architectures.x86_64.artifacts.metal.formats.iso.disk.location')\"\n\nmkdir -p ${WEBSRV_FOLDER}/images\ncurl -Lk ${ROOTFS_IMG_URL} -o ${WEBSRV_FOLDER}/images/${ROOTFS_IMG_URL##*/}\ncurl -Lk ${LIVE_ISO_URL} -o ${WEBSRV_FOLDER}/images/${LIVE_ISO_URL##*/}\nchmod -R 755 ${WEBSRV_FOLDER}/*\n\n## Run Webserver\npodman ps --noheading | grep -q websrv-ai\nif [[ $? == 0 ]];then\n    echo \"Launching Registry pod...\"\n    /usr/bin/podman run --name websrv-ai --net host -v /opt/srv:/usr/local/apache2/htdocs:z quay.io/alosadag/httpd:p8080\nfi\n</code></pre> <p>The script will create a folder under <code>/opt/srv</code>. This folder will contain the <code>images</code> for RHCOS provision in the worker nodes. To be more concrete, we need the <code>RootFS</code> and <code>LiveISO</code> artifacts found on the Openshift CI Release page.</p> <p>After the download, a container will run to host the images under a webserver. It uses a variation of the official httpd image, which also allows it to work with IPv6.</p>"},{"location":"labs/IPv6/hostedcluster/","title":"Index","text":"<p>Hosted Cluster, as mentioned in the documentation here, is essentially an OCP API endpoint managed by Hypershift. In this context, we will also include the term HostedControlPlane to enhance readability and comprehension. This terminology is further explained in the same link.</p> <p>The Hosted Cluster comprises two main components: - The Control Plane, which runs as pods in the management cluster. - The Data Plane, consisting of external nodes managed by the end user.</p> <p>With this foundational understanding, we can commence our Hosted Cluster deployment. Typically, an ACM/MCE user would utilize the web UI to create a cluster. However, in this scenario, we will leverage manifests, providing us with greater flexibility to modify the artifacts.</p>"},{"location":"labs/IPv6/hostedcluster/baremetalhost/","title":"Baremetalhost","text":""},{"location":"labs/IPv6/hostedcluster/baremetalhost/#bare-metal-hosts","title":"Bare Metal Hosts","text":"<p>A BareMetalHost is an openshift-machine-api object that encompasses both physical and logical details, allowing it to be identified by the Metal3 operator. Subsequently, these details are associated with other Assisted Service objects known as Agents. The structure of this object is as follows:</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: hosted-ipv6-worker0-bmc-secret\n  namespace: clusters-hosted-ipv6\ndata:\n  password: YWRtaW4=\n  username: YWRtaW4=\ntype: Opaque\n---\napiVersion: metal3.io/v1alpha1\nkind: BareMetalHost\nmetadata:\n  name: hosted-ipv6-worker0\n  namespace: clusters-hosted-ipv6\n  labels:\n    infraenvs.agent-install.openshift.io: hosted-ipv6\n  annotations:\n    inspect.metal3.io: disabled\n    bmac.agent-install.openshift.io/hostname: hosted-ipv6-worker0\nspec:\n  automatedCleaningMode: disabled\n  bmc:\n    disableCertificateVerification: true\n    address: redfish-virtualmedia://[192.168.125.1]:9000/redfish/v1/Systems/local/hosted-ipv6-worker0\n    credentialsName: hosted-ipv6-worker0-bmc-secret\n  bootMACAddress: aa:aa:aa:aa:03:11\n  online: true\n</code></pre> <p>Details:</p> <ul> <li>We will have at least 1 secret that holds the BMH credentials, so we will need to create at least 2 objects per worker node.</li> <li><code>spec.metadata.labels[\"infraenvs.agent-install.openshift.io\"]</code> serves as the link between the Assisted Installer and the BareMetalHost objects.</li> <li><code>spec.metadata.annotations[\"bmac.agent-install.openshift.io/hostname\"]</code> represents the node name it will adopt during deployment.</li> <li><code>spec.automatedCleaningMode</code> prevents the node from being erased by the Metal3 operator.</li> <li><code>spec.bmc.disableCertificateVerification</code> is set to <code>true</code> to bypass certificate validation from the client.</li> <li><code>spec.bmc.address</code> denotes the BMC address of the worker node.</li> <li><code>spec.bmc.credentialsName</code> points to the Secret where User/Password credentials are stored.</li> <li><code>spec.bootMACAddress</code> indicates the interface MACAddress from which the node will boot.</li> <li><code>spec.online</code> defines the desired state of the node once the BMH object is created.</li> </ul> <p>To deploy this object, simply follow the same procedure as before:</p> <p>Important</p> <p>Please create the virtual machines before you create the BareMetalHost and the destination Nodes.</p> <p>To deploy the BareMetalHost object, execute the following command:</p> <pre><code>oc apply -f 04-bmh.yaml\n</code></pre> <p>This will be the process:</p> <ul> <li> <p>Preparing (Trying to reach the nodes): <pre><code>NAMESPACE         NAME             STATE         CONSUMER   ONLINE   ERROR   AGE\nclusters-hosted   hosted-worker0   registering              true             2s\nclusters-hosted   hosted-worker1   registering              true             2s\nclusters-hosted   hosted-worker2   registering              true             2s\n</code></pre></p> </li> <li> <p>Provisioning (Nodes Booting up) <pre><code>NAMESPACE         NAME             STATE          CONSUMER   ONLINE   ERROR   AGE\nclusters-hosted   hosted-worker0   provisioning              true             16s\nclusters-hosted   hosted-worker1   provisioning              true             16s\nclusters-hosted   hosted-worker2   provisioning              true             16s\n</code></pre></p> </li> <li> <p>Provisioned (Nodes Booted up successfully) <pre><code>NAMESPACE         NAME             STATE         CONSUMER   ONLINE   ERROR   AGE\nclusters-hosted   hosted-worker0   provisioned              true             67s\nclusters-hosted   hosted-worker1   provisioned              true             67s\nclusters-hosted   hosted-worker2   provisioned              true             67s\n</code></pre></p> </li> </ul>"},{"location":"labs/IPv6/hostedcluster/baremetalhost/#agents-registration","title":"Agents registration","text":"<p>After the nodes have booted up, you will observe the appearance of agents within the namespace.</p> <pre><code>NAMESPACE         NAME                                   CLUSTER   APPROVED   ROLE          STAGE\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0411             true       auto-assign\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0412             true       auto-assign\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0413             true       auto-assign\n</code></pre> <p>These agents represent the nodes available for installation. To assign them to a HostedCluster, scale up the NodePool.</p>"},{"location":"labs/IPv6/hostedcluster/baremetalhost/#scaling-up-the-nodepool","title":"Scaling Up the Nodepool","text":"<p>Once we have the BareMetalHosts created, the statuses of these BareMetalHosts will transition from <code>Registering</code> (Attempting to reach the Node's BMC) to <code>Provisioning</code> (Node Booting Up), and finally to <code>Provisioned</code> (Successful node boot-up).</p> <p>The nodes will boot with the Agent's RHCOS LiveISO and a default pod named \"agent.\" This agent is responsible for receiving instructions from the Assisted Service Operator to install the Openshift payload.</p> <p>To accomplish this, execute the following command:</p> <pre><code>oc -n clusters scale nodepool hosted-ipv6 --replicas 3\n</code></pre> <p>After the NodePool scaling, you will notice that the agents are assigned to a Hosted Cluster.</p> <pre><code>NAMESPACE         NAME                                   CLUSTER   APPROVED   ROLE          STAGE\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0211   hosted    true       auto-assign\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0212   hosted    true       auto-assign\nclusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0213   hosted    true       auto-assign\n</code></pre> <p>And the NodePool replicas set</p> <pre><code>NAMESPACE   NAME     CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION                              UPDATINGVERSION   UPDATINGCONFIG   MESSAGE\nclusters    hosted   hosted    3                               False         False        4.14.0-0.nightly-2023-08-29-102237                                      Minimum availability requires 3 replicas, current 0 available\n</code></pre> <p>So now, we need to wait until the nodes join the cluster. The Agents will provide updates on their current stage and status. Initially, they may not post any status, but eventually, they will.</p>"},{"location":"labs/IPv6/hostedcluster/hostedcluster/","title":"Hostedcluster","text":"<p>In this section, we will focus on all the related objects necessary to achieve a Disconnected Hosted Cluster deployment.</p> <p>Premises:</p> <ul> <li>HostedCluster Name: <code>hosted-ipv6</code></li> <li>HostedCluster Namespace: <code>clusters</code></li> <li>Disconnected: <code>true</code></li> <li>Network Stack: <code>ipv6</code></li> </ul>"},{"location":"labs/IPv6/hostedcluster/hostedcluster/#openshift-objects","title":"Openshift Objects","text":""},{"location":"labs/IPv6/hostedcluster/hostedcluster/#namespaces","title":"Namespaces","text":"<p>In a typical situation, the operator would be responsible for creating the HCP (HostedControlPlane) namespace. However, in this case, we want to include all the objects before the operator begins reconciliation over the HostedCluster object. This way, when the operator commences the reconciliation process, it will find all the objects in place.</p> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: null\n  name: clusters-hosted-ipv6\nspec: {}\nstatus: {}\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: null\n  name: clusters\nspec: {}\nstatus: {}\n</code></pre> <p>Note</p> <p>We will not create objects one by one but will concatenate all of them in the same file and apply them with just one command.</p>"},{"location":"labs/IPv6/hostedcluster/hostedcluster/#configmap-and-secrets","title":"ConfigMap and Secrets","text":"<p>These are the ConfigMaps and Secrets that we will include in the HostedCluster deployment.</p> <pre><code>---\napiVersion: v1\ndata:\n  ca-bundle.crt: |\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\nkind: ConfigMap\nmetadata:\n  name: user-ca-bundle\n  namespace: clusters\n---\napiVersion: v1\ndata:\n  .dockerconfigjson: xxxxxxxxx\nkind: Secret\nmetadata:\n  creationTimestamp: null\n  name: hosted-ipv6-pull-secret\n  namespace: clusters\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: sshkey-cluster-hosted-ipv6\n  namespace: clusters\nstringData:\n  id_rsa.pub: ssh-rsa xxxxxxxxx\n---\napiVersion: v1\ndata:\n  key: nTPtVBEt03owkrKhIdmSW8jrWRxU57KO/fnZa8oaG0Y=\nkind: Secret\nmetadata:\n  creationTimestamp: null\n  name: hosted-ipv6-etcd-encryption-key\n  namespace: clusters\ntype: Opaque\n</code></pre>"},{"location":"labs/IPv6/hostedcluster/hostedcluster/#rbac-roles","title":"RBAC Roles","text":"<p>While not mandatory, it allows us to have the Assisted Service Agents located in the same HostedControlPlane namespace as the HostedControlPlane and still be managed by CAPI.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  creationTimestamp: null\n  name: capi-provider-role\n  namespace: clusters-hosted-ipv6\nrules:\n- apiGroups:\n  - agent-install.openshift.io\n  resources:\n  - agents\n  verbs:\n  - '*'\n</code></pre>"},{"location":"labs/IPv6/hostedcluster/hostedcluster/#hosted-cluster","title":"Hosted Cluster","text":"<p>This is a sample of the HostedCluster Object</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: HostedCluster\nmetadata:\n  name: hosted-ipv6\n  namespace: clusters\n  annotations:\n    hypershift.openshift.io/control-plane-operator-image: registry.ocp-edge-cluster-0.qe.lab.redhat.com:5005/openshift/release@sha256:31149e3e5f8c5e5b5b100ff2d89975cf5f7a73801b2c06c639bf6648766117f8\nspec:\n  additionalTrustBundle:\n    name: \"user-ca-bundle\"\n  olmCatalogPlacement: guest\n  imageContentSources:\n  - source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n    mirrors:\n    - registry.hypershiftbm.lab:5000/openshift/release\n  - source: quay.io/openshift-release-dev/ocp-release\n    mirrors:\n    - registry.hypershiftbm.lab:5000/openshift/release-images\n  - mirrors:\n  ...\n  ...\n  autoscaling: {}\n  controllerAvailabilityPolicy: SingleReplica\n  dns:\n    baseDomain: hypershiftbm.lab\n  etcd:\n    managed:\n      storage:\n        persistentVolume:\n          size: 8Gi\n        restoreSnapshotURL: null\n        type: PersistentVolume\n    managementType: Managed\n  fips: false\n  networking:\n    clusterNetwork:\n    - cidr: 10.132.0.0/14\n    networkType: OVNKubernetes\n    serviceNetwork:\n    - cidr: 172.31.0.0/16\n  platform:\n    agent:\n      agentNamespace: clusters-hosted-ipv6\n    type: Agent\n  pullSecret:\n    name: hosted-ipv6-pull-secret\n  release:\n    image: registry.hypershiftbm.lab:5000/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237\n  secretEncryption:\n    aescbc:\n      activeKey:\n        name: hosted-ipv6-etcd-encryption-key\n    type: aescbc\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: api.hosted-ipv6.hypershiftbm.lab\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: api.hosted-ipv6.hypershiftbm.lab\n      type: NodePort\n  - service: OIDC\n    servicePublishingStrategy:\n      nodePort:\n        address: api.hosted-ipv6.hypershiftbm.lab\n      type: NodePort\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: api.hosted-ipv6.hypershiftbm.lab\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: api.hosted-ipv6.hypershiftbm.lab\n      type: NodePort\n  sshKey:\n    name: sshkey-cluster-hosted-ipv6\nstatus:\n  controlPlaneEndpoint:\n    host: \"\"\n    port: 0\n</code></pre> <p>Note</p> <p>The <code>imageContentSources</code> section within the <code>spec</code> field contains mirror references for user workloads within the HostedCluster.</p> <p>As you can see, all the objects created before are referenced here. You can also refer to the documentation where all the fields are described.</p>"},{"location":"labs/IPv6/hostedcluster/hostedcluster/#deployment","title":"Deployment","text":"<p>To deploy these objects, simply concatenate them into the same file and apply them against the management cluster:</p> <pre><code>oc apply -f 01-4.14-hosted_cluster-nodeport.yaml\n</code></pre> <p>This will raise up a functional Hosted Control Plane.</p> <pre><code>NAME                                                  READY   STATUS    RESTARTS   AGE\ncapi-provider-5b57dbd6d5-pxlqc                        1/1     Running   0          3m57s\ncatalog-operator-9694884dd-m7zzv                      2/2     Running   0          93s\ncluster-api-f98b9467c-9hfrq                           1/1     Running   0          3m57s\ncluster-autoscaler-d7f95dd5-d8m5d                     1/1     Running   0          93s\ncluster-image-registry-operator-5ff5944b4b-648ht      1/2     Running   0          93s\ncluster-network-operator-77b896ddc-wpkq8              1/1     Running   0          94s\ncluster-node-tuning-operator-84956cd484-4hfgf         1/1     Running   0          94s\ncluster-policy-controller-5fd8595d97-rhbwf            1/1     Running   0          95s\ncluster-storage-operator-54dcf584b5-xrnts             1/1     Running   0          93s\ncluster-version-operator-9c554b999-l22s7              1/1     Running   0          95s\ncontrol-plane-operator-6fdc9c569-t7hr4                1/1     Running   0          3m57s\ncsi-snapshot-controller-785c6dc77c-8ljmr              1/1     Running   0          77s\ncsi-snapshot-controller-operator-7c6674bc5b-d9dtp     1/1     Running   0          93s\ndns-operator-6874b577f-9tc6b                          1/1     Running   0          94s\netcd-0                                                3/3     Running   0          3m39s\nhosted-cluster-config-operator-f5cf5c464-4nmbh        1/1     Running   0          93s\nignition-server-6b689748fc-zdqzk                      1/1     Running   0          95s\nignition-server-proxy-54d4bb9b9b-6zkg7                1/1     Running   0          95s\ningress-operator-6548dc758b-f9gtg                     1/2     Running   0          94s\nkonnectivity-agent-7767cdc6f5-tw782                   1/1     Running   0          95s\nkube-apiserver-7b5799b6c8-9f5bp                       4/4     Running   0          3m7s\nkube-controller-manager-5465bc4dd6-zpdlk              1/1     Running   0          44s\nkube-scheduler-5dd5f78b94-bbbck                       1/1     Running   0          2m36s\nmachine-approver-846c69f56-jxvfr                      1/1     Running   0          92s\noauth-openshift-79c7bf44bf-j975g                      2/2     Running   0          62s\nolm-operator-767f9584c-4lcl2                          2/2     Running   0          93s\nopenshift-apiserver-5d469778c6-pl8tj                  3/3     Running   0          2m36s\nopenshift-controller-manager-6475fdff58-hl4f7         1/1     Running   0          95s\nopenshift-oauth-apiserver-dbbc5cc5f-98574             2/2     Running   0          95s\nopenshift-route-controller-manager-5f6997b48f-s9vdc   1/1     Running   0          95s\npackageserver-67c87d4d4f-kl7qh                        2/2     Running   0          93s\n</code></pre> <p>And this is how the HostedCluster looks like:</p> <pre><code>NAMESPACE   NAME         VERSION   KUBECONFIG                PROGRESS   AVAILABLE   PROGRESSING   MESSAGE\nclusters    hosted-ipv6            hosted-admin-kubeconfig   Partial    True          False         The hosted control plane is available\n</code></pre> <p>After some time, we will have almost all the pieces in place, and the Control Plane operator awaits for the worker nodes to join the cluster. To achieve this, we need to create some more objects. Let's discuss the <code>InfraEnv</code> and the <code>BareMetalHost</code> in the following sections.</p>"},{"location":"labs/IPv6/hostedcluster/infraenv/","title":"Infraenv","text":"<p>The <code>InfraEnv</code> is an Assisted Service object that includes essential details such as the <code>pullSecretRef</code> and the <code>sshAuthorizedKey</code>. These details are used to create the RHCOS Boot Image customized specifically for the cluster. Below is the structure of this object:</p> <pre><code>---\napiVersion: agent-install.openshift.io/v1beta1\nkind: InfraEnv\nmetadata:\n  name: hosted-ipv6\n  namespace: clusters-hosted-ipv6\nspec:\n  pullSecretRef:\n    name: pull-secret\n  sshAuthorizedKey: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDk7ICaUE+/k4zTpxLk4+xFdHi4ZuDi5qjeF52afsNkw0w/glILHhwpL5gnp5WkRuL8GwJuZ1VqLC9EKrdmegn4MrmUlq7WTsP0VFOZFBfq2XRUxo1wrRdor2z0Bbh93ytR+ZsDbbLlGngXaMa0Vbt+z74FqlcajbHTZ6zBmTpBVq5RHtDPgKITdpE1fongp7+ZXQNBlkaavaqv8bnyrP4BWahLP4iO9/xJF9lQYboYwEEDzmnKLMW1VtCE6nJzEgWCufACTbxpNS7GvKtoHT/OVzw8ArEXhZXQUS1UY8zKsX2iXwmyhw5Sj6YboA8WICs4z+TrFP89LmxXY0j6536TQFyRz1iB4WWvCbH5n6W+ABV2e8ssJB1AmEy8QYNwpJQJNpSxzoKBjI73XxvPYYC/IjPFMySwZqrSZCkJYqQ023ySkaQxWZT7in4KeMu7eS2tC+Kn4deJ7KwwUycx8n6RHMeD8Qg9flTHCv3gmab8JKZJqN3hW1D378JuvmIX4V0=\n</code></pre> <p>Details:</p> <ul> <li><code>pullSecretRef</code> refers to the ConfigMap reference (in the same Namespace as the InfraEnv) where the PullSecret will be utilized.</li> <li><code>sshAuthorizedKey</code> represents the SSH Public key that will be injected into the Boot Image. This SSH key will, by default, allow access to the worker nodes as the <code>core</code> user.</li> </ul> <p>To deploy this object, follow the same procedure as before:</p> <pre><code>oc apply -f 03-infraenv.yaml\n</code></pre> <p>And this is how looks like</p> <pre><code>NAMESPACE              NAME     ISO CREATED AT\nclusters-hosted-ipv6   hosted   2023-09-11T15:14:10Z\n</code></pre>"},{"location":"labs/IPv6/hostedcluster/nodepool/","title":"Nodepool","text":"<p>A <code>NodePool</code> is a scalable set of worker nodes associated with a HostedCluster. NodePool machine architectures remain consistent within a specific pool and are independent of the underlying machine architecture of the control plane.</p> <p>Note</p> <p>Please ensure you modify the appropriate fields to align with your laboratory environment.</p> <p>Warning</p> <p>Before a day-1 patch, the release image set in the HostedCluster should use the digest rather than the tag. (e.g <code>quay.io/openshift-release-dev/ocp-release@sha256:e3ba11bd1e5e8ea5a0b36a75791c90f29afb0fdbe4125be4e48f69c76a5c47a0</code>)</p> <p>This is how one looks like:</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: NodePool\nmetadata:\n  creationTimestamp: null\n  name: hosted-ipv6\n  namespace: clusters\nspec:\n  arch: amd64\n  clusterName: hosted-ipv6\n  management:\n    autoRepair: false\n    upgradeType: InPlace\n  nodeDrainTimeout: 0s\n  nodeVolumeDetachTimeout: 0s\n  platform:\n    type: Agent\n  release:\n    image: registry.hypershiftbm.lab:5000/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237\n  replicas: 0\nstatus:\n  replicas: 0\n</code></pre> <p>Details:</p> <ul> <li>All the nodes included in this NodePool will be based on the Openshift version <code>4.14.0-0.nightly-2023-08-29-102237</code>.</li> <li>The Upgrade type is set to <code>InPlace</code>, indicating that the same bare-metal node will be reused during an upgrade.</li> <li>Autorepair is set to <code>false</code> because the node will not be recreated when it disappears.</li> <li>Replicas are set to <code>0</code> because we intend to scale them when needed.</li> </ul> <p>You can find more information about NodePool in the NodePool documentation.</p> <p>To deploy this object, simply follow the same procedure as before:</p> <pre><code>oc apply -f 02-nodepool.yaml\n</code></pre> <p>And this is how the NodePool looks like (at this point):</p> <pre><code>NAMESPACE   NAME          CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION                              UPDATINGVERSION   UPDATINGCONFIG   MESSAGE\nclusters    hosted-ipv6   hosted    0                               False         False        4.14.0-0.nightly-2023-08-29-102237\n</code></pre> <p>Important</p> <p>Keep the nodepool replicas to 0 until all the steps are in place.</p>"},{"location":"labs/IPv6/hostedcluster/worker-nodes/","title":"Worker nodes","text":"<p>Regarding the worker nodes, if you are working on real bare metal, this step is crucial to ensure that the details set in the <code>BareMetalHost</code> are correctly configured. If not, you will need to debug why it's not functioning as expected.</p> <p>However, if you are working with virtual machines, you can follow these steps to create empty ones that will be consumed by the Metal3 operator. To achieve this, we will utilize Kcli.</p>"},{"location":"labs/IPv6/hostedcluster/worker-nodes/#creating-virtual-machines","title":"Creating Virtual Machines","text":"<p>If this is not your first attempt, you must first delete the previous setup. To do so, please refer to the Deleting Virtual Machines section.</p> <p>Now, you can execute the following commands for VM creation:</p> <pre><code>kcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-ipv6 -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=[\"{\\\"name\\\": \\\"ipv6\\\", \\\"mac\\\": \\\"aa:aa:aa:aa:02:11\\\"}\"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0211 -P name=hosted-ipv6-worker0\nkcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-ipv6 -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=[\"{\\\"name\\\": \\\"ipv6\\\", \\\"mac\\\": \\\"aa:aa:aa:aa:02:12\\\"}\"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0212 -P name=hosted-ipv6-worker1\nkcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-ipv6 -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=[\"{\\\"name\\\": \\\"ipv6\\\", \\\"mac\\\": \\\"aa:aa:aa:aa:02:13\\\"}\"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0213 -P name=hosted-ipv6-worker2\n\nsleep 2\nsystemctl restart ksushy\n</code></pre> <p>Let's dissect the creation command:</p> <ul> <li><code>start=False</code>: The VM will not boot automatically upon creation.</li> <li><code>uefi_legacy=true</code>: We will use UEFI legacy boot to ensure compatibility with older UEFI implementations.</li> <li><code>plan=hosted-dual</code>: The plan name, which identifies a group of machines as a cluster.</li> <li><code>memory=8192</code> and <code>numcpus=16</code>: These parameters specify the resources for the VM, including RAM and CPU.</li> <li><code>disks=[200,200]</code>: We are creating 2 disks (thin provisioned) in the virtual machine.</li> <li><code>nets=[{\"name\": \"dual\", \"mac\": \"aa:aa:aa:aa:02:13\"}]</code>: Network details, including the network name it will be connected to and the MAC address for the primary interface.</li> <li>The <code>ksushy</code> restart is performed to make our <code>ksushy</code> (VM's BMC) aware of the new VMs added.</li> </ul> <p>This is what the command looks like:</p> <pre><code>+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+\n|         Name        | Status |         Ip        |                       Source                       |     Plan    | Profile |\n+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+\n|    hosted-worker0   |  down  |                   |                                                    | hosted-ipv6 |  kvirt  |\n|    hosted-worker1   |  down  |                   |                                                    | hosted-ipv6 |  kvirt  |\n|    hosted-worker2   |  down  |                   |                                                    | hosted-ipv6 |  kvirt  |\n+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+\n</code></pre>"},{"location":"labs/IPv6/hostedcluster/worker-nodes/#deleting-virtual-machines","title":"Deleting Virtual Machines","text":"<p>To delete the VMs, you simply need to delete the plan, which, in our case, is:</p> <pre><code>kcli delete plan hosted-ipv6\n</code></pre> <pre><code>$ kcli delete plan hosted-ipv6\nAre you sure? [y/N]: y\nhosted-worker0 deleted on local!\nhosted-worker1 deleted on local!\nhosted-worker2 deleted on local!\nPlan hosted-ipv6 deleted!\n</code></pre>"},{"location":"labs/IPv6/hypervisor/","title":"Hypervisor","text":"<p>This section is primarily focused on Virtual Machines. If you are working with real bare metal, you can skip this section and verify whether your laboratory has the necessary components to run the deployment properly. If you find that you've missed something, you can configure it separately to address any gaps.</p>"},{"location":"labs/IPv6/hypervisor/hypervisor-system-config/","title":"Hypervisor system config","text":"<p>This process is primarily intended for development environments. In production environments, it's essential to establish proper rules for the <code>firewalld</code> service and configure SELinux policies to maintain a secure environment.</p> <p>With these instructions, we allow various types of connections through the different virtual networks within the environment:</p> <pre><code>## SELinux\nsed -i s/^SELINUX=.*$/SELINUX=permissive/ /etc/selinux/config; setenforce 0\n\n## Firewalld\nsystemctl disable --now firewalld\n\n## Libvirtd\nsystemctl restart libvirtd\nsystemctl enable --now libvirtd\n</code></pre>"},{"location":"labs/IPv6/hypervisor/network-manager-dispatcher/","title":"Network manager dispatcher","text":"<p>This script modifies the system DNS resolver to prioritize pointing to the <code>dnsmasq</code> service (configured later). This ensures that virtual machines can resolve the various domains, routes, and registries required for the different steps of the process.</p> <p>To enable this, you need to create a script named <code>forcedns</code> in <code>/etc/NetworkManager/dispatcher.d/</code> with the following content:</p> <p>Note</p> <p>Please ensure you modify the appropriate fields to align with your laboratory environment.</p> IPv4IPv6Dual stack <pre><code>#!/bin/bash\n\nexport IP=\"192.168.125.1\"\nexport BASE_RESOLV_CONF=\"/run/NetworkManager/resolv.conf\"\n\nif ! [[ `grep -q \"$IP\" /etc/resolv.conf` ]]; then\nexport TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)\ncp $BASE_RESOLV_CONF $TMP_FILE\nchmod --reference=$BASE_RESOLV_CONF $TMP_FILE\nsed -i -e \"s/hypershiftbm.lab//\" -e \"s/search /&amp; hypershiftbm.lab /\" -e \"0,/nameserver/s/nameserver/&amp; $IP\\n&amp;/\" $TMP_FILE\nmv $TMP_FILE /etc/resolv.conf\nfi\necho \"ok\"\n</code></pre> <pre><code>#!/bin/bash\n\nexport IP=\"2620:52:0:1306::1\"\nexport BASE_RESOLV_CONF=\"/run/NetworkManager/resolv.conf\"\n\nif ! [[ `grep -q \"$IP\" /etc/resolv.conf` ]]; then\nexport TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)\ncp $BASE_RESOLV_CONF $TMP_FILE\nchmod --reference=$BASE_RESOLV_CONF $TMP_FILE\nsed -i -e \"s/hypershiftbm.lab//\" -e \"s/search /&amp; hypershiftbm.lab /\" -e \"0,/nameserver/s/nameserver/&amp; $IP\\n&amp;/\" $TMP_FILE\nmv $TMP_FILE /etc/resolv.conf\nfi\necho \"ok\"\n</code></pre> <pre><code>#!/bin/bash\n\nexport IP=\"192.168.126.1\"\nexport BASE_RESOLV_CONF=\"/run/NetworkManager/resolv.conf\"\n\nif ! [[ `grep -q \"$IP\" /etc/resolv.conf` ]]; then\nexport TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)\ncp $BASE_RESOLV_CONF $TMP_FILE\nchmod --reference=$BASE_RESOLV_CONF $TMP_FILE\nsed -i -e \"s/hypershiftbm.lab//\" -e \"s/search /&amp; hypershiftbm.lab /\" -e \"0,/nameserver/s/nameserver/&amp; $IP\\n&amp;/\" $TMP_FILE\nmv $TMP_FILE /etc/resolv.conf\nfi\necho \"ok\"\n</code></pre> <p>The <code>IP</code> variable at the beginning of the script must be modified to point to the IP address of the Hypervisor's interface hosting the Openshift management cluster.</p> <p>After creating the file, you need to add execution permissions using the command:</p> <pre><code>chmod 755 /etc/NetworkManager/dispatcher.d/forcedns\n</code></pre> <p>Then, execute it once. The output should indicate <code>ok</code>.</p>"},{"location":"labs/IPv6/hypervisor/packaging/","title":"Hypervisor Packaging","text":""},{"location":"labs/IPv6/hypervisor/packaging/#system","title":"System","text":"<p>These are the main packages that are needed to deploy a virtualized Openshift Management cluster.</p> <pre><code>sudo dnf install dnsmasq radvd vim golang podman bind-utils net-tools httpd-tools tree htop strace tmux -y\n</code></pre> <p>Additionally, you need to enable and start the Podman service using the following command:</p> <pre><code>systemctl enable --now podman\n</code></pre>"},{"location":"labs/IPv6/hypervisor/packaging/#kcli","title":"Kcli","text":"<p>We will utilize Kcli to deploy the Openshift Management cluster and various other virtualized components. To do so, you'll need to install and configure the hypervisor using the following commands:</p> <pre><code>sudo yum -y install libvirt libvirt-daemon-driver-qemu qemu-kvm\nsudo usermod -aG qemu,libvirt $(id -un)\nsudo newgrp libvirt\nsudo systemctl enable --now libvirtd\nsudo dnf -y copr enable karmab/kcli\nsudo dnf -y install kcli\nsudo kcli create pool -p /var/lib/libvirt/images default\nkcli create host kvm -H 127.0.0.1 local\nsudo setfacl -m u:$(id -un):rwx /var/lib/libvirt/images\nkcli create network  -c 192.168.122.0/24 default\n</code></pre> <p>For more info about Kcli please visit the official documentation.</p>"},{"location":"labs/IPv6/hypervisor/redfish-for-vms/","title":"Redfish for vms","text":"<p>In a bare metal environment, the preferred approach is to utilize the actual BMC (Baseboard Management Controller) of the nodes used for the management cluster, which can be managed by Metal3 for discovery and provisioning. However, in a virtual environment, this approach is not feasible. As a workaround, we will use <code>ksushy</code>, which is an implementation of <code>sushy-tools</code>, allowing us to simulate BMCs for the virtual machines.</p> <p>To configure <code>ksushy</code> we need to execute these commands:</p> <pre><code>sudo dnf install python3-pyOpenSSL.noarch python3-cherrypy -y\nkcli create sushy-service --ssl --ipv6 --port 9000\nsudo systemctl daemon-reload\nsystemctl enable --now ksushy\n</code></pre> <p>To test if this service is functioning correctly, you can check the service status with <code>systemctl status ksushy</code>. Additionally, you can execute a <code>curl</code> command against the exposed interface:</p> <pre><code>curl -Lk https://[2620:52:0:1305::1]:9000/redfish/v1\n</code></pre>"},{"location":"labs/IPv6/hypervisor/requisites/","title":"Hypervisor Prerequisites","text":"<ul> <li>CPU: The number of CPUs provided determines how many HostedClusters can run concurrently.</li> <li>Recommended: 16 CPUs per Node for 3 nodes.</li> <li> <p>Minimal Dev: In a development environment, you may manage with 12 CPUs per Node for 3 nodes.</p> </li> <li> <p>Memory: The amount of RAM impacts how many HostedClusters can be hosted.</p> </li> <li>Recommended: 48 GB of RAM per Node.</li> <li> <p>Minimal Dev: For minimal development, 18 GB of RAM per Node may suffice.</p> </li> <li> <p>Storage: Using SSD storage for MCE is crucial.</p> </li> <li>Management Cluster: 250 GB.</li> <li>Registry: Depends on the number of releases, operators, and images hosted. An acceptable number could be 500 GB, preferably separated from the disk where the HostedCluster is hosted.</li> <li> <p>Webserver: The required storage depends on the number of ISOs and images hosted. An acceptable number could be 500 GB.</p> </li> <li> <p>Production: For a production environment, it's advisable to keep these three components separated on different disks. A recommended configuration for production is as follows:</p> </li> <li>Registry: 2 TB.</li> <li>Management Cluster: 500 GB.</li> <li>WebServer: 2 TB.</li> </ul>"},{"location":"labs/IPv6/mce/","title":"Index","text":"<p>The Multicluster Engine (MCE) is a component of the ACM bundle. It plays a crucial role in deploying clusters across multiple providers.</p>"},{"location":"labs/IPv6/mce/#credentials-and-authorization","title":"Credentials and Authorization","text":"<p>To prepare for ACM/MCE deployment, certain prerequisites must be met. Additionally, you will need to request permissions to access internal builds. For detailed information, please refer to this documentation.</p> <p>ACM/MCE Deployment Agent Service Config</p>"},{"location":"labs/IPv6/mce/agentserviceconfig/","title":"Agentserviceconfig","text":"<p>The Agent Service Config object is an essential component of the Assisted Service addon included in MCE/ACM, responsible for Baremetal cluster deployment. When the addon is enabled, you must deploy an operand (CRD) named <code>AgentServiceConfig</code> to configure it.</p>"},{"location":"labs/IPv6/mce/agentserviceconfig/#agent-service-config-objects","title":"Agent Service Config Objects","text":"<p>You can find the CRD described here. In this context, we will focus on the main aspects to ensure its functionality in disconnected environments.</p> <p>In addition to configuring the Agent Service Config, to ensure that Multicluster Engine functions properly in a disconnected environment, we need to include some additional ConfigMaps.</p>"},{"location":"labs/IPv6/mce/agentserviceconfig/#custom-registries-configuration","title":"Custom Registries Configuration","text":"<p>This ConfigMap contains the disconnected details necessary to customize the deployment.</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: custom-registries\n  namespace: multicluster-engine\n  labels:\n    app: assisted-service\ndata:\n  ca-bundle.crt: |\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\n  registries.conf: |\n    unqualified-search-registries = [\"registry.access.redhat.com\", \"docker.io\"]\n\n    [[registry]]\n    prefix = \"\"\n    location = \"registry.redhat.io/openshift4\"\n    mirror-by-digest-only = true\n\n    [[registry.mirror]]\n      location = \"registry.hypershiftbm.lab:5000/openshift4\"\n\n    [[registry]]\n    prefix = \"\"\n    location = \"registry.redhat.io/rhacm2\"\n    mirror-by-digest-only = true\n    ...\n    ...\n</code></pre> <p>This object includes two fields:</p> <ol> <li>Custom CAs: This field contains the Certificate Authorities (CAs) that will be loaded into the various processes of the deployment.</li> <li>Registries: The <code>Registries.conf</code> field contains information about images and namespaces that need to be consumed from a mirror registry instead of the original source registry.</li> </ol>"},{"location":"labs/IPv6/mce/agentserviceconfig/#assisted-service-customization","title":"Assisted Service Customization","text":"<p>The Assisted Service Customization ConfigMap is consumed by the Assisted Service operator and contains variables that modify the behavior of the controllers.</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: assisted-service-config\n  namespace: multicluster-engine\ndata:\n  ALLOW_CONVERGED_FLOW: \"false\"\n</code></pre> <p>You can find documentation on how to customize the operator here.</p>"},{"location":"labs/IPv6/mce/agentserviceconfig/#assisted-service-config","title":"Assisted Service Config","text":"<p>The Assisted Service Config object includes the necessary information to ensure the correct functioning of the operator.</p> <pre><code>---\napiVersion: agent-install.openshift.io/v1beta1\nkind: AgentServiceConfig\nmetadata:\n  annotations:\n    unsupported.agent-install.openshift.io/assisted-service-configmap: assisted-service-config\n  name: agent\n  namespace: multicluster-engine\nspec:\n  mirrorRegistryRef:\n    name: custom-registries\n  databaseStorage:\n    storageClassName: lvms-vg1\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 10Gi\n  filesystemStorage:\n    storageClassName: lvms-vg1\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 20Gi\n  osImages:\n  - cpuArchitecture: x86_64\n    openshiftVersion: \"4.14\"\n    rootFSUrl: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202308281054-0-live-rootfs.x86_64.img\n    url: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202308281054-0-live.x86_64.iso\n    version: 414.92.202308281054-0\n</code></pre> <p>If your mirror registry doesn't require authentication in your pull secret, see this document on how to add it to the unauthenticated list in the AgentServiceConfig CR.</p> <p>In this section, we will emphasize the important aspects:</p> <ul> <li>The <code>metadata.annotations[\"unsupported.agent-install.openshift.io/assisted-service-configmap\"]</code> Annotation references the ConfigMap name to be consumed by the operator for customizing behavior.</li> <li>The <code>spec.mirrorRegistryRef.name</code> points to the ConfigMap containing disconnected registry information to be consumed by the Assisted Service Operator. This ConfigMap injects these resources during the deployment process.</li> <li>The <code>spec.osImages</code> field contains different versions available for deployment by this operator. These fields are mandatory.</li> </ul> <p>Let's fill in this section for the 4.14 dev preview ec4 version (sample):</p> <ul> <li>Release URL</li> <li>RHCOS Info</li> </ul> <p>Assuming you've already downloaded the RootFS and LiveIso files:</p> <pre><code>  - cpuArchitecture: x86_64\n    openshiftVersion: \"4.14\"\n    rootFSUrl: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202309101331-0-live-rootfs.x86_64.img\n    url: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202309101331-0-live.x86_64.iso\n    version: 414.92.202307250657-0\n</code></pre>"},{"location":"labs/IPv6/mce/agentserviceconfig/#deployment","title":"Deployment","text":"<p>To deploy all these objects, simply concatenate them into a single file and apply them to the Management Cluster.</p> <pre><code>oc apply -f agentServiceConfig.yaml\n</code></pre> <p>This will trigger 2 pods</p> <pre><code>assisted-image-service-0                               1/1     Running   2             11d\nassisted-service-668b49548-9m7xw                       2/2     Running   5             11d\n</code></pre> <p>Note</p> <p>The <code>Assisted Image Service</code> pod is responsible for creating the RHCOS Boot Image template, which will be customized for each cluster you deploy.</p> <p>The <code>Assisted Service</code> refers to the operator.</p>"},{"location":"labs/IPv6/mce/multicluster-engine/","title":"Multicluster engine","text":"<p>The deployment of each component will depend on your needs, follow the next links accordingly:</p> <ul> <li>ACM Deployment</li> <li>ACM Downstream builds</li> <li>MCE Deployment</li> <li>MCE Upgrade</li> <li>ACM/MCE Cluster deletion</li> <li>MulticlusterEngine deletion</li> <li>Last resource: nuke.sh</li> </ul>"},{"location":"labs/IPv6/mgmt-cluster/","title":"Index","text":""},{"location":"labs/IPv6/mgmt-cluster/#openshift-management-cluster","title":"Openshift Management Cluster","text":"<p>This section contains the necessary artifacts to set up an Openshift management cluster based on virtual machines using kcli as the primary tool. Another option is to use dev-scripts, which uses a different approach.</p> <p>Networking Openshift Compact Dual</p>"},{"location":"labs/IPv6/mgmt-cluster/compact-ipv6/","title":"Compact ipv6","text":"<p>In this section, we will discuss how to deploy the Openshift management cluster. To do that, we need to have the following files in place:</p> <ol> <li>Pull Secret</li> <li>Kcli Plan</li> </ol> <p>Ensure that these files are properly set up for the deployment process.</p>"},{"location":"labs/IPv6/mgmt-cluster/compact-ipv6/#pull-secret","title":"Pull Secret","text":"<p>The Pull Secret should be located in the same folder as the kcli plan and should be named <code>openshift_pull.json</code>.</p>"},{"location":"labs/IPv6/mgmt-cluster/compact-ipv6/#kcli-plan","title":"Kcli plan","text":"<p>The Kcli plan contains the Openshift definition. This is how looks like:</p> <p>Note</p> <p>Please ensure you modify the appropriate fields to align with your laboratory environment.</p> <ul> <li><code>mgmt-compact-hub-ipv6.yaml</code></li> </ul> <pre><code>plan: hub-ipv6\nforce: true\nversion: nightly\ntag: \"4.14.0-0.nightly-2023-08-29-102237\"\ncluster: \"hub-ipv6\"\nipv6: true\ndomain: hypershiftbm.lab\napi_ip: 2620:52:0:1305::2\ningress_ip: 2620:52:0:1305::3\ndisconnected_url: registry.hypershiftbm.lab:5000\ndisconnected_update: true\ndisconnected_user: dummy\ndisconnected_password: dummy\ndisconnected_operators_version: v4.13\ndisconnected_operators:\n- name: metallb-operator\n- name: lvms-operator\n  channels:\n  - name: stable-4.13\ndisconnected_extra_images:\n- quay.io/mavazque/trbsht:latest\n- quay.io/jparrill/hypershift:BMSelfManage-v4.14-rc-v3\n- registry.redhat.io/openshift4/ose-kube-rbac-proxy:v4.10\ndualstack: false\ndisk_size: 200\nextra_disks: [200]\nmemory: 48000\nnumcpus: 16\nctlplanes: 3\nworkers: 0\nmanifests: extra-manifests\nmetal3: true\nnetwork: ipv6\nusers_dev: developer\nusers_devpassword: developer\nusers_admin: admin\nusers_adminpassword: admin\nmetallb_pool: ipv6-virtual-network\nmetallb_ranges:\n- 2620:52:0:1305::150-2620:52:0:1305::190\nmetallb_autoassign: true\napps:\n- users\n- lvms-operator\n- metallb-operator\nvmrules:\n- hub-bootstrap:\n    nets:\n    - name: ipv6\n      mac: aa:aa:aa:aa:03:10\n- hub-ctlplane-0:\n    nets:\n    - name: ipv6\n      mac: aa:aa:aa:aa:03:01\n- hub-ctlplane-1:\n    nets:\n    - name: ipv6\n      mac: aa:aa:aa:aa:03:02\n- hub-ctlplane-2:\n    nets:\n    - name: ipv6\n      mac: aa:aa:aa:aa:03:03\n</code></pre> <p>Note</p> <p>To understand the meaning of each of these parameters, you can refer to the official documentation here.</p>"},{"location":"labs/IPv6/mgmt-cluster/compact-ipv6/#deployment","title":"Deployment","text":"<p>To initiate the provisioning procedure, execute the following:</p> <pre><code>kcli create cluster openshift --pf mgmt-compact-hub-ipv6.yaml\n</code></pre>"},{"location":"labs/IPv6/mgmt-cluster/network/","title":"Network","text":"<p>Firstly, we need to ensure that we have the right networks prepared for use in the Hypervisor. These networks will be used to host both the Management and Hosted clusters.</p> <p>To configure these networks, we will use the following <code>kcli</code> command:</p> <pre><code>kcli create network -c 2620:52:0:1305::0/64 -P dhcp=false -P dns=false --domain hypershiftbm.lab --nodhcp ipv6\n</code></pre> <p>Where:</p> <ul> <li><code>-c</code> remarks the CIDR used for that network</li> <li><code>-P dhcp=false</code> configures the network to disable the DHCP, this will be done by the dnsmasq we've configured before.</li> <li><code>-P dns=false</code> configures the network to disable the DNS, this will be done by the dnsmasq we've configured before.</li> <li><code>--domain</code> sets the domain to search into.</li> <li><code>ipv6</code> is the name of the network that will be created.</li> </ul> <p>This is what the network will look like once created:</p> <pre><code>[root@hypershiftbm ~]# kcli list network\nListing Networks...\n+---------+--------+---------------------+-------+------------------+------+\n| Network |  Type  |         Cidr        |  Dhcp |      Domain      | Mode |\n+---------+--------+---------------------+-------+------------------+------+\n| default | routed |   192.168.122.0/24  |  True |     default      | nat  |\n| ipv6    | routed |   192.168.125.0/24  | False | hypershiftbm.lab | nat  |\n| ipv6    | routed | 2620:52:0:1305::/64 | False | hypershiftbm.lab | nat  |\n+---------+--------+---------------------+-------+------------------+------+\n</code></pre> <pre><code>[root@hypershiftbm ~]# kcli info network ipv6\nProviding information about network ipv6...\ncidr: 2620:52:0:1305::/64\ndhcp: false\ndomain: hypershiftbm.lab\nmode: nat\nplan: kvirt\ntype: routed\n</code></pre>"},{"location":"labs/IPv6/mirror/","title":"Index","text":"<p>Important</p> <p>This section is only relevant in disconnected scenarios. If this does not apply to your situation, you can continue with the next section.</p> <p>Image mirroring is the process of fetching images from external registries, such as registry.redhat.com or quay.io, and storing them in your private registry.</p> <p>To learn how to perform image mirroring, follow the next sections:</p> <p>Image Mirroring Process ICSP and IDMS</p>"},{"location":"labs/IPv6/mirror/ICSP-IDMS/","title":"ICSP IDMS","text":"<p>Once the mirroring process is complete, you will have two main objects that need to be applied in the Management Cluster:</p> <ol> <li>ICSP (Image Content Source Policies) or IDMS (Image Digest Mirror Set).</li> <li>Catalog Sources.</li> </ol> <p>Using the <code>oc-mirror</code> tool, the output artifacts will be located in a new folder called <code>oc-mirror-workspace/results-XXXXXX/</code>.</p> <p>ICSP/IDMS will trigger a \"special\" MachineConfig change that will not reboot your nodes but will reboot the kubelet on each of them.</p> <p>Once all nodes are schedulable and marked as <code>READY</code>, you will need to apply the new catalog sources generated.</p> <p>The catalog sources will trigger some actions in the <code>openshift-marketplace operator</code>, such as downloading the catalog image and processing it to retrieve all the <code>PackageManifests</code> included in that image. You can check the new sources by executing <code>oc get packagemanifest</code> using the new CatalogSource as a source.</p>"},{"location":"labs/IPv6/mirror/ICSP-IDMS/#applying-the-artifacts","title":"Applying the Artifacts","text":"<p>First, we need to create the ICSP/IDMS artifacts:</p> <pre><code>oc apply -f oc-mirror-workspace/results-XXXXXX/imageContentSourcePolicy.yaml\n</code></pre> <p>Now, wait for the nodes to become ready again and execute the following command:</p> <pre><code>oc apply -f catalogSource-XXXXXXXX-index.yaml\n</code></pre>"},{"location":"labs/IPv6/mirror/mirroring/","title":"Mirroring","text":"<p>The mirroring step can take some time to complete, so we recommend starting with this part once the Registry server is up and running.</p> <p>For this purpose, we will use the <code>oc-mirror</code> tool, a binary that utilizes an object called <code>ImageSetConfiguration</code>.</p> <p>In this file, you can specify:</p> <ul> <li>The OpenShift versions to mirror (they should be located in quay.io).</li> <li>The additional operators to mirror, selecting packages individually.</li> <li>The extra images you want to add to the repository.</li> </ul> <p>Note</p> <p>Please ensure you modify the appropriate fields to align with your laboratory environment.</p> <p>Here is an example of the <code>ImageSetConfiguration</code> that we will use for our mirroring:</p> <pre><code>apiVersion: mirror.openshift.io/v1alpha2\nkind: ImageSetConfiguration\nstorageConfig:\n  registry:\n    imageURL: registry.hypershiftbm.lab:5000/openshift/release/metadata:latest\nmirror:\n  platform:\n    channels:\n    - name: candidate-4.14\n      minVersion: 4.14.0-ec.1\n      maxVersion: 4.14.0-ec.3\n      type: ocp\n    graph: true\n  additionalImages:\n  - name: quay.io/karmab/origin-keepalived-ipfailover:latest\n  - name: quay.io/karmab/kubectl:latest\n  - name: quay.io/karmab/haproxy:latest\n  - name: quay.io/karmab/mdns-publisher:latest\n  - name: quay.io/karmab/origin-coredns:latest\n  - name: quay.io/karmab/curl:latest\n  - name: quay.io/karmab/kcli:latest\n  - name: quay.io/mavazque/trbsht:latest\n  - name: quay.io/jparrill/hypershift:BMSelfManage-v4.14-rc-v3\n  - name: registry.redhat.io/openshift4/ose-kube-rbac-proxy:v4.10\n  operators:\n  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13\n    packages:\n    - name: lvms-operator\n    - name: local-storage-operator\n    - name: odf-csi-addons-operator\n    - name: odf-operator\n    - name: mcg-operator\n    - name: ocs-operator\n    - name: metallb-operator\n</code></pre> <p>Make sure you have your <code>${HOME}/.docker/config.json</code> file updated with the registries you are trying to mirror from and your private registry to push the images to.</p> <p>After that, we can begin the mirroring process:</p> <pre><code>oc-mirror --source-skip-tls --config imagesetconfig.yaml docker://${REGISTRY}\n</code></pre> <p>Once the mirror finishes, you will have a new folder called <code>oc-mirror-workspace/results-XXXXXX/</code> which contains the ICSP and the CatalogSources to be applied later on the HostedCluster.</p>"},{"location":"labs/IPv6/mirror/mirroring/#mirroring-nightly-and-ci-releases","title":"Mirroring Nightly and CI releases","text":"<p>The bad part in all of this, we cannot cover nightly or CI versions of Openshift so we will need to use the <code>oc adm release mirror</code> to mirror those versions.</p> <p>To mirror the nightly versions we need for this deployment, you need to execute this:</p> <pre><code>REGISTRY=registry.$(hostname --long):5000\n\noc adm release mirror \\\n  --from=registry.ci.openshift.org/ocp/release:4.14.0-0.nightly-2023-08-29-102237 \\\n  --to=${REGISTRY}/openshift/release \\\n  --to-release-image=${REGISTRY}/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237\n</code></pre> <p>For more detailed and updated information, you can visit the official Documentation or GitHub repository</p>"},{"location":"labs/IPv6/mirror/mirroring/#mirror-mce-internal-releases","title":"Mirror MCE internal releases","text":"<p>In order to mirror all the MCE latest images uploaded to quay.io or if it's internal and you can access the ACM documentation.</p> <ul> <li>Red Hat Official Documentation</li> <li>Red Hat Internal deployment Brew Registry deployment</li> </ul>"},{"location":"labs/IPv6/watching/","title":"Index","text":"<p>This section is purely done to let you know how we should monitor the deployment of a HostedCluster from two perspectives:</p> <ul> <li>Control Plane</li> <li>Data Plane</li> </ul> <p>So please follow these links accordingly.</p> <p>Control Plane perspective Data Plane perspective</p>"},{"location":"labs/IPv6/watching/watching-cp/","title":"Watching cp","text":"<p>Now it's a matter of waiting for the cluster to finish the deployment, so let's take a look at some useful commands on the Management cluster side:</p> <pre><code>export KUBECONFIG=/root/.kcli/clusters/hub-ipv4/auth/kubeconfig\n\nwatch \"oc get pod -n hypershift;echo;echo;oc get pod -n clusters-hosted-ipv4;echo;echo;oc get bmh -A;echo;echo;oc get agent -A;echo;echo;oc get infraenv -A;echo;echo;oc get hostedcluster -A;echo;echo;oc get nodepool -A;echo;echo;\"\n</code></pre> <p>This command will give you info about:</p> <ul> <li>What is the Hypershift Operator status</li> <li>The HostedControlPlane pod status</li> <li>The BareMetalHosts</li> <li>The Agents</li> <li>The Infraenv</li> <li>The HostedCluster and NodePool</li> </ul> <p>This is how it looks:</p> <p></p>"},{"location":"labs/IPv6/watching/watching-dp/","title":"Watching dp","text":"<p>If you check the Hosted cluster side you can check how the Operators are progressing and what is the status. To do that we will use these commands</p> <pre><code>oc get secret -n clusters-hosted-ipv4 admin-kubeconfig -o jsonpath='{.data.kubeconfig}' |base64 -d &gt; /root/hc_admin_kubeconfig.yaml\nexport KUBECONFIG=/root/hc_admin_kubeconfig.yaml\n\nwatch \"oc get clusterversion,nodes,co\"\n</code></pre> <p>This command will give you info about:</p> <ul> <li>Check the clusterversion</li> <li>Check if the Nodes has joined the cluster</li> <li>Check the ClusterOperators</li> </ul> <p>This is how looks like:</p> <p></p>"},{"location":"labs/common/registry/","title":"Registry","text":"<p>Important</p> <p>This section is exclusively applicable to disconnected scenarios. If this does not apply to your situation, please proceed to the next section.</p> <p>In this section, we will explain how to deploy a small, self-hosted registry using a Podman container. For production environments, we strongly recommend utilizing a more reliable solution such as Quay, Nexus, or Artifactory.</p> <ol> <li> <p>As a privileged user, please access the ${HOME} directory and proceed to create the following script provided below. The script will make certain assumptions, including the registry name based on the hypervisor hostname, as well as the necessary credentials and user access:</p> </li> <li> <p><code>registry.sh</code> <pre><code>#!/usr/bin/env bash\n\nset -euo pipefail\n\nPRIMARY_NIC=$(ls -1 /sys/class/net | grep -v podman | head -1)\nexport PATH=/root/bin:$PATH\nexport PULL_SECRET=\"/root/baremetal/hub/openshift_pull.json\"\n\nif [[ ! -f $PULL_SECRET ]];then\n  echo \"Pull Secret not found, exiting...\"\n  exit 1\nfi\n\ndnf -y install podman httpd httpd-tools jq skopeo libseccomp-devel\nexport IP=$(ip -o addr show $PRIMARY_NIC | head -1 | awk '{print $4}' | cut -d'/' -f1)\nREGISTRY_NAME=registry.$(hostname --long)\nREGISTRY_USER=dummy\nREGISTRY_PASSWORD=dummy\nKEY=$(echo -n $REGISTRY_USER:$REGISTRY_PASSWORD | base64)\necho \"{\\\"auths\\\": {\\\"$REGISTRY_NAME:5000\\\": {\\\"auth\\\": \\\"$KEY\\\", \\\"email\\\": \\\"sample-email@domain.ltd\\\"}}}\" &gt; /root/disconnected_pull.json\nmv ${PULL_SECRET} /root/openshift_pull.json.old\njq \".auths += {\\\"$REGISTRY_NAME:5000\\\": {\\\"auth\\\": \\\"$KEY\\\",\\\"email\\\": \\\"sample-email@domain.ltd\\\"}}\" &lt; /root/openshift_pull.json.old &gt; $PULL_SECRET\nmkdir -p /opt/registry/{auth,certs,data,conf}\ncat &lt;&lt;EOF &gt; /opt/registry/conf/config.yml\nversion: 0.1\nlog:\n  fields:\n    service: registry\nstorage:\n  cache:\n    blobdescriptor: inmemory\n  filesystem:\n    rootdirectory: /var/lib/registry\n  delete:\n    enabled: true\nhttp:\n  addr: :5000\n  headers:\n    X-Content-Type-Options: [nosniff]\nhealth:\n  storagedriver:\n    enabled: true\n    interval: 10s\n    threshold: 3\ncompatibility:\n  schema1:\n    enabled: true\nEOF\nopenssl req -newkey rsa:4096 -nodes -sha256 -keyout /opt/registry/certs/domain.key -x509 -days 3650 -out /opt/registry/certs/domain.crt -subj \"/C=US/ST=Madrid/L=San Bernardo/O=Karmalabs/OU=Guitar/CN=$REGISTRY_NAME\" -addext \"subjectAltName=DNS:$REGISTRY_NAME\"\ncp /opt/registry/certs/domain.crt /etc/pki/ca-trust/source/anchors/\nupdate-ca-trust extract\nhtpasswd -bBc /opt/registry/auth/htpasswd $REGISTRY_USER $REGISTRY_PASSWORD\npodman create --name registry --net host --security-opt label=disable --replace -v /opt/registry/data:/var/lib/registry:z -v /opt/registry/auth:/auth:z -v /opt/registry/conf/config.yml:/etc/docker/registry/config.yml -e \"REGISTRY_AUTH=htpasswd\" -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry\" -e \"REGISTRY_HTTP_SECRET=ALongRandomSecretForRegistry\" -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd -v /opt/registry/certs:/certs:z -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key docker.io/library/registry:latest\n[ \"$?\" == \"0\" ] || !!\nsystemctl enable --now registry\n</code></pre></p> </li> <li> <p>Kindly modify the location of the <code>PULL_SECRET</code> to point to the correct location.</p> </li> <li>Please adjust the permissions by adding the execution flag as follows: <code>chmod u+x ${HOME}/registry.sh</code></li> <li>You can execute the script without any parameters using the following command <code>${HOME}/registry.sh</code></li> </ol> <p>After executing the script <code>${HOME}/registry.sh</code>, the server should now be up and running. If the script has been properly configured and executed without errors, it should have started the server as intended.</p> <p>It will utilize a systemd service for management purposes. So, if you ever need to manage it, you can employ systemctl status/start/stop registry.</p> <p>The root folder for the registry is situated at /opt/registry, and it's structured as follows:</p> <ul> <li><code>certs</code> holds the TLS certificates.</li> <li><code>auth</code> keeps the credentials.</li> <li><code>data</code> contains the registry images.</li> <li><code>conf</code> will hold the registry configuration.</li> </ul>"},{"location":"labs/common/tls-certificates/","title":"Tls certificates","text":"<p>Important</p> <p>This section is only relevant in disconnected scenarios. If this doesn't apply to your situation, please proceed to the next section.</p> <p>In this section, we'll cover the TLS certificates involved in the process, primarily focusing on the private registries from which the images will be pulled. While there may be additional certificates, we'll concentrate on these particular ones.</p> <p>It's important to distinguish between the various methods and their impact on the associated cluster. All of these methods essentially modify the content of the following files on the OCP (OpenShift Container Platform) control plane (Master nodes) and data plane (worker nodes):</p> <ul> <li><code>/etc/pki/ca-trust/extracted/pem/</code></li> <li><code>/etc/pki/ca-trust/source/anchors/</code></li> <li><code>/etc/pki/tls/certs/</code></li> </ul>"},{"location":"labs/common/tls-certificates/#adding-a-ca-to-the-management-cluster","title":"Adding a CA to the Management Cluster","text":"<p>There exist numerous methods to accomplish this within the OpenShift environment. However, we have chosen to integrate the less intrusive approach.</p> <ol> <li>Initially, you must create a ConfigMap with a name of your choosing. In our specific case, we will utilize the name <code>registry-config</code> The content should resemble the following:</li> </ol> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: registry-config\n  namespace: openshift-config\ndata:\n  registry.hypershiftbm.lab..5000: |\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\n</code></pre> <p>Note</p> <pre><code>The data field ought to contain the registry name, while the value should encompass the Registry certificate. As is evident, the \":\" character is being replaced by \"..\"; therefore, it is imperative to ensure this correction.\n</code></pre> <ol> <li>Now we need to patch the clusterwide object <code>image.config.openshift.io</code> including this:</li> </ol> <pre><code>spec:\n  additionalTrustedCA:\n    name: registry-config\n</code></pre> <p>This modification will result in two significant consequences:</p> <ul> <li>Granting masters the capability to retrieve images from the private registry.</li> <li>Allowing the Hypershift Operator to extract the Openshift payload for the HostedCluster deployments.</li> </ul> <p>Note</p> <pre><code>The modification required several minutes to be successfully executed.\n</code></pre>"},{"location":"labs/common/tls-certificates/#alternative-adding-a-ca-to-the-management-cluster","title":"Alternative: Adding a CA to the Management Cluster","text":"<p>We consider this as an alternative, given that it entails the masters undergoing a reboot facilitated by the Machine Config Operator.</p> <p>It's described here. This method involves utilizing the <code>image-registry-operator</code>, which deploys the CAs to the OCP nodes.</p> <p>Hypershift's operators and controllers automatically handle this process, so if you're using a GA (Generally Available) released version, it should work seamlessly, and you won't need to apply these steps. This Hypershift feature is included in the payload of the 2.4 MCE release.</p> <p>However, if this feature is not working as expected or if it doesn't apply to your situation, you can follow this procedure:</p> <ul> <li>Check if the <code>openshift-config</code> namespace in the Management cluster contains a ConfigMap named <code>user-ca-bundle</code>.</li> <li>If the ConfigMap doesn't exist, execute the following command:</li> </ul> <pre><code>## REGISTRY_CERT_PATH=&lt;PATH/TO/YOUR/CERTIFICATE/FILE&gt;\nexport REGISTRY_CERT_PATH=/opt/registry/certs/domain.crt\n\noc create configmap user-ca-bundle -n openshift-config --from-file=ca-bundle.crt=${REGISTRY_CERT_PATH}\n</code></pre> <ul> <li>Otherwise, if that ConfigMap exists, execute this other command:</li> </ul> <pre><code>## REGISTRY_CERT_PATH=&lt;PATH/TO/YOUR/CERTIFICATE/FILE&gt;\nexport REGISTRY_CERT_PATH=/opt/registry/certs/domain.crt\nexport TMP_FILE=$(mktemp)\n\noc get cm -n openshift-config user-ca-bundle -ojsonpath='{.data.ca-bundle\\.crt}' &gt; ${TMP_FILE}\necho &gt;&gt; ${TMP_FILE}\necho \\#registry.$(hostname --long) &gt;&gt; ${TMP_FILE}\ncat ${REGISTRY_CERT_PATH} &gt;&gt; ${TMP_FILE}\noc create configmap user-ca-bundle -n openshift-config --from-file=ca-bundle.crt=${TMP_FILE} --dry-run=client -o yaml | kubectl apply -f -\n</code></pre> <p>You have a functional script located in the <code>assets/&lt;NetworkStack&gt;/09-tls-certificates/01-config.sh</code>, this is the sample for IPv6.</p>"},{"location":"labs/common/webserver/","title":"Webserver","text":"<p>Important</p> <p>This section is only relevant in disconnected scenarios, if this is not your case, you can continue with the next section.</p> <p>This section talks about an additional webserver that you need to configure to host the RHCOS images associated with the Openshift release you are trying to deploy as a HostedCluster.</p> <p>The script refers to this repository folder and it's the same for all three different network stacks.</p> <p>To do this, you can use this script:</p> <pre><code>#!/bin/bash\n\nWEBSRV_FOLDER=/opt/srv\nROOTFS_IMG_URL=\"$(../04-management-cluster/openshift-install coreos print-stream-json | jq -r '.architectures.x86_64.artifacts.metal.formats.pxe.rootfs.location')\"\nLIVE_ISO_URL=\"$(../04-management-cluster/openshift-install coreos print-stream-json | jq -r '.architectures.x86_64.artifacts.metal.formats.iso.disk.location')\"\n\nmkdir -p ${WEBSRV_FOLDER}/images\ncurl -Lk ${ROOTFS_IMG_URL} -o ${WEBSRV_FOLDER}/images/${ROOTFS_IMG_URL##*/}\ncurl -Lk ${LIVE_ISO_URL} -o ${WEBSRV_FOLDER}/images/${LIVE_ISO_URL##*/}\nchmod -R 755 ${WEBSRV_FOLDER}/*\n\n## Run Webserver\npodman ps --noheading | grep -q websrv-ai\nif [[ $? == 0 ]];then\n    echo \"Launching Registry pod...\"\n    /usr/bin/podman run --name websrv-ai --net host -v /opt/srv:/usr/local/apache2/htdocs:z quay.io/alosadag/httpd:p8080\nfi\n</code></pre> <p>The script will create a folder under <code>/opt/srv</code>. This folder will contain the <code>images</code> for RHCOS provision in the worker nodes. To be more concrete, we need the <code>RootFS</code> and <code>LiveISO</code> artifacts found on the Openshift CI Release page.</p> <p>After the download, a container will run to host the images under a webserver. It uses a variation of the official httpd image, which also allows it to work with IPv6.</p>"},{"location":"labs/common/hypervisor/hypervisor-system-config/","title":"Hypervisor system config","text":"<p>This process is primarily intended for development environments. In production environments, it's essential to establish proper rules for the <code>firewalld</code> service and configure SELinux policies to maintain a secure environment.</p> <p>With these instructions, we allow various types of connections through the different virtual networks within the environment:</p> <pre><code>## SELinux\nsed -i s/^SELINUX=.*$/SELINUX=permissive/ /etc/selinux/config; setenforce 0\n\n## Firewalld\nsystemctl disable --now firewalld\n\n## Libvirtd\nsystemctl restart libvirtd\nsystemctl enable --now libvirtd\n</code></pre>"},{"location":"labs/common/hypervisor/network-manager-dispatcher/","title":"Network manager dispatcher","text":"<p>This script modifies the system DNS resolver to prioritize pointing to the <code>dnsmasq</code> service (configured later). This ensures that virtual machines can resolve the various domains, routes, and registries required for the different steps of the process.</p> <p>To enable this, you need to create a script named <code>forcedns</code> in <code>/etc/NetworkManager/dispatcher.d/</code> with the following content:</p> <p>Note</p> <p>Please ensure you modify the appropriate fields to align with your laboratory environment.</p> IPv4IPv6Dual stack <pre><code>#!/bin/bash\n\nexport IP=\"192.168.125.1\"\nexport BASE_RESOLV_CONF=\"/run/NetworkManager/resolv.conf\"\n\nif ! [[ `grep -q \"$IP\" /etc/resolv.conf` ]]; then\nexport TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)\ncp $BASE_RESOLV_CONF $TMP_FILE\nchmod --reference=$BASE_RESOLV_CONF $TMP_FILE\nsed -i -e \"s/hypershiftbm.lab//\" -e \"s/search /&amp; hypershiftbm.lab /\" -e \"0,/nameserver/s/nameserver/&amp; $IP\\n&amp;/\" $TMP_FILE\nmv $TMP_FILE /etc/resolv.conf\nfi\necho \"ok\"\n</code></pre> <pre><code>#!/bin/bash\n\nexport IP=\"2620:52:0:1306::1\"\nexport BASE_RESOLV_CONF=\"/run/NetworkManager/resolv.conf\"\n\nif ! [[ `grep -q \"$IP\" /etc/resolv.conf` ]]; then\nexport TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)\ncp $BASE_RESOLV_CONF $TMP_FILE\nchmod --reference=$BASE_RESOLV_CONF $TMP_FILE\nsed -i -e \"s/hypershiftbm.lab//\" -e \"s/search /&amp; hypershiftbm.lab /\" -e \"0,/nameserver/s/nameserver/&amp; $IP\\n&amp;/\" $TMP_FILE\nmv $TMP_FILE /etc/resolv.conf\nfi\necho \"ok\"\n</code></pre> <pre><code>#!/bin/bash\n\nexport IP=\"192.168.126.1\"\nexport BASE_RESOLV_CONF=\"/run/NetworkManager/resolv.conf\"\n\nif ! [[ `grep -q \"$IP\" /etc/resolv.conf` ]]; then\nexport TMP_FILE=$(mktemp /etc/forcedns_resolv.conf.XXXXXX)\ncp $BASE_RESOLV_CONF $TMP_FILE\nchmod --reference=$BASE_RESOLV_CONF $TMP_FILE\nsed -i -e \"s/hypershiftbm.lab//\" -e \"s/search /&amp; hypershiftbm.lab /\" -e \"0,/nameserver/s/nameserver/&amp; $IP\\n&amp;/\" $TMP_FILE\nmv $TMP_FILE /etc/resolv.conf\nfi\necho \"ok\"\n</code></pre> <p>The <code>IP</code> variable at the beginning of the script must be modified to point to the IP address of the Hypervisor's interface hosting the Openshift management cluster.</p> <p>After creating the file, you need to add execution permissions using the command:</p> <pre><code>chmod 755 /etc/NetworkManager/dispatcher.d/forcedns\n</code></pre> <p>Then, execute it once. The output should indicate <code>ok</code>.</p>"},{"location":"labs/common/hypervisor/packaging/","title":"Hypervisor Packaging","text":""},{"location":"labs/common/hypervisor/packaging/#system","title":"System","text":"<p>These are the main packages that are needed to deploy a virtualized Openshift Management cluster.</p> <pre><code>sudo dnf install dnsmasq radvd vim golang podman bind-utils net-tools httpd-tools tree htop strace tmux -y\n</code></pre> <p>Additionally, you need to enable and start the Podman service using the following command:</p> <pre><code>systemctl enable --now podman\n</code></pre>"},{"location":"labs/common/hypervisor/packaging/#kcli","title":"Kcli","text":"<p>We will utilize Kcli to deploy the Openshift Management cluster and various other virtualized components. To do so, you'll need to install and configure the hypervisor using the following commands:</p> <pre><code>sudo yum -y install libvirt libvirt-daemon-driver-qemu qemu-kvm\nsudo usermod -aG qemu,libvirt $(id -un)\nsudo newgrp libvirt\nsudo systemctl enable --now libvirtd\nsudo dnf -y copr enable karmab/kcli\nsudo dnf -y install kcli\nsudo kcli create pool -p /var/lib/libvirt/images default\nkcli create host kvm -H 127.0.0.1 local\nsudo setfacl -m u:$(id -un):rwx /var/lib/libvirt/images\nkcli create network  -c 192.168.122.0/24 default\n</code></pre> <p>For more info about Kcli please visit the official documentation.</p>"},{"location":"labs/common/hypervisor/requisites/","title":"Hypervisor Prerequisites","text":"<ul> <li>CPU: The number of CPUs provided determines how many HostedClusters can run concurrently.</li> <li>Recommended: 16 CPUs per Node for 3 nodes.</li> <li> <p>Minimal Dev: In a development environment, you may manage with 12 CPUs per Node for 3 nodes.</p> </li> <li> <p>Memory: The amount of RAM impacts how many HostedClusters can be hosted.</p> </li> <li>Recommended: 48 GB of RAM per Node.</li> <li> <p>Minimal Dev: For minimal development, 18 GB of RAM per Node may suffice.</p> </li> <li> <p>Storage: Using SSD storage for MCE is crucial.</p> </li> <li>Management Cluster: 250 GB.</li> <li>Registry: Depends on the number of releases, operators, and images hosted. An acceptable number could be 500 GB, preferably separated from the disk where the HostedCluster is hosted.</li> <li> <p>Webserver: The required storage depends on the number of ISOs and images hosted. An acceptable number could be 500 GB.</p> </li> <li> <p>Production: For a production environment, it's advisable to keep these three components separated on different disks. A recommended configuration for production is as follows:</p> </li> <li>Registry: 2 TB.</li> <li>Management Cluster: 500 GB.</li> <li>WebServer: 2 TB.</li> </ul>"},{"location":"labs/common/mce/","title":"Index","text":"<p>The Multicluster Engine (MCE) is a component of the ACM bundle. It plays a crucial role in deploying clusters across multiple providers.</p>"},{"location":"labs/common/mce/#credentials-and-authorization","title":"Credentials and Authorization","text":"<p>To prepare for ACM/MCE deployment, certain prerequisites must be met. Additionally, you will need to request permissions to access internal builds. For detailed information, please refer to this documentation.</p> <p>ACM/MCE Deployment Agent Service Config</p>"},{"location":"labs/common/mce/agentserviceconfig/","title":"Agentserviceconfig","text":"<p>The Agent Service Config object is an essential component of the Assisted Service addon included in MCE/ACM, responsible for Baremetal cluster deployment. When the addon is enabled, you must deploy an operand (CRD) named <code>AgentServiceConfig</code> to configure it.</p>"},{"location":"labs/common/mce/agentserviceconfig/#agent-service-config-objects","title":"Agent Service Config Objects","text":"<p>You can find the CRD described here. In this context, we will focus on the main aspects to ensure its functionality in disconnected environments.</p> <p>In addition to configuring the Agent Service Config, to ensure that Multicluster Engine functions properly in a disconnected environment, we need to include some additional ConfigMaps.</p>"},{"location":"labs/common/mce/agentserviceconfig/#custom-registries-configuration","title":"Custom Registries Configuration","text":"<p>This ConfigMap contains the disconnected details necessary to customize the deployment.</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: custom-registries\n  namespace: multicluster-engine\n  labels:\n    app: assisted-service\ndata:\n  ca-bundle.crt: |\n    -----BEGIN CERTIFICATE-----\n    -----END CERTIFICATE-----\n  registries.conf: |\n    unqualified-search-registries = [\"registry.access.redhat.com\", \"docker.io\"]\n\n    [[registry]]\n    prefix = \"\"\n    location = \"registry.redhat.io/openshift4\"\n    mirror-by-digest-only = true\n\n    [[registry.mirror]]\n      location = \"registry.hypershiftbm.lab:5000/openshift4\"\n\n    [[registry]]\n    prefix = \"\"\n    location = \"registry.redhat.io/rhacm2\"\n    mirror-by-digest-only = true\n    ...\n    ...\n</code></pre> <p>This object includes two fields:</p> <ol> <li>Custom CAs: This field contains the Certificate Authorities (CAs) that will be loaded into the various processes of the deployment.</li> <li>Registries: The <code>Registries.conf</code> field contains information about images and namespaces that need to be consumed from a mirror registry instead of the original source registry.</li> </ol>"},{"location":"labs/common/mce/agentserviceconfig/#assisted-service-customization","title":"Assisted Service Customization","text":"<p>The Assisted Service Customization ConfigMap is consumed by the Assisted Service operator and contains variables that modify the behavior of the controllers.</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: assisted-service-config\n  namespace: multicluster-engine\ndata:\n  ALLOW_CONVERGED_FLOW: \"false\"\n</code></pre> <p>You can find documentation on how to customize the operator here.</p>"},{"location":"labs/common/mce/agentserviceconfig/#assisted-service-config","title":"Assisted Service Config","text":"<p>The Assisted Service Config object includes the necessary information to ensure the correct functioning of the operator.</p> <pre><code>---\napiVersion: agent-install.openshift.io/v1beta1\nkind: AgentServiceConfig\nmetadata:\n  annotations:\n    unsupported.agent-install.openshift.io/assisted-service-configmap: assisted-service-config\n  name: agent\n  namespace: multicluster-engine\nspec:\n  mirrorRegistryRef:\n    name: custom-registries\n  databaseStorage:\n    storageClassName: lvms-vg1\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 10Gi\n  filesystemStorage:\n    storageClassName: lvms-vg1\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 20Gi\n  osImages:\n  - cpuArchitecture: x86_64\n    openshiftVersion: \"4.14\"\n    rootFSUrl: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202308281054-0-live-rootfs.x86_64.img\n    url: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202308281054-0-live.x86_64.iso\n    version: 414.92.202308281054-0\n</code></pre> <p>If your mirror registry doesn't require authentication in your pull secret, see this document on how to add it to the unauthenticated list in the AgentServiceConfig CR.</p> <p>In this section, we will emphasize the important aspects:</p> <ul> <li>The <code>metadata.annotations[\"unsupported.agent-install.openshift.io/assisted-service-configmap\"]</code> Annotation references the ConfigMap name to be consumed by the operator for customizing behavior.</li> <li>The <code>spec.mirrorRegistryRef.name</code> points to the ConfigMap containing disconnected registry information to be consumed by the Assisted Service Operator. This ConfigMap injects these resources during the deployment process.</li> <li>The <code>spec.osImages</code> field contains different versions available for deployment by this operator. These fields are mandatory.</li> </ul> <p>Let's fill in this section for the 4.14 dev preview ec4 version (sample):</p> <ul> <li>Release URL</li> <li>RHCOS Info</li> </ul> <p>Assuming you've already downloaded the RootFS and LiveIso files:</p> <pre><code>  - cpuArchitecture: x86_64\n    openshiftVersion: \"4.14\"\n    rootFSUrl: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202309101331-0-live-rootfs.x86_64.img\n    url: http://registry.hypershiftbm.lab:8080/images/rhcos-414.92.202309101331-0-live.x86_64.iso\n    version: 414.92.202307250657-0\n</code></pre>"},{"location":"labs/common/mce/agentserviceconfig/#deployment","title":"Deployment","text":"<p>To deploy all these objects, simply concatenate them into a single file and apply them to the Management Cluster.</p> <pre><code>oc apply -f agentServiceConfig.yaml\n</code></pre> <p>This will trigger 2 pods</p> <pre><code>assisted-image-service-0                               1/1     Running   2             11d\nassisted-service-668b49548-9m7xw                       2/2     Running   5             11d\n</code></pre> <p>Note</p> <p>The <code>Assisted Image Service</code> pod is responsible for creating the RHCOS Boot Image template, which will be customized for each cluster you deploy.</p> <p>The <code>Assisted Service</code> refers to the operator.</p>"},{"location":"labs/common/mce/multicluster-engine/","title":"Multicluster engine","text":"<p>The deployment of each component will depend on your needs, follow the next links accordingly:</p> <ul> <li>ACM Deployment</li> <li>ACM Downstream builds</li> <li>MCE Deployment</li> <li>MCE Upgrade</li> <li>ACM/MCE Cluster deletion</li> <li>MulticlusterEngine deletion</li> <li>Last resource: nuke.sh</li> </ul>"},{"location":"labs/common/mirror/","title":"Index","text":"<p>Important</p> <p>This section is only relevant in disconnected scenarios. If this does not apply to your situation, you can continue with the next section.</p> <p>Image mirroring is the process of fetching images from external registries, such as registry.redhat.com or quay.io, and storing them in your private registry.</p> <p>To learn how to perform image mirroring, follow the next sections:</p> <p>Image Mirroring Process ICSP and IDMS</p>"},{"location":"labs/common/mirror/ICSP-IDMS/","title":"ICSP IDMS","text":"<p>Once the mirroring process is complete, you will have two main objects that need to be applied in the Management Cluster:</p> <ol> <li>ICSP (Image Content Source Policies) or IDMS (Image Digest Mirror Set).</li> <li>Catalog Sources.</li> </ol> <p>Using the <code>oc-mirror</code> tool, the output artifacts will be located in a new folder called <code>oc-mirror-workspace/results-XXXXXX/</code>.</p> <p>ICSP/IDMS will trigger a \"special\" MachineConfig change that will not reboot your nodes but will reboot the kubelet on each of them.</p> <p>Once all nodes are schedulable and marked as <code>READY</code>, you will need to apply the new catalog sources generated.</p> <p>The catalog sources will trigger some actions in the <code>openshift-marketplace operator</code>, such as downloading the catalog image and processing it to retrieve all the <code>PackageManifests</code> included in that image. You can check the new sources by executing <code>oc get packagemanifest</code> using the new CatalogSource as a source.</p>"},{"location":"labs/common/mirror/ICSP-IDMS/#applying-the-artifacts","title":"Applying the Artifacts","text":"<p>First, we need to create the ICSP/IDMS artifacts:</p> <pre><code>oc apply -f oc-mirror-workspace/results-XXXXXX/imageContentSourcePolicy.yaml\n</code></pre> <p>Now, wait for the nodes to become ready again and execute the following command:</p> <pre><code>oc apply -f catalogSource-XXXXXXXX-index.yaml\n</code></pre>"},{"location":"labs/common/mirror/mirroring/","title":"Mirroring","text":"<p>The mirroring step can take some time to complete, so we recommend starting with this part once the Registry server is up and running.</p> <p>For this purpose, we will use the <code>oc-mirror</code> tool, a binary that utilizes an object called <code>ImageSetConfiguration</code>.</p> <p>In this file, you can specify:</p> <ul> <li>The OpenShift versions to mirror (they should be located in quay.io).</li> <li>The additional operators to mirror, selecting packages individually.</li> <li>The extra images you want to add to the repository.</li> </ul> <p>Note</p> <p>Please ensure you modify the appropriate fields to align with your laboratory environment.</p> <p>Here is an example of the <code>ImageSetConfiguration</code> that we will use for our mirroring:</p> <pre><code>apiVersion: mirror.openshift.io/v1alpha2\nkind: ImageSetConfiguration\nstorageConfig:\n  registry:\n    imageURL: registry.hypershiftbm.lab:5000/openshift/release/metadata:latest\nmirror:\n  platform:\n    channels:\n    - name: candidate-4.14\n      minVersion: 4.14.0-ec.1\n      maxVersion: 4.14.0-ec.3\n      type: ocp\n    graph: true\n  additionalImages:\n  - name: quay.io/karmab/origin-keepalived-ipfailover:latest\n  - name: quay.io/karmab/kubectl:latest\n  - name: quay.io/karmab/haproxy:latest\n  - name: quay.io/karmab/mdns-publisher:latest\n  - name: quay.io/karmab/origin-coredns:latest\n  - name: quay.io/karmab/curl:latest\n  - name: quay.io/karmab/kcli:latest\n  - name: quay.io/mavazque/trbsht:latest\n  - name: quay.io/jparrill/hypershift:BMSelfManage-v4.14-rc-v3\n  - name: registry.redhat.io/openshift4/ose-kube-rbac-proxy:v4.10\n  operators:\n  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13\n    packages:\n    - name: lvms-operator\n    - name: local-storage-operator\n    - name: odf-csi-addons-operator\n    - name: odf-operator\n    - name: mcg-operator\n    - name: ocs-operator\n    - name: metallb-operator\n</code></pre> <p>Make sure you have your <code>${HOME}/.docker/config.json</code> file updated with the registries you are trying to mirror from and your private registry to push the images to.</p> <p>After that, we can begin the mirroring process:</p> <pre><code>oc-mirror --source-skip-tls --config imagesetconfig.yaml docker://${REGISTRY}\n</code></pre> <p>Once the mirror finishes, you will have a new folder called <code>oc-mirror-workspace/results-XXXXXX/</code> which contains the ICSP and the CatalogSources to be applied later on the HostedCluster.</p>"},{"location":"labs/common/mirror/mirroring/#mirroring-nightly-and-ci-releases","title":"Mirroring Nightly and CI releases","text":"<p>The bad part in all of this, we cannot cover nightly or CI versions of Openshift so we will need to use the <code>oc adm release mirror</code> to mirror those versions.</p> <p>To mirror the nightly versions we need for this deployment, you need to execute this:</p> <pre><code>REGISTRY=registry.$(hostname --long):5000\n\noc adm release mirror \\\n  --from=registry.ci.openshift.org/ocp/release:4.14.0-0.nightly-2023-08-29-102237 \\\n  --to=${REGISTRY}/openshift/release \\\n  --to-release-image=${REGISTRY}/openshift/release-images:4.14.0-0.nightly-2023-08-29-102237\n</code></pre> <p>For more detailed and updated information, you can visit the official Documentation or GitHub repository</p>"},{"location":"labs/common/mirror/mirroring/#mirror-mce-internal-releases","title":"Mirror MCE internal releases","text":"<p>In order to mirror all the MCE latest images uploaded to quay.io or if it's internal and you can access the ACM documentation.</p> <ul> <li>Red Hat Official Documentation</li> <li>Red Hat Internal deployment Brew Registry deployment</li> </ul>"},{"location":"labs/common/watching/","title":"Index","text":"<p>This section is purely done to let you know how we should monitor the deployment of a HostedCluster from two perspectives:</p> <ul> <li>Control Plane</li> <li>Data Plane</li> </ul> <p>So please follow these links accordingly.</p> <p>Control Plane perspective Data Plane perspective</p>"},{"location":"labs/common/watching/watching-cp/","title":"Watching cp","text":"<p>Now it's a matter of waiting for the cluster to finish the deployment, so let's take a look at some useful commands on the Management cluster side:</p> <pre><code>export KUBECONFIG=/root/.kcli/clusters/hub-ipv4/auth/kubeconfig\n\nwatch \"oc get pod -n hypershift;echo;echo;oc get pod -n clusters-hosted-ipv4;echo;echo;oc get bmh -A;echo;echo;oc get agent -A;echo;echo;oc get infraenv -A;echo;echo;oc get hostedcluster -A;echo;echo;oc get nodepool -A;echo;echo;\"\n</code></pre> <p>This command will give you info about:</p> <ul> <li>What is the Hypershift Operator status</li> <li>The HostedControlPlane pod status</li> <li>The BareMetalHosts</li> <li>The Agents</li> <li>The Infraenv</li> <li>The HostedCluster and NodePool</li> </ul> <p>This is how it looks:</p> <p></p>"},{"location":"labs/common/watching/watching-dp/","title":"Watching dp","text":"<p>If you check the Hosted cluster side you can check how the Operators are progressing and what is the status. To do that we will use these commands</p> <pre><code>oc get secret -n clusters-hosted-ipv4 admin-kubeconfig -o jsonpath='{.data.kubeconfig}' |base64 -d &gt; /root/hc_admin_kubeconfig.yaml\nexport KUBECONFIG=/root/hc_admin_kubeconfig.yaml\n\nwatch \"oc get clusterversion,nodes,co\"\n</code></pre> <p>This command will give you info about:</p> <ul> <li>Check the clusterversion</li> <li>Check if the Nodes has joined the cluster</li> <li>Check the ClusterOperators</li> </ul> <p>This is how looks like:</p> <p></p>"},{"location":"recipes/","title":"Recipes","text":"<p>In this section we will expose the more frequent recipes the people could use for different use cases, separated by providers.</p>"},{"location":"recipes/common/exposing-dataplane-with-metallb/","title":"Exposing dataplane with metallb","text":""},{"location":"recipes/common/exposing-dataplane-with-metallb/#configure-metallb-for-hostedclusters-data-plane","title":"Configure MetalLB for HostedCluster's Data Plane","text":"<ul> <li>Deploy the MetalLB Operator using the OLM, applying this manifest or using the UI Console:</li> </ul> <pre><code>apiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: metallb-operator\n  namespace: openshift-operators\nspec:\n  channel: \"stable\"\n  name: metallb-operator\n  source: redhat-operators\n  sourceNamespace: openshift-marketplace\n  installPlanApproval: Automatic\n</code></pre> <ul> <li>Deploy the MetalLB CR:</li> </ul> <pre><code>---\napiVersion: metallb.io/v1beta1\nkind: MetalLB\nmetadata:\n  name: metallb\n  namespace: openshift-operators\n</code></pre> <p>This deploys the metallb-controller-manager and the webhook-server.</p> <ul> <li>Configure the IPAddressPool and the L2Advertisement:</li> </ul> <pre><code>---\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: lab-network\n  namespace: openshift-operators\nspec:\n  autoAssign: true\n  addresses:\n  - 192.168.126.160-192.168.126.165\n  - 2620:52:0:1306::160-2620:52:0:1306::169\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: advertise-lab-network\n  namespace: openshift-operators\nspec:\n  ipAddressPools:\n  - lab-network\n</code></pre> <p>Note</p> <p>The sample config is based on a DualStack layout. If your deployment uses only one stack, specify the IPAddressPool for that stack.</p> <ul> <li>Expose the OpenShift service. This is usually done in both the Control Plane for MGMT configuration and the Data Plane to configure the Ingress:</li> </ul> <pre><code>kind: Service\napiVersion: v1\nmetadata:\n  annotations:\n    metallb.universe.tf/address-pool: lab-network\n  name: metallb-ingress\n  namespace: openshift-ingress\nspec:\n  ipFamilies:\n  - IPv4\n  - IPv6\n  ipFamilyPolicy: PreferDualStack\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 80\n    - name: https\n      protocol: TCP\n      port: 443\n      targetPort: 443\n  selector:\n    ingresscontroller.operator.openshift.io/deployment-ingresscontroller: default\n  type: LoadBalancer\n</code></pre> <p>Note</p> <p>The sample config is based on a DualStack layout. If your deployment uses only one stack, specify the ipFamilies for that stack and modify the ipFamilyPolicy accordingly.</p> <p>The usual configuration for the Hosted Cluster in the BareMetal case is a mix between Route and LoadBalancer strategies:</p> <pre><code>spec:\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      type: LoadBalancer\n  - service: OAuthServer\n    servicePublishingStrategy:\n      type: Route\n  - service: OIDC\n    servicePublishingStrategy:\n      type: Route\n      Route:\n        hostname: &lt;URL&gt;\n  - service: Konnectivity\n    servicePublishingStrategy:\n      type: Route\n  - service: Ignition\n    servicePublishingStrategy:\n      type: Route\n</code></pre> <p>This way, the API server is configured as a LoadBalancer, and the rest of the services are exposed via Route.</p> <p>Note</p> <p>You can specify a specific URL to expose the service you want.</p>"},{"location":"recipes/common/replace-crio-runtime/","title":"Replacing the Worker Node's CRI-O Runtime","text":"<p>Starting with OpenShift version 4.18, the Machine Config Operator (MCO) will switch the default runtime from <code>runc</code> to <code>crun</code>, simplifying much of the internals, crun is a simplified implementation of runc written in C. You can find more information about this change here.</p>"},{"location":"recipes/common/replace-crio-runtime/#switching-from-runc-to-crun","title":"Switching from <code>runc</code> to <code>crun</code>","text":"<p>The most common use case is when someone using OpenShift 4.16 or 4.17 wants to test their workload performance with <code>crun</code> before upgrading to 4.18, where <code>crun</code> will become the default runtime.</p> <p>Ensure you follow these steps to make the change.</p>"},{"location":"recipes/common/replace-crio-runtime/#creating-the-machineconfig-patch-for-the-nodepool","title":"Creating the MachineConfig patch for the NodePool","text":"<p>The following manifest sets <code>crun</code> as the default runtime.</p> HCP Regular Workloads (CRUN)HCP High Performance Workloads (CRUN) <ul> <li>Put the crio configuration into a file</li> </ul> <pre><code>cat &lt;&lt;EOF &gt; crio-config.conf\n[crio.runtime]\ndefault_runtime=\"crun\"\n\n[crio.runtime.runtimes.crun]\nruntime_path = \"/bin/crun\"\nruntime_type = \"oci\"\nruntime_root = \"/run/crun\"\nEOF\n</code></pre> <ul> <li>Get the base64 hash of the file content</li> </ul> <pre><code>export CONFIG_HASH=$(cat crio-config.conf | base64 -w0)\n</code></pre> <ul> <li>Create the MachineConfig file with content in Base64</li> </ul> <pre><code>cat &lt;&lt;EOF &gt; mc-crun-default-runtime.yaml\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  name: 60-crun-default-runtime\nspec:\n  config:\n    ignition:\n      version: 3.2.0\n    storage:\n      files:\n      - contents:\n          source: data:text/plain;charset=utf-8;base64,${CONFIG_HASH}\n        mode: 420\n        path: /etc/crio/crio.conf.d/99-crun-runtime.conf\nEOF\n</code></pre> <ul> <li>Run this command to create a ConfigMap containing the MachineConfig patch for regular workloads</li> </ul> <pre><code>oc create -n &lt;hostedcluster_namespace&gt; configmap mcp-crun-default --from-file config=mc-crun-default-runtime.yaml\n</code></pre> <ul> <li>Use this command to patch the NodePool resource by adding the newly created ConfigMap as a MachineConfig change</li> </ul> <pre><code>oc patch -n &lt;hostedcluster_namespace&gt; nodepool &lt;nodepool_name&gt; --type=json -p='[{\"op\": \"add\", \"path\": \"/spec/config\", \"value\": [{\"name\": \"mcp-crun-default\"}]}]'\n</code></pre> <ul> <li>Put the crio configuration into a file</li> </ul> <pre><code>cat &lt;&lt;EOF &gt; crio-config.conf\n[crio.runtime]\ndefault_runtime=\"crun\"\n# You will need to adjust the CPU set assignation depending on the infrastructure\ninfra_ctr_cpuset = \"0-1,24-25\"\n\n[crio.runtime.runtimes.crun]\nruntime_path = \"/bin/crun\"\nruntime_type = \"oci\"\nruntime_root = \"/run/crun\"\n\nallowed_annotations = [\n    \"io.containers.trace-syscall\",\n    \"io.kubernetes.cri-o.Devices\",\n    \"io.kubernetes.cri-o.LinkLogs\",\n    \"cpu-load-balancing.crio.io\",\n    \"cpu-quota.crio.io\",\n    \"irq-load-balancing.crio.io\",\n]\nEOF\n</code></pre> <ul> <li>Get the base64 hash of the file content</li> </ul> <pre><code>export CONFIG_HASH=$(cat crio-config.conf | base64 -w0)\n</code></pre> <ul> <li>Create the MachineConfig file with content in Base64</li> </ul> <pre><code>cat &lt;&lt;EOF &gt; mc-crun-high-perf-default-runtime.yaml\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  name: 60-crun-high-perf-default-runtime\nspec:\n  config:\n    ignition:\n      version: 3.2.0\n    storage:\n      files:\n      - contents:\n          source: data:text/plain;charset=utf-8;base64,${CONFIG_HASH}\n        mode: 420\n        path: /etc/crio/crio.conf.d/99-crun-high-perf-runtime.conf\nEOF\n</code></pre> <ul> <li>Create the ConfigMap for high-performance workloads using the following command</li> </ul> <pre><code>oc create -n &lt;hostedcluster_namespace&gt; configmap mcp-crun-hp-default --from-file config=mc-crun-high-perf-default-runtime.yaml\n</code></pre> <ul> <li>Patch the NodePool resource with the newly created ConfigMap</li> </ul> <pre><code>oc patch -n &lt;hostedcluster_namespace&gt; nodepool &lt;nodepool_name&gt; --type=json -p='[{\"op\": \"add\", \"path\": \"/spec/config\", \"value\": [{\"name\": \"mcp-crun-hp-default\"}]}]'\n</code></pre>"},{"location":"recipes/common/replace-crio-runtime/#switching-from-crun-to-runc","title":"Switching from <code>crun</code> to <code>runc</code>","text":"<p>If you experience performance issues or similar problems and want to revert to <code>runc</code> as the default runtime, follow this guide to set <code>runc</code> as the default CRI-O runtime for your worker nodes, even after upgrading to 4.18. This process is also compatible with 4.17 if you plan to upgrade but want to retain <code>runc</code> as the runtime.</p>"},{"location":"recipes/common/replace-crio-runtime/#creating-the-machineconfig-patch-for-the-nodepool_1","title":"Creating the MachineConfig patch for the NodePool","text":"<p>The following manifest sets <code>runc</code> as the default runtime.</p> HCP Regular Workloads (RUNC)HCP High Performance Workloads (RUNC) <ul> <li>Put the crio configuration into a file</li> </ul> <pre><code>cat &lt;&lt;EOF &gt; crio-config.conf\n[crio.runtime]\ndefault_runtime=\"runc\"\n\n[crio.runtime.runtimes.runc]\nruntime_path = \"/bin/runc\"\nruntime_type = \"oci\"\nruntime_root = \"/run/runc\"\nEOF\n</code></pre> <ul> <li>Get the base64 hash of the file content</li> </ul> <pre><code>export CONFIG_HASH=$(cat crio-config.conf | base64 -w0)\n</code></pre> <ul> <li>Create the MachineConfig file with content in Base64</li> </ul> <pre><code>cat &lt;&lt;EOF &gt; mc-runc-default-runtime.yaml\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  name: 60-runc-default-runtime\nspec:\n  config:\n    ignition:\n      version: 3.2.0\n    storage:\n      files:\n      - contents:\n          source: data:text/plain;charset=utf-8;base64,${CONFIG_HASH}\n        mode: 420\n        path: /etc/crio/crio.conf.d/99-runc-runtime.conf\nEOF\n</code></pre> <ul> <li>Create the ConfigMap for regular workloads using</li> </ul> <pre><code>oc create -n &lt;hostedcluster_namespace&gt; configmap mcp-runc-default --from-file config=mc-runc-default-runtime.yaml\n</code></pre> <ul> <li>Patch the NodePool resource with the newly created ConfigMap</li> </ul> <pre><code>oc patch -n &lt;hostedcluster_namespace&gt; nodepool &lt;nodepool_name&gt; --type=json -p='[{\"op\": \"add\", \"path\": \"/spec/config\", \"value\": [{\"name\": \"mcp-runc-default\"}]}]'\n</code></pre> <ul> <li>Put the crio configuration into a file</li> </ul> <pre><code>cat &lt;&lt;EOF &gt; crio-config.conf\n[crio.runtime]\ndefault_runtime=\"runc\"\n# You will need to adjust the CPU set assignation depending on the infrastructure\ninfra_ctr_cpuset = \"0-1,24-25\"\n\n[crio.runtime.runtimes.runc]\nruntime_path = \"/bin/runc\"\nruntime_type = \"oci\"\nruntime_root = \"/run/runc\"\n\nallowed_annotations = [\n    \"io.containers.trace-syscall\",\n    \"io.kubernetes.cri-o.Devices\",\n    \"io.kubernetes.cri-o.LinkLogs\",\n    \"cpu-load-balancing.crio.io\",\n    \"cpu-quota.crio.io\",\n    \"irq-load-balancing.crio.io\",\n]\nEOF\n</code></pre> <ul> <li>Get the base64 hash of the file content</li> </ul> <pre><code>export CONFIG_HASH=$(cat crio-config.conf | base64 -w0)\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt; mc-runc-high-perf-default-runtime.yaml\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  name: 60-runc-high-perf-default-runtime\nspec:\n  config:\n    ignition:\n      version: 3.2.0\n    storage:\n      files:\n      - contents:\n          source: data:text/plain;charset=utf-8;base64,${CONFIG_HASH}\n        mode: 420\n        path: /etc/crio/crio.conf.d/99-runc-high-perf-runtime.conf\nEOF\n</code></pre> <ul> <li>Create the ConfigMap for high-performance workloads</li> </ul> <pre><code>oc create -n &lt;hostedcluster_namespace&gt; configmap mcp-runc-hp-default --from-file config=mc-runc-high-perf-default-runtime.yaml\n</code></pre> <ul> <li>Patch the NodePool resource with the newly created ConfigMap</li> </ul> <pre><code>oc patch -n &lt;hostedcluster_namespace&gt; nodepool &lt;nodepool_name&gt; --type=json -p='[{\"op\": \"add\", \"path\": \"/spec/config\", \"value\": [{\"name\": \"mcp-runc-hp-default\"}]}]'\n</code></pre>"},{"location":"reference/","title":"Reference","text":"<p>This section of the HyperShift documentation contains references.</p>"},{"location":"reference/SLOs/","title":"SLOs","text":"<p>This project is committed to satisfy a number of internal SLOs.  These SLOs can be taken as reference by consumers to aggregate them and help define their own SLOs.</p> <p>These SLOs/SLIs are currently just referential and monitored as part of our CI runs. They are valid under the following circumstances:</p> <ul> <li>Running raw Hypershift operator in an OCP management cluster with bare HostedCluster and NodePool resources.</li> <li>Management cluster has 2 m6i.4xlarge instances with 16 cores to allocate HostedClusters.</li> <li>No more than 20 concurrent HostedClusters.</li> </ul>"},{"location":"reference/SLOs/#slosslis","title":"SLOs/SLIs","text":"<ul> <li>Time for cluster to become available should be &lt; 5min.</li> <li>Time for cluster to complete rollout should be &lt; 15min.</li> <li>Cluster Memory consumption per single replica HostedCluster should be &lt; 7GB.</li> <li>Cluster Memory consumption per high available HostedCluster should be &lt; 18GB.</li> <li>Time to NodePool availability should be &lt; 8min.</li> <li>Time to cloud resources deletion in a raw cluster should be &lt; 3min.</li> <li>Time to cluster deletion in a raw cluster should be &lt; 5min.</li> </ul> <p>TBD:</p> <ul> <li>Time to instance creation</li> <li>Time to node joining cluster</li> <li>Time for node to become active after joining</li> </ul> <p>Dashboards with these metrics are currently stored in a temporary Grafana instance:</p> <ul> <li>Deletion SLIs</li> <li>Creation/Running SLIs</li> </ul>"},{"location":"reference/api/","title":"HyperShift API Reference","text":"<p>Packages:</p> <ul> <li> hypershift.openshift.io/v1beta1 </li> </ul>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1","title":"hypershift.openshift.io/v1beta1","text":"<p> <p>Package v1beta1 contains the HyperShift API.</p> <p>The HyperShift API enables creating and managing lightweight, flexible, heterogeneous OpenShift clusters at scale.</p> <p>HyperShift clusters are deployed in a topology which isolates the \u201ccontrol plane\u201d (e.g. etcd, the API server, controller manager, etc.) from the \u201cdata plane\u201d (e.g. worker nodes and their kubelets, and the infrastructure on which they run). This enables \u201chosted control plane as a service\u201d use cases.</p> </p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.CertificateSigningRequestApproval","title":"CertificateSigningRequestApproval","text":"<p> <p>CertificateSigningRequestApproval defines the desired state of CertificateSigningRequestApproval</p> </p> Field Description <code>apiVersion</code> string <code> hypershift.openshift.io/v1beta1 </code> <code>kind</code> string  <code>CertificateSigningRequestApproval</code> <code>metadata</code>  Kubernetes meta/v1.ObjectMeta  (Optional) <p>metadata is standard object metadata.</p> Refer to the Kubernetes API documentation for the fields of the <code>metadata</code> field.  <code>spec</code>  CertificateSigningRequestApprovalSpec  (Optional) <p>spec is the specification of the desired behavior of the CertificateSigningRequestApproval.</p> <code>status</code>  CertificateSigningRequestApprovalStatus  (Optional) <p>status is the most recently observed status of the CertificateSigningRequestApproval.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.GCPPrivateServiceConnect","title":"GCPPrivateServiceConnect","text":"<p> <p>GCPPrivateServiceConnect represents GCP Private Service Connect infrastructure. This resource is feature-gated behind the GCPPlatform feature gate.</p> </p> Field Description <code>apiVersion</code> string <code> hypershift.openshift.io/v1beta1 </code> <code>kind</code> string  <code>GCPPrivateServiceConnect</code> <code>metadata</code>  Kubernetes meta/v1.ObjectMeta  (Optional) <p>metadata is the metadata for the GCPPrivateServiceConnect.</p> Refer to the Kubernetes API documentation for the fields of the <code>metadata</code> field.  <code>spec</code>  GCPPrivateServiceConnectSpec  (Optional) <p>spec is the specification for the GCPPrivateServiceConnect.</p> <code>forwardingRuleName</code>  string  <p>forwardingRuleName is the name of the Internal Load Balancer forwarding rule</p> <code>consumerAcceptList</code>  []string  <p>consumerAcceptList specifies which customer projects can connect Accepts both project IDs (e.g. \u201cmy-project-123\u201d) and project numbers (e.g. \u201c123456789012\u201d)</p> <code>natSubnet</code>  string  (Optional) <p>natSubnet is the subnet used for NAT by the Service Attachment Auto-populated by the HyperShift Operator</p> <code>status</code>  GCPPrivateServiceConnectStatus  (Optional) <p>status is the status of the GCPPrivateServiceConnect.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.HostedCluster","title":"HostedCluster","text":"<p> <p>HostedCluster is the primary representation of a HyperShift cluster and encapsulates the control plane and common data plane configuration. Creating a HostedCluster results in a fully functional OpenShift control plane with no attached nodes. To support workloads (e.g. pods), a HostedCluster may have one or more associated NodePool resources.</p> </p> Field Description <code>apiVersion</code> string <code> hypershift.openshift.io/v1beta1 </code> <code>kind</code> string  <code>HostedCluster</code> <code>metadata</code>  Kubernetes meta/v1.ObjectMeta  (Optional) <p>metadata is the metadata for the HostedCluster.</p> Refer to the Kubernetes API documentation for the fields of the <code>metadata</code> field.  <code>spec</code>  HostedClusterSpec  (Optional) <p>spec is the desired behavior of the HostedCluster.</p> <code>release</code>  Release  <p>release specifies the desired OCP release payload for all the hosted cluster components. This includes those components running management side like the Kube API Server and the CVO but also the operands which land in the hosted cluster data plane like the ingress controller, ovn agents, etc. The maximum and minimum supported release versions are determined by the running Hypersfhit Operator. Attempting to use an unsupported version will result in the HostedCluster being degraded and the validateReleaseImage condition being false. Attempting to use a release with a skew against a NodePool release bigger than N-2 for the y-stream will result in leaving the NodePool in an unsupported state. Changing this field will trigger a rollout of the control plane components. The behavior of the rollout will be driven by the ControllerAvailabilityPolicy and InfrastructureAvailabilityPolicy for PDBs and maxUnavailable and surce policies.</p> <code>controlPlaneRelease</code>  Release  (Optional) <p>controlPlaneRelease is like spec.release but only for the components running on the management cluster. This excludes any operand which will land in the hosted cluster data plane. It is useful when you need to apply patch management side like a CVE, transparently for the hosted cluster. Version input for this field is free, no validation is performed against spec.release or maximum and minimum is performed. If defined, it will dicate the version of the components running management side, while spec.release will dictate the version of the components landing in the hosted cluster data plane. If not defined, spec.release is used for both. Changing this field will trigger a rollout of the control plane. The behavior of the rollout will be driven by the ControllerAvailabilityPolicy and InfrastructureAvailabilityPolicy for PDBs and maxUnavailable and surce policies.</p> <code>clusterID</code>  string  (Optional) <p>clusterID uniquely identifies this cluster. This is expected to be an RFC4122 UUID value (xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx in hexadecimal digits). As with a Kubernetes metadata.uid, this ID uniquely identifies this cluster in space and time. This value identifies the cluster in metrics pushed to telemetry and metrics produced by the control plane operators. If a value is not specified, a random clusterID will be generated and set by the controller. Once set, this value is immutable.</p> <code>infraID</code>  string  (Optional) <p>infraID is a globally unique identifier for the cluster. It must consist of lowercase alphanumeric characters and hyphens (\u2018-\u2019) only, and start and end with an alphanumeric character. It must be no more than 253 characters in length. This identifier will be used to associate various cloud resources with the HostedCluster and its associated NodePools. infraID is used to compute and tag created resources with \u201ckubernetes.io/cluster/\u201d+hcluster.Spec.InfraID which has contractual meaning for the cloud provider implementations. If a value is not specified, a random infraID will be generated and set by the controller. Once set, this value is immutable.</p> <code>updateService</code>  github.com/openshift/api/config/v1.URL  (Optional) <p>updateService may be used to specify the preferred upstream update service. If omitted we will use the appropriate update service for the cluster and region. This is used by the control plane operator to determine and signal the appropriate available upgrades in the hostedCluster.status.</p> <code>channel</code>  string  (Optional) <p>channel is an identifier for explicitly requesting that a non-default set of updates be applied to this cluster. If omitted no particular upgrades are suggested. TODO(alberto): Consider the backend to use the default channel by default. Default channel will contain stable updates that are appropriate for production clusters.</p> <code>platform</code>  PlatformSpec  <p>platform specifies the underlying infrastructure provider for the cluster and is used to configure platform specific behavior.</p> <code>kubeAPIServerDNSName</code>  string  (Optional) <p>kubeAPIServerDNSName specifies a desired DNS name to resolve to the KAS. When set, the controller will automatically generate a secret with kubeconfig and expose it in the hostedCluster Status.customKubeconfig field. If it\u2019s set or removed day 2, the kubeconfig generated secret will be created, recreated or deleted. The DNS entries should be resolvable from the cluster, so this should be manually configured in the DNS provider. This field works in conjunction with configuration.APIServer.ServingCerts.NamedCertificates to enable access to the API server via a custom domain name. The NamedCertificates provide the TLS certificates for the custom domain, while this field triggers the generation of a kubeconfig that uses those certificates. This API endpoint only works in OCP version 4.19 or later. Older versions will result in a no-op.</p> <code>controllerAvailabilityPolicy</code>  AvailabilityPolicy  (Optional) <p>controllerAvailabilityPolicy specifies the availability policy applied to critical control plane components like the Kube API Server. Possible values are HighlyAvailable and SingleReplica. The default value is HighlyAvailable. This field is immutable.</p> <code>infrastructureAvailabilityPolicy</code>  AvailabilityPolicy  (Optional) <p>infrastructureAvailabilityPolicy specifies the availability policy applied to infrastructure services which run on the hosted cluster data plane like the ingress controller and image registry controller. Possible values are HighlyAvailable and SingleReplica. The default value is SingleReplica.</p> <code>dns</code>  DNSSpec  (Optional) <p>dns specifies the DNS configuration for the hosted cluster ingress.</p> <code>networking</code>  ClusterNetworking  <p>networking specifies network configuration for the hosted cluster. Defaults to OVNKubernetes with a cluster network of cidr: \u201c10.132.0.0/14\u201d and a service network of cidr: \u201c172.31.0.0/16\u201d.</p> <code>autoscaling</code>  ClusterAutoscaling  (Optional) <p>autoscaling specifies auto-scaling behavior that applies to all NodePools associated with this HostedCluster.</p> <code>autoNode</code>  AutoNode  (Optional) <p>autoNode specifies the configuration for the autoNode feature.</p> <code>etcd</code>  EtcdSpec  <p>etcd specifies configuration for the control plane etcd cluster. The default managementType is Managed. Once set, the managementType cannot be changed.</p> <code>services</code>  []ServicePublishingStrategyMapping  <p>services specifies how individual control plane services endpoints are published for consumption. This requires APIServer;OAuthServer;Konnectivity;Ignition. This field is immutable for all platforms but IBMCloud. Max is 6 to account for OIDC;OVNSbDb for backward compatibility though they are no-op.</p> <p>-kubebuilder:validation:XValidation:rule=\u201cself.all(s, !(s.service == \u2018APIServer\u2019 &amp;&amp; s.servicePublishingStrategy.type == \u2018Route\u2019) || has(s.servicePublishingStrategy.route.hostname))\u201d,message=\u201cIf serviceType is \u2018APIServer\u2019 and publishing strategy is \u2018Route\u2019, then hostname must be set\u201d -kubebuilder:validation:XValidation:rule=\u201cself.platform.type == \u2018IBMCloud\u2019 ? [\u2018APIServer\u2019, \u2018OAuthServer\u2019, \u2018Konnectivity\u2019].all(requiredType, self.exists(s, s.service == requiredType))\u201d,message=\u201cServices list must contain at least \u2018APIServer\u2019, \u2018OAuthServer\u2019, and \u2018Konnectivity\u2019 service types\u201d : [\u2018APIServer\u2019, \u2018OAuthServer\u2019, \u2018Konnectivity\u2019, \u2018Ignition\u2019].all(requiredType, self.exists(s, s.service == requiredType))\u201c,message=\u201cServices list must contain at least \u2018APIServer\u2019, \u2018OAuthServer\u2019, \u2018Konnectivity\u2019, and \u2018Ignition\u2019 service types\u201d -kubebuilder:validation:XValidation:rule=\u201cself.filter(s, s.servicePublishingStrategy.type == \u2018Route\u2019 &amp;&amp; has(s.servicePublishingStrategy.route) &amp;&amp; has(s.servicePublishingStrategy.route.hostname)).all(x, self.filter(y, y.servicePublishingStrategy.type == \u2018Route\u2019 &amp;&amp; (has(y.servicePublishingStrategy.route) &amp;&amp; has(y.servicePublishingStrategy.route.hostname) &amp;&amp; y.servicePublishingStrategy.route.hostname == x.servicePublishingStrategy.route.hostname)).size() &lt;= 1)\u201d,message=\u201cEach route publishingStrategy \u2018hostname\u2019 must be unique within the Services list.\u201d -kubebuilder:validation:XValidation:rule=\u201cself.filter(s, s.servicePublishingStrategy.type == \u2018NodePort\u2019 &amp;&amp; has(s.servicePublishingStrategy.nodePort) &amp;&amp; has(s.servicePublishingStrategy.nodePort.address) &amp;&amp; has(s.servicePublishingStrategy.nodePort.port)).all(x, self.filter(y, y.servicePublishingStrategy.type == \u2018NodePort\u2019 &amp;&amp; (has(y.servicePublishingStrategy.nodePort) &amp;&amp; has(y.servicePublishingStrategy.nodePort.address) &amp;&amp; y.servicePublishingStrategy.nodePort.address == x.servicePublishingStrategy.nodePort.address &amp;&amp; has(y.servicePublishingStrategy.nodePort.port) &amp;&amp; y.servicePublishingStrategy.nodePort.port == x.servicePublishingStrategy.nodePort.port )).size() &lt;= 1)\u201d,message=\u201cEach nodePort publishingStrategy \u2018nodePort\u2019 and \u2018hostname\u2019 must be unique within the Services list.\u201d TODO(alberto): this breaks the cost budget for &lt; 4.17. We should figure why and enable it back. And If not fixable, consider imposing a minimum version on the management cluster.</p> <code>pullSecret</code>  Kubernetes core/v1.LocalObjectReference  <p>pullSecret is a local reference to a Secret that must have a \u201c.dockerconfigjson\u201d key whose content must be a valid Openshift pull secret JSON. If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state. TODO(alberto): Signal this in a condition. This pull secret will be part of every payload generated by the controllers for any NodePool of the HostedCluster and it will be injected into the container runtime of all NodePools. Changing this value will trigger a rollout for all existing NodePools in the cluster. Changing the content of the secret inplace will not trigger a rollout and might result in unpredictable behaviour. TODO(alberto): have our own local reference type to include our opinions and avoid transparent changes.</p> <code>sshKey</code>  Kubernetes core/v1.LocalObjectReference  (Optional) <p>sshKey is a local reference to a Secret that must have a \u201cid_rsa.pub\u201d key whose content must be the public part of 1..N SSH keys. If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state. TODO(alberto): Signal this in a condition. When sshKey is set, the controllers will generate a machineConfig with the sshAuthorizedKeys https://coreos.github.io/ignition/configuration-v3_2/ populated with this value. This MachineConfig will be part of every payload generated by the controllers for any NodePool of the HostedCluster. Changing this value will trigger a rollout for all existing NodePools in the cluster.</p> <code>issuerURL</code>  string  (Optional) <p>issuerURL is an OIDC issuer URL which will be used as the issuer in all ServiceAccount tokens generated by the control plane API server via \u2013service-account-issuer kube api server flag. https://k8s-docs.netlify.app/en/docs/reference/command-line-tools-reference/kube-apiserver/ https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection The default value is kubernetes.default.svc, which only works for in-cluster validation. If the platform is AWS and this value is set, the controller will update an s3 object with the appropriate OIDC documents (using the serviceAccountSigningKey info) into that issuerURL. The expectation is for this s3 url to be backed by an OIDC provider in the AWS IAM.</p> <code>serviceAccountSigningKey</code>  Kubernetes core/v1.LocalObjectReference  (Optional) <p>serviceAccountSigningKey is a local reference to a secret that must have a \u201ckey\u201d key whose content must be the private key used by the service account token issuer. If not specified, a service account signing key will be generated automatically for the cluster. When specifying a service account signing key, an IssuerURL must also be specified. If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state. TODO(alberto): Signal this in a condition.</p> <code>configuration</code>  ClusterConfiguration  (Optional) <p>configuration specifies configuration for individual OCP components in the cluster, represented as embedded resources that correspond to the openshift configuration API.</p> <code>operatorConfiguration</code>  OperatorConfiguration  (Optional) <p>operatorConfiguration specifies configuration for individual OCP operators in the cluster.</p> <code>auditWebhook</code>  Kubernetes core/v1.LocalObjectReference  (Optional) <p>auditWebhook contains metadata for configuring an audit webhook endpoint for a cluster to process cluster audit events. It references a secret that contains the webhook information for the audit webhook endpoint. It is a secret because if the endpoint has mTLS the kubeconfig will contain client keys. The kubeconfig needs to be stored in the secret with a secret key name that corresponds to the constant AuditWebhookKubeconfigKey.</p> <code>imageContentSources</code>  []ImageContentSource  (Optional) <p>imageContentSources specifies image mirrors that can be used by cluster nodes to pull content. When imageContentSources is set, the controllers will generate a machineConfig. This MachineConfig will be part of every payload generated by the controllers for any NodePool of the HostedCluster. Changing this value will trigger a rollout for all existing NodePools in the cluster.</p> <code>additionalTrustBundle</code>  Kubernetes core/v1.LocalObjectReference  (Optional) <p>additionalTrustBundle is a local reference to a ConfigMap that must have a \u201cca-bundle.crt\u201d key whose content must be a PEM-encoded X.509 certificate bundle that will be added to the hosted controlplane and nodes If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state. TODO(alberto): Signal this in a condition. This will be part of every payload generated by the controllers for any NodePool of the HostedCluster. Changing this value will trigger a rollout for all existing NodePools in the cluster.</p> <code>secretEncryption</code>  SecretEncryptionSpec  (Optional) <p>secretEncryption specifies a Kubernetes secret encryption strategy for the control plane.</p> <code>fips</code>  bool  (Optional) <p>fips indicates whether this cluster\u2019s nodes will be running in FIPS mode. If set to true, the control plane\u2019s ignition server will be configured to expect that nodes joining the cluster will be FIPS-enabled.</p> <code>pausedUntil</code>  string  (Optional) <p>pausedUntil is a field that can be used to pause reconciliation on the HostedCluster controller, resulting in any change to the HostedCluster being ignored. Either a date can be provided in RFC3339 format or a boolean as in \u2018true\u2019, \u2018false\u2019, \u2018True\u2019, \u2018False\u2019. If a date is provided: reconciliation is paused on the resource until that date. If the boolean true is provided: reconciliation is paused on the resource until the field is removed.</p> <code>olmCatalogPlacement</code>  OLMCatalogPlacement  (Optional) <p>olmCatalogPlacement specifies the placement of OLM catalog components. By default, this is set to management and OLM catalog components are deployed onto the management cluster. If set to guest, the OLM catalog components will be deployed onto the guest cluster.</p> <code>nodeSelector</code>  map[string]string  (Optional) <p>nodeSelector when specified, is propagated to all control plane Deployments and Stateful sets running management side. It must be satisfied by the management Nodes for the pods to be scheduled. Otherwise the HostedCluster will enter a degraded state. Changes to this field will propagate to existing Deployments and StatefulSets. TODO(alberto): add additional validation for the map key/values.</p> <code>tolerations</code>  []Kubernetes core/v1.Toleration  (Optional) <p>tolerations when specified, define what custom tolerations are added to the hcp pods.</p> <code>labels</code>  map[string]string  (Optional) <p>labels when specified, define what custom labels are added to the hcp pods. Changing this day 2 will cause a rollout of all hcp pods. Duplicate keys are not supported. If duplicate keys are defined, only the last key/value pair is preserved. Valid values are those in https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</p> <p>-kubebuilder:validation:XValidation:rule=<code>self.all(key, size(key) &lt;= 317 &amp;&amp; key.matches('^(([A-Za-z0-9]+(\\\\.[A-Za-z0-9]+)?)*[A-Za-z0-9]\\\\/)?(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])$'))</code>, message=\u201clabel key must have two segments: an optional prefix and name, separated by a slash (/). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (), dots (.), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (.), not longer than 253 characters in total, followed by a slash (/)\u201d -kubebuilder:validation:XValidation:rule=<code>self.all(key, size(self[key]) &lt;= 63 &amp;&amp; self[key].matches('^(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?$'))</code>, message=\u201clabel value must be 63 characters or less (can be empty), consist of alphanumeric characters, dashes (-), underscores () or dots (.), and begin and end with an alphanumeric character\u201d TODO: key/value validations break cost budget for &lt;=4.17. We should figure why and enable it back.</p> <code>capabilities</code>  Capabilities  (Optional) <p>capabilities allows for disabling optional components at cluster install time. This field is optional and once set cannot be changed.</p> <code>status</code>  HostedClusterStatus  (Optional) <p>status is the latest observed status of the HostedCluster.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.NodePool","title":"NodePool","text":"<p> <p>NodePool is a scalable set of worker nodes attached to a HostedCluster. NodePool machine architectures are uniform within a given pool, and are independent of the control plane\u2019s underlying machine architecture.</p> </p> Field Description <code>apiVersion</code> string <code> hypershift.openshift.io/v1beta1 </code> <code>kind</code> string  <code>NodePool</code> <code>metadata</code>  Kubernetes meta/v1.ObjectMeta  (Optional) <p>metadata is the metadata for the NodePool.</p> Refer to the Kubernetes API documentation for the fields of the <code>metadata</code> field.  <code>spec</code>  NodePoolSpec  (Optional) <p>spec is the desired behavior of the NodePool.</p> <code>clusterName</code>  string  <p>clusterName is the name of the HostedCluster this NodePool belongs to. If a HostedCluster with this name doesn\u2019t exist, the controller will no-op until it exists.</p> <code>release</code>  Release  <p>release specifies the OCP release used for this NodePool. It drives the machine ignition configuration (including the kubelet version) and other platform-specific properties (e.g. an AMI on AWS).</p> <p>Version-skew rules and effects: - The minor-version skew relative to the control-plane release must be &lt;= N-2. This is not currently enforced, but exceeding this limit is unsupported and may lead to unpredictable behavior. - If the specified release is higher than the HostedCluster\u2019s release, the NodePool will be degraded and the ValidReleaseImage condition will be false. - If the specified release is lower than the NodePool\u2019s current y-stream, the NodePool will be degraded and the ValidReleaseImage condition will be false.</p> <p>Changing this field triggers a NodePool rollout.</p> <code>platform</code>  NodePoolPlatform  <p>platform specifies the underlying infrastructure provider for the NodePool and is used to configure platform specific behavior.</p> <code>replicas</code>  int32  (Optional) <p>replicas is the desired number of nodes the pool should maintain. If unset, the controller default value is 0. replicas is mutually exclusive with autoscaling. If autoscaling is configured, replicas must be omitted and autoscaling will control the NodePool size internally.</p> <code>management</code>  NodePoolManagement  <p>management specifies behavior for managing nodes in the pool, such as upgrade strategies and auto-repair behaviors.</p> <code>autoScaling</code>  NodePoolAutoScaling  (Optional) <p>autoScaling specifies auto-scaling behavior for the NodePool. autoScaling is mutually exclusive with replicas. If replicas is set, this field must be omitted.</p> <code>config</code>  []Kubernetes core/v1.LocalObjectReference  (Optional) <p>config is a list of references to ConfigMaps containing serialized MachineConfig resources to be injected into the ignition configurations of nodes in the NodePool. The MachineConfig API schema is defined here:</p> <p>https://github.com/openshift/machine-config-operator/blob/18963e4f8fe66e8c513ca4b131620760a414997f/pkg/apis/machineconfiguration.openshift.io/v1/types.go#L185</p> <p>Each ConfigMap must have a single key named \u201cconfig\u201d whose value is the YML with one or more serialized machineconfiguration.openshift.io resources:</p> <ul> <li>KubeletConfig</li> <li>ContainerRuntimeConfig</li> <li>MachineConfig</li> <li>ClusterImagePolicy</li> <li>ImageContentSourcePolicy</li> <li>ImageDigestMirrorSet</li> </ul> <p>This is validated in the backend and signaled back via validMachineConfig condition. Changing this field will trigger a NodePool rollout.</p> <code>nodeDrainTimeout</code>  Kubernetes meta/v1.Duration  (Optional) <p>nodeDrainTimeout is the maximum amount of time that the controller will spend on retrying to drain a node until it succeeds. The default value is 0, meaning that the node can retry drain without any time limitations. Changing this field propagate inplace into existing Nodes.</p> <code>nodeVolumeDetachTimeout</code>  Kubernetes meta/v1.Duration  (Optional) <p>nodeVolumeDetachTimeout is the maximum amount of time that the controller will spend on detaching volumes from a node. The default value is 0, meaning that the volumes will be detached from the node without any time limitations. After the timeout, any remaining attached volumes will be ignored and the removal of the machine will continue. Changing this field propagate inplace into existing Nodes.</p> <code>nodeLabels</code>  map[string]string  (Optional) <p>nodeLabels propagates a list of labels to Nodes, only once on creation. Valid values are those in https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</p> <code>taints</code>  []Taint  (Optional) <p>taints if specified, propagates a list of taints to Nodes, only once on creation. These taints are additive to the ones applied by other controllers</p> <code>pausedUntil</code>  string  (Optional) <p>pausedUntil is a field that can be used to pause reconciliation on the NodePool controller. Resulting in any change to the NodePool being ignored. Either a date can be provided in RFC3339 format or a boolean as in \u2018true\u2019, \u2018false\u2019, \u2018True\u2019, \u2018False\u2019. If a date is provided: reconciliation is paused on the resource until that date. If the boolean true is provided: reconciliation is paused on the resource until the field is removed.</p> <code>tuningConfig</code>  []Kubernetes core/v1.LocalObjectReference  (Optional) <p>tuningConfig is a list of references to ConfigMaps containing serialized Tuned or PerformanceProfile resources to define the tuning configuration to be applied to nodes in the NodePool. The Tuned API is defined here:</p> <p>https://github.com/openshift/cluster-node-tuning-operator/blob/2c76314fb3cc8f12aef4a0dcd67ddc3677d5b54f/pkg/apis/tuned/v1/tuned_types.go</p> <p>The PerformanceProfile API is defined here: https://github.com/openshift/cluster-node-tuning-operator/tree/b41042d42d4ba5bb2e99960248cf1d6ae4935018/pkg/apis/performanceprofile/v2</p> <p>Each ConfigMap must have a single key named \u201ctuning\u201d whose value is the JSON or YAML of a serialized Tuned or PerformanceProfile. Changing this field will trigger a NodePool rollout.</p> <code>arch</code>  string  (Optional) <p>arch is the preferred processor architecture for the NodePool. Different platforms might have different supported architectures. TODO: This is set as optional to prevent validation from failing due to a limitation on client side validation with open API machinery: https://github.com/kubernetes/kubernetes/issues/108768#issuecomment-1253912215</p> <code>status</code>  NodePoolStatus  (Optional) <p>status is the latest observed status of the NodePool.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AESCBCSpec","title":"AESCBCSpec","text":"<p> (Appears on: SecretEncryptionSpec) </p> <p> <p>AESCBCSpec defines metadata about the AESCBC secret encryption strategy</p> </p> Field Description <code>activeKey</code>  Kubernetes core/v1.LocalObjectReference  <p>activeKey defines the active key used to encrypt new secrets</p> <code>backupKey</code>  Kubernetes core/v1.LocalObjectReference  (Optional) <p>backupKey defines the old key during the rotation process so previously created secrets can continue to be decrypted until they are all re-encrypted with the active key.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.APIServerNetworking","title":"APIServerNetworking","text":"<p> (Appears on: ClusterNetworking) </p> <p> <p>APIServerNetworking specifies how the APIServer is exposed inside a cluster node.</p> </p> Field Description <code>advertiseAddress</code>  string  (Optional) <p>advertiseAddress is the address that pods within the nodes will use to talk to the API server. This is an address associated with the loopback adapter of each node. If not specified, the controller will take default values. The default values will be set as 172.20.0.1 or fd00::1. This value is immutable.</p> <code>port</code>  int32  (Optional) <p>port is the port at which the APIServer is exposed inside a node. Other pods using host networking cannot listen on this port. If omitted 6443 is used. This is useful to choose a port other than the default one which might interfere with customer environments e.g. https://github.com/openshift/hypershift/pull/356. Setting this to 443 is possible only for backward compatibility reasons and it\u2019s discouraged. Doing so, it would result in the controller overriding the KAS endpoint in the guest cluster having a discrepancy with the KAS Pod and potentially causing temporarily network failures. This value is immutable.</p> <code>allowedCIDRBlocks</code>  []CIDRBlock  (Optional) <p>allowedCIDRBlocks is an allow list of CIDR blocks that can access the APIServer. If not specified, traffic is allowed from all addresses. This field is enforced for ARO (Azure Red Hat OpenShift) via the shared-ingress HAProxy. For platforms other than ARO, the enforcement depends on whether the underlying cloud provider supports the Service LoadBalancerSourceRanges field. If the platform does not support LoadBalancerSourceRanges, this field may have no effect.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AWSCloudProviderConfig","title":"AWSCloudProviderConfig","text":"<p> (Appears on: AWSPlatformSpec) </p> <p> <p>AWSCloudProviderConfig specifies AWS networking configuration.</p> </p> Field Description <code>subnet</code>  AWSResourceReference  (Optional) <p>subnet is the subnet to use for control plane cloud resources.</p> <code>zone</code>  string  (Optional) <p>zone is the availability zone where control plane cloud resources are created.</p> <code>vpc</code>  string  <p>vpc is the VPC to use for control plane cloud resources.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AWSEndpointAccessType","title":"AWSEndpointAccessType","text":"<p> (Appears on: AWSPlatformSpec) </p> <p> <p>AWSEndpointAccessType specifies the publishing scope of cluster endpoints.</p> </p> Value Description <p>\"Private\"</p> <p>Private endpoint access allows only private API server access and private node communication with the control plane.</p> <p>\"Public\"</p> <p>Public endpoint access allows public API server access and public node communication with the control plane.</p> <p>\"PublicAndPrivate\"</p> <p>PublicAndPrivate endpoint access allows public API server access and private node communication with the control plane.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AWSKMSAuthSpec","title":"AWSKMSAuthSpec","text":"<p> (Appears on: AWSKMSSpec) </p> <p> <p>AWSKMSAuthSpec defines metadata about the management of credentials used to interact and encrypt data via AWS KMS key. The referenced role must have a trust relationship that allows it to be assumed via web identity. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html. Example:</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"Federated\": \"{{ .ProviderARN }}\"\n},\n\"Action\": \"sts:AssumeRoleWithWebIdentity\",\n\"Condition\": {\n\"StringEquals\": {\n\"{{ .ProviderName }}:sub\": {{ .ServiceAccounts }}\n}\n}\n}\n]\n}\n</code></pre> </p> Field Description <code>awsKms</code>  string  <p>awsKms is an ARN value referencing a role appropriate for managing the auth via the AWS KMS key.</p> <p>The following is an example of a valid policy document:</p> <p>{ \u201cVersion\u201d: \u201c2012-10-17\u201d, \u201cStatement\u201d: [ { \u201cEffect\u201d: \u201cAllow\u201d, \u201cAction\u201d: [ \u201ckms:Encrypt\u201d, \u201ckms:Decrypt\u201d, \u201ckms:ReEncrypt\u201d, \u201ckms:GenerateDataKey\u201d, \u201ckms:DescribeKey\u201d ], \u201cResource\u201d: %q } ] }</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AWSKMSKeyEntry","title":"AWSKMSKeyEntry","text":"<p> (Appears on: AWSKMSSpec) </p> <p> <p>AWSKMSKeyEntry defines metadata to locate the encryption key in AWS</p> </p> Field Description <code>arn</code>  string  <p>arn is the Amazon Resource Name for the encryption key</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AWSKMSSpec","title":"AWSKMSSpec","text":"<p> (Appears on: KMSSpec) </p> <p> <p>AWSKMSSpec defines metadata about the configuration of the AWS KMS Secret Encryption provider</p> </p> Field Description <code>region</code>  string  <p>region contains the AWS region</p> <code>activeKey</code>  AWSKMSKeyEntry  <p>activeKey defines the active key used to encrypt new secrets</p> <code>backupKey</code>  AWSKMSKeyEntry  (Optional) <p>backupKey defines the old key during the rotation process so previously created secrets can continue to be decrypted until they are all re-encrypted with the active key.</p> <code>auth</code>  AWSKMSAuthSpec  <p>auth defines metadata about the management of credentials used to interact with AWS KMS</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AWSNodePoolPlatform","title":"AWSNodePoolPlatform","text":"<p> (Appears on: NodePoolPlatform) </p> <p> <p>AWSNodePoolPlatform specifies the configuration of a NodePool when operating on AWS.</p> </p> Field Description <code>instanceType</code>  string  <p>instanceType is an ec2 instance type for node instances (e.g. m5.large).</p> <code>instanceProfile</code>  string  (Optional) <p>instanceProfile is the AWS EC2 instance profile, which is a container for an IAM role that the EC2 instance uses.</p> <code>subnet</code>  AWSResourceReference  <p>subnet is the subnet to use for node instances.</p> <code>ami</code>  string  (Optional) <p>ami is the image id to use for node instances. If unspecified, the default is chosen based on the NodePool release payload image.</p> <code>imageType</code>  ImageType  (Optional) <p>imageType specifies the type of image to use for node instances. Valid values are \u201cLinux\u201d or \u201cWindows\u201d When set to Windows, the controller will automatically populate the AMI field with a Windows-compatible AMI based on the region and OpenShift version. When the AMI field is also set, it will take precedence over automatic selection based on imageType. Also note that Windows ImageType is only compatible with an Arch of amd64</p> <code>securityGroups</code>  []AWSResourceReference  (Optional) <p>securityGroups is an optional set of security groups to associate with node instances.</p> <code>rootVolume</code>  Volume  (Optional) <p>rootVolume specifies configuration for the root volume of node instances.</p> <code>resourceTags</code>  []AWSResourceTag  (Optional) <p>resourceTags is an optional list of additional tags to apply to AWS node instances. Changes to this field will be propagated in-place to AWS EC2 instances and their initial EBS volumes. Volumes created by the storage operator and attached to instances after they are created do not get these tags applied.</p> <p>These will be merged with HostedCluster scoped tags, which take precedence in case of conflicts. These take precedence over tags defined out of band (i.e., tags added manually or by other tools outside of HyperShift) in AWS in case of conflicts.</p> <p>See https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html for information on tagging AWS resources. AWS supports a maximum of 50 tags per resource. OpenShift reserves 25 tags for its use, leaving 25 tags available for the user.</p> <code>placement</code>  PlacementOptions  (Optional) <p>placement specifies the placement options for the EC2 instances.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AWSPlatformSpec","title":"AWSPlatformSpec","text":"<p> (Appears on: PlatformSpec) </p> <p> <p>AWSPlatformSpec specifies configuration for clusters running on Amazon Web Services.</p> </p> Field Description <code>region</code>  string  <p>region is the AWS region in which the cluster resides. This configures the OCP control plane cloud integrations, and is used by NodePool to resolve the correct boot AMI for a given release.</p> <code>cloudProviderConfig</code>  AWSCloudProviderConfig  (Optional) <p>cloudProviderConfig specifies AWS networking configuration for the control plane. This is mainly used for cloud provider controller config: https://github.com/kubernetes/kubernetes/blob/f5be5052e3d0808abb904aebd3218fe4a5c2dd82/staging/src/k8s.io/legacy-cloud-providers/aws/aws.go#L1347-L1364 TODO(dan): should this be named AWSNetworkConfig?</p> <code>serviceEndpoints</code>  []AWSServiceEndpoint  (Optional) <p>serviceEndpoints specifies optional custom endpoints which will override the default service endpoint of specific AWS Services.</p> <p>There must be only one ServiceEndpoint for a given service name.</p> <code>rolesRef</code>  AWSRolesRef  <p>rolesRef contains references to various AWS IAM roles required to enable integrations such as OIDC.</p> <code>resourceTags</code>  []AWSResourceTag  (Optional) <p>resourceTags is a list of additional tags to apply to AWS resources created for the cluster. See https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html for information on tagging AWS resources. AWS supports a maximum of 50 tags per resource. OpenShift reserves 25 tags for its use, leaving 25 tags available for the user. Changes to this field will be propagated in-place to AWS resources (VPC Endpoints, EC2 instances, initial EBS volumes and default/endpoint security groups). These tags will be propagated to the infrastructure CR in the guest cluster, where other OCP operators might choose to honor this input to reconcile AWS resources created by them. Please consult the official documentation for a list of all AWS resources that support in-place tag updates. These take precedence over tags defined out of band (i.e., tags added manually or by other tools outside of HyperShift) in AWS in case of conflicts.</p> <code>endpointAccess</code>  AWSEndpointAccessType  (Optional) <p>endpointAccess specifies the publishing scope of cluster endpoints. The default is Public.</p> <code>additionalAllowedPrincipals</code>  []string  (Optional) <p>additionalAllowedPrincipals specifies a list of additional allowed principal ARNs to be added to the hosted control plane\u2019s VPC Endpoint Service to enable additional VPC Endpoint connection requests to be automatically accepted. See https://docs.aws.amazon.com/vpc/latest/privatelink/configure-endpoint-service.html for more details around VPC Endpoint Service allowed principals.</p> <code>multiArch</code>  bool  (Optional) <p>multiArch specifies whether the Hosted Cluster will be expected to support NodePools with different CPU architectures, i.e., supporting arm64 NodePools and supporting amd64 NodePools on the same Hosted Cluster. Deprecated: This field is no longer used. The HyperShift Operator now performs multi-arch validations automatically despite the platform type. The HyperShift Operator will set HostedCluster.Status.PayloadArch based on the HostedCluster release image. This field is used by the NodePool controller to validate the NodePool.Spec.Arch is supported.</p> <code>sharedVPC</code>  AWSSharedVPC  (Optional) <p>sharedVPC contains fields that must be specified if the HostedCluster must use a VPC that is created in a different AWS account and is shared with the AWS account where the HostedCluster will be created.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AWSPlatformStatus","title":"AWSPlatformStatus","text":"<p> (Appears on: PlatformStatus) </p> <p> <p>AWSPlatformStatus contains status specific to the AWS platform</p> </p> Field Description <code>defaultWorkerSecurityGroupID</code>  string  (Optional) <p>defaultWorkerSecurityGroupID is the ID of a security group created by the control plane operator. It is always added to worker machines in addition to any security groups specified in the NodePool.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AWSResourceReference","title":"AWSResourceReference","text":"<p> (Appears on: AWSCloudProviderConfig,  AWSNodePoolPlatform) </p> <p> <p>AWSResourceReference is a reference to a specific AWS resource by ID or filters. Only one of ID or Filters may be specified. Specifying more than one will result in a validation error.</p> </p> Field Description <code>id</code>  string  (Optional) <p>id of resource</p> <code>filters</code>  []Filter  (Optional) <p>filters is a set of key/value pairs used to identify a resource They are applied according to the rules defined by the AWS API: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Filtering.html</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AWSResourceTag","title":"AWSResourceTag","text":"<p> (Appears on: AWSNodePoolPlatform,  AWSPlatformSpec) </p> <p> <p>AWSResourceTag is a tag to apply to AWS resources created for the cluster.</p> </p> Field Description <code>key</code>  string  <p>key is the key of the tag.</p> <code>value</code>  string  <p>value is the value of the tag.</p> <p>Some AWS service do not support empty values. Since tags are added to resources in many services, the length of the tag value must meet the requirements of all services.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AWSRoleCredentials","title":"AWSRoleCredentials","text":"Field Description <code>arn</code>  string  <p>arn is the ARN of the role.</p> <code>namespace</code>  string  <p>namespace is the namespace of the role.</p> <code>name</code>  string  <p>name is the name of the role.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AWSRolesRef","title":"AWSRolesRef","text":"<p> (Appears on: AWSPlatformSpec) </p> <p> <p>AWSRolesRef contains references to various AWS IAM roles required for operators to make calls against the AWS API. The referenced role must have a trust relationship that allows it to be assumed via web identity. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html. Example:</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"Federated\": \"{{ .ProviderARN }}\"\n},\n\"Action\": \"sts:AssumeRoleWithWebIdentity\",\n\"Condition\": {\n\"StringEquals\": {\n\"{{ .ProviderName }}:sub\": {{ .ServiceAccounts }}\n}\n}\n}\n]\n}\n</code></pre> </p> Field Description <code>ingressARN</code>  string  <p>ingressARN is an ARN value referencing a role appropriate for the Ingress Operator.</p> <p>The following is an example of a valid policy document:</p> <p>{ \u201cVersion\u201d: \u201c2012-10-17\u201d, \u201cStatement\u201d: [ { \u201cEffect\u201d: \u201cAllow\u201d, \u201cAction\u201d: [ \u201celasticloadbalancing:DescribeLoadBalancers\u201d, \u201ctag:GetResources\u201d, \u201croute53:ListHostedZones\u201d ], \u201cResource\u201d: \u201c*\u201d }, { \u201cEffect\u201d: \u201cAllow\u201d, \u201cAction\u201d: [ \u201croute53:ChangeResourceRecordSets\u201d ], \u201cResource\u201d: [ \u201carn:aws:route53:::PUBLIC_ZONE_ID\u201d, \u201carn:aws:route53:::PRIVATE_ZONE_ID\u201d ] } ] }</p> <code>imageRegistryARN</code>  string  <p>imageRegistryARN is an ARN value referencing a role appropriate for the Image Registry Operator.</p> <p>The following is an example of a valid policy document:</p> <p>{ \u201cVersion\u201d: \u201c2012-10-17\u201d, \u201cStatement\u201d: [ { \u201cEffect\u201d: \u201cAllow\u201d, \u201cAction\u201d: [ \u201cs3:CreateBucket\u201d, \u201cs3:DeleteBucket\u201d, \u201cs3:PutBucketTagging\u201d, \u201cs3:GetBucketTagging\u201d, \u201cs3:PutBucketPublicAccessBlock\u201d, \u201cs3:GetBucketPublicAccessBlock\u201d, \u201cs3:PutEncryptionConfiguration\u201d, \u201cs3:GetEncryptionConfiguration\u201d, \u201cs3:PutLifecycleConfiguration\u201d, \u201cs3:GetLifecycleConfiguration\u201d, \u201cs3:GetBucketLocation\u201d, \u201cs3:ListBucket\u201d, \u201cs3:GetObject\u201d, \u201cs3:PutObject\u201d, \u201cs3:DeleteObject\u201d, \u201cs3:ListBucketMultipartUploads\u201d, \u201cs3:AbortMultipartUpload\u201d, \u201cs3:ListMultipartUploadParts\u201d ], \u201cResource\u201d: \u201c*\u201d } ] }</p> <code>storageARN</code>  string  <p>storageARN is an ARN value referencing a role appropriate for the Storage Operator.</p> <p>The following is an example of a valid policy document:</p> <p>{ \u201cVersion\u201d: \u201c2012-10-17\u201d, \u201cStatement\u201d: [ { \u201cEffect\u201d: \u201cAllow\u201d, \u201cAction\u201d: [ \u201cec2:AttachVolume\u201d, \u201cec2:CreateSnapshot\u201d, \u201cec2:CreateTags\u201d, \u201cec2:CreateVolume\u201d, \u201cec2:DeleteSnapshot\u201d, \u201cec2:DeleteTags\u201d, \u201cec2:DeleteVolume\u201d, \u201cec2:DescribeInstances\u201d, \u201cec2:DescribeSnapshots\u201d, \u201cec2:DescribeTags\u201d, \u201cec2:DescribeVolumes\u201d, \u201cec2:DescribeVolumesModifications\u201d, \u201cec2:DetachVolume\u201d, \u201cec2:ModifyVolume\u201d ], \u201cResource\u201d: \u201c*\u201d } ] }</p> <code>networkARN</code>  string  <p>networkARN is an ARN value referencing a role appropriate for the Network Operator.</p> <p>The following is an example of a valid policy document:</p> <p>{ \u201cVersion\u201d: \u201c2012-10-17\u201d, \u201cStatement\u201d: [ { \u201cEffect\u201d: \u201cAllow\u201d, \u201cAction\u201d: [ \u201cec2:DescribeInstances\u201d, \u201cec2:DescribeInstanceStatus\u201d, \u201cec2:DescribeInstanceTypes\u201d, \u201cec2:UnassignPrivateIpAddresses\u201d, \u201cec2:AssignPrivateIpAddresses\u201d, \u201cec2:UnassignIpv6Addresses\u201d, \u201cec2:AssignIpv6Addresses\u201d, \u201cec2:DescribeSubnets\u201d, \u201cec2:DescribeNetworkInterfaces\u201d ], \u201cResource\u201d: \u201c*\u201d } ] }</p> <code>kubeCloudControllerARN</code>  string  <p>kubeCloudControllerARN is an ARN value referencing a role appropriate for the KCM/KCC. Source: https://cloud-provider-aws.sigs.k8s.io/prerequisites/#iam-policies</p> <p>The following is an example of a valid policy document:</p> <p>{ \u201cVersion\u201d: \u201c2012-10-17\u201d, \u201cStatement\u201d: [ { \u201cAction\u201d: [ \u201cautoscaling:DescribeAutoScalingGroups\u201d, \u201cautoscaling:DescribeLaunchConfigurations\u201d, \u201cautoscaling:DescribeTags\u201d, \u201cec2:DescribeAvailabilityZones\u201d, \u201cec2:DescribeInstances\u201d, \u201cec2:DescribeImages\u201d, \u201cec2:DescribeRegions\u201d, \u201cec2:DescribeRouteTables\u201d, \u201cec2:DescribeSecurityGroups\u201d, \u201cec2:DescribeSubnets\u201d, \u201cec2:DescribeVolumes\u201d, \u201cec2:CreateSecurityGroup\u201d, \u201cec2:CreateTags\u201d, \u201cec2:CreateVolume\u201d, \u201cec2:ModifyInstanceAttribute\u201d, \u201cec2:ModifyVolume\u201d, \u201cec2:AttachVolume\u201d, \u201cec2:AuthorizeSecurityGroupIngress\u201d, \u201cec2:CreateRoute\u201d, \u201cec2:DeleteRoute\u201d, \u201cec2:DeleteSecurityGroup\u201d, \u201cec2:DeleteVolume\u201d, \u201cec2:DetachVolume\u201d, \u201cec2:RevokeSecurityGroupIngress\u201d, \u201cec2:DescribeVpcs\u201d, \u201celasticloadbalancing:AddTags\u201d, \u201celasticloadbalancing:AttachLoadBalancerToSubnets\u201d, \u201celasticloadbalancing:ApplySecurityGroupsToLoadBalancer\u201d, \u201celasticloadbalancing:CreateLoadBalancer\u201d, \u201celasticloadbalancing:CreateLoadBalancerPolicy\u201d, \u201celasticloadbalancing:CreateLoadBalancerListeners\u201d, \u201celasticloadbalancing:ConfigureHealthCheck\u201d, \u201celasticloadbalancing:DeleteLoadBalancer\u201d, \u201celasticloadbalancing:DeleteLoadBalancerListeners\u201d, \u201celasticloadbalancing:DescribeLoadBalancers\u201d, \u201celasticloadbalancing:DescribeLoadBalancerAttributes\u201d, \u201celasticloadbalancing:DetachLoadBalancerFromSubnets\u201d, \u201celasticloadbalancing:DeregisterInstancesFromLoadBalancer\u201d, \u201celasticloadbalancing:ModifyLoadBalancerAttributes\u201d, \u201celasticloadbalancing:RegisterInstancesWithLoadBalancer\u201d, \u201celasticloadbalancing:SetLoadBalancerPoliciesForBackendServer\u201d, \u201celasticloadbalancing:AddTags\u201d, \u201celasticloadbalancing:CreateListener\u201d, \u201celasticloadbalancing:CreateTargetGroup\u201d, \u201celasticloadbalancing:DeleteListener\u201d, \u201celasticloadbalancing:DeleteTargetGroup\u201d, \u201celasticloadbalancing:DeregisterTargets\u201d, \u201celasticloadbalancing:DescribeListeners\u201d, \u201celasticloadbalancing:DescribeLoadBalancerPolicies\u201d, \u201celasticloadbalancing:DescribeTargetGroups\u201d, \u201celasticloadbalancing:DescribeTargetHealth\u201d, \u201celasticloadbalancing:ModifyListener\u201d, \u201celasticloadbalancing:ModifyTargetGroup\u201d, \u201celasticloadbalancing:RegisterTargets\u201d, \u201celasticloadbalancing:SetLoadBalancerPoliciesOfListener\u201d, \u201ciam:CreateServiceLinkedRole\u201d, \u201ckms:DescribeKey\u201d ], \u201cResource\u201d: [ \u201c*\u201d ], \u201cEffect\u201d: \u201cAllow\u201d } ] }</p> <code>nodePoolManagementARN</code>  string  <p>nodePoolManagementARN is an ARN value referencing a role appropriate for the CAPI Controller.</p> <p>The following is an example of a valid policy document:</p> <p>{ \u201cVersion\u201d: \u201c2012-10-17\u201d, \u201cStatement\u201d: [ { \u201cAction\u201d: [ \u201cec2:AssociateRouteTable\u201d, \u201cec2:AttachInternetGateway\u201d, \u201cec2:AuthorizeSecurityGroupIngress\u201d, \u201cec2:CreateInternetGateway\u201d, \u201cec2:CreateNatGateway\u201d, \u201cec2:CreateRoute\u201d, \u201cec2:CreateRouteTable\u201d, \u201cec2:CreateSecurityGroup\u201d, \u201cec2:CreateSubnet\u201d, \u201cec2:CreateTags\u201d, \u201cec2:DeleteInternetGateway\u201d, \u201cec2:DeleteNatGateway\u201d, \u201cec2:DeleteRouteTable\u201d, \u201cec2:DeleteSecurityGroup\u201d, \u201cec2:DeleteSubnet\u201d, \u201cec2:DeleteTags\u201d, \u201cec2:DescribeAccountAttributes\u201d, \u201cec2:DescribeAddresses\u201d, \u201cec2:DescribeAvailabilityZones\u201d, \u201cec2:DescribeImages\u201d, \u201cec2:DescribeInstances\u201d, \u201cec2:DescribeInternetGateways\u201d, \u201cec2:DescribeNatGateways\u201d, \u201cec2:DescribeNetworkInterfaces\u201d, \u201cec2:DescribeNetworkInterfaceAttribute\u201d, \u201cec2:DescribeRouteTables\u201d, \u201cec2:DescribeSecurityGroups\u201d, \u201cec2:DescribeSubnets\u201d, \u201cec2:DescribeVpcs\u201d, \u201cec2:DescribeVpcAttribute\u201d, \u201cec2:DescribeVolumes\u201d, \u201cec2:DetachInternetGateway\u201d, \u201cec2:DisassociateRouteTable\u201d, \u201cec2:DisassociateAddress\u201d, \u201cec2:ModifyInstanceAttribute\u201d, \u201cec2:ModifyNetworkInterfaceAttribute\u201d, \u201cec2:ModifySubnetAttribute\u201d, \u201cec2:RevokeSecurityGroupIngress\u201d, \u201cec2:RunInstances\u201d, \u201cec2:TerminateInstances\u201d, \u201ctag:GetResources\u201d, \u201cec2:CreateLaunchTemplate\u201d, \u201cec2:CreateLaunchTemplateVersion\u201d, \u201cec2:DescribeLaunchTemplates\u201d, \u201cec2:DescribeLaunchTemplateVersions\u201d, \u201cec2:DeleteLaunchTemplate\u201d, \u201cec2:DeleteLaunchTemplateVersions\u201d ], \u201cResource\u201d: [ \u201c\u201d ], \u201cEffect\u201d: \u201cAllow\u201d }, { \u201cCondition\u201d: { \u201cStringLike\u201d: { \u201ciam:AWSServiceName\u201d: \u201celasticloadbalancing.amazonaws.com\u201d } }, \u201cAction\u201d: [ \u201ciam:CreateServiceLinkedRole\u201d ], \u201cResource\u201d: [ \u201carn::iam:::role/aws-service-role/elasticloadbalancing.amazonaws.com/AWSServiceRoleForElasticLoadBalancing\u201d ], \u201cEffect\u201d: \u201cAllow\u201d }, { \u201cAction\u201d: [ \u201ciam:PassRole\u201d ], \u201cResource\u201d: [ \u201carn::iam:::role/-worker-role\u201d ], \u201cEffect\u201d: \u201cAllow\u201d }, { \u201cEffect\u201d: \u201cAllow\u201d, \u201cAction\u201d: [ \u201ckms:Decrypt\u201d, \u201ckms:ReEncrypt\u201d, \u201ckms:GenerateDataKeyWithoutPlainText\u201d, \u201ckms:DescribeKey\u201d ], \u201cResource\u201d: \u201c\u201d }, { \u201cEffect\u201d: \u201cAllow\u201d, \u201cAction\u201d: [ \u201ckms:CreateGrant\u201d ], \u201cResource\u201d: \u201c\u201d, \u201cCondition\u201d: { \u201cBool\u201d: { \u201ckms:GrantIsForAWSResource\u201d: true } } } ] }</p> <code>controlPlaneOperatorARN</code>  string  <p>controlPlaneOperatorARN  is an ARN value referencing a role appropriate for the Control Plane Operator.</p> <p>The following is an example of a valid policy document:</p> <p>{ \u201cVersion\u201d: \u201c2012-10-17\u201d, \u201cStatement\u201d: [ { \u201cEffect\u201d: \u201cAllow\u201d, \u201cAction\u201d: [ \u201cec2:CreateVpcEndpoint\u201d, \u201cec2:DescribeVpcEndpoints\u201d, \u201cec2:ModifyVpcEndpoint\u201d, \u201cec2:DeleteVpcEndpoints\u201d, \u201cec2:CreateTags\u201d, \u201croute53:ListHostedZones\u201d, \u201cec2:CreateSecurityGroup\u201d, \u201cec2:AuthorizeSecurityGroupIngress\u201d, \u201cec2:AuthorizeSecurityGroupEgress\u201d, \u201cec2:DeleteSecurityGroup\u201d, \u201cec2:RevokeSecurityGroupIngress\u201d, \u201cec2:RevokeSecurityGroupEgress\u201d, \u201cec2:DescribeSecurityGroups\u201d, \u201cec2:DescribeVpcs\u201d, ], \u201cResource\u201d: \u201c*\u201d }, { \u201cEffect\u201d: \u201cAllow\u201d, \u201cAction\u201d: [ \u201croute53:ChangeResourceRecordSets\u201d, \u201croute53:ListResourceRecordSets\u201d ], \u201cResource\u201d: \u201carn:aws:route53:::%s\u201d } ] }</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AWSServiceEndpoint","title":"AWSServiceEndpoint","text":"<p> (Appears on: AWSPlatformSpec) </p> <p> <p>AWSServiceEndpoint stores the configuration for services to override existing defaults of AWS Services.</p> </p> Field Description <code>name</code>  string  <p>name is the name of the AWS service. This must be provided and cannot be empty.</p> <code>url</code>  string  <p>url is fully qualified URI with scheme https, that overrides the default generated endpoint for a client. This must be provided and cannot be empty.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AWSSharedVPC","title":"AWSSharedVPC","text":"<p> (Appears on: AWSPlatformSpec) </p> <p> <p>AWSSharedVPC contains fields needed to create a HostedCluster using a VPC that has been created and shared from a different AWS account than the AWS account where the cluster is getting created.</p> </p> Field Description <code>rolesRef</code>  AWSSharedVPCRolesRef  <p>rolesRef contains references to roles in the VPC owner account that enable a HostedCluster on a shared VPC.</p> <code>localZoneID</code>  string  <p>localZoneID is the ID of the route53 hosted zone for [cluster-name].hypershift.local that is associated with the HostedCluster\u2019s VPC and exists in the VPC owner account.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AWSSharedVPCRolesRef","title":"AWSSharedVPCRolesRef","text":"<p> (Appears on: AWSSharedVPC) </p> <p> <p>AWSSharedVPCRolesRef contains references to AWS IAM roles required for a shared VPC hosted cluster. These roles must exist in the VPC owner\u2019s account.</p> </p> Field Description <code>ingressARN</code>  string  <p>ingressARN is an ARN value referencing the role in the VPC owner account that allows the ingress operator in the cluster account to create and manage records in the private DNS hosted zone.</p> <p>The referenced role must have a trust relationship that allows it to be assumed by the ingress operator role in the VPC creator account. Example: { \u201cVersion\u201d: \u201c2012-10-17\u201d, \u201cStatement\u201d: [ { \u201cSid\u201d: \u201cStatement1\u201d, \u201cEffect\u201d: \u201cAllow\u201d, \u201cPrincipal\u201d: { \u201cAWS\u201d: \u201carn:aws:iam::[cluster-creator-account-id]:role/[infra-id]-openshift-ingress\u201d }, \u201cAction\u201d: \u201csts:AssumeRole\u201d } ] }</p> <p>The following is an example of the policy document for this role. (Based on https://docs.openshift.com/rosa/rosa_install_access_delete_clusters/rosa-shared-vpc-config.html#rosa-sharing-vpc-dns-and-roles_rosa-shared-vpc-config)</p> <p>{ \u201cVersion\u201d: \u201c2012-10-17\u201d, \u201cStatement\u201d: [ { \u201cEffect\u201d: \u201cAllow\u201d, \u201cAction\u201d: [ \u201croute53:ListHostedZones\u201d, \u201croute53:ListHostedZonesByName\u201d, \u201croute53:ChangeTagsForResource\u201d, \u201croute53:GetAccountLimit\u201d, \u201croute53:GetChange\u201d, \u201croute53:GetHostedZone\u201d, \u201croute53:ListTagsForResource\u201d, \u201croute53:UpdateHostedZoneComment\u201d, \u201ctag:GetResources\u201d, \u201ctag:UntagResources\u201d \u201croute53:ChangeResourceRecordSets\u201d, \u201croute53:ListResourceRecordSets\u201d ], \u201cResource\u201d: \u201c*\u201d }, ] }</p> <code>controlPlaneARN</code>  string  <p>controlPlaneARN is an ARN value referencing the role in the VPC owner account that allows the control plane operator in the cluster account to create and manage a VPC endpoint, its corresponding Security Group, and DNS records in the hypershift local hosted zone.</p> <p>The referenced role must have a trust relationship that allows it to be assumed by the control plane operator role in the VPC creator account. Example: { \u201cVersion\u201d: \u201c2012-10-17\u201d, \u201cStatement\u201d: [ { \u201cSid\u201d: \u201cStatement1\u201d, \u201cEffect\u201d: \u201cAllow\u201d, \u201cPrincipal\u201d: { \u201cAWS\u201d: \u201carn:aws:iam::[cluster-creator-account-id]:role/[infra-id]-control-plane-operator\u201d }, \u201cAction\u201d: \u201csts:AssumeRole\u201d } ] }</p> <p>The following is an example of the policy document for this role.</p> <p>{ \u201cVersion\u201d: \u201c2012-10-17\u201d, \u201cStatement\u201d: [ { \u201cEffect\u201d: \u201cAllow\u201d, \u201cAction\u201d: [ \u201cec2:CreateVpcEndpoint\u201d, \u201cec2:DescribeVpcEndpoints\u201d, \u201cec2:ModifyVpcEndpoint\u201d, \u201cec2:DeleteVpcEndpoints\u201d, \u201cec2:CreateTags\u201d, \u201croute53:ListHostedZones\u201d, \u201cec2:CreateSecurityGroup\u201d, \u201cec2:AuthorizeSecurityGroupIngress\u201d, \u201cec2:AuthorizeSecurityGroupEgress\u201d, \u201cec2:DeleteSecurityGroup\u201d, \u201cec2:RevokeSecurityGroupIngress\u201d, \u201cec2:RevokeSecurityGroupEgress\u201d, \u201cec2:DescribeSecurityGroups\u201d, \u201cec2:DescribeVpcs\u201d, \u201croute53:ChangeResourceRecordSets\u201d, \u201croute53:ListResourceRecordSets\u201d ], \u201cResource\u201d: \u201c*\u201d } ] }</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AddressPair","title":"AddressPair","text":"<p> (Appears on: PortSpec) </p> <p> </p> Field Description <code>ipAddress</code>  string  <p>ipAddress is the IP address of the allowed address pair. Depending on the configuration of Neutron, it may be supported to specify a CIDR instead of a specific IP address.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AgentNodePoolPlatform","title":"AgentNodePoolPlatform","text":"<p> (Appears on: NodePoolPlatform) </p> <p> <p>AgentNodePoolPlatform specifies the configuration of a NodePool when operating on the Agent platform.</p> </p> Field Description <code>agentLabelSelector</code>  Kubernetes meta/v1.LabelSelector  (Optional) <p>agentLabelSelector contains labels that must be set on an Agent in order to be selected for a Machine.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AgentPlatformSpec","title":"AgentPlatformSpec","text":"<p> (Appears on: PlatformSpec) </p> <p> <p>AgentPlatformSpec specifies configuration for agent-based installations.</p> </p> Field Description <code>agentNamespace</code>  string  <p>agentNamespace is the namespace where to search for Agents for this cluster</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AllocationPool","title":"AllocationPool","text":"<p> (Appears on: SubnetSpec) </p> <p> </p> Field Description <code>start</code>  string  <p>start represents the start of the AllocationPool, that is the lowest IP of the pool.</p> <code>end</code>  string  <p>end represents the end of the AlloctionPool, that is the highest IP of the pool.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AutoNode","title":"AutoNode","text":"<p> (Appears on: HostedClusterSpec,  HostedControlPlaneSpec) </p> <p> <p>We expose here internal configuration knobs that won\u2019t be exposed to the service.</p> </p> Field Description <code>provisionerConfig</code>  ProvisionerConfig  <p>provisionerConfig is the implementation used for Node auto provisioning.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AvailabilityPolicy","title":"AvailabilityPolicy","text":"<p> (Appears on: HostedClusterSpec,  HostedControlPlaneSpec) </p> <p> <p>availabilityPolicy specifies a high level availability policy for components.</p> </p> Value Description <p>\"HighlyAvailable\"</p> <p>HighlyAvailable means components should be resilient to problems across fault boundaries as defined by the component to which the policy is attached. This usually means running critical workloads with 3 replicas and with little or no toleration of disruption of the component.</p> <p>\"SingleReplica\"</p> <p>SingleReplica means components are not expected to be resilient to problems across most fault boundaries associated with high availability. This usually means running critical workloads with just 1 replica and with toleration of full disruption of the component.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzureAuthenticationConfiguration","title":"AzureAuthenticationConfiguration","text":"<p> (Appears on: AzurePlatformSpec) </p> <p> <p>azureAuthenticationConfiguration is a discriminated union type that contains the Azure authentication configuration for a Hosted Cluster. This configuration is used to determine how the Hosted Cluster authenticates with Azure\u2019s API, either with managed identities or workload identities.</p> </p> Field Description <code>azureAuthenticationConfigType</code>  AzureAuthenticationType  <p>azureAuthenticationConfigType is the type of identity configuration used in the Hosted Cluster. This field is used to determine which identity configuration is being used. Valid values are \u201cManagedIdentities\u201d and \u201cWorkloadIdentities\u201d.</p> <code>managedIdentities</code>  AzureResourceManagedIdentities  (Optional) <p>managedIdentities contains the managed identities needed for HCP control plane and data plane components that authenticate with Azure\u2019s API.</p> <p>These are required for managed Azure, also known as ARO HCP.</p> <code>workloadIdentities</code>  AzureWorkloadIdentities  (Optional) <p>workloadIdentities is a struct of client IDs for each component that needs to authenticate with Azure\u2019s API in self-managed Azure. These client IDs are used to authenticate with Azure cloud on both the control plane and data plane.</p> <p>This is required for self-managed Azure.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzureAuthenticationType","title":"AzureAuthenticationType","text":"<p> (Appears on: AzureAuthenticationConfiguration) </p> <p> <p>AzureAuthenticationType is a discriminated union type that contains the Azure authentication configuration for an Azure Hosted Cluster. This type is used to determine which authentication configuration is being used. Valid values are \u201cManagedIdentities\u201d and \u201cWorkloadIdentities\u201d.</p> </p> Value Description <p>\"ManagedIdentities\"</p> <p>\u201cManagedIdentities\u201d means that the Hosted Cluster is using managed identities to authenticate with Azure\u2019s API. This is only valid for managed Azure, also known as ARO HCP.</p> <p>\"WorkloadIdentities\"</p> <p>\u201cWorkloadIdentities\u201d means that the Hosted Cluster is using workload identities to authenticate with Azure\u2019s API. This is only valid for self-managed Azure.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzureClientID","title":"AzureClientID","text":"<p> (Appears on: ManagedIdentity,  WorkloadIdentity) </p> <p> <p>AzureClientID is a string that represents the client ID of a managed identity.</p> </p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzureDiagnosticsStorageAccountType","title":"AzureDiagnosticsStorageAccountType","text":"<p> (Appears on: Diagnostics) </p> <p> <p>AzureDiagnosticsStorageAccountType specifies the type of storage account for storing Azure VM diagnostics data.</p> </p> Value Description <p>\"Disabled\"</p> <p>\"Managed\"</p> <p>\"UserManaged\"</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzureDiskPersistence","title":"AzureDiskPersistence","text":"<p> (Appears on: AzureNodePoolOSDisk) </p> <p> </p> Value Description <p>\"Ephemeral\"</p> <p>EphemeralDiskPersistence is the ephemeral disk type.</p> <p>\"Persistent\"</p> <p>PersistentDiskPersistence is the persistent disk type.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzureDiskStorageAccountType","title":"AzureDiskStorageAccountType","text":"<p> (Appears on: AzureNodePoolOSDisk) </p> <p> </p> Value Description <p>\"Premium_LRS\"</p> <p>DiskStorageAccountTypesPremiumLRS - Premium SSD locally redundant storage. Best for production and performance sensitive workloads.</p> <p>\"PremiumV2_LRS\"</p> <p>DiskStorageAccountTypesPremiumV2LRS - Premium SSD v2 locally redundant storage. Best for production and performance-sensitive workloads that consistently require low latency and high IOPS and throughput.</p> <p>\"Standard_LRS\"</p> <p>DiskStorageAccountTypesStandardLRS - Standard HDD locally redundant storage. Best for backup, non-critical, and infrequent access.</p> <p>\"StandardSSD_LRS\"</p> <p>DiskStorageAccountTypesStandardSSDLRS - Standard SSD locally redundant storage. Best for web servers, lightly used enterprise applications and dev/test.</p> <p>\"UltraSSD_LRS\"</p> <p>DiskStorageAccountTypesUltraSSDLRS - Ultra SSD locally redundant storage. Best for IO-intensive workloads such as SAP HANA, top tier databases (for example, SQL, Oracle), and other transaction-heavy workloads.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzureKMSKey","title":"AzureKMSKey","text":"<p> (Appears on: AzureKMSSpec) </p> <p> </p> Field Description <code>keyVaultName</code>  string  <p>keyVaultName is the name of the keyvault. Must match criteria specified at https://docs.microsoft.com/en-us/azure/key-vault/general/about-keys-secrets-certificates#vault-name-and-object-name Your Microsoft Entra application used to create the cluster must be authorized to access this keyvault, e.g using the AzureCLI: <code>az keyvault set-policy -n $KEYVAULT_NAME --key-permissions decrypt encrypt --spn &lt;YOUR APPLICATION CLIENT ID&gt;</code></p> <code>keyName</code>  string  <p>keyName is the name of the keyvault key used for encrypt/decrypt</p> <code>keyVersion</code>  string  <p>keyVersion contains the version of the key to use</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzureKMSSpec","title":"AzureKMSSpec","text":"<p> (Appears on: KMSSpec) </p> <p> <p>AzureKMSSpec defines metadata about the configuration of the Azure KMS Secret Encryption provider using Azure key vault</p> </p> Field Description <code>activeKey</code>  AzureKMSKey  <p>activeKey defines the active key used to encrypt new secrets</p> <code>backupKey</code>  AzureKMSKey  (Optional) <p>backupKey defines the old key during the rotation process so previously created secrets can continue to be decrypted until they are all re-encrypted with the active key.</p> <code>kms</code>  ManagedIdentity  <p>kms is a pre-existing managed identity used to authenticate with Azure KMS.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzureMarketplaceImage","title":"AzureMarketplaceImage","text":"<p> (Appears on: AzureVMImage) </p> <p> <p>AzureMarketplaceImage specifies the information needed to create an Azure VM from an Azure Marketplace image. This struct supports two usage patterns: 1. Specify only imageGeneration to use marketplace defaults from the release payload (HyperShift will select the appropriate image) 2. Specify publisher, offer, sku, and version to use an explicit marketplace image (with optional imageGeneration)</p> </p> Field Description <code>imageGeneration</code>  AzureVMImageGeneration  (Optional) <p>imageGeneration specifies the Hyper-V generation of the Azure Marketplace image to use for the nodes. This field is used by HyperShift to select the appropriate marketplace image (Gen1 or Gen2) from the release payload metadata when publisher, offer, sku, and version are not explicitly provided. It is not passed to CAPZ (Cluster API Provider Azure); the generation information is encoded into the SKU field that CAPZ uses. Valid values are Gen1 and Gen2. If unspecified, defaults to Gen2.</p> <code>publisher</code>  string  (Optional) <p>publisher is the name of the organization that created the image. It must be between 3 and 50 characters in length, and consist of only lowercase letters, numbers, and hyphens (-) and underscores (_). It must start with a lowercase letter or a number. TODO: Can we explain where a user might find this value, or provide an example of one they might want to use</p> <code>offer</code>  string  (Optional) <p>offer specifies the name of a group of related images created by the publisher. TODO: What is the valid character set for this field? What about minimum and maximum lengths?</p> <code>sku</code>  string  (Optional) <p>sku specifies an instance of an offer, such as a major release of a distribution. For example, 2204-lts-gen2, 8-lvm-gen2. The value must consist only of lowercase letters, numbers, and hyphens (-) and underscores (). TODO: What about length limits?</p> <code>version</code>  string  (Optional) <p>version specifies the version of an image sku. The allowed formats are Major.Minor.Build or \u2018latest\u2019. Major, Minor, and Build are decimal numbers, e.g. \u20181.2.0\u2019. Specify \u2018latest\u2019 to use the latest version of an image available at deployment time. Even if you use \u2018latest\u2019, the VM image will not automatically update after deploy time even if a new version becomes available.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzureNodePoolOSDisk","title":"AzureNodePoolOSDisk","text":"<p> (Appears on: AzureNodePoolPlatform) </p> <p> </p> Field Description <code>sizeGiB</code>  int32  (Optional) <p>sizeGiB is the size in GiB (1024^3 bytes) to assign to the OS disk. This should be between 16 and 65,536 when using the UltraSSD_LRS storage account type and between 16 and 32,767 when using any other storage account type. When not set, this means no opinion and the platform is left to choose a reasonable default, which is subject to change over time. The current default is 30.</p> <code>diskStorageAccountType</code>  AzureDiskStorageAccountType  (Optional) <p>diskStorageAccountType is the disk storage account type to use. Valid values are Premium_LRS, PremiumV2_LRS, Standard_LRS, StandardSSD_LRS, UltraSSD_LRS. Note that Standard means a HDD. The disk performance is tied to the disk type, please refer to the Azure documentation for further details https://docs.microsoft.com/en-us/azure/virtual-machines/disks-types#disk-type-comparison. When omitted this means no opinion and the platform is left to choose a reasonable default, which is subject to change over time. The current default is Premium SSD LRS.</p> <code>encryptionSetID</code>  string  (Optional) <p>encryptionSetID is the ID of the DiskEncryptionSet resource to use to encrypt the OS disks for the VMs. Configuring a DiskEncyptionSet allows greater control over the encryption of the VM OS disk at rest. Can be used with either platform (Azure) managed, or customer managed encryption keys. This needs to exist in the same subscription id listed in the Hosted Cluster, HostedCluster.Spec.Platform.Azure.SubscriptionID. DiskEncryptionSetID should also exist in a resource group under the same subscription id and the same location listed in the Hosted Cluster, HostedCluster.Spec.Platform.Azure.Location. The encryptionSetID should be in the format <code>/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Copmute/diskEncryptionSets/{resourceName}</code>. The subscriptionId in the encryptionSetID must be a valid UUID. It should be 5 groups of hyphen separated hexadecimal characters in the form 8-4-4-4-12. The resourceGroupName should be between 1 and 90 characters, consisting only of alphanumeric characters, hyphens, underscores, periods and parenthesis and must not end with a period (.) character. The resourceName should be between 1 and 80 characters, consisting only of alphanumeric characters, hyphens and underscores. TODO: Are there other encryption related options we may want to expose, should this be in a struct as well?</p> <code>persistence</code>  AzureDiskPersistence  (Optional) <p>persistence determines whether the OS disk should be persisted beyond the life of the VM. Valid values are Persistent and Ephemeral. When set to Ephmeral, the OS disk will not be persisted to Azure storage and implies restrictions to the VM size and caching type. Full details can be found in the Azure documentation https://learn.microsoft.com/en-us/azure/virtual-machines/ephemeral-os-disks. Ephmeral disks are primarily used for stateless applications, provide lower latency than Persistent disks and also incur no storage costs. When not set, this means no opinion and the platform is left to choose a reasonable default, which is subject to change over time.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzureNodePoolPlatform","title":"AzureNodePoolPlatform","text":"<p> (Appears on: NodePoolPlatform) </p> <p> <p>AzureNodePoolPlatform is the platform specific configuration for an Azure node pool.</p> </p> Field Description <code>vmSize</code>  string  <p>vmSize is the Azure VM instance type to use for the nodes being created in the nodepool. The size naming convention is documented here https://learn.microsoft.com/en-us/azure/virtual-machines/vm-naming-conventions. Size names should start with a Family name, which is represented by one of more capital letters, and then be followed by the CPU count. This is followed by 0 or more additional features, represented by a, b, d, i, l, m, p, t, s, C, and NP, refer to the Azure documentation for an explanation of these features. Optionally an accelerator such as a GPU can be added, prefixed by an underscore, for example A100, H100 or MI300X. The size may also be versioned, in which case it should be suffixed with _v where the version is a number. For example, \u201cD32ads_v5\u201d would be a suitable general purpose VM size, or \u201cND96_MI300X_v5\u201d would represent a GPU accelerated VM. <code>image</code>  AzureVMImage  <p>image is used to configure the VM boot image. If unset, the default image at the location below will be used and is expected to exist: subscription//resourceGroups//providers/Microsoft.Compute/images/rhcos.x86_64.vhd. The  and the  are expected to be the same resource group documented in the Hosted Cluster specification respectively, HostedCluster.Spec.Platform.Azure.SubscriptionID and HostedCluster.Spec.Platform.Azure.ResourceGroupName. <code>osDisk</code>  AzureNodePoolOSDisk  <p>osDisk provides configuration for the OS disk for the nodepool. This can be used to configure the size, storage account type, encryption options and whether the disk is persistent or ephemeral. When not provided, the platform will choose reasonable defaults which are subject to change over time. Review the fields within the osDisk for more details.</p> <code>availabilityZone</code>  string  (Optional) <p>availabilityZone is the failure domain identifier where the VM should be attached to. This must not be specified for clusters in a location that does not support AvailabilityZone because it would cause a failure from Azure API.</p> <code>encryptionAtHost</code>  string  (Optional) <p>encryptionAtHost enables encryption at host on virtual machines. According to Microsoft documentation, this means data stored on the VM host is encrypted at rest and flows encrypted to the Storage service. See https://learn.microsoft.com/en-us/azure/virtual-machines/disks-enable-host-based-encryption-portal?tabs=azure-powershell for more information.</p> <code>subnetID</code>  string  <p>subnetID is the subnet ID of an existing subnet where the nodes in the nodepool will be created. This can be a different subnet than the one listed in the HostedCluster, HostedCluster.Spec.Platform.Azure.SubnetID, but must exist in the same network, HostedCluster.Spec.Platform.Azure.VnetID, and must exist under the same subscription ID, HostedCluster.Spec.Platform.Azure.SubscriptionID. subnetID is immutable once set. The subnetID should be in the format <code>/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Network/virtualNetworks/{vnetName}/subnets/{subnetName}</code>. The subscriptionId in the encryptionSetID must be a valid UUID. It should be 5 groups of hyphen separated hexadecimal characters in the form 8-4-4-4-12. The resourceGroupName should be between 1 and 90 characters, consisting only of alphanumeric characters, hyphens, underscores, periods and parenthesis and must not end with a period (.) character. The vnetName should be between 2 and 64 characters, consisting only of alphanumeric characters, hyphens, underscores and periods and must not end with either a period (.) or hyphen (-) character. The subnetName should be between 1 and 80 characters, consisting only of alphanumeric characters, hyphens and underscores and must start with an alphanumeric character and must not end with a period (.) or hyphen (-) character.</p> <code>diagnostics</code>  Diagnostics  (Optional) <p>diagnostics specifies the diagnostics settings for a virtual machine. If not specified, then Boot diagnostics will be disabled.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzurePlatformSpec","title":"AzurePlatformSpec","text":"<p> (Appears on: PlatformSpec) </p> <p> <p>AzurePlatformSpec specifies configuration for clusters running on Azure. Generally, the HyperShift API assumes bring your own (BYO) cloud infrastructure resources. For example, resources like a resource group, a subnet, or a vnet would be pre-created and then their names would be used respectively in the ResourceGroupName, SubnetName, VnetName fields of the Hosted Cluster CR. An existing cloud resource is expected to exist under the same SubscriptionID.</p> </p> Field Description <code>cloud</code>  string  (Optional) <p>cloud is the cloud environment identifier, valid values could be found here: https://github.com/Azure/go-autorest/blob/4c0e21ca2bbb3251fe7853e6f9df6397f53dd419/autorest/azure/environments.go#L33</p> <code>location</code>  string  <p>location is the Azure region in where all the cloud infrastructure resources will be created.</p> <p>Example: eastus</p> <code>resourceGroup</code>  string  <p>resourceGroup is the name of an existing resource group where all cloud resources created by the Hosted Cluster are to be placed. The resource group is expected to exist under the same subscription as SubscriptionID.</p> <p>In ARO HCP, this will be the managed resource group where customer cloud resources will be created.</p> <p>Resource group naming requirements can be found here: https://azure.github.io/PSRule.Rules.Azure/en/rules/Azure.ResourceGroup.Name/.</p> <p>Example: if your resource group ID is /subscriptions//resourceGroups/, your ResourceGroupName is . <code>vnetID</code>  string  <p>vnetID is the ID of an existing VNET to use in creating VMs. The VNET can exist in a different resource group other than the one specified in ResourceGroupName, but it must exist under the same subscription as SubscriptionID.</p> <p>In ARO HCP, this will be the ID of the customer provided VNET.</p> <p>Example: /subscriptions//resourceGroups//providers/Microsoft.Network/virtualNetworks/ <code>subnetID</code>  string  <p>subnetID is the subnet ID of an existing subnet where the nodes in the nodepool will be created. This can be a different subnet than the one listed in the HostedCluster, HostedCluster.Spec.Platform.Azure.SubnetID, but must exist in the same network, HostedCluster.Spec.Platform.Azure.VnetID, and must exist under the same subscription ID, HostedCluster.Spec.Platform.Azure.SubscriptionID. subnetID is immutable once set. The subnetID should be in the format <code>/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Network/virtualNetworks/{vnetName}/subnets/{subnetName}</code>. The subscriptionId in the encryptionSetID must be a valid UUID. It should be 5 groups of hyphen separated hexadecimal characters in the form 8-4-4-4-12. The resourceGroupName should be between 1 and 90 characters, consisting only of alphanumeric characters, hyphens, underscores, periods and parenthesis and must not end with a period (.) character. The vnetName should be between 2 and 64 characters, consisting only of alphanumeric characters, hyphens, underscores and periods and must not end with either a period (.) or hyphen (-) character. The subnetName should be between 1 and 80 characters, consisting only of alphanumeric characters, hyphens and underscores and must start with an alphanumeric character and must not end with a period (.) or hyphen (-) character.</p> <code>subscriptionID</code>  string  <p>subscriptionID is a unique identifier for an Azure subscription used to manage resources.</p> <code>securityGroupID</code>  string  <p>securityGroupID is the ID of an existing security group on the SubnetID. This field is provided as part of the configuration for the Azure cloud provider, aka Azure cloud controller manager (CCM). This security group is expected to exist under the same subscription as SubscriptionID.</p> <code>azureAuthenticationConfig</code>  AzureAuthenticationConfiguration  <p>azureAuthenticationConfig is the type of Azure authentication configuration to use to authenticate with Azure\u2019s Cloud API.</p> <code>tenantID</code>  string  <p>tenantID is a unique identifier for the tenant where Azure resources will be created and managed in.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzureResourceManagedIdentities","title":"AzureResourceManagedIdentities","text":"<p> (Appears on: AzureAuthenticationConfiguration) </p> <p> <p>AzureResourceManagedIdentities contains the managed identities needed for HCP control plane and data plane components that authenticate with Azure\u2019s API.</p> </p> Field Description <code>controlPlane</code>  ControlPlaneManagedIdentities  <p>controlPlane contains the client IDs of all the managed identities on the HCP control plane needing to authenticate with Azure\u2019s API.</p> <code>dataPlane</code>  DataPlaneManagedIdentities  <p>dataPlane contains the client IDs of all the managed identities on the data plane needing to authenticate with Azure\u2019s API.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzureVMImage","title":"AzureVMImage","text":"<p> (Appears on: AzureNodePoolPlatform) </p> <p> <p>AzureVMImage represents the different types of boot image sources that can be provided for an Azure VM.</p> </p> Field Description <code>type</code>  AzureVMImageType  <p>type is the type of image data that will be provided to the Azure VM. Valid values are \u201cImageID\u201d and \u201cAzureMarketplace\u201d. ImageID means is used for legacy managed VM images. This is where the user uploads a VM image directly to their resource group. AzureMarketplace means the VM will boot from an Azure Marketplace image. Marketplace images are preconfigured and published by the OS vendors and may include preconfigured software for the VM. When Type is \u201cAzureMarketplace\u201d, you can either: 1. Specify only imageGeneration to use marketplace defaults from the release payload 2. Specify publisher, offer, sku, and version to use an explicit marketplace image 3. Specify all fields (imageGeneration along with publisher, offer, sku, version)</p> <code>imageID</code>  string  (Optional) <p>imageID is the Azure resource ID of a VHD image to use to boot the Azure VMs from. TODO: What is the valid character set for this field? What about minimum and maximum lengths?</p> <code>azureMarketplace</code>  AzureMarketplaceImage  (Optional) <p>azureMarketplace contains the Azure Marketplace image info to use to boot the Azure VMs from.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzureVMImageGeneration","title":"AzureVMImageGeneration","text":"<p> (Appears on: AzureMarketplaceImage) </p> <p> <p>AzureVMImageGeneration represents the Hyper-V generation of an Azure VM image.</p> </p> Value Description <p>\"Gen1\"</p> <p>Gen1 represents Hyper-V Generation 1 VMs</p> <p>\"Gen2\"</p> <p>Gen2 represents Hyper-V Generation 2 VMs</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzureVMImageType","title":"AzureVMImageType","text":"<p> (Appears on: AzureVMImage) </p> <p> <p>AzureVMImageType is used to specify the source of the Azure VM boot image. Valid values are ImageID and AzureMarketplace.</p> </p> Value Description <p>\"AzureMarketplace\"</p> <p>AzureMarketplace is used to specify the Azure Marketplace image info to use to boot the Azure VMs from.</p> <p>\"ImageID\"</p> <p>ImageID is the used to specify that an Azure resource ID of a VHD image is used to boot the Azure VMs from.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.AzureWorkloadIdentities","title":"AzureWorkloadIdentities","text":"<p> (Appears on: AzureAuthenticationConfiguration) </p> <p> <p>AzureWorkloadIdentities is a struct that contains the client IDs of all the managed identities in self-managed Azure needing to authenticate with Azure\u2019s API.</p> </p> Field Description <code>imageRegistry</code>  WorkloadIdentity  <p>imageRegistry is the client ID of a federated managed identity, associated with cluster-image-registry-operator, used in workload identity authentication.</p> <code>ingress</code>  WorkloadIdentity  <p>ingress is the client ID of a federated managed identity, associated with cluster-ingress-operator, used in workload identity authentication.</p> <code>file</code>  WorkloadIdentity  <p>file is the client ID of a federated managed identity, associated with cluster-storage-operator-file, used in workload identity authentication.</p> <code>disk</code>  WorkloadIdentity  <p>disk is the client ID of a federated managed identity, associated with cluster-storage-operator-disk, used in workload identity authentication.</p> <code>nodePoolManagement</code>  WorkloadIdentity  <p>nodePoolManagement is the client ID of a federated managed identity, associated with cluster-api-provider-azure, used in workload identity authentication.</p> <code>cloudProvider</code>  WorkloadIdentity  <p>cloudProvider is the client ID of a federated managed identity, associated with azure-cloud-provider, used in workload identity authentication.</p> <code>network</code>  WorkloadIdentity  <p>network is the client ID of a federated managed identity, associated with cluster-network-operator, used in workload identity authentication.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.CIDRBlock","title":"CIDRBlock","text":"<p> (Appears on: APIServerNetworking) </p> <p> </p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.Capabilities","title":"Capabilities","text":"<p> (Appears on: HostedClusterSpec,  HostedControlPlaneSpec) </p> <p> <p>capabilities allows enabling or disabling optional components at install time. When this is not supplied, the cluster will use the DefaultCapabilitySet defined for the respective OpenShift version, minus the baremetal capability. Once set, it cannot be changed.</p> </p> Field Description <code>enabled</code>  []OptionalCapability  (Optional) <p>enabled when specified, explicitly enables the specified capabilit\u00edes on the hosted cluster. Once set, this field cannot be changed.</p> <code>disabled</code>  []OptionalCapability  (Optional) <p>disabled when specified, explicitly disables the specified capabilit\u00edes on the hosted cluster. Once set, this field cannot be changed.</p> <p>Note: Disabling \u2018openshift-samples\u2019,\u2018Insights\u2019, \u2018Console\u2019, \u2018NodeTuning\u2019, \u2018Ingress\u2019 are only supported in OpenShift versions 4.20 and above.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.CapacityReservationOptions","title":"CapacityReservationOptions","text":"<p> (Appears on: PlacementOptions) </p> <p> <p>CapacityReservationOptions specifies Capacity Reservation options for the NodePool instances.</p> </p> Field Description <code>id</code>  string  (Optional) <p>id specifies the target Capacity Reservation into which the EC2 instances should be launched. Must follow the format: cr- followed by 17 lowercase hexadecimal characters. For example: cr-0123456789abcdef0 When empty, no specific Capacity Reservation is targeted.</p> <p>When specified, preference cannot be set to \u2018None\u2019 or \u2018Open\u2019 as these are mutually exclusive with targeting a specific reservation. Use preference \u2018CapacityReservationsOnly\u2019 or omit preference field when targeting a specific reservation.</p> <code>marketType</code>  MarketType  (Optional) <p>marketType specifies the market type of the CapacityReservation for the EC2 instances. Valid values are OnDemand, CapacityBlocks and omitted: - \u201cOnDemand\u201d: EC2 instances run as standard On-Demand instances. - \u201cCapacityBlocks\u201d: scheduled pre-purchased compute capacity. Capacity Blocks is recommended when GPUs are needed to support ML workloads. When omitted, this means no opinion and the platform is left to choose a reasonable default, which is subject to change over time. The current default value is CapacityBlocks.</p> <p>When set to \u2018CapacityBlocks\u2019, a specific Capacity Reservation ID must be provided.</p> <code>preference</code>  CapacityReservationPreference  (Optional) <p>preference specifies the preference for use of Capacity Reservations by the instance. Valid values include: - \u201c\u201d: No preference (platform default) - \u201cOpen\u201d: The instance may make use of open Capacity Reservations that match its AZ and InstanceType - \u201cNone\u201d: The instance may not make use of any Capacity Reservations. This is to conserve open reservations for desired workloads - \u201cCapacityReservationsOnly\u201d: The instance will only run if matched or targeted to a Capacity Reservation</p> <p>Cannot be set to \u2018None\u2019 or \u2018Open\u2019 when a specific Capacity Reservation ID is provided, as targeting a specific reservation is mutually exclusive with these general preference settings.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.CapacityReservationPreference","title":"CapacityReservationPreference","text":"<p> (Appears on: CapacityReservationOptions) </p> <p> <p>CapacityReservationPreference describes the preferred use of capacity reservations of an instance</p> </p> Value Description <p>\"None\"</p> <p>CapacityReservationPreferenceNone the instance may not make use of any Capacity Reservations. This is to conserve open reservations for desired workloads</p> <p>\"CapacityReservationsOnly\"</p> <p>CapacityReservationPreferenceOnly the instance will only run if matched or targeted to a Capacity Reservation</p> <p>\"Open\"</p> <p>CapacityReservationPreferenceOpen the instance may make use of open Capacity Reservations that match its AZ and InstanceType.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.CertificateSigningRequestApprovalSpec","title":"CertificateSigningRequestApprovalSpec","text":"<p> (Appears on: CertificateSigningRequestApproval) </p> <p> <p>CertificateSigningRequestApprovalSpec defines the desired state of CertificateSigningRequestApproval</p> </p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.CertificateSigningRequestApprovalStatus","title":"CertificateSigningRequestApprovalStatus","text":"<p> (Appears on: CertificateSigningRequestApproval) </p> <p> <p>CertificateSigningRequestApprovalStatus defines the observed state of CertificateSigningRequestApproval</p> </p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ClusterAutoscaling","title":"ClusterAutoscaling","text":"<p> (Appears on: HostedClusterSpec,  HostedControlPlaneSpec) </p> <p> <p>ClusterAutoscaling specifies auto-scaling behavior that applies to all NodePools associated with a control plane.</p> </p> Field Description <code>scaling</code>  ScalingType  (Optional) <p>scaling defines the scaling behavior for the cluster autoscaler. ScaleUpOnly means the autoscaler will only scale up nodes, never scale down. ScaleUpAndScaleDown means the autoscaler will both scale up and scale down nodes. When set to ScaleUpAndScaleDown, the scaleDown field can be used to configure scale down behavior.</p> <p>Note: This field is only supported in OpenShift versions 4.19 and above.</p> <code>scaleDown</code>  ScaleDownConfig  (Optional) <p>scaleDown configures the behavior of the Cluster Autoscaler scale down operation. This field is only valid when scaling is set to ScaleUpAndScaleDown.</p> <code>balancingIgnoredLabels</code>  []string  (Optional) <p>balancingIgnoredLabels sets \u201c\u2013balancing-ignore-label \u201d flag on cluster-autoscaler for each listed label. This option specifies labels that cluster autoscaler should ignore when considering node group similarity. For example, if you have nodes with \u201ctopology.ebs.csi.aws.com/zone\u201d label, you can add name of this label here to prevent cluster autoscaler from splitting nodes into different node groups based on its value. <p>HyperShift automatically appends platform-specific balancing ignore labels: - AWS: \u201clifecycle\u201d, \u201ck8s.amazonaws.com/eniConfig\u201d, \u201ctopology.k8s.aws/zone-id\u201d - Azure: \u201cagentpool\u201d, \u201ckubernetes.azure.com/agentpool\u201d - Common: - \u201chypershift.openshift.io/nodePool\u201d - \u201ctopology.ebs.csi.aws.com/zone\u201d - \u201ctopology.disk.csi.azure.com/zone\u201d - \u201cibm-cloud.kubernetes.io/worker-id\u201d - \u201cvpc-block-csi-driver-labels\u201d These labels are added by default and do not need to be manually specified.</p> <code>maxNodesTotal</code>  int32  (Optional) <p>maxNodesTotal is the maximum allowable number of nodes for the Autoscaler scale out to be operational. The autoscaler will not grow the cluster beyond this number. If omitted, the autoscaler will not have a maximum limit. number.</p> <code>maxPodGracePeriod</code>  int32  (Optional) <p>maxPodGracePeriod is the maximum seconds to wait for graceful pod termination before scaling down a NodePool. The default is 600 seconds.</p> <code>maxNodeProvisionTime</code>  string  (Optional) <p>maxNodeProvisionTime is the maximum time to wait for node provisioning before considering the provisioning to be unsuccessful, expressed as a Go duration string. The default is 15 minutes.</p> <code>maxFreeDifferenceRatioPercent</code>  int32  (Optional) <p>maxFreeDifferenceRatioPercent sets the maximum difference ratio for free resources between similar node groups. This parameter controls how strict the similarity check is when comparing node groups for load balancing. The value represents a percentage from 0 to 100. When set to 0, this means node groups must have exactly the same free resources to be considered similar (no difference allowed). When set to 100, this means node groups will be considered similar regardless of their free resource differences (any difference allowed). A value between 0 and 100 represents the maximum allowed difference ratio for free resources between node groups to be considered similar. When omitted, the autoscaler defaults to 10%. This affects the \u201c\u2013max-free-difference-ratio\u201d flag on cluster-autoscaler.</p> <code>podPriorityThreshold</code>  int32  (Optional) <p>podPriorityThreshold enables users to schedule \u201cbest-effort\u201d pods, which shouldn\u2019t trigger autoscaler actions, but only run when there are spare resources available. The default is -10.</p> <p>See the following for more details: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-cluster-autoscaler-work-with-pod-priority-and-preemption</p> <code>expanders</code>  []ExpanderString  (Optional) <p>expanders guide the autoscaler in choosing node groups during scale-out. Sets the order of expanders for scaling out node groups. Options include: * LeastWaste - selects the group with minimal idle CPU and memory after scaling. * Priority - selects the group with the highest user-defined priority. * Random - selects a group randomly. If not specified, <code>[Priority, LeastWaste]</code> is the default. Maximum of 3 expanders can be specified.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ClusterConfiguration","title":"ClusterConfiguration","text":"<p> (Appears on: HostedClusterSpec,  HostedControlPlaneSpec) </p> <p> <p>ClusterConfiguration specifies configuration for individual OCP components in the cluster, represented as embedded resources that correspond to the openshift configuration API.</p> <p>The API for individual configuration items is at: https://docs.openshift.com/container-platform/4.7/rest_api/config_apis/config-apis-index.html</p> </p> Field Description <code>apiServer</code>  github.com/openshift/api/config/v1.APIServerSpec  (Optional) <p>apiServer holds configuration (like serving certificates, client CA and CORS domains) shared by all API servers in the system, among them especially kube-apiserver and openshift-apiserver.</p> <code>authentication</code>  github.com/openshift/api/config/v1.AuthenticationSpec  (Optional) <p>authentication specifies cluster-wide settings for authentication (like OAuth and webhook token authenticators).</p> <code>featureGate</code>  github.com/openshift/api/config/v1.FeatureGateSpec  (Optional) <p>featureGate holds cluster-wide information about feature gates.</p> <code>image</code>  github.com/openshift/api/config/v1.ImageSpec  (Optional) <p>image governs policies related to imagestream imports and runtime configuration for external registries. It allows cluster admins to configure which registries OpenShift is allowed to import images from, extra CA trust bundles for external registries, and policies to block or allow registry hostnames. When exposing OpenShift\u2019s image registry to the public, this also lets cluster admins specify the external hostname. This input will be part of every payload generated by the controllers for any NodePool of the HostedCluster. Changing this value will trigger a rollout for all existing NodePools in the cluster.</p> <code>ingress</code>  github.com/openshift/api/config/v1.IngressSpec  (Optional) <p>ingress holds cluster-wide information about ingress, including the default ingress domain used for routes.</p> <code>network</code>  github.com/openshift/api/config/v1.NetworkSpec  (Optional) <p>network holds cluster-wide information about the network. It is used to configure the desired network configuration, such as: IP address pools for services/pod IPs, network plugin, etc. Please view network.spec for an explanation on what applies when configuring this resource. TODO (csrwng): Add validation here to exclude changes that conflict with networking settings in the HostedCluster.Spec.Networking field.</p> <code>oauth</code>  github.com/openshift/api/config/v1.OAuthSpec  (Optional) <p>oauth holds cluster-wide information about OAuth. It is used to configure the integrated OAuth server. This configuration is only honored when the top level Authentication config has type set to IntegratedOAuth.</p> <code>operatorhub</code>  github.com/openshift/api/config/v1.OperatorHubSpec  (Optional) <p>operatorhub specifies the configuration for the Operator Lifecycle Manager in the HostedCluster. This is only configured at deployment time but the controller are not reconcilling over it. The OperatorHub configuration will be constantly reconciled if catalog placement is management, but only on cluster creation otherwise.</p> <code>scheduler</code>  github.com/openshift/api/config/v1.SchedulerSpec  (Optional) <p>scheduler holds cluster-wide config information to run the Kubernetes Scheduler and influence its placement decisions. The canonical name for this config is <code>cluster</code>.</p> <code>proxy</code>  github.com/openshift/api/config/v1.ProxySpec  (Optional) <p>proxy holds cluster-wide information on how to configure default proxies for the cluster. This affects traffic flowing from the hosted cluster data plane. The controllers will generate a machineConfig with the proxy config for the cluster. This MachineConfig will be part of every payload generated by the controllers for any NodePool of the HostedCluster. Changing this value will trigger a rollout for all existing NodePools in the cluster.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ClusterNetworkEntry","title":"ClusterNetworkEntry","text":"<p> (Appears on: ClusterNetworking) </p> <p> <p>ClusterNetworkEntry is a single IP address block for pod IP blocks. IP blocks are allocated with size 2^HostSubnetLength.</p> </p> Field Description <code>cidr</code>  github.com/openshift/hypershift/api/util/ipnet.IPNet  <p>cidr is the IP block address pool.</p> <code>hostPrefix</code>  int32  (Optional) <p>hostPrefix is the prefix size to allocate to each node from the CIDR. For example, 24 would allocate 2^(32-24)=2^8=256 addresses to each node. If this field is not used by the plugin, it can be left unset.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ClusterNetworkOperatorSpec","title":"ClusterNetworkOperatorSpec","text":"<p> (Appears on: OperatorConfiguration) </p> <p> </p> Field Description <code>disableMultiNetwork</code>  bool  (Optional) <p>disableMultiNetwork when set to true disables the Multus CNI plugin and related components in the hosted cluster. This prevents the installation of multus daemon sets in the guest cluster and the multus-admission-controller in the management cluster. Default is false (Multus is enabled). This field is immutable. This field can only be set to true when NetworkType is \u201cOther\u201d. Setting it to true with any other NetworkType will result in a validation error during cluster creation.</p> <code>ovnKubernetesConfig</code>  OVNKubernetesConfig  (Optional) <p>ovnKubernetesConfig holds OVN-Kubernetes specific configuration. This is only consumed when NetworkType is OVNKubernetes.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ClusterNetworking","title":"ClusterNetworking","text":"<p> (Appears on: HostedClusterSpec,  HostedControlPlaneSpec) </p> <p> <p>clusterNetworking specifies network configuration for a cluster. All CIDRs must be unique. Additional validation to check for CIDRs overlap and consistent network stack is performed by the controllers. Failing that validation will result in the HostedCluster being degraded and the validConfiguration condition being false. TODO this is available in vanilla kube from 1.31 API servers and in Openshift from 4.16. TODO(alberto): Use CEL cidr library for all these validation when all management clusters are &gt;= 1.31.</p> </p> Field Description <code>machineNetwork</code>  []MachineNetworkEntry  (Optional) <p>machineNetwork is the list of IP address pools for machines. This might be used among other things to generate appropriate networking security groups in some clouds providers. Currently only one entry or two for dual stack is supported. This field is immutable.</p> <code>clusterNetwork</code>  []ClusterNetworkEntry  (Optional) <p>clusterNetwork is the list of IP address pools for pods. Defaults to cidr: \u201c10.132.0.0/14\u201d. Currently only one entry is supported. This field is immutable.</p> <code>serviceNetwork</code>  []ServiceNetworkEntry  (Optional) <p>serviceNetwork is the list of IP address pools for services. Defaults to cidr: \u201c172.31.0.0/16\u201d. Currently only one entry is supported. This field is immutable.</p> <code>networkType</code>  NetworkType  (Optional) <p>networkType specifies the SDN provider used for cluster networking. Defaults to OVNKubernetes. This field is required and immutable. kubebuilder:validation:XValidation:rule=\u201cself == oldSelf\u201d, message=\u201cnetworkType is immutable\u201d</p> <code>apiServer</code>  APIServerNetworking  (Optional) <p>apiServer contains advanced network settings for the API server that affect how the APIServer is exposed inside a hosted cluster node.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ClusterVersionOperatorSpec","title":"ClusterVersionOperatorSpec","text":"<p> (Appears on: OperatorConfiguration) </p> <p> <p>ClusterVersionOperatorSpec is the specification of the desired behavior of the Cluster Version Operator.</p> </p> Field Description <code>operatorLogLevel</code>  LogLevel  (Optional) <p>operatorLogLevel is an intent based logging for the operator itself. It does not give fine-grained control, but it is a simple way to manage coarse grained logging choices that operators have to interpret for themselves.</p> <p>Valid values are: \u201cNormal\u201d, \u201cDebug\u201d, \u201cTrace\u201d, \u201cTraceAll\u201d. Defaults to \u201cNormal\u201d.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ClusterVersionStatus","title":"ClusterVersionStatus","text":"<p> (Appears on: HostedClusterStatus,  HostedControlPlaneStatus) </p> <p> <p>ClusterVersionStatus reports the status of the cluster versioning, including any upgrades that are in progress. The current field will be set to whichever version the cluster is reconciling to, and the conditions array will report whether the update succeeded, is in progress, or is failing.</p> </p> Field Description <code>desired</code>  github.com/openshift/api/config/v1.Release  <p>desired is the version that the cluster is reconciling towards. If the cluster is not yet fully initialized desired will be set with the information available, which may be an image or a tag.</p> <code>history</code>  []github.com/openshift/api/config/v1.UpdateHistory  (Optional) <p>history contains a list of the most recent versions applied to the cluster. This value may be empty during cluster startup, and then will be updated when a new update is being applied. The newest update is first in the list and it is ordered by recency. Updates in the history have state Completed if the rollout completed - if an update was failing or halfway applied the state will be Partial. Only a limited amount of update history is preserved.</p> <code>observedGeneration</code>  int64  <p>observedGeneration reports which version of the spec is being synced. If this value is not equal to metadata.generation, then the desired and conditions fields may represent a previous version.</p> <code>availableUpdates</code>  []github.com/openshift/api/config/v1.Release  <p>availableUpdates contains updates recommended for this cluster. Updates which appear in conditionalUpdates but not in availableUpdates may expose this cluster to known issues. This list may be empty if no updates are recommended, if the update service is unavailable, or if an invalid channel has been specified.</p> <code>conditionalUpdates</code>  []github.com/openshift/api/config/v1.ConditionalUpdate  (Optional) <p>conditionalUpdates contains the list of updates that may be recommended for this cluster if it meets specific required conditions. Consumers interested in the set of updates that are actually recommended for this cluster should use availableUpdates. This list may be empty if no updates are recommended, if the update service is unavailable, or if an empty or invalid channel has been specified.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ComponentResource","title":"ComponentResource","text":"<p> (Appears on: ControlPlaneComponentStatus) </p> <p> <p>ComponentResource defines a resource reconciled by a ControlPlaneComponent.</p> </p> Field Description <code>kind</code>  string  <p>kind is the name of the resource schema.</p> <code>group</code>  string  <p>group is the API group for this resource type.</p> <code>name</code>  string  <p>name is the name of this resource.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ConditionType","title":"ConditionType","text":"Value Description <p>\"AWSDefaultSecurityGroupCreated\"</p> <p>AWSDefaultSecurityGroupCreated indicates whether the default security group for AWS workers has been created. A failure here indicates that NodePools without a security group will be blocked from creating machines.</p> <p>\"AWSDefaultSecurityGroupDeleted\"</p> <p>AWSDefaultSecurityGroupDeleted indicates whether the default security group for AWS workers has been deleted. A failure here indicates that the Security Group has some dependencies that there are still pending cloud resources to be deleted that are using that SG.</p> <p>\"AWSEndpointAvailable\"</p> <p>AWSEndpointServiceAvailable indicates whether the AWS Endpoint has been created in the guest VPC</p> <p>\"AWSEndpointServiceAvailable\"</p> <p>AWSEndpointServiceAvailable indicates whether the AWS Endpoint Service has been created for the specified NLB in the management VPC</p> <p>\"CVOScaledDown\"</p> <p>\"CloudResourcesDestroyed\"</p> <p>CloudResourcesDestroyed bubbles up the same condition from HCP. It signals if the cloud provider infrastructure created by Kubernetes in the consumer cloud provider account was destroyed. A failure here may require external user intervention to resolve. E.g. cloud provider perms were corrupted. E.g. the guest cluster was broken and kube resource deletion that affects cloud infra like service type load balancer can\u2019t succeed.</p> <p>\"ClusterVersionAvailable\"</p> <p>ClusterVersionAvailable bubbles up Failing configv1.OperatorAvailable from the CVO.</p> <p>\"ClusterVersionFailing\"</p> <p>ClusterVersionFailing bubbles up Failing from the CVO.</p> <p>\"ClusterVersionProgressing\"</p> <p>ClusterVersionProgressing bubbles up configv1.OperatorProgressing from the CVO.</p> <p>\"ClusterVersionReleaseAccepted\"</p> <p>ClusterVersionReleaseAccepted bubbles up Failing ReleaseAccepted from the CVO.</p> <p>\"ClusterVersionRetrievedUpdates\"</p> <p>ClusterVersionRetrievedUpdates bubbles up RetrievedUpdates from the CVO.</p> <p>\"ClusterVersionSucceeding\"</p> <p>ClusterVersionSucceeding indicates the current status of the desired release version of the HostedCluster as indicated by the Failing condition in the underlying cluster\u2019s ClusterVersion.</p> <p>\"ClusterVersionUpgradeable\"</p> <p>ClusterVersionUpgradeable indicates the Upgradeable condition in the underlying cluster\u2019s ClusterVersion.</p> <p>\"Available\"</p> <p>ControlPlaneComponentAvailable indicates whether the ControlPlaneComponent is available.</p> <p>\"RolloutComplete\"</p> <p>ControlPlaneComponentRolloutComplete indicates whether the ControlPlaneComponent has completed its rollout.</p> <p>\"EtcdAvailable\"</p> <p>EtcdAvailable bubbles up the same condition from HCP. It signals if etcd is available. A failure here often means a software bug or a non-stable cluster.</p> <p>\"EtcdRecoveryActive\"</p> <p>EtcdRecoveryActive indicates that the Etcd cluster is failing and the recovery job was triggered.</p> <p>\"EtcdSnapshotRestored\"</p> <p>\"ExternalDNSReachable\"</p> <p>ExternalDNSReachable bubbles up the same condition from HCP. It signals if the configured external DNS is reachable. A failure here requires external user intervention to resolve. E.g. changing the external DNS domain or making sure the domain is created and registered correctly.</p> <p>\"GCPDNSAvailable\"</p> <p>GCPDNSAvailable indicates whether the DNS configuration has been created in the customer VPC</p> <p>\"GCPEndpointAvailable\"</p> <p>GCPEndpointAvailable indicates whether the GCP PSC Endpoint has been created in the customer VPC</p> <p>\"GCPPrivateServiceConnectAvailable\"</p> <p>GCPPrivateServiceConnectAvailable indicates overall PSC infrastructure availability</p> <p>\"GCPServiceAttachmentAvailable\"</p> <p>GCPServiceAttachmentAvailable indicates whether the GCP Service Attachment has been created for the specified Internal Load Balancer in the management VPC</p> <p>\"Available\"</p> <p>HostedClusterAvailable indicates whether the HostedCluster has a healthy control plane. When this is false for too long and there\u2019s no clear indication in the \u201cReason\u201d, please check the remaining more granular conditions.</p> <p>\"Degraded\"</p> <p>HostedClusterDegraded indicates whether the HostedCluster is encountering an error that may require user intervention to resolve.</p> <p>\"HostedClusterDestroyed\"</p> <p>HostedClusterDestroyed indicates that a hosted has finished destroying and that it is waiting for a destroy grace period to go away. The grace period is determined by the hypershift.openshift.io/destroy-grace-period annotation in the HostedCluster if present.</p> <p>\"Progressing\"</p> <p>HostedClusterProgressing indicates whether the HostedCluster is attempting an initial deployment or upgrade. When this is false for too long and there\u2019s no clear indication in the \u201cReason\u201d, please check the remaining more granular conditions.</p> <p>\"HostedClusterRestoredFromBackup\"</p> <p>HostedClusterRestoredFromBackup indicates that the HostedCluster was restored from backup. This condition is set to true when the HostedCluster is restored from backup and the recovery process is complete. This condition is used to track the status of the recovery process and to determine if the HostedCluster is ready to be used after restoration.</p> <p>\"Available\"</p> <p>\"Degraded\"</p> <p>\"IgnitionEndpointAvailable\"</p> <p>IgnitionEndpointAvailable indicates whether the ignition server for the HostedCluster is available to handle ignition requests. A failure here often means a software bug or a non-stable cluster.</p> <p>\"IgnitionServerValidReleaseInfo\"</p> <p>IgnitionServerValidReleaseInfo indicates if the release contains all the images used by the local ignition provider and reports missing images if any.</p> <p>\"InfrastructureReady\"</p> <p>InfrastructureReady bubbles up the same condition from HCP. It signals if the infrastructure for a control plane to be operational, e.g. load balancers were created successfully. A failure here may require external user intervention to resolve. E.g. hitting quotas on the cloud provider.</p> <p>\"KubeAPIServerAvailable\"</p> <p>KubeAPIServerAvailable bubbles up the same condition from HCP. It signals if the kube API server is available. A failure here often means a software bug or a non-stable cluster.</p> <p>\"KubeVirtNodesLiveMigratable\"</p> <p>KubeVirtNodesLiveMigratable indicates if all nodes (VirtualMachines) of the kubevirt hosted cluster can be live migrated without experiencing a node restart</p> <p>\"PlatformCredentialsFound\"</p> <p>PlatformCredentialsFound indicates that credentials required for the desired platform are valid. A failure here is unlikely to resolve without the changing user input.</p> <p>\"ReconciliationActive\"</p> <p>ReconciliationActive indicates if reconciliation of the HostedCluster is active or paused hostedCluster.spec.pausedUntil.</p> <p>\"ReconciliationSucceeded\"</p> <p>ReconciliationSucceeded indicates if the HostedCluster reconciliation succeeded. A failure here often means a software bug or a non-stable cluster.</p> <p>\"SupportedHostedCluster\"</p> <p>SupportedHostedCluster indicates whether a HostedCluster is supported by the current configuration of the hypershift-operator. e.g. If HostedCluster requests endpointAcess Private but the hypershift-operator is running on a management cluster outside AWS or is not configured with AWS credentials, the HostedCluster is not supported. A failure here is unlikely to resolve without the changing user input.</p> <p>\"UnmanagedEtcdAvailable\"</p> <p>UnmanagedEtcdAvailable indicates whether a user-managed etcd cluster is healthy.</p> <p>\"ValidAWSIdentityProvider\"</p> <p>ValidAWSIdentityProvider indicates if the Identity Provider referenced in the cloud credentials is healthy. E.g. for AWS the idp ARN is referenced in the iam roles. \u201cVersion\u201d: \u201c2012-10-17\u201d, \u201cStatement\u201d: [ { \u201cEffect\u201d: \u201cAllow\u201d, \u201cPrincipal\u201d: { \u201cFederated\u201d: \u201c{{ .ProviderARN }}\u201d }, \u201cAction\u201d: \u201csts:AssumeRoleWithWebIdentity\u201d, \u201cCondition\u201d: { \u201cStringEquals\u201d: { \u201c{{ .ProviderName }}:sub\u201d: {{ .ServiceAccounts }} } } } ]</p> <p>A failure here may require external user intervention to resolve.</p> <p>\"ValidAWSKMSConfig\"</p> <p>ValidAWSKMSConfig indicates whether the AWS KMS role and encryption key are valid and operational A failure here indicates that the role or the key are invalid, or the role doesn\u2019t have access to use the key.</p> <p>\"ValidAzureKMSConfig\"</p> <p>ValidAzureKMSConfig indicates whether the given KMS input for the Azure platform is valid and operational A failure here indicates that the input is invalid, or permissions are missing to use the encryption key.</p> <p>\"ValidConfiguration\"</p> <p>ValidHostedClusterConfiguration signals if the hostedCluster input is valid and supported by the underlying management cluster. A failure here is unlikely to resolve without the changing user input.</p> <p>\"ValidHostedControlPlaneConfiguration\"</p> <p>ValidHostedControlPlaneConfiguration bubbles up the same condition from HCP. It signals if the hostedControlPlane input is valid and supported by the underlying management cluster. A failure here is unlikely to resolve without the changing user input.</p> <p>\"ValidIDPConfiguration\"</p> <p>ValidIDPConfiguration indicates if the Identity Provider configuration is valid. A failure here may require external user intervention to resolve e.g. the user-provided IDP configuration provided is invalid or the IDP is not reachable.</p> <p>\"ValidKubeVirtInfraNetworkMTU\"</p> <p>ValidKubeVirtInfraNetworkMTU indicates if the MTU configured on an infra cluster hosting a guest cluster utilizing kubevirt platform is a sufficient value that will avoid performance degradation due to fragmentation of the double encapsulation in ovn-kubernetes</p> <p>\"ValidOIDCConfiguration\"</p> <p>ValidOIDCConfiguration indicates if an AWS cluster\u2019s OIDC condition is detected as invalid. A failure here may require external user intervention to resolve. E.g. oidc was deleted out of band.</p> <p>\"ValidReleaseImage\"</p> <p>ValidReleaseImage indicates if the release image set in the spec is valid for the HostedCluster. For example, this can be set false if the HostedCluster itself attempts an unsupported version before 4.9 or an unsupported upgrade e.g y-stream upgrade before 4.11. A failure here is unlikely to resolve without the changing user input.</p> <p>\"ValidReleaseInfo\"</p> <p>ValidReleaseInfo bubbles up the same condition from HCP. It indicates if the release contains all the images used by hypershift and reports missing images if any.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ConfigurationStatus","title":"ConfigurationStatus","text":"<p> (Appears on: HostedClusterStatus,  HostedControlPlaneStatus) </p> <p> <p>ConfigurationStatus contains the status of HostedCluster configuration</p> </p> Field Description <code>authentication</code>  github.com/openshift/api/config/v1.AuthenticationStatus  (Optional) <p>authentication contains the observed authentication configuration status from the hosted cluster. This field reflects the current state of the cluster authentication including OAuth metadata, OIDC client status, and other authentication-related configurations.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ControlPlaneComponent","title":"ControlPlaneComponent","text":"<p> <p>ControlPlaneComponent specifies the state of a ControlPlane Component</p> </p> Field Description <code>metadata</code>  Kubernetes meta/v1.ObjectMeta  (Optional) <p>metadata is the metadata for the ControlPlaneComponent.</p> Refer to the Kubernetes API documentation for the fields of the <code>metadata</code> field.  <code>spec</code>  ControlPlaneComponentSpec  (Optional) <p>spec is the specification for the ControlPlaneComponent.</p> <code>status</code>  ControlPlaneComponentStatus  (Optional) <p>status is the status of the ControlPlaneComponent.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ControlPlaneComponentSpec","title":"ControlPlaneComponentSpec","text":"<p> (Appears on: ControlPlaneComponent) </p> <p> <p>ControlPlaneComponentSpec defines the desired state of ControlPlaneComponent</p> </p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ControlPlaneComponentStatus","title":"ControlPlaneComponentStatus","text":"<p> (Appears on: ControlPlaneComponent) </p> <p> <p>ControlPlaneComponentStatus defines the observed state of ControlPlaneComponent</p> </p> Field Description <code>conditions</code>  []Kubernetes meta/v1.Condition  (Optional) <p>conditions contains details for the current state of the ControlPlane Component. If there is an error, then the Available condition will be false.</p> <p>Current condition types are: \u201cAvailable\u201d</p> <code>version</code>  string  (Optional) <p>version reports the current version of this component.</p> <code>resources</code>  []ComponentResource  (Optional) <p>resources is a list of the resources reconciled by this component.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ControlPlaneManagedIdentities","title":"ControlPlaneManagedIdentities","text":"<p> (Appears on: AzureResourceManagedIdentities) </p> <p> <p>ControlPlaneManagedIdentities contains the managed identities on the HCP control plane needing to authenticate with Azure\u2019s API.</p> </p> Field Description <code>managedIdentitiesKeyVault</code>  ManagedAzureKeyVault  <p>managedIdentitiesKeyVault contains information on the management cluster\u2019s managed identities Azure Key Vault. This Key Vault is where the managed identities certificates are stored. These certificates are pulled out of the Key Vault by the Secrets Store CSI driver and mounted into a volume on control plane pods requiring authentication with Azure API.</p> <p>More information on how the Secrets Store CSI driver works to do this can be found here: https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-driver.</p> <code>cloudProvider</code>  ManagedIdentity  <p>cloudProvider is a pre-existing managed identity associated with the azure cloud provider, aka cloud controller manager.</p> <code>nodePoolManagement</code>  ManagedIdentity  <p>nodePoolManagement is a pre-existing managed identity associated with the operator managing the NodePools.</p> <code>controlPlaneOperator</code>  ManagedIdentity  <p>controlPlaneOperator is a pre-existing managed identity associated with the control plane operator.</p> <code>imageRegistry</code>  ManagedIdentity  (Optional) <p>imageRegistry is a pre-existing managed identity associated with the cluster-image-registry-operator.</p> <code>ingress</code>  ManagedIdentity  <p>ingress is a pre-existing managed identity associated with the cluster-ingress-operator.</p> <code>network</code>  ManagedIdentity  <p>network is a pre-existing managed identity associated with the cluster-network-operator.</p> <code>disk</code>  ManagedIdentity  <p>disk is a pre-existing managed identity associated with the azure-disk-controller.</p> <code>file</code>  ManagedIdentity  <p>file is a pre-existing managed identity associated with the azure-disk-controller.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.DNSSpec","title":"DNSSpec","text":"<p> (Appears on: HostedClusterSpec,  HostedControlPlaneSpec) </p> <p> <p>DNSSpec specifies the DNS configuration for the hosted cluster ingress.</p> </p> Field Description <code>baseDomain</code>  string  <p>baseDomain is the base domain of the hosted cluster. It will be used to configure ingress in the hosted cluster through the subdomain baseDomainPrefix.baseDomain. If baseDomainPrefix is omitted, the hostedCluster.name will be used as the subdomain. Once set, this field is immutable. When the value is the empty string \u201c\u201d, the controller might default to a value depending on the platform.</p> <code>baseDomainPrefix</code>  string  (Optional) <p>baseDomainPrefix is the base domain prefix for the hosted cluster ingress. It will be used to configure ingress in the hosted cluster through the subdomain baseDomainPrefix.baseDomain. If baseDomainPrefix is omitted, the hostedCluster.name will be used as the subdomain. Set baseDomainPrefix to an empty string \u201c\u201d, if you don\u2019t want a prefix at all (not even hostedCluster.name) to be prepended to baseDomain. This field is immutable.</p> <code>publicZoneID</code>  string  (Optional) <p>publicZoneID is the Hosted Zone ID where all the DNS records that are publicly accessible to the internet exist. This field is optional and mainly leveraged in cloud environments where the DNS records for the .baseDomain are created by controllers in this zone. Once set, this value is immutable.</p> <code>privateZoneID</code>  string  (Optional) <p>privateZoneID is the Hosted Zone ID where all the DNS records that are only available internally to the cluster exist. This field is optional and mainly leveraged in cloud environments where the DNS records for the .baseDomain are created by controllers in this zone. Once set, this value is immutable.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.DataPlaneManagedIdentities","title":"DataPlaneManagedIdentities","text":"<p> (Appears on: AzureResourceManagedIdentities) </p> <p> <p>DataPlaneManagedIdentities contains the client IDs of all the managed identities on the data plane needing to authenticate with Azure\u2019s API.</p> </p> Field Description <code>imageRegistryMSIClientID</code>  string  <p>imageRegistryMSIClientID is the client ID of a pre-existing managed identity ID associated with the image registry controller.</p> <code>diskMSIClientID</code>  string  <p>diskMSIClientID is the client ID of a pre-existing managed identity ID associated with the CSI Disk driver.</p> <code>fileMSIClientID</code>  string  <p>fileMSIClientID is the client ID of a pre-existing managed identity ID associated with the CSI File driver.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.Diagnostics","title":"Diagnostics","text":"<p> (Appears on: AzureNodePoolPlatform) </p> <p> <p>Diagnostics specifies the diagnostics settings for a virtual machine.</p> </p> Field Description <code>storageAccountType</code>  AzureDiagnosticsStorageAccountType  (Optional) <p>storageAccountType determines if the storage account for storing the diagnostics data should be disabled (Disabled), provisioned by Azure (Managed) or by the user (UserManaged).</p> <code>userManaged</code>  UserManagedDiagnostics  (Optional) <p>userManaged specifies the diagnostics settings for a virtual machine when the storage account is managed by the user.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.EtcdManagementType","title":"EtcdManagementType","text":"<p> (Appears on: EtcdSpec) </p> <p> <p>EtcdManagementType is a enum specifying the strategy for managing the cluster\u2019s etcd instance</p> </p> Value Description <p>\"Managed\"</p> <p>Managed means HyperShift should provision and operator the etcd cluster automatically.</p> <p>\"Unmanaged\"</p> <p>Unmanaged means HyperShift will not provision or manage the etcd cluster, and the user is responsible for doing so.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.EtcdSpec","title":"EtcdSpec","text":"<p> (Appears on: HostedClusterSpec,  HostedControlPlaneSpec) </p> <p> <p>EtcdSpec specifies configuration for a control plane etcd cluster.</p> </p> Field Description <code>managementType</code>  EtcdManagementType  <p>managementType defines how the etcd cluster is managed. This can be either Managed or Unmanaged. This field is immutable.</p> <code>managed</code>  ManagedEtcdSpec  (Optional) <p>managed specifies the behavior of an etcd cluster managed by HyperShift.</p> <code>unmanaged</code>  UnmanagedEtcdSpec  (Optional) <p>unmanaged specifies configuration which enables the control plane to integrate with an externally managed etcd cluster.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.EtcdTLSConfig","title":"EtcdTLSConfig","text":"<p> (Appears on: UnmanagedEtcdSpec) </p> <p> <p>EtcdTLSConfig specifies TLS configuration for HTTPS etcd client endpoints.</p> </p> Field Description <code>clientSecret</code>  Kubernetes core/v1.LocalObjectReference  <p>clientSecret refers to a secret for client mTLS authentication with the etcd cluster. It may have the following key/value pairs:</p> <pre><code>etcd-client-ca.crt: Certificate Authority value\netcd-client.crt: Client certificate value\netcd-client.key: Client certificate key value\n</code></pre>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ExpanderString","title":"ExpanderString","text":"<p> (Appears on: ClusterAutoscaling) </p> <p> <p>ExpanderString contains the name of an expander to be used by the cluster autoscaler.</p> </p> Value Description <p>\"LeastWaste\"</p> <p>\"Priority\"</p> <p>Selects the node group with the least idle resources.</p> <p>\"Random\"</p> <p>Selects the node group with the highest priority.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.Filter","title":"Filter","text":"<p> (Appears on: AWSResourceReference) </p> <p> <p>Filter is a filter used to identify an AWS resource</p> </p> Field Description <code>name</code>  string  <p>name is the name of the filter.</p> <code>values</code>  []string  <p>values is a list of values for the filter.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.FilterByNeutronTags","title":"FilterByNeutronTags","text":"<p> (Appears on: NetworkFilter,  RouterFilter,  SubnetFilter) </p> <p> </p> Field Description <code>tags</code>  []NeutronTag  (Optional) <p>tags is a list of tags to filter by. If specified, the resource must have all of the tags specified to be included in the result.</p> <code>tagsAny</code>  []NeutronTag  (Optional) <p>tagsAny is a list of tags to filter by. If specified, the resource must have at least one of the tags specified to be included in the result.</p> <code>notTags</code>  []NeutronTag  (Optional) <p>notTags is a list of tags to filter by. If specified, resources which contain all of the given tags will be excluded from the result.</p> <code>notTagsAny</code>  []NeutronTag  (Optional) <p>notTagsAny is a list of tags to filter by. If specified, resources which contain any of the given tags will be excluded from the result.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.GCPEndpointAccessType","title":"GCPEndpointAccessType","text":"<p> (Appears on: GCPPlatformSpec) </p> <p> <p>GCPEndpointAccessType defines the endpoint access type for GCP clusters. Equivalent to AWS EndpointAccessType but adapted for GCP networking model.</p> </p> Value Description <p>\"Private\"</p> <p>GCPEndpointAccessPrivate endpoint access allows only private API server access and private node communication with the control plane via Private Service Connect.</p> <p>\"PublicAndPrivate\"</p> <p>GCPEndpointAccessPublicAndPrivate endpoint access allows public API server access and private node communication with the control plane via Private Service Connect.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.GCPNetworkConfig","title":"GCPNetworkConfig","text":"<p> (Appears on: GCPPlatformSpec) </p> <p> <p>GCPNetworkConfig specifies VPC configuration for GCP clusters and Private Service Connect endpoint creation.</p> </p> Field Description <code>network</code>  GCPResourceReference  <p>network is the VPC network name</p> <code>privateServiceConnectSubnet</code>  GCPResourceReference  <p>privateServiceConnectSubnet is the subnet for Private Service Connect endpoints</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.GCPPlatformSpec","title":"GCPPlatformSpec","text":"<p> (Appears on: PlatformSpec) </p> <p> <p>GCPPlatformSpec specifies configuration for clusters running on Google Cloud Platform.</p> </p> Field Description <code>project</code>  string  <p>project is the GCP project ID. A valid project ID must satisfy the following rules: length: Must be between 6 and 30 characters, inclusive characters: Only lowercase letters (<code>a-z</code>), digits (<code>0-9</code>), and hyphens (<code>-</code>) are allowed start and end: Must begin with a lowercase letter and must not end with a hyphen hyphens: No consecutive hyphens are allowed (e.g., \u201cmy\u2013project\u201d is invalid) valid examples: \u201cmy-project\u201d, \u201cmy-project-1\u201d, \u201cmy-project-123\u201d.</p> <code>region</code>  string  <p>region is the GCP region in which the cluster resides. A valid region must satisfy the following rules: format: Must be in the form <code>&lt;letters&gt;-&lt;lettersOrDigits&gt;&lt;digit&gt;</code> characters: Only lowercase letters (<code>a-z</code>), digits (<code>0-9</code>), and a single hyphen (<code>-</code>) separator valid examples: \u201cus-central1\u201d, \u201ceurope-west2\u201d region must not include zone suffixes (e.g., \u201c-a\u201d). For a full list of valid regions, see: https://cloud.google.com/compute/docs/regions-zones.</p> <code>networkConfig</code>  GCPNetworkConfig  <p>networkConfig specifies VPC configuration for Private Service Connect. Required for VPC configuration in Private Service Connect deployments.</p> <code>endpointAccess</code>  GCPEndpointAccessType  (Optional) <p>endpointAccess controls API endpoint accessibility for the HostedControlPlane on GCP. Allowed values: \u201cPrivate\u201d, \u201cPublicAndPrivate\u201d. Defaults to \u201cPrivate\u201d.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.GCPPrivateServiceConnectSpec","title":"GCPPrivateServiceConnectSpec","text":"<p> (Appears on: GCPPrivateServiceConnect) </p> <p> <p>GCPPrivateServiceConnectSpec defines the desired state of PSC infrastructure</p> </p> Field Description <code>forwardingRuleName</code>  string  <p>forwardingRuleName is the name of the Internal Load Balancer forwarding rule</p> <code>consumerAcceptList</code>  []string  <p>consumerAcceptList specifies which customer projects can connect Accepts both project IDs (e.g. \u201cmy-project-123\u201d) and project numbers (e.g. \u201c123456789012\u201d)</p> <code>natSubnet</code>  string  (Optional) <p>natSubnet is the subnet used for NAT by the Service Attachment Auto-populated by the HyperShift Operator</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.GCPPrivateServiceConnectStatus","title":"GCPPrivateServiceConnectStatus","text":"<p> (Appears on: GCPPrivateServiceConnect) </p> <p> <p>GCPPrivateServiceConnectStatus defines the observed state of PSC infrastructure</p> </p> Field Description <code>conditions</code>  []Kubernetes meta/v1.Condition  (Optional) <p>conditions represent the current state of PSC infrastructure Current condition types are: \u201cGCPPrivateServiceConnectAvailable\u201d, \u201cGCPServiceAttachmentAvailable\u201d, \u201cGCPEndpointAvailable\u201d, \u201cGCPDNSAvailable\u201d</p> <code>serviceAttachmentName</code>  string  (Optional) <p>serviceAttachmentName is the name of the created Service Attachment</p> <code>serviceAttachmentURI</code>  string  (Optional) <p>serviceAttachmentURI is the URI customers use to connect Format: projects/{project}/regions/{region}/serviceAttachments/{name}</p> <code>endpointIP</code>  string  (Optional) <p>endpointIP is the reserved IP address for the PSC endpoint</p> <code>dnsZoneName</code>  string  (Optional) <p>dnsZoneName is the private DNS zone name</p> <code>dnsRecords</code>  []string  (Optional) <p>dnsRecords lists the created DNS A records</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.GCPResourceReference","title":"GCPResourceReference","text":"<p> (Appears on: GCPNetworkConfig) </p> <p> <p>GCPResourceReference represents a reference to a GCP resource by name. Follows GCP naming patterns (name-based APIs, not ID-based like AWS). See https://google.aip.dev/122 for GCP resource name standards.</p> </p> Field Description <code>name</code>  string  <p>name is the name of the GCP resource</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.HostedClusterSpec","title":"HostedClusterSpec","text":"<p> (Appears on: HostedCluster) </p> <p> </p> Field Description <code>release</code>  Release  <p>release specifies the desired OCP release payload for all the hosted cluster components. This includes those components running management side like the Kube API Server and the CVO but also the operands which land in the hosted cluster data plane like the ingress controller, ovn agents, etc. The maximum and minimum supported release versions are determined by the running Hypersfhit Operator. Attempting to use an unsupported version will result in the HostedCluster being degraded and the validateReleaseImage condition being false. Attempting to use a release with a skew against a NodePool release bigger than N-2 for the y-stream will result in leaving the NodePool in an unsupported state. Changing this field will trigger a rollout of the control plane components. The behavior of the rollout will be driven by the ControllerAvailabilityPolicy and InfrastructureAvailabilityPolicy for PDBs and maxUnavailable and surce policies.</p> <code>controlPlaneRelease</code>  Release  (Optional) <p>controlPlaneRelease is like spec.release but only for the components running on the management cluster. This excludes any operand which will land in the hosted cluster data plane. It is useful when you need to apply patch management side like a CVE, transparently for the hosted cluster. Version input for this field is free, no validation is performed against spec.release or maximum and minimum is performed. If defined, it will dicate the version of the components running management side, while spec.release will dictate the version of the components landing in the hosted cluster data plane. If not defined, spec.release is used for both. Changing this field will trigger a rollout of the control plane. The behavior of the rollout will be driven by the ControllerAvailabilityPolicy and InfrastructureAvailabilityPolicy for PDBs and maxUnavailable and surce policies.</p> <code>clusterID</code>  string  (Optional) <p>clusterID uniquely identifies this cluster. This is expected to be an RFC4122 UUID value (xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx in hexadecimal digits). As with a Kubernetes metadata.uid, this ID uniquely identifies this cluster in space and time. This value identifies the cluster in metrics pushed to telemetry and metrics produced by the control plane operators. If a value is not specified, a random clusterID will be generated and set by the controller. Once set, this value is immutable.</p> <code>infraID</code>  string  (Optional) <p>infraID is a globally unique identifier for the cluster. It must consist of lowercase alphanumeric characters and hyphens (\u2018-\u2019) only, and start and end with an alphanumeric character. It must be no more than 253 characters in length. This identifier will be used to associate various cloud resources with the HostedCluster and its associated NodePools. infraID is used to compute and tag created resources with \u201ckubernetes.io/cluster/\u201d+hcluster.Spec.InfraID which has contractual meaning for the cloud provider implementations. If a value is not specified, a random infraID will be generated and set by the controller. Once set, this value is immutable.</p> <code>updateService</code>  github.com/openshift/api/config/v1.URL  (Optional) <p>updateService may be used to specify the preferred upstream update service. If omitted we will use the appropriate update service for the cluster and region. This is used by the control plane operator to determine and signal the appropriate available upgrades in the hostedCluster.status.</p> <code>channel</code>  string  (Optional) <p>channel is an identifier for explicitly requesting that a non-default set of updates be applied to this cluster. If omitted no particular upgrades are suggested. TODO(alberto): Consider the backend to use the default channel by default. Default channel will contain stable updates that are appropriate for production clusters.</p> <code>platform</code>  PlatformSpec  <p>platform specifies the underlying infrastructure provider for the cluster and is used to configure platform specific behavior.</p> <code>kubeAPIServerDNSName</code>  string  (Optional) <p>kubeAPIServerDNSName specifies a desired DNS name to resolve to the KAS. When set, the controller will automatically generate a secret with kubeconfig and expose it in the hostedCluster Status.customKubeconfig field. If it\u2019s set or removed day 2, the kubeconfig generated secret will be created, recreated or deleted. The DNS entries should be resolvable from the cluster, so this should be manually configured in the DNS provider. This field works in conjunction with configuration.APIServer.ServingCerts.NamedCertificates to enable access to the API server via a custom domain name. The NamedCertificates provide the TLS certificates for the custom domain, while this field triggers the generation of a kubeconfig that uses those certificates. This API endpoint only works in OCP version 4.19 or later. Older versions will result in a no-op.</p> <code>controllerAvailabilityPolicy</code>  AvailabilityPolicy  (Optional) <p>controllerAvailabilityPolicy specifies the availability policy applied to critical control plane components like the Kube API Server. Possible values are HighlyAvailable and SingleReplica. The default value is HighlyAvailable. This field is immutable.</p> <code>infrastructureAvailabilityPolicy</code>  AvailabilityPolicy  (Optional) <p>infrastructureAvailabilityPolicy specifies the availability policy applied to infrastructure services which run on the hosted cluster data plane like the ingress controller and image registry controller. Possible values are HighlyAvailable and SingleReplica. The default value is SingleReplica.</p> <code>dns</code>  DNSSpec  (Optional) <p>dns specifies the DNS configuration for the hosted cluster ingress.</p> <code>networking</code>  ClusterNetworking  <p>networking specifies network configuration for the hosted cluster. Defaults to OVNKubernetes with a cluster network of cidr: \u201c10.132.0.0/14\u201d and a service network of cidr: \u201c172.31.0.0/16\u201d.</p> <code>autoscaling</code>  ClusterAutoscaling  (Optional) <p>autoscaling specifies auto-scaling behavior that applies to all NodePools associated with this HostedCluster.</p> <code>autoNode</code>  AutoNode  (Optional) <p>autoNode specifies the configuration for the autoNode feature.</p> <code>etcd</code>  EtcdSpec  <p>etcd specifies configuration for the control plane etcd cluster. The default managementType is Managed. Once set, the managementType cannot be changed.</p> <code>services</code>  []ServicePublishingStrategyMapping  <p>services specifies how individual control plane services endpoints are published for consumption. This requires APIServer;OAuthServer;Konnectivity;Ignition. This field is immutable for all platforms but IBMCloud. Max is 6 to account for OIDC;OVNSbDb for backward compatibility though they are no-op.</p> <p>-kubebuilder:validation:XValidation:rule=\u201cself.all(s, !(s.service == \u2018APIServer\u2019 &amp;&amp; s.servicePublishingStrategy.type == \u2018Route\u2019) || has(s.servicePublishingStrategy.route.hostname))\u201d,message=\u201cIf serviceType is \u2018APIServer\u2019 and publishing strategy is \u2018Route\u2019, then hostname must be set\u201d -kubebuilder:validation:XValidation:rule=\u201cself.platform.type == \u2018IBMCloud\u2019 ? [\u2018APIServer\u2019, \u2018OAuthServer\u2019, \u2018Konnectivity\u2019].all(requiredType, self.exists(s, s.service == requiredType))\u201d,message=\u201cServices list must contain at least \u2018APIServer\u2019, \u2018OAuthServer\u2019, and \u2018Konnectivity\u2019 service types\u201d : [\u2018APIServer\u2019, \u2018OAuthServer\u2019, \u2018Konnectivity\u2019, \u2018Ignition\u2019].all(requiredType, self.exists(s, s.service == requiredType))\u201c,message=\u201cServices list must contain at least \u2018APIServer\u2019, \u2018OAuthServer\u2019, \u2018Konnectivity\u2019, and \u2018Ignition\u2019 service types\u201d -kubebuilder:validation:XValidation:rule=\u201cself.filter(s, s.servicePublishingStrategy.type == \u2018Route\u2019 &amp;&amp; has(s.servicePublishingStrategy.route) &amp;&amp; has(s.servicePublishingStrategy.route.hostname)).all(x, self.filter(y, y.servicePublishingStrategy.type == \u2018Route\u2019 &amp;&amp; (has(y.servicePublishingStrategy.route) &amp;&amp; has(y.servicePublishingStrategy.route.hostname) &amp;&amp; y.servicePublishingStrategy.route.hostname == x.servicePublishingStrategy.route.hostname)).size() &lt;= 1)\u201d,message=\u201cEach route publishingStrategy \u2018hostname\u2019 must be unique within the Services list.\u201d -kubebuilder:validation:XValidation:rule=\u201cself.filter(s, s.servicePublishingStrategy.type == \u2018NodePort\u2019 &amp;&amp; has(s.servicePublishingStrategy.nodePort) &amp;&amp; has(s.servicePublishingStrategy.nodePort.address) &amp;&amp; has(s.servicePublishingStrategy.nodePort.port)).all(x, self.filter(y, y.servicePublishingStrategy.type == \u2018NodePort\u2019 &amp;&amp; (has(y.servicePublishingStrategy.nodePort) &amp;&amp; has(y.servicePublishingStrategy.nodePort.address) &amp;&amp; y.servicePublishingStrategy.nodePort.address == x.servicePublishingStrategy.nodePort.address &amp;&amp; has(y.servicePublishingStrategy.nodePort.port) &amp;&amp; y.servicePublishingStrategy.nodePort.port == x.servicePublishingStrategy.nodePort.port )).size() &lt;= 1)\u201d,message=\u201cEach nodePort publishingStrategy \u2018nodePort\u2019 and \u2018hostname\u2019 must be unique within the Services list.\u201d TODO(alberto): this breaks the cost budget for &lt; 4.17. We should figure why and enable it back. And If not fixable, consider imposing a minimum version on the management cluster.</p> <code>pullSecret</code>  Kubernetes core/v1.LocalObjectReference  <p>pullSecret is a local reference to a Secret that must have a \u201c.dockerconfigjson\u201d key whose content must be a valid Openshift pull secret JSON. If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state. TODO(alberto): Signal this in a condition. This pull secret will be part of every payload generated by the controllers for any NodePool of the HostedCluster and it will be injected into the container runtime of all NodePools. Changing this value will trigger a rollout for all existing NodePools in the cluster. Changing the content of the secret inplace will not trigger a rollout and might result in unpredictable behaviour. TODO(alberto): have our own local reference type to include our opinions and avoid transparent changes.</p> <code>sshKey</code>  Kubernetes core/v1.LocalObjectReference  (Optional) <p>sshKey is a local reference to a Secret that must have a \u201cid_rsa.pub\u201d key whose content must be the public part of 1..N SSH keys. If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state. TODO(alberto): Signal this in a condition. When sshKey is set, the controllers will generate a machineConfig with the sshAuthorizedKeys https://coreos.github.io/ignition/configuration-v3_2/ populated with this value. This MachineConfig will be part of every payload generated by the controllers for any NodePool of the HostedCluster. Changing this value will trigger a rollout for all existing NodePools in the cluster.</p> <code>issuerURL</code>  string  (Optional) <p>issuerURL is an OIDC issuer URL which will be used as the issuer in all ServiceAccount tokens generated by the control plane API server via \u2013service-account-issuer kube api server flag. https://k8s-docs.netlify.app/en/docs/reference/command-line-tools-reference/kube-apiserver/ https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection The default value is kubernetes.default.svc, which only works for in-cluster validation. If the platform is AWS and this value is set, the controller will update an s3 object with the appropriate OIDC documents (using the serviceAccountSigningKey info) into that issuerURL. The expectation is for this s3 url to be backed by an OIDC provider in the AWS IAM.</p> <code>serviceAccountSigningKey</code>  Kubernetes core/v1.LocalObjectReference  (Optional) <p>serviceAccountSigningKey is a local reference to a secret that must have a \u201ckey\u201d key whose content must be the private key used by the service account token issuer. If not specified, a service account signing key will be generated automatically for the cluster. When specifying a service account signing key, an IssuerURL must also be specified. If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state. TODO(alberto): Signal this in a condition.</p> <code>configuration</code>  ClusterConfiguration  (Optional) <p>configuration specifies configuration for individual OCP components in the cluster, represented as embedded resources that correspond to the openshift configuration API.</p> <code>operatorConfiguration</code>  OperatorConfiguration  (Optional) <p>operatorConfiguration specifies configuration for individual OCP operators in the cluster.</p> <code>auditWebhook</code>  Kubernetes core/v1.LocalObjectReference  (Optional) <p>auditWebhook contains metadata for configuring an audit webhook endpoint for a cluster to process cluster audit events. It references a secret that contains the webhook information for the audit webhook endpoint. It is a secret because if the endpoint has mTLS the kubeconfig will contain client keys. The kubeconfig needs to be stored in the secret with a secret key name that corresponds to the constant AuditWebhookKubeconfigKey.</p> <code>imageContentSources</code>  []ImageContentSource  (Optional) <p>imageContentSources specifies image mirrors that can be used by cluster nodes to pull content. When imageContentSources is set, the controllers will generate a machineConfig. This MachineConfig will be part of every payload generated by the controllers for any NodePool of the HostedCluster. Changing this value will trigger a rollout for all existing NodePools in the cluster.</p> <code>additionalTrustBundle</code>  Kubernetes core/v1.LocalObjectReference  (Optional) <p>additionalTrustBundle is a local reference to a ConfigMap that must have a \u201cca-bundle.crt\u201d key whose content must be a PEM-encoded X.509 certificate bundle that will be added to the hosted controlplane and nodes If the reference is set but none of the above requirements are met, the HostedCluster will enter a degraded state. TODO(alberto): Signal this in a condition. This will be part of every payload generated by the controllers for any NodePool of the HostedCluster. Changing this value will trigger a rollout for all existing NodePools in the cluster.</p> <code>secretEncryption</code>  SecretEncryptionSpec  (Optional) <p>secretEncryption specifies a Kubernetes secret encryption strategy for the control plane.</p> <code>fips</code>  bool  (Optional) <p>fips indicates whether this cluster\u2019s nodes will be running in FIPS mode. If set to true, the control plane\u2019s ignition server will be configured to expect that nodes joining the cluster will be FIPS-enabled.</p> <code>pausedUntil</code>  string  (Optional) <p>pausedUntil is a field that can be used to pause reconciliation on the HostedCluster controller, resulting in any change to the HostedCluster being ignored. Either a date can be provided in RFC3339 format or a boolean as in \u2018true\u2019, \u2018false\u2019, \u2018True\u2019, \u2018False\u2019. If a date is provided: reconciliation is paused on the resource until that date. If the boolean true is provided: reconciliation is paused on the resource until the field is removed.</p> <code>olmCatalogPlacement</code>  OLMCatalogPlacement  (Optional) <p>olmCatalogPlacement specifies the placement of OLM catalog components. By default, this is set to management and OLM catalog components are deployed onto the management cluster. If set to guest, the OLM catalog components will be deployed onto the guest cluster.</p> <code>nodeSelector</code>  map[string]string  (Optional) <p>nodeSelector when specified, is propagated to all control plane Deployments and Stateful sets running management side. It must be satisfied by the management Nodes for the pods to be scheduled. Otherwise the HostedCluster will enter a degraded state. Changes to this field will propagate to existing Deployments and StatefulSets. TODO(alberto): add additional validation for the map key/values.</p> <code>tolerations</code>  []Kubernetes core/v1.Toleration  (Optional) <p>tolerations when specified, define what custom tolerations are added to the hcp pods.</p> <code>labels</code>  map[string]string  (Optional) <p>labels when specified, define what custom labels are added to the hcp pods. Changing this day 2 will cause a rollout of all hcp pods. Duplicate keys are not supported. If duplicate keys are defined, only the last key/value pair is preserved. Valid values are those in https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</p> <p>-kubebuilder:validation:XValidation:rule=<code>self.all(key, size(key) &lt;= 317 &amp;&amp; key.matches('^(([A-Za-z0-9]+(\\\\.[A-Za-z0-9]+)?)*[A-Za-z0-9]\\\\/)?(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])$'))</code>, message=\u201clabel key must have two segments: an optional prefix and name, separated by a slash (/). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (), dots (.), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (.), not longer than 253 characters in total, followed by a slash (/)\u201d -kubebuilder:validation:XValidation:rule=<code>self.all(key, size(self[key]) &lt;= 63 &amp;&amp; self[key].matches('^(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?$'))</code>, message=\u201clabel value must be 63 characters or less (can be empty), consist of alphanumeric characters, dashes (-), underscores () or dots (.), and begin and end with an alphanumeric character\u201d TODO: key/value validations break cost budget for &lt;=4.17. We should figure why and enable it back.</p> <code>capabilities</code>  Capabilities  (Optional) <p>capabilities allows for disabling optional components at cluster install time. This field is optional and once set cannot be changed.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.HostedClusterStatus","title":"HostedClusterStatus","text":"<p> (Appears on: HostedCluster) </p> <p> <p>HostedClusterStatus is the latest observed status of a HostedCluster.</p> </p> Field Description <code>conditions</code>  []Kubernetes meta/v1.Condition  (Optional) <p>conditions represents the latest available observations of a control plane\u2019s current state.</p> <code>version</code>  ClusterVersionStatus  (Optional) <p>version is the status of the release version applied to the HostedCluster.</p> <code>kubeconfig</code>  Kubernetes core/v1.LocalObjectReference  (Optional) <p>kubeconfig is a reference to the secret containing the default kubeconfig for the cluster.</p> <code>customKubeconfig</code>  Kubernetes core/v1.LocalObjectReference  (Optional) <p>customKubeconfig is a local secret reference to the external custom kubeconfig. Once the hypershift operator sets this status field, it will generate a secret with the specified name containing a kubeconfig within the <code>HostedCluster</code> namespace.</p> <code>kubeadminPassword</code>  Kubernetes core/v1.LocalObjectReference  (Optional) <p>kubeadminPassword is a reference to the secret that contains the initial kubeadmin user password for the guest cluster.</p> <code>ignitionEndpoint</code>  string  (Optional) <p>ignitionEndpoint is the endpoint injected in the ign config userdata. It exposes the config for instances to become kubernetes nodes.</p> <code>controlPlaneEndpoint</code>  APIEndpoint  (Optional) <p>controlPlaneEndpoint contains the endpoint information by which external clients can access the control plane. This is populated after the infrastructure is ready.</p> <code>oauthCallbackURLTemplate</code>  string  (Optional) <p>oauthCallbackURLTemplate contains a template for the URL to use as a callback for identity providers. The [identity-provider-name] placeholder must be replaced with the name of an identity provider defined on the HostedCluster. This is populated after the infrastructure is ready.</p> <code>payloadArch</code>  PayloadArchType  (Optional) <p>payloadArch represents the CPU architecture type of the HostedCluster.Spec.Release.Image. The valid values are: Multi, ARM64, AMD64, S390X, or PPC64LE.</p> <code>platform</code>  PlatformStatus  (Optional) <p>platform contains platform-specific status of the HostedCluster</p> <code>configuration</code>  ConfigurationStatus  (Optional) <p>configuration contains the cluster configuration status of the HostedCluster</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.HostedControlPlaneSpec","title":"HostedControlPlaneSpec","text":"<p> <p>HostedControlPlaneSpec defines the desired state of HostedControlPlane</p> </p> Field Description <code>releaseImage</code>  string  <p>releaseImage is the release image applied to the hosted control plane.</p> <code>controlPlaneReleaseImage</code>  string  (Optional) <p>controlPlaneReleaseImage specifies the desired OCP release payload for control plane components running on the management cluster. If not defined, ReleaseImage is used</p> <code>updateService</code>  github.com/openshift/api/config/v1.URL  (Optional) <p>updateService may be used to specify the preferred upstream update service. By default it will use the appropriate update service for the cluster and region.</p> <code>channel</code>  string  (Optional) <p>channel is an identifier for explicitly requesting that a non-default set of updates be applied to this cluster. The default channel will be contain stable updates that are appropriate for production clusters.</p> <code>pullSecret</code>  Kubernetes core/v1.LocalObjectReference  <p>pullSecret is a reference to a secret containing the pull secret for the hosted control plane.</p> <code>issuerURL</code>  string  <p>issuerURL is an OIDC issuer URL which is used as the issuer in all ServiceAccount tokens generated by the control plane API server. The default value is kubernetes.default.svc, which only works for in-cluster validation.</p> <code>networking</code>  ClusterNetworking  (Optional) <p>networking specifies network configuration for the cluster. Temporarily optional for backward compatibility, required in future releases.</p> <code>sshKey</code>  Kubernetes core/v1.LocalObjectReference  <p>sshKey is a reference to a secret containing the SSH key for the hosted control plane.</p> <code>clusterID</code>  string  (Optional) <p>clusterID is the unique id that identifies the cluster externally. Making it optional here allows us to keep compatibility with previous versions of the control-plane-operator that have no knowledge of this field.</p> <code>infraID</code>  string  <p>infraID is the unique id that identifies the cluster internally.</p> <code>platform</code>  PlatformSpec  <p>platform is the platform configuration for the cluster.</p> <code>dns</code>  DNSSpec  <p>dns is the DNS configuration for the cluster.</p> <code>serviceAccountSigningKey</code>  Kubernetes core/v1.LocalObjectReference  (Optional) <p>serviceAccountSigningKey is a reference to a secret containing the private key used by the service account token issuer. The secret is expected to contain a single key named \u201ckey\u201d. If not specified, a service account signing key will be generated automatically for the cluster.</p> <code>controllerAvailabilityPolicy</code>  AvailabilityPolicy  (Optional) <p>controllerAvailabilityPolicy specifies the availability policy applied to critical control plane components. The default value is SingleReplica.</p> <code>infrastructureAvailabilityPolicy</code>  AvailabilityPolicy  (Optional) <p>infrastructureAvailabilityPolicy specifies the availability policy applied to infrastructure services which run on cluster nodes. The default value is SingleReplica.</p> <code>fips</code>  bool  (Optional) <p>fips specifies if the nodes for the cluster will be running in FIPS mode</p> <code>kubeconfig</code>  KubeconfigSecretRef  (Optional) <p>kubeconfig specifies the name and key for the kubeconfig secret</p> <code>kubeAPIServerDNSName</code>  string  (Optional) <p>kubeAPIServerDNSName specifies a desired DNS name to resolve to the KAS. When set, the controller will automatically generate a secret with kubeconfig and expose it in the hostedCluster Status.customKubeconfig field. If it\u2019s set or removed day 2, the kubeconfig generated secret will be created, recreated or deleted. The DNS entries should be resolvable from the cluster, so this should be manually configured in the DNS provider. This field works in conjunction with configuration.APIServer.ServingCerts.NamedCertificates to enable access to the API server via a custom domain name. The NamedCertificates provide the TLS certificates for the custom domain, while this field triggers the generation of a kubeconfig that uses those certificates.</p> <code>services</code>  []ServicePublishingStrategyMapping  <p>services defines metadata about how control plane services are published in the management cluster.</p> <code>auditWebhook</code>  Kubernetes core/v1.LocalObjectReference  (Optional) <p>auditWebhook contains metadata for configuring an audit webhook endpoint for a cluster to process cluster audit events. It references a secret that contains the webhook information for the audit webhook endpoint. It is a secret because if the endpoint has MTLS the kubeconfig will contain client keys. This is currently only supported in IBM Cloud. The kubeconfig needs to be stored in the secret with a secret key name that corresponds to the constant AuditWebhookKubeconfigKey.</p> <code>etcd</code>  EtcdSpec  <p>etcd contains metadata about the etcd cluster the hypershift managed Openshift control plane components use to store data.</p> <code>configuration</code>  ClusterConfiguration  (Optional) <p>configuration embeds resources that correspond to the openshift configuration API: https://docs.openshift.com/container-platform/4.7/rest_api/config_apis/config-apis-index.html</p> <code>operatorConfiguration</code>  OperatorConfiguration  (Optional) <p>operatorConfiguration specifies configuration for individual OCP operators in the cluster.</p> <code>imageContentSources</code>  []ImageContentSource  (Optional) <p>imageContentSources lists sources/repositories for the release-image content.</p> <code>additionalTrustBundle</code>  Kubernetes core/v1.LocalObjectReference  (Optional) <p>additionalTrustBundle references a ConfigMap containing a PEM-encoded X.509 certificate bundle</p> <code>secretEncryption</code>  SecretEncryptionSpec  (Optional) <p>secretEncryption contains metadata about the kubernetes secret encryption strategy being used for the cluster when applicable.</p> <code>pausedUntil</code>  string  (Optional) <p>pausedUntil is a field that can be used to pause reconciliation on a resource. Either a date can be provided in RFC3339 format or a boolean. If a date is provided: reconciliation is paused on the resource until that date. If the boolean true is provided: reconciliation is paused on the resource until the field is removed.</p> <code>olmCatalogPlacement</code>  OLMCatalogPlacement  (Optional) <p>olmCatalogPlacement specifies the placement of OLM catalog components. By default, this is set to management and OLM catalog components are deployed onto the management cluster. If set to guest, the OLM catalog components will be deployed onto the guest cluster.</p> <code>autoscaling</code>  ClusterAutoscaling  (Optional) <p>autoscaling specifies auto-scaling behavior that applies to all NodePools associated with the control plane.</p> <code>autoNode</code>  AutoNode  (Optional) <p>autoNode specifies the configuration for the autoNode feature.</p> <code>nodeSelector</code>  map[string]string  (Optional) <p>nodeSelector when specified, must be true for the pods managed by the HostedCluster to be scheduled.</p> <code>tolerations</code>  []Kubernetes core/v1.Toleration  (Optional) <p>tolerations when specified, define what custom tolerations are added to the hcp pods.</p> <code>labels</code>  map[string]string  (Optional) <p>labels when specified, define what custom labels are added to the hcp pods. Changing this day 2 will cause a rollout of all hcp pods. Duplicate keys are not supported. If duplicate keys are defined, only the last key/value pair is preserved. Valid values are those in https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</p> <p>-kubebuilder:validation:XValidation:rule=<code>self.all(key, size(key) &lt;= 317 &amp;&amp; key.matches('^(([A-Za-z0-9]+(\\\\.[A-Za-z0-9]+)?)*[A-Za-z0-9]\\\\/)?(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])$'))</code>, message=\u201clabel key must have two segments: an optional prefix and name, separated by a slash (/). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (), dots (.), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (.), not longer than 253 characters in total, followed by a slash (/)\u201d -kubebuilder:validation:XValidation:rule=<code>self.all(key, size(self[key]) &lt;= 63 &amp;&amp; self[key].matches('^(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?$'))</code>, message=\u201clabel value must be 63 characters or less (can be empty), consist of alphanumeric characters, dashes (-), underscores () or dots (.), and begin and end with an alphanumeric character\u201d TODO: key/value validations break cost budget for &lt;=4.17. We should figure why and enable it back.</p> <code>capabilities</code>  Capabilities  (Optional) <p>capabilities allows for disabling optional components at cluster install time. This field is optional and once set cannot be changed.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.HostedControlPlaneStatus","title":"HostedControlPlaneStatus","text":"<p> <p>HostedControlPlaneStatus defines the observed state of HostedControlPlane</p> </p> Field Description <code>conditions</code>  []Kubernetes meta/v1.Condition  (Optional) <p>conditions contains details for one aspect of the current state of the HostedControlPlane. Current condition types are: \u201cAvailable\u201d</p> <code>ready</code>  bool  <p>ready denotes that the HostedControlPlane API Server is ready to receive requests This satisfies CAPI contract https://github.com/kubernetes-sigs/cluster-api/blob/cd3a694deac89d5ebeb888307deaa61487207aa0/controllers/cluster_controller_phases.go#L226-L230</p> <code>initialized</code>  bool  <p>initialized denotes whether or not the control plane has provided a kubeadm-config. Once this condition is marked true, its value is never changed. See the Ready condition for an indication of the current readiness of the cluster\u2019s control plane. This satisfies CAPI contract https://github.com/kubernetes-sigs/cluster-api/blob/cd3a694deac89d5ebeb888307deaa61487207aa0/controllers/cluster_controller_phases.go#L238-L252</p> <code>externalManagedControlPlane</code>  bool  (Optional) <p>externalManagedControlPlane indicates to cluster-api that the control plane is managed by an external service. https://github.com/kubernetes-sigs/cluster-api/blob/65e5385bffd71bf4aad3cf34a537f11b217c7fab/controllers/machine_controller.go#L468</p> <code>controlPlaneEndpoint</code>  APIEndpoint  (Optional) <p>controlPlaneEndpoint contains the endpoint information by which external clients can access the control plane.  This is populated after the infrastructure is ready.</p> <code>oauthCallbackURLTemplate</code>  string  (Optional) <p>oauthCallbackURLTemplate contains a template for the URL to use as a callback for identity providers. The [identity-provider-name] placeholder must be replaced with the name of an identity provider defined on the HostedCluster. This is populated after the infrastructure is ready.</p> <code>versionStatus</code>  ClusterVersionStatus  (Optional) <p>versionStatus is the status of the release version applied by the hosted control plane operator.</p> <code>version</code>  string  (Optional) <p>version is the semantic version of the release applied by the hosted control plane operator</p> <p>Deprecated: Use versionStatus.desired.version instead.</p> <code>releaseImage</code>  string  (Optional) <p>releaseImage is the release image applied to the hosted control plane.</p> <p>Deprecated: Use versionStatus.desired.image instead.</p> <code>lastReleaseImageTransitionTime</code>  Kubernetes meta/v1.Time  (Optional) <p>lastReleaseImageTransitionTime is the time of the last update to the current releaseImage property.</p> <p>Deprecated: Use versionStatus.history[0].startedTime instead.</p> <code>kubeConfig</code>  KubeconfigSecretRef  (Optional) <p>kubeConfig is a reference to the secret containing the default kubeconfig for this control plane.</p> <code>customKubeconfig</code>  KubeconfigSecretRef  (Optional) <p>customKubeconfig references an external custom kubeconfig secret. This field is populated in the status when a custom kubeconfig secret has been generated for the hosted cluster. It contains the name and key of the secret located in the hostedCluster namespace. This field is only populated when kubeApiExternalName is set. If this field is removed during a day 2 operation, the referenced secret will be deleted and this field will be removed from the hostedCluster status.</p> <code>kubeadminPassword</code>  Kubernetes core/v1.LocalObjectReference  (Optional) <p>kubeadminPassword is a reference to the secret containing the initial kubeadmin password for the guest cluster.</p> <code>platform</code>  PlatformStatus  (Optional) <p>platform contains platform-specific status of the HostedCluster</p> <code>nodeCount</code>  int  (Optional) <p>nodeCount tracks the number of nodes in the HostedControlPlane.</p> <code>configuration</code>  ConfigurationStatus  (Optional) <p>configuration contains the cluster configuration status of the HostedCluster</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthSpec","title":"IBMCloudKMSAuthSpec","text":"<p> (Appears on: IBMCloudKMSSpec) </p> <p> <p>IBMCloudKMSAuthSpec defines metadata for how authentication is done with IBM Cloud KMS</p> </p> Field Description <code>type</code>  IBMCloudKMSAuthType  <p>type defines the IBM Cloud KMS authentication strategy</p> <code>unmanaged</code>  IBMCloudKMSUnmanagedAuthSpec  (Optional) <p>unmanaged defines the auth metadata the customer provides to interact with IBM Cloud KMS</p> <code>managed</code>  IBMCloudKMSManagedAuthSpec  (Optional) <p>managed defines metadata around the service to service authentication strategy for the IBM Cloud KMS system (all provider managed).</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.IBMCloudKMSAuthType","title":"IBMCloudKMSAuthType","text":"<p> (Appears on: IBMCloudKMSAuthSpec) </p> <p> <p>IBMCloudKMSAuthType defines the IBM Cloud KMS authentication strategy</p> </p> Value Description <p>\"Managed\"</p> <p>IBMCloudKMSManagedAuth defines the KMS authentication strategy where the IKS/ROKS platform uses service to service auth to call IBM Cloud KMS APIs (no customer credentials required)</p> <p>\"Unmanaged\"</p> <p>IBMCloudKMSUnmanagedAuth defines the KMS authentication strategy where a customer supplies IBM Cloud authentication to interact with IBM Cloud KMS APIs</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.IBMCloudKMSKeyEntry","title":"IBMCloudKMSKeyEntry","text":"<p> (Appears on: IBMCloudKMSSpec) </p> <p> <p>IBMCloudKMSKeyEntry defines metadata for an IBM Cloud KMS encryption key</p> </p> Field Description <code>crkID</code>  string  <p>crkID is the customer rook key id</p> <code>instanceID</code>  string  <p>instanceID is the id for the key protect instance</p> <code>correlationID</code>  string  <p>correlationID is an identifier used to track all api call usage from hypershift</p> <code>url</code>  string  <p>url is the url to call key protect apis over</p> <code>keyVersion</code>  int  <p>keyVersion is a unique number associated with the key. The number increments whenever a new key is enabled for data encryption.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.IBMCloudKMSManagedAuthSpec","title":"IBMCloudKMSManagedAuthSpec","text":"<p> (Appears on: IBMCloudKMSAuthSpec) </p> <p> <p>IBMCloudKMSManagedAuthSpec defines metadata around the service to service authentication strategy for the IBM Cloud KMS system (all provider managed).</p> </p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.IBMCloudKMSSpec","title":"IBMCloudKMSSpec","text":"<p> (Appears on: KMSSpec) </p> <p> <p>IBMCloudKMSSpec defines metadata for the IBM Cloud KMS encryption strategy</p> </p> Field Description <code>region</code>  string  <p>region is the IBM Cloud region</p> <code>auth</code>  IBMCloudKMSAuthSpec  <p>auth defines metadata for how authentication is done with IBM Cloud KMS</p> <code>keyList</code>  []IBMCloudKMSKeyEntry  <p>keyList defines the list of keys used for data encryption</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.IBMCloudKMSUnmanagedAuthSpec","title":"IBMCloudKMSUnmanagedAuthSpec","text":"<p> (Appears on: IBMCloudKMSAuthSpec) </p> <p> <p>IBMCloudKMSUnmanagedAuthSpec defines the auth metadata the customer provides to interact with IBM Cloud KMS</p> </p> Field Description <code>credentials</code>  Kubernetes core/v1.LocalObjectReference  <p>credentials should reference a secret with a key field of IBMCloudIAMAPIKeySecretKey that contains a apikey to call IBM Cloud KMS APIs</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.IBMCloudPlatformSpec","title":"IBMCloudPlatformSpec","text":"<p> (Appears on: NodePoolPlatform,  PlatformSpec) </p> <p> <p>IBMCloudPlatformSpec defines IBMCloud specific settings for components</p> </p> Field Description <code>providerType</code>  github.com/openshift/api/config/v1.IBMCloudProviderType  (Optional) <p>providerType is a specific supported infrastructure provider within IBM Cloud.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ImageContentSource","title":"ImageContentSource","text":"<p> (Appears on: HostedClusterSpec,  HostedControlPlaneSpec) </p> <p> <p>ImageContentSource specifies image mirrors that can be used by cluster nodes to pull content. For cluster workloads, if a container image registry host of the pullspec matches Source then one of the Mirrors are substituted as hosts in the pullspec and tried in order to fetch the image.</p> </p> Field Description <code>source</code>  string  <p>source is the repository that users refer to, e.g. in image pull specifications.</p> <code>mirrors</code>  []string  (Optional) <p>mirrors are one or more repositories that may also contain the same images.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ImageType","title":"ImageType","text":"<p> (Appears on: AWSNodePoolPlatform) </p> <p> <p>ImageType specifies the type of image to use for node instances.</p> </p> Value Description <p>\"Linux\"</p> <p>ImageTypeLinux represents the default image type (Linux/RHCOS). This is used when ImageType is empty or unspecified.</p> <p>\"Windows\"</p> <p>ImageTypeWindows represents a Windows-based image type. When set, the controller will automatically populate the AMI field with a Windows-compatible AMI based on the region and OpenShift version.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.InPlaceUpgrade","title":"InPlaceUpgrade","text":"<p> (Appears on: NodePoolManagement) </p> <p> <p>InPlaceUpgrade specifies an upgrade strategy which upgrades nodes in-place without any new nodes being created or any old nodes being deleted.</p> </p> Field Description <code>maxUnavailable</code>  k8s.io/apimachinery/pkg/util/intstr.IntOrString  (Optional) <p>maxUnavailable is the maximum number of nodes that can be unavailable during the update.</p> <p>Value can be an absolute number (ex: 5) or a percentage of desired nodes (ex: 10%).</p> <p>Absolute number is calculated from percentage by rounding down.</p> <p>Defaults to 1.</p> <p>Example: when this is set to 30%, a max of 30% of the nodes can be made unschedulable/unavailable immediately when the update starts. Once a set of nodes is updated, more nodes can be made unschedulable for update, ensuring that the total number of nodes schedulable at all times during the update is at least 70% of desired nodes.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.InstanceType","title":"InstanceType","text":""},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KMSProvider","title":"KMSProvider","text":"<p> (Appears on: KMSSpec) </p> <p> <p>KMSProvider defines the supported KMS providers</p> </p> Value Description <p>\"AWS\"</p> <p>\"Azure\"</p> <p>\"IBMCloud\"</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KMSSpec","title":"KMSSpec","text":"<p> (Appears on: SecretEncryptionSpec) </p> <p> <p>KMSSpec defines metadata about the kms secret encryption strategy</p> </p> Field Description <code>provider</code>  KMSProvider  <p>provider defines the KMS provider</p> <code>ibmcloud</code>  IBMCloudKMSSpec  (Optional) <p>ibmcloud defines metadata for the IBM Cloud KMS encryption strategy</p> <code>aws</code>  AWSKMSSpec  (Optional) <p>aws defines metadata about the configuration of the AWS KMS Secret Encryption provider</p> <code>azure</code>  AzureKMSSpec  (Optional) <p>azure defines metadata about the configuration of the Azure KMS Secret Encryption provider using Azure key vault</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KarpenterAWSConfig","title":"KarpenterAWSConfig","text":"<p> (Appears on: KarpenterConfig) </p> <p> </p> Field Description <code>roleARN</code>  string  <p>roleARN specifies the ARN of the Karpenter provisioner.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KarpenterConfig","title":"KarpenterConfig","text":"<p> (Appears on: ProvisionerConfig) </p> <p> </p> Field Description <code>platform</code>  PlatformType  <p>platform specifies the platform-specific configuration for Karpenter.</p> <code>aws</code>  KarpenterAWSConfig  (Optional) <p>aws specifies the AWS-specific configuration for Karpenter.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubeVirtNodePoolStatus","title":"KubeVirtNodePoolStatus","text":"<p> (Appears on: NodePoolPlatformStatus) </p> <p> <p>KubeVirtNodePoolStatus contains the KubeVirt platform statuses</p> </p> Field Description <code>cacheName</code>  string  (Optional) <p>cacheName holds the name of the cache DataVolume, if exists</p> <code>credentials</code>  KubevirtPlatformCredentials  (Optional) <p>credentials shows the client credentials used when creating KubeVirt virtual machines. This filed is only exists when the KubeVirt virtual machines are being placed on a cluster separate from the one hosting the Hosted Control Plane components.</p> <p>The default behavior when Credentials is not defined is for the KubeVirt VMs to be placed on the same cluster and namespace as the Hosted Control Plane.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtCachingStrategy","title":"KubevirtCachingStrategy","text":"<p> (Appears on: KubevirtRootVolume) </p> <p> <p>KubevirtCachingStrategy defines the boot image caching strategy</p> </p> Field Description <code>type</code>  KubevirtCachingStrategyType  <p>type is the type of the caching strategy</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtCachingStrategyType","title":"KubevirtCachingStrategyType","text":"<p> (Appears on: KubevirtCachingStrategy) </p> <p> <p>KubevirtCachingStrategyType is the type of the boot image caching mechanism for the KubeVirt provider</p> </p> Value Description <p>\"None\"</p> <p>KubevirtCachingStrategyNone means that hypershift will not cache the boot image</p> <p>\"PVC\"</p> <p>KubevirtCachingStrategyPVC means that hypershift will cache the boot image into a PVC; only relevant when using a QCOW boot image, and is ignored when using a container image</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtCompute","title":"KubevirtCompute","text":"<p> (Appears on: KubevirtNodePoolPlatform) </p> <p> <p>KubevirtCompute contains values associated with the virtual compute hardware requested for the VM.</p> </p> Field Description <code>memory</code>  k8s.io/apimachinery/pkg/api/resource.Quantity  (Optional) <p>memory represents how much guest memory the VM should have</p> <code>cores</code>  uint32  (Optional) <p>cores is the number of CPU cores for the KubeVirt VM.</p> <code>qosClass</code>  QoSClass  (Optional) <p>qosClass if set to \u201cGuaranteed\u201d, requests the scheduler to place the VirtualMachineInstance on a node with limit memory and CPU, equal to be the requested values, to set the VMI as a Guaranteed QoS Class; See here for more details: https://kubevirt.io/user-guide/operations/node_overcommit/#requesting-the-right-qos-class-for-virtualmachineinstances</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtDiskImage","title":"KubevirtDiskImage","text":"<p> (Appears on: KubevirtRootVolume) </p> <p> <p>KubevirtDiskImage contains values representing where the rhcos image is located</p> </p> Field Description <code>containerDiskImage</code>  string  (Optional) <p>containerDiskImage is a string representing the container image that holds the root disk</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtHostDevice","title":"KubevirtHostDevice","text":"<p> (Appears on: KubevirtNodePoolPlatform) </p> <p> </p> Field Description <code>deviceName</code>  string  <p>deviceName is the name of the host device that is desired to be utilized in the HostedCluster\u2019s NodePool The device can be any supported PCI device, including GPU, either as a passthrough or a vGPU slice.</p> <code>count</code>  int  (Optional) <p>count is the number of instances the specified host device will be attached to each of the NodePool\u2019s nodes. Default is 1.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtManualStorageDriverConfig","title":"KubevirtManualStorageDriverConfig","text":"<p> (Appears on: KubevirtStorageDriverSpec) </p> <p> </p> Field Description <code>storageClassMapping</code>  []KubevirtStorageClassMapping  (Optional) <p>storageClassMapping maps StorageClasses on the infra cluster hosting the KubeVirt VMs to StorageClasses that are made available within the Guest Cluster.</p> <p>NOTE: It is possible that not all capabilities of an infra cluster\u2019s storageclass will be present for the corresponding guest clusters storageclass.</p> <code>volumeSnapshotClassMapping</code>  []KubevirtVolumeSnapshotClassMapping  (Optional) <p>volumeSnapshotClassMapping maps VolumeSnapshotClasses on the infra cluster hosting the KubeVirt VMs to VolumeSnapshotClasses that are made available within the Guest Cluster.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtNetwork","title":"KubevirtNetwork","text":"<p> (Appears on: KubevirtNodePoolPlatform) </p> <p> <p>KubevirtNetwork specifies the configuration for a virtual machine network interface</p> </p> Field Description <code>name</code>  string  <p>name specify the network attached to the nodes it is a value with the format \u201c[namespace]/[name]\u201d to reference the multus network attachment definition</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtNodePoolPlatform","title":"KubevirtNodePoolPlatform","text":"<p> (Appears on: NodePoolPlatform) </p> <p> <p>KubevirtNodePoolPlatform specifies the configuration of a NodePool when operating on KubeVirt platform.</p> </p> Field Description <code>rootVolume</code>  KubevirtRootVolume  <p>rootVolume represents values associated with the VM volume that will host rhcos</p> <code>compute</code>  KubevirtCompute  (Optional) <p>compute contains values representing the virtual hardware requested for the VM</p> <code>networkInterfaceMultiqueue</code>  MultiQueueSetting  (Optional) <p>networkInterfaceMultiqueue if set to \u201cEnable\u201d, virtual network interfaces configured with a virtio bus will also enable the vhost multiqueue feature for network devices. The number of queues created depends on additional factors of the VirtualMachineInstance, like the number of guest CPUs.</p> <code>additionalNetworks</code>  []KubevirtNetwork  (Optional) <p>additionalNetworks specify the extra networks attached to the nodes</p> <code>attachDefaultNetwork</code>  bool  (Optional) <p>attachDefaultNetwork specify if the default pod network should be attached to the nodes this can only be set to false if AdditionalNetworks are configured</p> <code>nodeSelector</code>  map[string]string  (Optional) <p>nodeSelector is a selector which must be true for the kubevirt VirtualMachine to fit on a node. Selector which must match a node\u2019s labels for the VM to be scheduled on that node. More info: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/</p> <code>hostDevices</code>  []KubevirtHostDevice  (Optional) <p>hostDevices specifies the host devices (e.g. GPU devices) to be passed from the management cluster, to the nodepool nodes</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtPersistentVolume","title":"KubevirtPersistentVolume","text":"<p> (Appears on: KubevirtVolume) </p> <p> <p>KubevirtPersistentVolume contains the values involved with provisioning persistent storage for a KubeVirt VM.</p> </p> Field Description <code>size</code>  k8s.io/apimachinery/pkg/api/resource.Quantity  (Optional) <p>size is the size of the persistent storage volume</p> <code>storageClass</code>  string  (Optional) <p>storageClass is the storageClass used for the underlying PVC that hosts the volume</p> <code>accessModes</code>  []PersistentVolumeAccessMode  (Optional) <p>accessModes is an array that contains the desired Access Modes the root volume should have. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes</p> <code>volumeMode</code>  Kubernetes core/v1.PersistentVolumeMode  (Optional) <p>volumeMode defines what type of volume is required by the claim. Value of Filesystem is implied when not included in claim spec.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtPlatformCredentials","title":"KubevirtPlatformCredentials","text":"<p> (Appears on: KubeVirtNodePoolStatus,  KubevirtPlatformSpec) </p> <p> </p> Field Description <code>infraKubeConfigSecret</code>  KubeconfigSecretRef  (Optional) <p>infraKubeConfigSecret is a reference to the secret containing the kubeconfig of an external infrastructure cluster for kubevirt provider</p> <code>infraNamespace</code>  string  <p>infraNamespace is the namespace in the external infrastructure cluster where kubevirt resources will be created</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtPlatformSpec","title":"KubevirtPlatformSpec","text":"<p> (Appears on: PlatformSpec) </p> <p> <p>KubevirtPlatformSpec specifies configuration for kubevirt guest cluster installations</p> </p> Field Description <code>baseDomainPassthrough</code>  bool  (Optional) <p>baseDomainPassthrough toggles whether or not an automatically generated base domain for the guest cluster should be used that is a subdomain of the management cluster\u2019s *.apps DNS.</p> <p>For the KubeVirt platform, the basedomain can be autogenerated using the *.apps domain of the management/infra hosting cluster This makes the guest cluster\u2019s base domain a subdomain of the hypershift infra/mgmt cluster\u2019s base domain.</p> <p>Example: Infra/Mgmt cluster\u2019s DNS Base: example.com Cluster: mgmt-cluster.example.com Apps:    *.apps.mgmt-cluster.example.com KubeVirt Guest cluster\u2019s DNS Base: apps.mgmt-cluster.example.com Cluster: guest.apps.mgmt-cluster.example.com Apps: *.apps.guest.apps.mgmt-cluster.example.com</p> <p>This is possible using OCP wildcard routes</p> <code>generateID</code>  string  (Optional) <p>generateID is used to uniquely apply a name suffix to resources associated with kubevirt infrastructure resources</p> <code>credentials</code>  KubevirtPlatformCredentials  (Optional) <p>credentials defines the client credentials used when creating KubeVirt virtual machines. Defining credentials is only necessary when the KubeVirt virtual machines are being placed on a cluster separate from the one hosting the Hosted Control Plane components.</p> <p>The default behavior when Credentials is not defined is for the KubeVirt VMs to be placed on the same cluster and namespace as the Hosted Control Plane.</p> <code>storageDriver</code>  KubevirtStorageDriverSpec  (Optional) <p>storageDriver defines how the KubeVirt CSI driver exposes StorageClasses on the infra cluster (hosting the VMs) to the guest cluster.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtRootVolume","title":"KubevirtRootVolume","text":"<p> (Appears on: KubevirtNodePoolPlatform) </p> <p> <p>KubevirtRootVolume represents the volume that the rhcos disk will be stored and run from.</p> </p> Field Description <code>diskImage</code>  KubevirtDiskImage  (Optional) <p>diskImage represents what rhcos image to use for the node pool</p> <code>KubevirtVolume</code>  KubevirtVolume  <p> (Members of <code>KubevirtVolume</code> are embedded into this type.) </p> <p>kubevirtVolume represents of type of storage to run the image on</p> <code>cacheStrategy</code>  KubevirtCachingStrategy  (Optional) <p>cacheStrategy defines the boot image caching strategy. Default - no caching</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtStorageClassMapping","title":"KubevirtStorageClassMapping","text":"<p> (Appears on: KubevirtManualStorageDriverConfig) </p> <p> </p> Field Description <code>group</code>  string  (Optional) <p>group contains which group this mapping belongs to.</p> <code>infraStorageClassName</code>  string  <p>infraStorageClassName is the name of the infra cluster storage class that will be exposed to the guest.</p> <code>guestStorageClassName</code>  string  <p>guestStorageClassName is the name that the corresponding storageclass will be called within the guest cluster</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtStorageDriverConfigType","title":"KubevirtStorageDriverConfigType","text":"<p> (Appears on: KubevirtStorageDriverSpec) </p> <p> <p>KubevirtStorageDriverConfigType defines how the kubevirt storage driver is configured.</p> </p> Value Description <p>\"Default\"</p> <p>DefaultKubevirtStorageDriverConfigType means the kubevirt storage driver maps to the underlying infra cluster\u2019s default storageclass</p> <p>\"Manual\"</p> <p>ManualKubevirtStorageDriverConfigType means the kubevirt storage driver mapping is explicitly defined.</p> <p>\"None\"</p> <p>NoneKubevirtStorageDriverConfigType means no kubevirt storage driver is used</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtStorageDriverSpec","title":"KubevirtStorageDriverSpec","text":"<p> (Appears on: KubevirtPlatformSpec) </p> <p> </p> Field Description <code>type</code>  KubevirtStorageDriverConfigType  (Optional) <p>type represents the type of kubevirt csi driver configuration to use</p> <code>manual</code>  KubevirtManualStorageDriverConfig  (Optional) <p>manual is used to explicitly define how the infra storageclasses are mapped to guest storageclasses</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtVolume","title":"KubevirtVolume","text":"<p> (Appears on: KubevirtRootVolume) </p> <p> <p>KubevirtVolume represents what kind of storage to use for a KubeVirt VM volume</p> </p> Field Description <code>type</code>  KubevirtVolumeType  (Optional) <p>type represents the type of storage to associate with the kubevirt VMs.</p> <code>persistent</code>  KubevirtPersistentVolume  (Optional) <p>persistent volume type means the VM\u2019s storage is backed by a PVC VMs that use persistent volumes can survive disruption events like restart and eviction This is the default type used when no storage type is defined.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtVolumeSnapshotClassMapping","title":"KubevirtVolumeSnapshotClassMapping","text":"<p> (Appears on: KubevirtManualStorageDriverConfig) </p> <p> </p> Field Description <code>group</code>  string  (Optional) <p>group contains which group this mapping belongs to.</p> <code>infraVolumeSnapshotClassName</code>  string  <p>infraVolumeSnapshotClassName is the name of the infra cluster volume snapshot class that will be exposed to the guest.</p> <code>guestVolumeSnapshotClassName</code>  string  <p>guestVolumeSnapshotClassName is the name that the corresponding volumeSnapshotClass will be called within the guest cluster</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.KubevirtVolumeType","title":"KubevirtVolumeType","text":"<p> (Appears on: KubevirtVolume) </p> <p> <p>KubevirtVolumeType is a specific supported KubeVirt volumes</p> </p> Value Description <p>\"Persistent\"</p> <p>KubevirtVolumeTypePersistent represents persistent volume for kubevirt VMs</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.LoadBalancerPublishingStrategy","title":"LoadBalancerPublishingStrategy","text":"<p> (Appears on: ServicePublishingStrategy) </p> <p> <p>LoadBalancerPublishingStrategy specifies setting used to expose a service as a LoadBalancer.</p> </p> Field Description <code>hostname</code>  string  (Optional) <p>hostname is the name of the DNS record that will be created pointing to the LoadBalancer and passed through to consumers of the service. If omitted, the value will be inferred from the corev1.Service Load balancer type .status.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.LogLevel","title":"LogLevel","text":"<p> (Appears on: ClusterVersionOperatorSpec) </p> <p> </p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.MachineNetworkEntry","title":"MachineNetworkEntry","text":"<p> (Appears on: ClusterNetworking) </p> <p> <p>MachineNetworkEntry is a single IP address block for node IP blocks.</p> </p> Field Description <code>cidr</code>  github.com/openshift/hypershift/api/util/ipnet.IPNet  <p>cidr is the IP block address pool for machines within the cluster.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ManagedAzureKeyVault","title":"ManagedAzureKeyVault","text":"<p> (Appears on: ControlPlaneManagedIdentities) </p> <p> <p>ManagedAzureKeyVault is an Azure Key Vault on the management cluster.</p> </p> Field Description <code>name</code>  string  <p>name is the name of the Azure Key Vault on the management cluster.</p> <code>tenantID</code>  string  <p>tenantID is the tenant ID of the Azure Key Vault on the management cluster.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ManagedEtcdSpec","title":"ManagedEtcdSpec","text":"<p> (Appears on: EtcdSpec) </p> <p> <p>ManagedEtcdSpec specifies the behavior of an etcd cluster managed by HyperShift.</p> </p> Field Description <code>storage</code>  ManagedEtcdStorageSpec  <p>storage specifies how etcd data is persisted.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ManagedEtcdStorageSpec","title":"ManagedEtcdStorageSpec","text":"<p> (Appears on: ManagedEtcdSpec) </p> <p> <p>ManagedEtcdStorageSpec describes the storage configuration for etcd data.</p> </p> Field Description <code>type</code>  ManagedEtcdStorageType  <p>type is the kind of persistent storage implementation to use for etcd. Only PersistentVolume is supported at the moment.</p> <code>persistentVolume</code>  PersistentVolumeEtcdStorageSpec  (Optional) <p>persistentVolume is the configuration for PersistentVolume etcd storage. With this implementation, a PersistentVolume will be allocated for every etcd member (either 1 or 3 depending on the HostedCluster control plane availability configuration).</p> <code>restoreSnapshotURL</code>  []string  (Optional) <p>restoreSnapshotURL allows an optional URL to be provided where an etcd snapshot can be downloaded, for example a pre-signed URL referencing a storage service. This snapshot will be restored on initial startup, only when the etcd PV is empty.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ManagedEtcdStorageType","title":"ManagedEtcdStorageType","text":"<p> (Appears on: ManagedEtcdStorageSpec) </p> <p> <p>ManagedEtcdStorageType is a storage type for an etcd cluster.</p> </p> Value Description <p>\"PersistentVolume\"</p> <p>PersistentVolumeEtcdStorage uses PersistentVolumes for etcd storage.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ManagedIdentity","title":"ManagedIdentity","text":"<p> (Appears on: AzureKMSSpec,  ControlPlaneManagedIdentities) </p> <p> <p>ManagedIdentity contains the client ID, and its certificate name, of a managed identity. This managed identity is used, by an HCP component, to authenticate with the Azure API.</p> </p> Field Description <code>clientID</code>  AzureClientID  (Optional) <p>clientID is the client ID of a managed identity associated with CredentialsSecretName. This field is optional and mainly used for CI purposes.</p> <code>objectEncoding</code>  ObjectEncodingFormat  <p>objectEncoding represents the encoding for the Azure Key Vault secret containing the certificate related to the managed identity. objectEncoding needs to match the encoding format used when the certificate was stored in the Azure Key Vault. If objectEncoding doesn\u2019t match the encoding format of the certificate, the certificate will unsuccessfully be read by the Secrets CSI driver and an error will occur. This error will only be visible on the SecretProviderClass custom resource related to the managed identity.</p> <p>The default value is utf-8.</p> <p>See this for more info - https://github.com/Azure/secrets-store-csi-driver-provider-azure/blob/master/website/content/en/getting-started/usage/_index.md</p> <code>credentialsSecretName</code>  string  <p>credentialsSecretName is the name of an Azure Key Vault secret. This field assumes the secret contains the JSON format of a UserAssignedIdentityCredentials struct. At a minimum, the secret needs to contain the ClientId, ClientSecret, AuthenticationEndpoint, NotBefore, and NotAfter, and TenantId.</p> <p>More info on this struct can be found here - https://github.com/Azure/msi-dataplane/blob/63fb37d3a1aaac130120624674df795d2e088083/pkg/dataplane/internal/generated_client.go#L156.</p> <p>credentialsSecretName must be between 1 and 127 characters and use only alphanumeric characters and hyphens. credentialsSecretName must also be unique within the Azure Key Vault. See more details here - https://azure.github.io/PSRule.Rules.Azure/en/rules/Azure.KeyVault.SecretName/.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.MarketType","title":"MarketType","text":"<p> (Appears on: CapacityReservationOptions) </p> <p> <p>MarketType describes the market type of the CapacityReservation for an Instance.</p> </p> Value Description <p>\"CapacityBlocks\"</p> <p>MarketTypeCapacityBlock is a MarketType enum value</p> <p>\"OnDemand\"</p> <p>MarketTypeOnDemand is a MarketType enum value</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.MultiQueueSetting","title":"MultiQueueSetting","text":"<p> (Appears on: KubevirtNodePoolPlatform) </p> <p> </p> Value Description <p>\"Disable\"</p> <p>\"Enable\"</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.NetworkFilter","title":"NetworkFilter","text":"<p> (Appears on: NetworkParam) </p> <p> <p>NetworkFilter specifies a query to select an OpenStack network. At least one property must be set.</p> </p> Field Description <code>name</code>  string  (Optional) <p>name is the name of the network to filter by.</p> <code>description</code>  string  (Optional) <p>description is the description of the network to filter by.</p> <code>projectID</code>  string  (Optional) <p>projectID is the project ID of the network to filter by.</p> <code>FilterByNeutronTags</code>  FilterByNeutronTags  <p> (Members of <code>FilterByNeutronTags</code> are embedded into this type.) </p> (Optional) <p>FilterByNeutronTags specifies tags to filter by.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.NetworkParam","title":"NetworkParam","text":"<p> (Appears on: OpenStackPlatformSpec,  PortSpec) </p> <p> <p>NetworkParam specifies an OpenStack network. It may be specified by either ID or Filter, but not both.</p> </p> Field Description <code>id</code>  string  (Optional) <p>id is the ID of the network to use. If ID is provided, the other filters cannot be provided. Must be in UUID format.</p> <code>filter</code>  NetworkFilter  (Optional) <p>filter specifies a filter to select an OpenStack network. If provided, cannot be empty.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.NetworkType","title":"NetworkType","text":"<p> (Appears on: ClusterNetworking) </p> <p> <p>NetworkType specifies the SDN provider used for cluster networking.</p> </p> Value Description <p>\"Calico\"</p> <p>Calico specifies Calico as the SDN provider</p> <p>\"OVNKubernetes\"</p> <p>OVNKubernetes specifies OVN as the SDN provider</p> <p>\"OpenShiftSDN\"</p> <p>OpenShiftSDN specifies OpenShiftSDN as the SDN provider</p> <p>\"Other\"</p> <p>Other specifies an undefined SDN provider</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.NeutronTag","title":"NeutronTag","text":"<p> (Appears on: FilterByNeutronTags) </p> <p> <p>NeutronTag represents a tag on a Neutron resource. It may not be empty and may not contain commas.</p> </p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.NodePoolAutoScaling","title":"NodePoolAutoScaling","text":"<p> (Appears on: NodePoolSpec) </p> <p> <p>NodePoolAutoScaling specifies auto-scaling behavior for a NodePool.</p> </p> Field Description <code>min</code>  int32  <p>min is the minimum number of nodes to maintain in the pool. Must be &gt;= 1 and &lt;= .Max.</p> <code>max</code>  int32  <p>max is the maximum number of nodes allowed in the pool. Must be &gt;= 1 and &gt;= Min.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.NodePoolCondition","title":"NodePoolCondition","text":"<p> (Appears on: NodePoolStatus) </p> <p> <p>We define our own condition type since metav1.Condition has validation for Reason that might be broken by what we bubble up from CAPI. NodePoolCondition defines an observation of NodePool resource operational state.</p> </p> Field Description <code>type</code>  string  <p>type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important.</p> <code>status</code>  Kubernetes core/v1.ConditionStatus  <p>status of the condition, one of True, False, Unknown.</p> <code>severity</code>  string  (Optional) <p>severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False.</p> <code>lastTransitionTime</code>  Kubernetes meta/v1.Time  <p>lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable.</p> <code>reason</code>  string  (Optional) <p>reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty.</p> <code>message</code>  string  (Optional) <p>message is a human readable message indicating details about the transition. This field may be empty.</p> <code>observedGeneration</code>  int64  (Optional) <p>observedGeneration represents the .metadata.generation that the condition was set based upon.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.NodePoolManagement","title":"NodePoolManagement","text":"<p> (Appears on: NodePoolSpec) </p> <p> <p>NodePoolManagement specifies behavior for managing nodes in a NodePool, such as upgrade strategies and auto-repair behaviors.</p> </p> Field Description <code>upgradeType</code>  UpgradeType  <p>upgradeType specifies the type of strategy for handling upgrades. This can be either \u201cReplace\u201d or \u201cInPlace\u201d. \u201cReplace\u201d will update Nodes by recreating the underlying instances. \u201cInPlace\u201d will update Nodes by applying changes to the existing instances. This might or might not result in a reboot.</p> <code>replace</code>  ReplaceUpgrade  (Optional) <p>replace is the configuration for rolling upgrades. It defaults to a RollingUpdate strategy with maxSurge of 1 and maxUnavailable of 0.</p> <code>inPlace</code>  InPlaceUpgrade  (Optional) <p>inPlace is the configuration for in-place upgrades.</p> <code>autoRepair</code>  bool  (Optional) <p>autoRepair specifies whether health checks should be enabled for machines in the NodePool. The default is false. Enabling this feature will cause the controller to automatically delete unhealthy machines. The unhealthy criteria is reserved for the controller implementation and subject to change. But generally it\u2019s determined by checking the Node ready condition is true and a timeout that might vary depending on the platform provider. AutoRepair will no-op when more than 2 Nodes are unhealthy at the same time. Giving time for the cluster to stabilize or to the user to manually intervene.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.NodePoolPlatform","title":"NodePoolPlatform","text":"<p> (Appears on: NodePoolSpec) </p> <p> <p>NodePoolPlatform specifies the underlying infrastructure provider for the NodePool and is used to configure platform specific behavior.</p> </p> Field Description <code>type</code>  PlatformType  <p>type specifies the platform name.</p> <code>aws</code>  AWSNodePoolPlatform  (Optional) <p>aws specifies the configuration used when operating on AWS.</p> <code>ibmcloud</code>  IBMCloudPlatformSpec  (Optional) <p>ibmcloud defines IBMCloud specific settings for components</p> <code>kubevirt</code>  KubevirtNodePoolPlatform  (Optional) <p>kubevirt specifies the configuration used when operating on KubeVirt platform.</p> <code>agent</code>  AgentNodePoolPlatform  (Optional) <p>agent specifies the configuration used when using Agent platform.</p> <code>azure</code>  AzureNodePoolPlatform  (Optional) <p>azure specifies the configuration used when using Azure platform.</p> <code>powervs</code>  PowerVSNodePoolPlatform  (Optional) <p>powervs specifies the configuration used when using IBMCloud PowerVS platform.</p> <code>openstack</code>  OpenStackNodePoolPlatform  (Optional) <p>openstack specifies the configuration used when using OpenStack platform.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.NodePoolPlatformStatus","title":"NodePoolPlatformStatus","text":"<p> (Appears on: NodePoolStatus) </p> <p> <p>NodePoolPlatformStatus struct contains platform-specific status information.</p> </p> Field Description <code>kubeVirt</code>  KubeVirtNodePoolStatus  (Optional) <p>kubeVirt contains the KubeVirt platform statuses</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.NodePoolSpec","title":"NodePoolSpec","text":"<p> (Appears on: NodePool) </p> <p> <p>NodePoolSpec is the desired behavior of a NodePool.</p> </p> Field Description <code>clusterName</code>  string  <p>clusterName is the name of the HostedCluster this NodePool belongs to. If a HostedCluster with this name doesn\u2019t exist, the controller will no-op until it exists.</p> <code>release</code>  Release  <p>release specifies the OCP release used for this NodePool. It drives the machine ignition configuration (including the kubelet version) and other platform-specific properties (e.g. an AMI on AWS).</p> <p>Version-skew rules and effects: - The minor-version skew relative to the control-plane release must be &lt;= N-2. This is not currently enforced, but exceeding this limit is unsupported and may lead to unpredictable behavior. - If the specified release is higher than the HostedCluster\u2019s release, the NodePool will be degraded and the ValidReleaseImage condition will be false. - If the specified release is lower than the NodePool\u2019s current y-stream, the NodePool will be degraded and the ValidReleaseImage condition will be false.</p> <p>Changing this field triggers a NodePool rollout.</p> <code>platform</code>  NodePoolPlatform  <p>platform specifies the underlying infrastructure provider for the NodePool and is used to configure platform specific behavior.</p> <code>replicas</code>  int32  (Optional) <p>replicas is the desired number of nodes the pool should maintain. If unset, the controller default value is 0. replicas is mutually exclusive with autoscaling. If autoscaling is configured, replicas must be omitted and autoscaling will control the NodePool size internally.</p> <code>management</code>  NodePoolManagement  <p>management specifies behavior for managing nodes in the pool, such as upgrade strategies and auto-repair behaviors.</p> <code>autoScaling</code>  NodePoolAutoScaling  (Optional) <p>autoScaling specifies auto-scaling behavior for the NodePool. autoScaling is mutually exclusive with replicas. If replicas is set, this field must be omitted.</p> <code>config</code>  []Kubernetes core/v1.LocalObjectReference  (Optional) <p>config is a list of references to ConfigMaps containing serialized MachineConfig resources to be injected into the ignition configurations of nodes in the NodePool. The MachineConfig API schema is defined here:</p> <p>https://github.com/openshift/machine-config-operator/blob/18963e4f8fe66e8c513ca4b131620760a414997f/pkg/apis/machineconfiguration.openshift.io/v1/types.go#L185</p> <p>Each ConfigMap must have a single key named \u201cconfig\u201d whose value is the YML with one or more serialized machineconfiguration.openshift.io resources:</p> <ul> <li>KubeletConfig</li> <li>ContainerRuntimeConfig</li> <li>MachineConfig</li> <li>ClusterImagePolicy</li> <li>ImageContentSourcePolicy</li> <li>ImageDigestMirrorSet</li> </ul> <p>This is validated in the backend and signaled back via validMachineConfig condition. Changing this field will trigger a NodePool rollout.</p> <code>nodeDrainTimeout</code>  Kubernetes meta/v1.Duration  (Optional) <p>nodeDrainTimeout is the maximum amount of time that the controller will spend on retrying to drain a node until it succeeds. The default value is 0, meaning that the node can retry drain without any time limitations. Changing this field propagate inplace into existing Nodes.</p> <code>nodeVolumeDetachTimeout</code>  Kubernetes meta/v1.Duration  (Optional) <p>nodeVolumeDetachTimeout is the maximum amount of time that the controller will spend on detaching volumes from a node. The default value is 0, meaning that the volumes will be detached from the node without any time limitations. After the timeout, any remaining attached volumes will be ignored and the removal of the machine will continue. Changing this field propagate inplace into existing Nodes.</p> <code>nodeLabels</code>  map[string]string  (Optional) <p>nodeLabels propagates a list of labels to Nodes, only once on creation. Valid values are those in https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set</p> <code>taints</code>  []Taint  (Optional) <p>taints if specified, propagates a list of taints to Nodes, only once on creation. These taints are additive to the ones applied by other controllers</p> <code>pausedUntil</code>  string  (Optional) <p>pausedUntil is a field that can be used to pause reconciliation on the NodePool controller. Resulting in any change to the NodePool being ignored. Either a date can be provided in RFC3339 format or a boolean as in \u2018true\u2019, \u2018false\u2019, \u2018True\u2019, \u2018False\u2019. If a date is provided: reconciliation is paused on the resource until that date. If the boolean true is provided: reconciliation is paused on the resource until the field is removed.</p> <code>tuningConfig</code>  []Kubernetes core/v1.LocalObjectReference  (Optional) <p>tuningConfig is a list of references to ConfigMaps containing serialized Tuned or PerformanceProfile resources to define the tuning configuration to be applied to nodes in the NodePool. The Tuned API is defined here:</p> <p>https://github.com/openshift/cluster-node-tuning-operator/blob/2c76314fb3cc8f12aef4a0dcd67ddc3677d5b54f/pkg/apis/tuned/v1/tuned_types.go</p> <p>The PerformanceProfile API is defined here: https://github.com/openshift/cluster-node-tuning-operator/tree/b41042d42d4ba5bb2e99960248cf1d6ae4935018/pkg/apis/performanceprofile/v2</p> <p>Each ConfigMap must have a single key named \u201ctuning\u201d whose value is the JSON or YAML of a serialized Tuned or PerformanceProfile. Changing this field will trigger a NodePool rollout.</p> <code>arch</code>  string  (Optional) <p>arch is the preferred processor architecture for the NodePool. Different platforms might have different supported architectures. TODO: This is set as optional to prevent validation from failing due to a limitation on client side validation with open API machinery: https://github.com/kubernetes/kubernetes/issues/108768#issuecomment-1253912215</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.NodePoolStatus","title":"NodePoolStatus","text":"<p> (Appears on: NodePool) </p> <p> <p>NodePoolStatus is the latest observed status of a NodePool.</p> </p> Field Description <code>replicas</code>  int32  (Optional) <p>replicas is the latest observed number of nodes in the pool.</p> <code>version</code>  string  (Optional) <p>version is the semantic version of the latest applied release specified by the NodePool.</p> <code>platform</code>  NodePoolPlatformStatus  (Optional) <p>platform holds the specific statuses</p> <code>conditions</code>  []NodePoolCondition  (Optional) <p>conditions represents the latest available observations of the node pool\u2019s current state.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.NodePortPublishingStrategy","title":"NodePortPublishingStrategy","text":"<p> (Appears on: ServicePublishingStrategy) </p> <p> <p>NodePortPublishingStrategy specifies a NodePort used to expose a service.</p> </p> Field Description <code>address</code>  string  <p>address is the host/ip that the NodePort service is exposed over.</p> <code>port</code>  int32  (Optional) <p>port is the port of the NodePort service. If &lt;=0, the port is dynamically assigned when the service is created.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.OLMCatalogPlacement","title":"OLMCatalogPlacement","text":"<p> (Appears on: HostedClusterSpec,  HostedControlPlaneSpec) </p> <p> <p>OLMCatalogPlacement is an enum specifying the placement of OLM catalog components.</p> </p> Value Description <p>\"guest\"</p> <p>GuestOLMCatalogPlacement indicates OLM catalog components will be placed in the guest cluster.</p> <p>\"management\"</p> <p>ManagementOLMCatalogPlacement indicates OLM catalog components will be placed in the management cluster.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.OVNIPv4Config","title":"OVNIPv4Config","text":"<p> (Appears on: OVNKubernetesConfig) </p> <p> <p>OVNIPv4Config contains IPv4-specific configuration options for OVN-Kubernetes. https://github.com/openshift/api/blob/6d3c4e25a8d3aeb57ad61649d80c38cbd27d1cc8/operator/v1/types_network.go#L473-L503</p> </p> Field Description <code>internalTransitSwitchSubnet</code>  string  (Optional) <p>internalTransitSwitchSubnet is a v4 subnet in IPV4 CIDR format used internally by OVN-Kubernetes for the distributed transit switch in the OVN Interconnect architecture that connects the cluster routers on each node together to enable east west traffic. The subnet chosen should not overlap with other networks specified for OVN-Kubernetes as well as other networks used on the host. When omitted, this means no opinion and the platform is left to choose a reasonable default which is subject to change over time. The current default subnet is 100.88.0.0/16 The subnet must be large enough to accommodate one IP per node in your cluster The value must be in proper IPV4 CIDR format</p> <code>internalJoinSubnet</code>  string  (Optional) <p>internalJoinSubnet is a v4 subnet used internally by ovn-kubernetes in case the default one is being already used by something else. It must not overlap with any other subnet being used by OpenShift or by the node network. The size of the subnet must be larger than the number of nodes. The current default value is 100.64.0.0/16 The subnet must be large enough to accommodate one IP per node in your cluster The value must be in proper IPV4 CIDR format</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.OVNKubernetesConfig","title":"OVNKubernetesConfig","text":"<p> (Appears on: ClusterNetworkOperatorSpec) </p> <p> <p>OVNKubernetesConfig contains OVN-Kubernetes specific configuration options. https://github.com/openshift/api/blob/6d3c4e25a8d3aeb57ad61649d80c38cbd27d1cc8/operator/v1/types_network.go#L400-L471</p> </p> Field Description <code>ipv4</code>  OVNIPv4Config  (Optional) <p>ipv4 allows users to configure IP settings for IPv4 connections. When omitted, this means no opinions and the default configuration is used. Check individual fields within ipv4 for details of default values.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ObjectEncodingFormat","title":"ObjectEncodingFormat","text":"<p> (Appears on: ManagedIdentity) </p> <p> <p>objectEncoding represents the encoding for the Azure Key Vault secret containing the certificate related to CertificateName. objectEncoding needs to match the encoding format used when the certificate was stored in the Azure Key Vault. If objectEncoding doesn\u2019t match the encoding format of the certificate, the certificate will unsuccessfully be read by the Secrets CSI driver and an error will occur. This error will only be visible on the SecretProviderClass custom resource related to the managed identity.</p> <p>The default value is utf-8.</p> <p>See this for more info - https://github.com/Azure/secrets-store-csi-driver-provider-azure/blob/master/website/content/en/getting-started/usage/_index.md</p> </p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.OpenStackIdentityReference","title":"OpenStackIdentityReference","text":"<p> (Appears on: OpenStackPlatformSpec) </p> <p> <p>OpenStackIdentityReference is a reference to an infrastructure provider identity to be used to provision cluster resources.</p> </p> Field Description <code>name</code>  string  <p>name is the name of a secret in the same namespace as the resource being provisioned. The secret must contain a key named <code>clouds.yaml</code> which contains an OpenStack clouds.yaml file. The secret may optionally contain a key named <code>cacert</code> containing a PEM-encoded CA certificate.</p> <code>cloudName</code>  string  <p>cloudName specifies the name of the entry in the clouds.yaml file to use.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.OpenStackNodePoolPlatform","title":"OpenStackNodePoolPlatform","text":"<p> (Appears on: NodePoolPlatform) </p> <p> </p> Field Description <code>flavor</code>  string  <p>flavor is the OpenStack flavor to use for the node instances.</p> <code>imageName</code>  string  (Optional) <p>imageName is the OpenStack Glance image name to use for node instances. If unspecified, the default is chosen based on the NodePool release payload image.</p> <code>availabilityZone</code>  string  (Optional) <p>availabilityZone is the nova availability zone in which the provider will create the VM. If not specified, the VM will be created in the default availability zone specified in the nova configuration. Availability zone names must NOT contain : since it is used by admin users to specify hosts where instances are launched in server creation. Also, it must not contain spaces otherwise it will lead to node that belongs to this availability zone register failure, see kubernetes/cloud-provider-openstack#1379 for further information. The maximum length of availability zone name is 63 as per labels limits.</p> <code>additionalPorts</code>  []PortSpec  (Optional) <p>additionalPorts is a list of additional ports to create on the node instances.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.OpenStackPlatformSpec","title":"OpenStackPlatformSpec","text":"<p> (Appears on: PlatformSpec) </p> <p> <p>OpenStackPlatformSpec specifies configuration for clusters running on OpenStack.</p> </p> Field Description <code>identityRef</code>  OpenStackIdentityReference  <p>identityRef is a reference to a secret holding OpenStack credentials to be used when reconciling the hosted cluster.</p> <code>managedSubnets</code>  []SubnetSpec  (Optional) <p>managedSubnets describe the OpenStack Subnet to be created. Cluster actuator will create a network, and a subnet with the defined DNSNameservers, AllocationPools and the CIDR defined in the HostedCluster MachineNetwork, and a router connected to the subnet. Currently only one IPv4 subnet is supported.</p> <code>router</code>  RouterParam  (Optional) <p>router specifies an existing router to be used if ManagedSubnets are specified. If specified, no new router will be created.</p> <code>network</code>  NetworkParam  (Optional) <p>network specifies an existing network to use if no ManagedSubnets are specified.</p> <code>subnets</code>  []SubnetParam  (Optional) <p>subnets specifies existing subnets to use if not ManagedSubnets are specified. All subnets must be in the network specified by Network. There can be zero, one, or two subnets. If no subnets are specified, all subnets in Network will be used. If 2 subnets are specified, one must be IPv4 and the other IPv6.</p> <code>networkMTU</code>  int  (Optional) <p>networkMTU is the MTU for the network.</p> <code>externalNetwork</code>  NetworkParam  (Optional) <p>externalNetwork is the OpenStack Network to be used to get public internet to the VMs. This option is ignored if DisableExternalNetwork is set to true.</p> <p>If ExternalNetwork is defined it must refer to exactly one external network.</p> <p>If ExternalNetwork is not defined or is empty the controller will use any existing external network as long as there is only one. It is an error if ExternalNetwork is not defined and there are multiple external networks unless DisableExternalNetwork is also set.</p> <p>If ExternalNetwork is not defined and there are no external networks the controller will proceed as though DisableExternalNetwork was set.</p> <code>disableExternalNetwork</code>  bool  (Optional) <p>disableExternalNetwork specifies whether or not to attempt to connect the cluster to an external network. This allows for the creation of clusters when connecting to an external network is not possible or desirable, e.g. if using a provider network.</p> <code>tags</code>  []string  (Optional) <p>tags to set on all resources in cluster which support tags</p> <code>ingressFloatingIP</code>  string  (Optional) <p>ingressFloatingIP is an available floating IP in your OpenStack cluster that will be associated with the OpenShift ingress port. When not specified, an IP address will be assigned randomly by the OpenStack cloud provider. When specified, the floating IP has to be pre-created.  If the specified value is not a floating IP or is already claimed, the OpenStack cloud provider won\u2019t be able to provision the load balancer. This value must be a valid IPv4 or IPv6 address.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.OperatorConfiguration","title":"OperatorConfiguration","text":"<p> (Appears on: HostedClusterSpec,  HostedControlPlaneSpec) </p> <p> <p>OperatorConfiguration specifies configuration for individual OCP operators in the cluster.</p> </p> Field Description <code>clusterVersionOperator</code>  ClusterVersionOperatorSpec  (Optional) <p>clusterVersionOperator specifies the configuration for the Cluster Version Operator in the hosted cluster.</p> <code>clusterNetworkOperator</code>  ClusterNetworkOperatorSpec  (Optional) <p>clusterNetworkOperator specifies the configuration for the Cluster Network Operator in the hosted cluster.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.OptionalCapability","title":"OptionalCapability","text":"<p> (Appears on: Capabilities) </p> <p> </p> Value Description <p>\"baremetal\"</p> <p>\"Console\"</p> <p>\"ImageRegistry\"</p> <p>\"Ingress\"</p> <p>\"Insights\"</p> <p>\"NodeTuning\"</p> <p>\"openshift-samples\"</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PayloadArchType","title":"PayloadArchType","text":"<p> (Appears on: HostedClusterStatus) </p> <p> </p> Value Description <p>\"AMD64\"</p> <p>\"ARM64\"</p> <p>\"Multi\"</p> <p>\"PPC64LE\"</p> <p>\"S390X\"</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PersistentVolumeAccessMode","title":"PersistentVolumeAccessMode","text":"<p> (Appears on: KubevirtPersistentVolume) </p> <p> </p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PersistentVolumeEtcdStorageSpec","title":"PersistentVolumeEtcdStorageSpec","text":"<p> (Appears on: ManagedEtcdStorageSpec) </p> <p> <p>PersistentVolumeEtcdStorageSpec is the configuration for PersistentVolume etcd storage.</p> </p> Field Description <code>storageClassName</code>  string  (Optional) <p>storageClassName is the StorageClass of the data volume for each etcd member. See https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1. TODO(alberto): This shouldn\u2019t really be a pointer. There\u2019s no real different semantic for nil and empty string. Revisit all pointer vs non-pointer choices.</p> <code>size</code>  k8s.io/apimachinery/pkg/api/resource.Quantity  (Optional) <p>size is the minimum size of the data volume for each etcd member. Default is 8Gi. This field is immutable</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PlacementOptions","title":"PlacementOptions","text":"<p> (Appears on: AWSNodePoolPlatform) </p> <p> <p>PlacementOptions specifies the placement options for the EC2 instances.</p> </p> Field Description <code>tenancy</code>  string  (Optional) <p>tenancy indicates if instance should run on shared or single-tenant hardware.</p> <p>Possible values: - \u201cdefault\u201d: NodePool instances run on shared hardware. - \u201cdedicated\u201d: Each NodePool instance runs on single-tenant hardware (Dedicated Instances). - \u201chost\u201d: NodePool instances run on user\u2019s pre-allocated dedicated hosts (Dedicated Hosts).</p> <p>When tenancy is set to \u201chost\u201d, capacityReservation cannot be specified as AWS does not support Capacity Reservations with Dedicated Hosts.</p> <code>capacityReservation</code>  CapacityReservationOptions  (Optional) <p>capacityReservation specifies Capacity Reservation options for the NodePool instances.</p> <p>Cannot be specified when tenancy is set to \u201chost\u201d as Dedicated Hosts do not support Capacity Reservations. Compatible with \u201cdefault\u201d and \u201cdedicated\u201d tenancy.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PlatformSpec","title":"PlatformSpec","text":"<p> (Appears on: HostedClusterSpec,  HostedControlPlaneSpec) </p> <p> <p>PlatformSpec specifies the underlying infrastructure provider for the cluster and is used to configure platform specific behavior.</p> </p> Field Description <code>type</code>  PlatformType  <p>type is the type of infrastructure provider for the cluster.</p> <code>aws</code>  AWSPlatformSpec  (Optional) <p>aws specifies configuration for clusters running on Amazon Web Services.</p> <code>agent</code>  AgentPlatformSpec  (Optional) <p>agent specifies configuration for agent-based installations.</p> <code>ibmcloud</code>  IBMCloudPlatformSpec  (Optional) <p>ibmcloud defines IBMCloud specific settings for components</p> <code>azure</code>  AzurePlatformSpec  (Optional) <p>azure defines azure specific settings</p> <code>powervs</code>  PowerVSPlatformSpec  (Optional) <p>powervs specifies configuration for clusters running on IBMCloud Power VS Service. This field is immutable. Once set, It can\u2019t be changed.</p> <code>kubevirt</code>  KubevirtPlatformSpec  (Optional) <p>kubevirt defines KubeVirt specific settings for cluster components.</p> <code>openstack</code>  OpenStackPlatformSpec  (Optional) <p>openstack specifies configuration for clusters running on OpenStack.</p> <code>gcp</code>  GCPPlatformSpec  (Optional) <p>gcp specifies configuration for clusters running on Google Cloud Platform.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PlatformStatus","title":"PlatformStatus","text":"<p> (Appears on: HostedClusterStatus,  HostedControlPlaneStatus) </p> <p> <p>PlatformStatus contains platform-specific status</p> </p> Field Description <code>aws</code>  AWSPlatformStatus  (Optional) <p>aws contains platform-specific status for AWS</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PlatformType","title":"PlatformType","text":"<p> (Appears on: KarpenterConfig,  NodePoolPlatform,  PlatformSpec) </p> <p> <p>PlatformType is a specific supported infrastructure provider.</p> </p> Value Description <p>\"AWS\"</p> <p>AWSPlatform represents Amazon Web Services infrastructure.</p> <p>\"Agent\"</p> <p>AgentPlatform represents user supplied insfrastructure booted with agents.</p> <p>\"Azure\"</p> <p>AzurePlatform represents Azure infrastructure.</p> <p>\"GCP\"</p> <p>GCPPlatform represents Google Cloud Platform infrastructure.</p> <p>\"IBMCloud\"</p> <p>IBMCloudPlatform represents IBM Cloud infrastructure.</p> <p>\"KubeVirt\"</p> <p>KubevirtPlatform represents Kubevirt infrastructure.</p> <p>\"None\"</p> <p>NonePlatform represents user supplied (e.g. bare metal) infrastructure.</p> <p>\"OpenStack\"</p> <p>OpenStackPlatform represents OpenStack infrastructure.</p> <p>\"PowerVS\"</p> <p>PowerVSPlatform represents PowerVS infrastructure.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PortSecurityPolicy","title":"PortSecurityPolicy","text":"<p> (Appears on: PortSpec) </p> <p> <p>PortSecurityPolicy defines whether or not to enable port security on a port.</p> </p> Value Description <p>\"\"</p> <p>PortSecurityDefault uses the default port security policy.</p> <p>\"Disabled\"</p> <p>PortSecurityDisabled disables port security on a port.</p> <p>\"Enabled\"</p> <p>PortSecurityEnabled enables port security on a port.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PortSpec","title":"PortSpec","text":"<p> (Appears on: OpenStackNodePoolPlatform) </p> <p> <p>PortSpec specifies the options for creating a port.</p> </p> Field Description <code>network</code>  NetworkParam  (Optional) <p>network is a query for an openstack network that the port will be created or discovered on. This will fail if the query returns more than one network.</p> <code>description</code>  string  (Optional) <p>description is a human-readable description for the port.</p> <code>allowedAddressPairs</code>  []AddressPair  (Optional) <p>allowedAddressPairs is a list of address pairs which Neutron will allow the port to send traffic from in addition to the port\u2019s addresses. If not specified, the MAC Address will be the MAC Address of the port. Depending on the configuration of Neutron, it may be supported to specify a CIDR instead of a specific IP address.</p> <code>vnicType</code>  string  (Optional) <p>vnicType specifies the type of vNIC which this port should be attached to. This is used to determine which mechanism driver(s) to be used to bind the port. The valid values are normal, macvtap, direct, baremetal, direct-physical, virtio-forwarder, smart-nic and remote-managed, although these values will not be validated in this API to ensure compatibility with future neutron changes or custom implementations. What type of vNIC is actually available depends on deployments. If not specified, the Neutron default value is used.</p> <code>portSecurityPolicy</code>  PortSecurityPolicy  (Optional) <p>portSecurityPolicy specifies whether or not to enable port security on the port. Allowed values are \u201cEnabled\u201d, \u201cDisabled\u201d and omitted. When not set, it takes the value of the corresponding field at the network level.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PowerVSNodePoolImageDeletePolicy","title":"PowerVSNodePoolImageDeletePolicy","text":"<p> (Appears on: PowerVSNodePoolPlatform) </p> <p> <p>PowerVSNodePoolImageDeletePolicy defines image delete policy to be used for PowerVSNodePoolPlatform</p> </p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PowerVSNodePoolPlatform","title":"PowerVSNodePoolPlatform","text":"<p> (Appears on: NodePoolPlatform) </p> <p> <p>PowerVSNodePoolPlatform specifies the configuration of a NodePool when operating on IBMCloud PowerVS platform.</p> </p> Field Description <code>systemType</code>  string  (Optional) <p>systemType is the System type used to host the instance. systemType determines the number of cores and memory that is available. Few of the supported SystemTypes are s922,e880,e980. e880 systemType available only in Dallas Datacenters. e980 systemType available in Datacenters except Dallas and Washington. When omitted, this means that the user has no opinion and the platform is left to choose a reasonable default. The current default is s922 which is generally available.</p> <code>processorType</code>  PowerVSNodePoolProcType  (Optional) <p>processorType is the VM instance processor type. It must be set to one of the following values: Dedicated, Capped or Shared.</p> <p>Dedicated: resources are allocated for a specific client, The hypervisor makes a 1:1 binding of a partition\u2019s processor to a physical processor core. Shared: Shared among other clients. Capped: Shared, but resources do not expand beyond those that are requested, the amount of CPU time is Capped to the value specified for the entitlement.</p> <p>if the processorType is selected as Dedicated, then Processors value cannot be fractional. When omitted, this means that the user has no opinion and the platform is left to choose a reasonable default. The current default is shared.</p> <code>processors</code>  k8s.io/apimachinery/pkg/util/intstr.IntOrString  (Optional) <p>processors is the number of virtual processors in a virtual machine. when the processorType is selected as Dedicated the processors value cannot be fractional. maximum value for the Processors depends on the selected SystemType. when SystemType is set to e880 or e980 maximum Processors value is 143. when SystemType is set to s922 maximum Processors value is 15. minimum value for Processors depends on the selected ProcessorType. when ProcessorType is set as Shared or Capped, The minimum processors is 0.5. when ProcessorType is set as Dedicated, The minimum processors is 1. When omitted, this means that the user has no opinion and the platform is left to choose a reasonable default. The default is set based on the selected ProcessorType. when ProcessorType selected as Dedicated, the default is set to 1. when ProcessorType selected as Shared or Capped, the default is set to 0.5.</p> <code>memoryGiB</code>  int32  (Optional) <p>memoryGiB is the size of a virtual machine\u2019s memory, in GiB. maximum value for the MemoryGiB depends on the selected SystemType. when SystemType is set to e880 maximum MemoryGiB value is 7463 GiB. when SystemType is set to e980 maximum MemoryGiB value is 15307 GiB. when SystemType is set to s922 maximum MemoryGiB value is 942 GiB. The minimum memory is 32 GiB.</p> <p>When omitted, this means the user has no opinion and the platform is left to choose a reasonable default. The current default is 32.</p> <code>image</code>  PowerVSResourceReference  (Optional) <p>image used for deploying the nodes. If unspecified, the default is chosen based on the NodePool release payload image.</p> <code>storageType</code>  PowerVSNodePoolStorageType  (Optional) <p>storageType for the image and nodes, this will be ignored if Image is specified. The storage tiers in PowerVS are based on I/O operations per second (IOPS). It means that the performance of your storage volumes is limited to the maximum number of IOPS based on volume size and storage tier. Although, the exact numbers might change over time, the Tier 3 storage is currently set to 3 IOPS/GB, and the Tier 1 storage is currently set to 10 IOPS/GB.</p> <p>The default is tier1</p> <code>imageDeletePolicy</code>  PowerVSNodePoolImageDeletePolicy  (Optional) <p>imageDeletePolicy is policy for the image deletion.</p> <p>delete: delete the image from the infrastructure. retain: delete the image from the openshift but retain in the infrastructure.</p> <p>The default is delete</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PowerVSNodePoolProcType","title":"PowerVSNodePoolProcType","text":"<p> (Appears on: PowerVSNodePoolPlatform) </p> <p> <p>PowerVSNodePoolProcType defines processor type to be used for PowerVSNodePoolPlatform</p> </p> Value Description <p>\"capped\"</p> <p>PowerVSNodePoolCappedProcType defines capped processor type</p> <p>\"dedicated\"</p> <p>PowerVSNodePoolDedicatedProcType defines dedicated processor type</p> <p>\"shared\"</p> <p>PowerVSNodePoolSharedProcType defines shared processor type</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PowerVSNodePoolStorageType","title":"PowerVSNodePoolStorageType","text":"<p> (Appears on: PowerVSNodePoolPlatform) </p> <p> <p>PowerVSNodePoolStorageType defines storage type to be used for PowerVSNodePoolPlatform</p> </p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PowerVSPlatformSpec","title":"PowerVSPlatformSpec","text":"<p> (Appears on: PlatformSpec) </p> <p> <p>PowerVSPlatformSpec defines IBMCloud PowerVS specific settings for components</p> </p> Field Description <code>accountID</code>  string  <p>accountID is the IBMCloud account id. This field is immutable. Once set, It can\u2019t be changed.</p> <code>cisInstanceCRN</code>  string  <p>cisInstanceCRN is the IBMCloud CIS Service Instance\u2019s Cloud Resource Name This field is immutable. Once set, It can\u2019t be changed.</p> <code>resourceGroup</code>  string  <p>resourceGroup is the IBMCloud Resource Group in which the cluster resides. This field is immutable. Once set, It can\u2019t be changed.</p> <code>region</code>  string  <p>region is the IBMCloud region in which the cluster resides. This configures the OCP control plane cloud integrations, and is used by NodePool to resolve the correct boot image for a given release. This field is immutable. Once set, It can\u2019t be changed.</p> <code>zone</code>  string  <p>zone is the availability zone where control plane cloud resources are created. This field is immutable. Once set, It can\u2019t be changed.</p> <code>subnet</code>  PowerVSResourceReference  <p>subnet is the subnet to use for control plane cloud resources. This field is immutable. Once set, It can\u2019t be changed.</p> <code>serviceInstanceID</code>  string  <p>serviceInstanceID is the reference to the Power VS service on which the server instance(VM) will be created. Power VS service is a container for all Power VS instances at a specific geographic region. serviceInstance can be created via IBM Cloud catalog or CLI. ServiceInstanceID is the unique identifier that can be obtained from IBM Cloud UI or IBM Cloud cli.</p> <p>More detail about Power VS service instance. https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-creating-power-virtual-server</p> <p>This field is immutable. Once set, It can\u2019t be changed.</p> <code>vpc</code>  PowerVSVPC  <p>vpc specifies IBM Cloud PowerVS Load Balancing configuration for the control plane. This field is immutable. Once set, It can\u2019t be changed.</p> <code>kubeCloudControllerCreds</code>  Kubernetes core/v1.LocalObjectReference  <p>kubeCloudControllerCreds is a reference to a secret containing cloud credentials with permissions matching the cloud controller policy. This field is immutable. Once set, It can\u2019t be changed.</p> <p>TODO(dan): document the \u201ccloud controller policy\u201d</p> <code>nodePoolManagementCreds</code>  Kubernetes core/v1.LocalObjectReference  <p>nodePoolManagementCreds is a reference to a secret containing cloud credentials with permissions matching the node pool management policy. This field is immutable. Once set, It can\u2019t be changed.</p> <p>TODO(dan): document the \u201cnode pool management policy\u201d</p> <code>ingressOperatorCloudCreds</code>  Kubernetes core/v1.LocalObjectReference  <p>ingressOperatorCloudCreds is a reference to a secret containing ibm cloud credentials for ingress operator to get authenticated with ibm cloud.</p> <code>storageOperatorCloudCreds</code>  Kubernetes core/v1.LocalObjectReference  <p>storageOperatorCloudCreds is a reference to a secret containing ibm cloud credentials for storage operator to get authenticated with ibm cloud.</p> <code>imageRegistryOperatorCloudCreds</code>  Kubernetes core/v1.LocalObjectReference  <p>imageRegistryOperatorCloudCreds is a reference to a secret containing ibm cloud credentials for image registry operator to get authenticated with ibm cloud.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PowerVSResourceReference","title":"PowerVSResourceReference","text":"<p> (Appears on: PowerVSNodePoolPlatform,  PowerVSPlatformSpec) </p> <p> <p>PowerVSResourceReference is a reference to a specific IBMCloud PowerVS resource by ID, or Name. Only one of ID, or Name may be specified. Specifying more than one will result in a validation error.</p> </p> Field Description <code>id</code>  string  (Optional) <p>id of resource</p> <code>name</code>  string  (Optional) <p>name of resource</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PowerVSVPC","title":"PowerVSVPC","text":"<p> (Appears on: PowerVSPlatformSpec) </p> <p> <p>PowerVSVPC specifies IBM Cloud PowerVS LoadBalancer configuration for the control plane.</p> </p> Field Description <code>name</code>  string  <p>name for VPC to used for all the service load balancer. This field is immutable. Once set, It can\u2019t be changed.</p> <code>region</code>  string  <p>region is the IBMCloud region in which VPC gets created, this VPC used for all the ingress traffic into the OCP cluster. This field is immutable. Once set, It can\u2019t be changed.</p> <code>zone</code>  string  (Optional) <p>zone is the availability zone where load balancer cloud resources are created. This field is immutable. Once set, It can\u2019t be changed.</p> <code>subnet</code>  string  (Optional) <p>subnet is the subnet to use for load balancer. This field is immutable. Once set, It can\u2019t be changed.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.Provisioner","title":"Provisioner","text":"<p> (Appears on: ProvisionerConfig) </p> <p> <p>provisioner is a enum specifying the strategy for auto managing Nodes.</p> </p> Value Description <p>\"Karpenter\"</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ProvisionerConfig","title":"ProvisionerConfig","text":"<p> (Appears on: AutoNode) </p> <p> <p>ProvisionerConfig is a enum specifying the strategy for auto managing Nodes.</p> </p> Field Description <code>name</code>  Provisioner  <p>name specifies the name of the provisioner to use.</p> <code>karpenter</code>  KarpenterConfig  (Optional) <p>karpenter specifies the configuration for the Karpenter provisioner.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.PublishingStrategyType","title":"PublishingStrategyType","text":"<p> (Appears on: ServicePublishingStrategy) </p> <p> <p>PublishingStrategyType defines publishing strategies for services.</p> </p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.QoSClass","title":"QoSClass","text":"<p> (Appears on: KubevirtCompute) </p> <p> </p> Value Description <p>\"Burstable\"</p> <p>\"Guaranteed\"</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.Release","title":"Release","text":"<p> (Appears on: HostedClusterSpec,  NodePoolSpec) </p> <p> <p>Release represents the metadata for an OCP release payload image.</p> </p> Field Description <code>image</code>  string  <p>image is the image pullspec of an OCP release payload image. See https://quay.io/repository/openshift-release-dev/ocp-release?tab=tags for a list of available images.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ReplaceUpgrade","title":"ReplaceUpgrade","text":"<p> (Appears on: NodePoolManagement) </p> <p> <p>ReplaceUpgrade specifies upgrade behavior that replaces existing nodes according to a given strategy.</p> </p> Field Description <code>strategy</code>  UpgradeStrategy  (Optional) <p>strategy is the node replacement strategy for nodes in the pool. In can be either \u201cRollingUpdate\u201d or \u201cOnDelete\u201d. RollingUpdate will rollout Nodes honoring maxSurge and maxUnavailable. OnDelete provide more granular control and will replace nodes as the old ones are manually deleted.</p> <code>rollingUpdate</code>  RollingUpdate  (Optional) <p>rollingUpdate specifies a rolling update strategy which upgrades nodes by creating new nodes and deleting the old ones.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.RetentionPolicy","title":"RetentionPolicy","text":"<p> <p>RetentionPolicy defines the policy for handling resources associated with a cluster when the cluster is deleted.</p> </p> Value Description <p>\"Orphan\"</p> <p>OrphanRetentionPolicy will keep the resources associated with the cluster when the cluster is deleted.</p> <p>\"Prune\"</p> <p>PruneRetentionPolicy will delete the resources associated with the cluster when the cluster is deleted.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.RollingUpdate","title":"RollingUpdate","text":"<p> (Appears on: ReplaceUpgrade) </p> <p> <p>RollingUpdate specifies a rolling update strategy which upgrades nodes by creating new nodes and deleting the old ones.</p> </p> Field Description <code>maxUnavailable</code>  k8s.io/apimachinery/pkg/util/intstr.IntOrString  (Optional) <p>maxUnavailable is the maximum number of nodes that can be unavailable during the update.</p> <p>Value can be an absolute number (ex: 5) or a percentage of desired nodes (ex: 10%).</p> <p>Absolute number is calculated from percentage by rounding down.</p> <p>This can not be 0 if MaxSurge is 0.</p> <p>Defaults to 0.</p> <p>Example: when this is set to 30%, old nodes can be deleted down to 70% of desired nodes immediately when the rolling update starts. Once new nodes are ready, more old nodes be deleted, followed by provisioning new nodes, ensuring that the total number of nodes available at all times during the update is at least 70% of desired nodes.</p> <code>maxSurge</code>  k8s.io/apimachinery/pkg/util/intstr.IntOrString  (Optional) <p>maxSurge is the maximum number of nodes that can be provisioned above the desired number of nodes.</p> <p>Value can be an absolute number (ex: 5) or a percentage of desired nodes (ex: 10%).</p> <p>Absolute number is calculated from percentage by rounding up.</p> <p>This can not be 0 if MaxUnavailable is 0.</p> <p>Defaults to 1.</p> <p>Example: when this is set to 30%, new nodes can be provisioned immediately when the rolling update starts, such that the total number of old and new nodes do not exceed 130% of desired nodes. Once old nodes have been deleted, new nodes can be provisioned, ensuring that total number of nodes running at any time during the update is at most 130% of desired nodes.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.RoutePublishingStrategy","title":"RoutePublishingStrategy","text":"<p> (Appears on: ServicePublishingStrategy) </p> <p> <p>RoutePublishingStrategy specifies options for exposing a service as a Route.</p> </p> Field Description <code>hostname</code>  string  (Optional) <p>hostname is the name of the DNS record that will be created pointing to the Route and passed through to consumers of the service. If omitted, the value will be inferred from management ingress.Spec.Domain.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.RouterFilter","title":"RouterFilter","text":"<p> (Appears on: RouterParam) </p> <p> <p>RouterFilter specifies a query to select an OpenStack router. At least one property must be set.</p> </p> Field Description <code>name</code>  string  (Optional) <p>name is the name of the router to filter by.</p> <code>description</code>  string  (Optional) <p>description is the description of the router to filter by.</p> <code>projectID</code>  string  (Optional) <p>projectID is the project ID of the router to filter by.</p> <code>FilterByNeutronTags</code>  FilterByNeutronTags  <p> (Members of <code>FilterByNeutronTags</code> are embedded into this type.) </p> (Optional) <p>FilterByNeutronTags specifies tags to filter by.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.RouterParam","title":"RouterParam","text":"<p> (Appears on: OpenStackPlatformSpec) </p> <p> <p>RouterParam specifies an OpenStack router to use. It may be specified by either ID or filter, but not both.</p> </p> Field Description <code>id</code>  string  (Optional) <p>id is the ID of the router to use. If ID is provided, the other filters cannot be provided. Must be in UUID format.</p> <code>filter</code>  RouterFilter  (Optional) <p>filter specifies a filter to select an OpenStack router. If provided, cannot be empty.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ScaleDownConfig","title":"ScaleDownConfig","text":"<p> (Appears on: ClusterAutoscaling) </p> <p> <p>Configures when and how to scale down cluster nodes.</p> </p> Field Description <code>delayAfterAddSeconds</code>  int32  (Optional) <p>delayAfterAddSeconds sets how long after scale up the scale down evaluation resumes in seconds. It must be between 0 and 86400 (24 hours). When set to 0, this means scale down evaluation will resume immediately after scale up, without any delay. When omitted, the autoscaler defaults to 600s (10 minutes).</p> <code>delayAfterDeleteSeconds</code>  int32  (Optional) <p>delayAfterDeleteSeconds sets how long after node deletion, scale down evaluation resumes, defaults to scan-interval. It must be between 0 and 86400 (24 hours). When set to 0, this means scale down evaluation will resume immediately after node deletion, without any delay. When omitted, the autoscaler defaults to 0s.</p> <code>delayAfterFailureSeconds</code>  int32  (Optional) <p>delayAfterFailureSeconds sets how long after a scale down failure, scale down evaluation resumes. It must be between 0 and 86400 (24 hours). When set to 0, this means scale down evaluation will resume immediately after a scale down failure, without any delay. When omitted, the autoscaler defaults to 180s (3 minutes).</p> <code>unneededDurationSeconds</code>  int32  (Optional) <p>unneededDurationSeconds establishes how long a node should be unneeded before it is eligible for scale down in seconds. It must be between 0 and 86400 (24 hours). When omitted, the autoscaler defaults to 600s (10 minutes).</p> <code>utilizationThresholdPercent</code>  int32  (Optional) <p>utilizationThresholdPercent determines the node utilization level, defined as sum of requested resources divided by capacity, below which a node can be considered for scale down. The value represents a percentage from 0 to 100. When set to 0, this means nodes will only be considered for scale down if they are completely idle (0% utilization). When set to 100, this means nodes will be considered for scale down regardless of their utilization level. A value between 0 and 100 represents the utilization threshold below which a node can be considered for scale down. When omitted, the autoscaler defaults to 50%.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ScalingType","title":"ScalingType","text":"<p> (Appears on: ClusterAutoscaling) </p> <p> <p>ScalingType defines the scaling behavior for the cluster autoscaler.</p> </p> Value Description <p>\"ScaleUpAndScaleDown\"</p> <p>ScaleUpAndScaleDown means the autoscaler will both scale up and scale down nodes.</p> <p>\"ScaleUpOnly\"</p> <p>ScaleUpOnly means the autoscaler will only scale up nodes, never scale down.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.SecretEncryptionSpec","title":"SecretEncryptionSpec","text":"<p> (Appears on: HostedClusterSpec,  HostedControlPlaneSpec) </p> <p> <p>SecretEncryptionSpec contains metadata about the kubernetes secret encryption strategy being used for the cluster when applicable.</p> </p> Field Description <code>type</code>  SecretEncryptionType  <p>type defines the type of kube secret encryption being used</p> <code>kms</code>  KMSSpec  (Optional) <p>kms defines metadata about the kms secret encryption strategy</p> <code>aescbc</code>  AESCBCSpec  (Optional) <p>aescbc defines metadata about the AESCBC secret encryption strategy</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.SecretEncryptionType","title":"SecretEncryptionType","text":"<p> (Appears on: SecretEncryptionSpec) </p> <p> <p>SecretEncryptionType defines the type of kube secret encryption being used.</p> </p> Value Description <p>\"aescbc\"</p> <p>AESCBC uses AES-CBC with PKCS#7 padding to do secret encryption</p> <p>\"kms\"</p> <p>KMS integrates with a cloud provider\u2019s key management service to do secret encryption</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ServiceNetworkEntry","title":"ServiceNetworkEntry","text":"<p> (Appears on: ClusterNetworking) </p> <p> <p>ServiceNetworkEntry is a single IP address block for the service network.</p> </p> Field Description <code>cidr</code>  github.com/openshift/hypershift/api/util/ipnet.IPNet  <p>cidr is the IP block address pool for services within the cluster in CIDR format (e.g., 192.168.1.0/24 or 2001:0db8::/64)</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ServicePublishingStrategy","title":"ServicePublishingStrategy","text":"<p> (Appears on: ServicePublishingStrategyMapping) </p> <p> </p> Field Description <code>type</code>  PublishingStrategyType  <p>type is the publishing strategy used for the service. It can be LoadBalancer;NodePort;Route;None;S3</p> <code>nodePort</code>  NodePortPublishingStrategy  (Optional) <p>nodePort configures exposing a service using a NodePort.</p> <code>loadBalancer</code>  LoadBalancerPublishingStrategy  (Optional) <p>loadBalancer configures exposing a service using a dedicated LoadBalancer.</p> <code>route</code>  RoutePublishingStrategy  (Optional) <p>route configures exposing a service using a Route through and an ingress controller behind a cloud Load Balancer. The specifics of the setup are platform dependent.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ServicePublishingStrategyMapping","title":"ServicePublishingStrategyMapping","text":"<p> (Appears on: HostedClusterSpec,  HostedControlPlaneSpec) </p> <p> <p>ServicePublishingStrategyMapping specifies how individual control plane services endpoints are published for consumption. This includes APIServer;OAuthServer;Konnectivity;Ignition. If a given service is not present in this list, it will be exposed publicly by default.</p> </p> Field Description <code>service</code>  ServiceType  <p>service identifies the type of service being published. It can be APIServer;OAuthServer;Konnectivity;Ignition OVNSbDb;OIDC are no-op and kept for backward compatibility. This field is immutable.</p> <code>servicePublishingStrategy</code>  ServicePublishingStrategy  <p>servicePublishingStrategy specifies how to publish a service endpoint.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.ServiceType","title":"ServiceType","text":"<p> (Appears on: ServicePublishingStrategyMapping) </p> <p> <p>ServiceType defines what control plane services can be exposed from the management control plane.</p> </p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.SubnetFilter","title":"SubnetFilter","text":"<p> (Appears on: SubnetParam) </p> <p> <p>SubnetFilter specifies a filter to select a subnet. At least one parameter must be specified.</p> </p> Field Description <code>name</code>  string  (Optional) <p>name is the name of the subnet to filter by.</p> <code>description</code>  string  (Optional) <p>description is the description of the subnet to filter by.</p> <code>projectID</code>  string  (Optional) <p>projectID is the project ID of the subnet to filter by.</p> <code>ipVersion</code>  int  (Optional) <p>ipVersion is the IP version of the subnet to filter by.</p> <code>gatewayIP</code>  string  (Optional) <p>gatewayIP is the gateway IP of the subnet to filter by.</p> <code>cidr</code>  string  (Optional) <p>cidr is the CIDR of the subnet to filter by.</p> <code>ipv6AddressMode</code>  string  (Optional) <p>ipv6AddressMode is the IPv6 address mode of the subnet to filter by.</p> <code>ipv6RAMode</code>  string  (Optional) <p>ipv6RAMode is the IPv6 RA mode of the subnet to filter by.</p> <code>FilterByNeutronTags</code>  FilterByNeutronTags  <p> (Members of <code>FilterByNeutronTags</code> are embedded into this type.) </p> (Optional) <p>FilterByNeutronTags specifies tags to filter by.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.SubnetParam","title":"SubnetParam","text":"<p> (Appears on: OpenStackPlatformSpec) </p> <p> <p>SubnetParam specifies an OpenStack subnet to use. It may be specified by either ID or filter, but not both.</p> </p> Field Description <code>id</code>  string  (Optional) <p>id is the uuid of the subnet. It will not be validated.</p> <code>filter</code>  SubnetFilter  (Optional) <p>filter specifies a filter to select the subnet. It must match exactly one subnet.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.SubnetSpec","title":"SubnetSpec","text":"<p> (Appears on: OpenStackPlatformSpec) </p> <p> </p> Field Description <code>dnsNameservers</code>  []string  (Optional) <p>dnsNameservers holds a list of DNS server addresses that will be provided when creating the subnet. These addresses need to have the same IP version as CIDR.</p> <code>allocationPools</code>  []AllocationPool  (Optional) <p>allocationPools is an array of AllocationPool objects that will be applied to OpenStack Subnet being created. If set, OpenStack will only allocate these IPs for Machines. It will still be possible to create ports from outside of these ranges manually.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.Taint","title":"Taint","text":"<p> (Appears on: NodePoolSpec) </p> <p> <p>taint is as v1 Core but without TimeAdded. https://github.com/kubernetes/kubernetes/blob/ed8cad1e80d096257921908a52ac69cf1f41a098/staging/src/k8s.io/api/core/v1/types.go#L3037-L3053 Validation replicates the same validation as the upstream https://github.com/kubernetes/kubernetes/blob/9a2a7537f035969a68e432b4cc276dbce8ce1735/pkg/util/taints/taints.go#L273. See also https://kubernetes.io/docs/concepts/overview/working-with-objects/names/.</p> </p> Field Description <code>key</code>  string  <p>key is the taint key to be applied to a node.</p> <code>value</code>  string  (Optional) <p>value is the taint value corresponding to the taint key.</p> <code>effect</code>  Kubernetes core/v1.TaintEffect  <p>effect is the effect of the taint on pods that do not tolerate the taint. Valid effects are NoSchedule, PreferNoSchedule and NoExecute.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.UnmanagedEtcdSpec","title":"UnmanagedEtcdSpec","text":"<p> (Appears on: EtcdSpec) </p> <p> <p>UnmanagedEtcdSpec specifies configuration which enables the control plane to integrate with an eternally managed etcd cluster.</p> </p> Field Description <code>endpoint</code>  string  <p>endpoint is the full etcd cluster client endpoint URL. For example:</p> <pre><code>https://etcd-client:2379\n</code></pre> <p>If the URL uses an HTTPS scheme, the TLS field is required.</p> <code>tls</code>  EtcdTLSConfig  <p>tls specifies TLS configuration for HTTPS etcd client endpoints.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.UpgradeStrategy","title":"UpgradeStrategy","text":"<p> (Appears on: ReplaceUpgrade) </p> <p> <p>UpgradeStrategy is a specific strategy for upgrading nodes in a NodePool.</p> </p> Value Description <p>\"OnDelete\"</p> <p>UpgradeStrategyOnDelete replaces old nodes when the deletion of the associated node instances are completed.</p> <p>\"RollingUpdate\"</p> <p>UpgradeStrategyRollingUpdate means use a rolling update for nodes.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.UpgradeType","title":"UpgradeType","text":"<p> (Appears on: NodePoolManagement) </p> <p> <p>UpgradeType is a type of high-level upgrade behavior nodes in a NodePool.</p> </p> Value Description <p>\"InPlace\"</p> <p>UpgradeTypeInPlace is a strategy which replaces nodes in-place with no additional node capacity requirements.</p> <p>\"Replace\"</p> <p>UpgradeTypeReplace is a strategy which replaces nodes using surge node capacity.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.UserManagedDiagnostics","title":"UserManagedDiagnostics","text":"<p> (Appears on: Diagnostics) </p> <p> <p>UserManagedDiagnostics specifies the diagnostics settings for a virtual machine when the storage account is managed by the user.</p> </p> Field Description <code>storageAccountURI</code>  string  <p>storageAccountURI is the URI of the user-managed storage account. The URI typically will be <code>https://&lt;mystorageaccountname&gt;.blob.core.windows.net/</code> but may differ if you are using Azure DNS zone endpoints. You can find the correct endpoint by looking for the Blob Primary Endpoint in the endpoints tab in the Azure console or with the CLI by issuing <code>az storage account list --query='[].{name: name, \"resource group\": resourceGroup, \"blob endpoint\": primaryEndpoints.blob}'</code>.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.Volume","title":"Volume","text":"<p> (Appears on: AWSNodePoolPlatform) </p> <p> <p>Volume specifies the configuration options for node instance storage devices.</p> </p> Field Description <code>size</code>  int64  <p>size is the size of the volume in gibibytes (GiB).</p> <p>Must be greater than the image snapshot size or 8 (whichever is greater).</p> <code>type</code>  string  <p>type is the type of volume to provision.</p> <code>iops</code>  int64  (Optional) <p>iops is the number of IOPS requested for the disk. This is only valid for type io1.</p> <code>encrypted</code>  bool  (Optional) <p>encrypted indicates whether the EBS volume should be encrypted or not.</p> <code>encryptionKey</code>  string  (Optional) <p>encryptionKey is the KMS key to use for volume encryption.</p>"},{"location":"reference/api/#hypershift.openshift.io/v1beta1.WorkloadIdentity","title":"WorkloadIdentity","text":"<p> (Appears on: AzureWorkloadIdentities) </p> <p> <p>WorkloadIdentity is a struct that contains the client ID of a federated managed identity used in workload identity authentication.</p> </p> Field Description <code>clientID</code>  AzureClientID  <p>clientID is client ID of a federated managed identity used in workload identity authentication</p>"},{"location":"reference/concepts-and-personas/","title":"Concepts and Personas","text":""},{"location":"reference/concepts-and-personas/#concepts","title":"Concepts","text":"<p>Management Cluster</p> <p>An OpenShift cluster which hosts HyperShift and zero to many Hosted Clusters.</p> <p>Hosted Cluster</p> <p>An OpenShift API endpoint managed by HyperShift.</p> <p>Hosted Control Plane</p> <p>An OpenShift control plane running on the Management Cluster which is exposed by a Hosted Cluster's API endpoint. The component parts of a control plane include etcd, apiserver, kube-controller-manager, vpn, etc.</p> <p>Hosted Control Plane Infrastructure</p> <p>Resources on the Management Cluster or external cloud provider which are prerequisites to running Hosted Control Plane processes.</p> <p>Management Cluster Infrastructure</p> <p>Network, compute, storage, etc. of the Management Cluster.</p> <p>Hosted Cluster Infrastructure</p> <p>Network, compute, storage, etc. that exist in customer cloud account.</p>"},{"location":"reference/concepts-and-personas/#personas","title":"Personas","text":"<p>Cluster Service Provider</p> <p>The user hosting cluster control planes, responsible for up-time. UI for fleet wide alerts, configuring AWS account to host control planes in, user provisioned infra (host awareness of available compute), where to pull VMs from. Has cluster admin management.</p> <p>Cluster Service Consumer</p> <p>The user empowered to request control planes, request workers, and drive upgrades or modify externalized configuration. Likely not empowered to manage or access cloud credentials or infrastructure encryption keys.</p> <p>Cluster Instance Admin</p> <p>The user with cluster-admin role in the provisioned cluster, but may have no power over when/how cluster is upgraded or configured. May see some configuration projected into the cluster in read-only fashion.</p> <p>Cluster (Instance) User</p> <p>Maps to a developer today in standalone OCP. They will not have a view or insight into OperatorHub, Machines, etc.</p>"},{"location":"reference/controller-architecture/","title":"Controller architecture","text":""},{"location":"reference/controller-architecture/#high-level-overview","title":"High Level Overview","text":""},{"location":"reference/controller-architecture/#physical-layout-and-operating-model","title":"Physical layout and operating model","text":"<p>Legend:</p> <ul> <li>Yellow box: namespace</li> <li>Rounded box: processes</li> <li>Rectangle: CR instances</li> </ul> flowchart LR   subgraph hypershift     cluster-operator([HyperShift Operator])   end    subgraph user-clusters     HostedClusterA     NodePoolA   end    subgraph cluster-a     control-plane-operator([Control Plane Operator])      capi-manager([CAPI Manager])     capi-provider([CAPI Provider])      HostedControlPlane     ExternalInfraCluster      cp-components([Control Plane Components])      capi-cluster[CAPICluster]      capi-machine-template[CAPIInfrastructureMachineTemplate]     capi-machineset[CAPI MachineSet]     capi-machine[CAPI Machine]     capi-provider-machine[CAPIInfrastructureMachine]   end    cluster-operator--&gt;|reconciles|HostedClusterA   cluster-operator--&gt;|operates|control-plane-operator   cluster-operator--&gt;|operates|capi-manager   cluster-operator--&gt;|operates|capi-provider   cluster-operator--&gt;|creates|HostedControlPlane   cluster-operator--&gt;|creates|capi-cluster   cluster-operator--&gt;|creates|ExternalInfraCluster   cluster-operator--&gt;|reconciles|NodePoolA   cluster-operator--&gt;|creates|capi-machine-template   cluster-operator--&gt;|creates|capi-machineset    control-plane-operator--&gt;|operates|cp-components    control-plane-operator--&gt;|reconciles|HostedControlPlane    capi-manager--&gt;|reconciles|capi-cluster   capi-manager--&gt;|reconciles|capi-machineset   capi-manager--&gt;|creates|capi-machine    capi-provider--&gt;|reconciles|capi-machine   capi-provider--&gt;|creates|capi-provider-machine  <p>TODO: 1. How do we (or should we) represent an input/output or \"consumes\" relationship (e.g. the hypershift operator creates and syncs machine templates, and the CAPI provider reads the template, but nothing actively watches templates and does work in reaction to them directly)</p>"},{"location":"reference/controller-architecture/#major-components","title":"Major Components","text":""},{"location":"reference/controller-architecture/#hypershift-operator","title":"HyperShift Operator","text":"<p>The HyperShift Operator is a singleton within the management cluster that manages the lifecycle of hosted clusters represented by <code>HostedCluster</code> resources.</p> <p>A single version of the the HyperShift Operator knows how to manage multiple hosted OCP versions.</p> <p>The HyperShift Operator is responsible for:</p> <ul> <li>Processing <code>HostedCluster</code> and <code>NodePool</code> resources and managing Control Plane Operator and Cluster API (CAPI) deployments which do the actual work of installing a control plane.</li> <li>Managing the lifecycle of the hosted cluster by handling rollouts of new Control Plane Operator and CAPI deployments based on version changes to <code>HostedCluster</code> and <code>NodePool</code> resources.</li> <li>Aggregating and surfacing information about clusters.</li> </ul>"},{"location":"reference/controller-architecture/#hostedcluster-controller","title":"HostedCluster Controller","text":"graph TD   hosted-cluster-controller[HostedCluster Controller] --&gt; reconcile([Reconcile HostedCluster])   reconcile --&gt; is-deleted{{Deleted?}}   is-deleted --&gt;|Yes| teardown([Teardown])   is-deleted --&gt;|No| sync([Sync])    teardown --&gt;teardown-complete{{Teardown complete?}}   teardown-complete --&gt;|Yes| return   teardown-complete --&gt;|No| reconcile    sync --&gt; create-namespace([Create Namespace])   create-namespace --&gt; deploy-cp-operator([Deploy Control Plane Operator])   deploy-cp-operator --&gt; deploy-capi-manager([Deploy CAPI Manager])   deploy-capi-manager --&gt; deploy-capi-provider([Deploy CAPI Provider])   deploy-capi-provider --&gt; create-capi-cluster([Create CAPICluster])   create-capi-cluster --&gt; create-hosted-control-plane([Create HostedControlPlane])   create-hosted-control-plane --&gt; create-external-infra-cluster([Create ExternalInfraCluster])   create-external-infra-cluster --&gt;has-initial-nodes{{HostedCluster has initial nodes?}}   has-initial-nodes --&gt;|Yes| create-node-pool([Create NodePool])   has-initial-nodes --&gt;|No| return   create-node-pool --&gt; return    return([End])"},{"location":"reference/controller-architecture/#nodepool-controller","title":"NodePool Controller","text":"graph TD   nodepool-controller[NodePool Controller] --&gt; reconcile([Reconcile NodePool])   reconcile --&gt; is-deleted{{Deleted?}}   is-deleted --&gt;|Yes| teardown([Teardown])   is-deleted --&gt;|No| sync([Sync])    sync --&gt; create-capi-machineset([Create CAPIMachineSet])   create-capi-machineset --&gt; create-capi-infra-machine-template([Create CAPIInfrastructureMachineTemplate])    create-capi-infra-machine-template --&gt; return    teardown --&gt;teardown-complete{{Teardown complete?}}   teardown-complete --&gt;|Yes| return   teardown-complete --&gt;|No| reconcile    return([End])"},{"location":"reference/controller-architecture/#externalinfracluster-controller","title":"ExternalInfraCluster Controller","text":"graph TD   external-infra-cluster-controller[ExternalInfraCluster Controller] --&gt; reconcile([Reconcile ExternalInfraCluster])   reconcile --&gt; is-deleted{{Deleted?}}   is-deleted --&gt;|Yes| teardown([Teardown])   is-deleted --&gt;|No| sync([Sync])    teardown --&gt;teardown-complete{{Teardown complete?}}   teardown-complete --&gt;|Yes| return   teardown-complete --&gt;|No| reconcile    sync --&gt; get-hosted-control-plane([Get HostedControlPlane])   get-hosted-control-plane --&gt;is-hcp-ready{{Is HostedControlPlane ready?}}   is-hcp-ready --&gt;|No| reconcile   is-hcp-ready --&gt;|Yes| update-infra-status([Update ExternalInfraCluster status])   update-infra-status --&gt; return    return([End])"},{"location":"reference/controller-architecture/#control-plane-operator","title":"Control Plane Operator","text":"<p>The Control Plane Operator is deployed by the HyperShift Operator into a hosted control plane namespace and manages the rollout of a single version of the the hosted cluster's control plane.</p> <p>The Control Plane Operator is versioned in lockstep with a specific OCP version and is decoupled from the management cluster's version.</p> <p>The Control Plane Operator is responsible for:</p> <ul> <li>Provisioning all the infrastructure required to host a control plane (whether this means creating or adopting existing infrastructure). This infrastructure may be management cluster resources, external cloud provider resources, etc.</li> <li>Deploying an OCP control plane configured to run in the context of the provisioned infrastructure.</li> <li>Implementing any versioned behavior necessary to rollout the new version (e.g. version specific changes at layers above OCP itself, like configuration or infrastructure changes).</li> </ul>"},{"location":"reference/controller-architecture/#hostedcontrolplane-controller","title":"HostedControlPlane Controller","text":"graph TD   hosted-control-plane-controller[HostedControlPlane Controller] --&gt; reconcile([Reconcile HostedControlPlane])   reconcile --&gt; is-deleted{{Deleted?}}   is-deleted --&gt;|Yes| teardown([Teardown])   is-deleted --&gt;|No| sync([Sync])    teardown --&gt;teardown-complete{{Teardown complete?}}   teardown-complete --&gt;|Yes| return   teardown-complete --&gt;|No| reconcile    sync --&gt; create-infra([Deploy Control PlaneComponents])   create-infra --&gt; create-config-operator([Deploy Hosted ClusterConfig Operator])   create-config-operator --&gt;is-infra-ready{{Infra ready?}}   is-infra-ready --&gt;|Yes| update-hosted-controlplane-ready([Update HostedControlPlane status])   is-infra-ready --&gt;|No| reconcile   update-hosted-controlplane-ready --&gt; return    return([End])"},{"location":"reference/controller-architecture/#hosted-cluster-config-operator","title":"Hosted Cluster Config Operator","text":"<p>The Hosted Cluster Config Operator is a control plane component maintained by HyperShift that's a peer to other control plane components (e.g., etcd, apiserver, controller-manager), and is managed by the Control Plane Operator in the same way as those other control plane components.</p> <p>The Hosted Cluster Config Operator is versioned in lockstep with a specific OCP version and is decoupled from the management cluster's version.</p> <p>The Hosted Cluster Config Operator is responsible for:</p> <ul> <li>Reading CAs from the hosted cluster to configure the kube controller manager CA bundle running in the hosted control plane</li> <li>Reconciling resources that live on the hosted cluster:<ul> <li>CRDs created by operators that are absent from the hosted cluster (RequestCount CRD created by cluster-kube-apiserver-operator)</li> <li>Clearing any user changes to the ClusterVersion resource (all updates should be driven via HostedCluster API)</li> <li>ClusterOperator stubs for control plane components that run outside.</li> <li>Global Configuration that is managed via the HostedCluster API</li> <li>Namespaces that are normally created by operators that are absent from the cluster.</li> <li>RBAC that is normally created by operators that are absent from the cluster.</li> <li>Registry configuration</li> <li>Default ingress controller</li> <li>Control Plane PKI (kubelet serving CA, control plane signer CA)</li> <li>Konnectivity Agent</li> <li>OpenShift APIServer resources (APIServices, Service, Endpoints)</li> <li>OpenShift OAuth APIServer resources (APIServices, Service, Endpoints)</li> <li>Monitoring Configuration (set node selector to non-master nodes)</li> <li>Pull Secret</li> <li>OAuth serving cert CA</li> <li>OAuthClients required by the console</li> <li>Cloud Credential Secrets (contain STS role for components that need cloud access)</li> <li>OLM CatalogSources</li> <li>OLM PackageServer resources (APIService, Service, Endpoints)</li> </ul> </li> </ul>"},{"location":"reference/controller-architecture/#resource-dependency-diagram","title":"Resource dependency diagram","text":"<ul> <li>Dotted lines are dependencies (ownerRefs)</li> <li>Solid lines are associations (e.g. infrastructureRefs or controlPlaneRefs on specs)</li> </ul> classDiagram   HostedCluster   HostedControlPlane ..&gt; CAPICluster   ExternalInfraCluster ..&gt; CAPICluster   CAPICluster ..&gt; HostedCluster   CAPICluster --&gt; HostedControlPlane   CAPICluster --&gt; ExternalInfraCluster   CAPIMachineSet ..&gt; CAPICluster   CAPIMachineSet --&gt; CAPIInfrastructureMachineTemplate   CAPIMachine ..&gt;CAPIMachineSet   CAPIMachine --&gt;CAPIInfrastructureMachine   CAPIInfrastructureMachine ..&gt;CAPIMachine   CAPIInfrastructureMachineTemplate ..&gt;CAPICluster"},{"location":"reference/controller-architecture/#transformations","title":"Transformations","text":"<p>Trying to show how certain important resources are derived from others. These are resources created by our operators, not by CAPI.</p> classDiagram   CAPICluster ..&gt; HostedControlPlane   CAPICluster ..&gt; ExternalInfraCluster   HostedControlPlane ..&gt; HostedCluster   ExternalInfraCluster ..&gt; HostedCluster   classDiagram   CAPIInfrastructureTemplate ..&gt; NodePool   CAPIInfrastructureTemplate ..&gt; HostedCluster   CAPIMachineSet ..&gt; NodePool   CAPIMachineSet ..&gt; HostedCluster   CAPIMachineSet ..&gt; CAPIInfrastructureTemplate"},{"location":"reference/goals-and-design-invariants/","title":"Project goals","text":"<p>These are desired project goals which drive the design invariants stated below. Goals and scope may vary as the project evolves.</p> <ul> <li>Provide an API to express intent to create OpenShift Container Platform (OCP) clusters with a hosted control plane topology on existing infrastructure.</li> <li>Decouple control and data plane.</li> <li>Enable segregation of ownership and responsibility for different personas.</li> <li>Security.</li> <li>Cost efficiency.</li> </ul>"},{"location":"reference/goals-and-design-invariants/#design-invariants","title":"Design invariants","text":"<ul> <li>Communication between management cluster and a hosted cluster is unidirectional. A hosted cluster has no awareness of a management cluster.</li> <li>Communication between management cluster and a hosted cluster is only allowed from within each particular control plane namespace.</li> <li>Compute worker Nodes should not run anything beyond user workloads.</li> <li>A hosted cluster should not expose CRDs, CRs or Pods that enable users to manipulate HyperShift owned features.</li> <li>HyperShift components should not own or manage user infrastructure platform credentials.</li> </ul>"},{"location":"reference/konnectivity/","title":"Konnectivity in HyperShift","text":""},{"location":"reference/konnectivity/#what-is-konnectivity","title":"What is Konnectivity?","text":"<p>The Konnectivity proxy, also known as apiserver-network-proxy, is a component in Kubernetes that facilitates secure network communication between the Kubernetes API server and cluster nodes. It allows the Kubernetes API server to initiate communication to worker nodes or kubernetes services running in the data plane.</p> <p>Upstream References:</p> <ul> <li>Setup Konnectivity</li> <li>Proposal</li> <li>Reference Implementation</li> </ul>"},{"location":"reference/konnectivity/#reference-network-diagram","title":"Reference Network Diagram","text":"flowchart LR     subgraph hosted control plane         subgraph kube-apiserver pod             kube-apiserver --&gt; konnectivity-server         end         subgraph openshift-apiserver pod             openshift-apiserver --&gt; konnectivity-https-proxy-oapi[konnectivity-https-proxy]         end         subgraph oauth-apiserver pod             oauth-apiserver --&gt; konnectivity-socks5-proxy-oauth[konnectivity-socks5-proxy]             oauth-apiserver --&gt; konnectivity-https-proxy-oauth[konnectivity-https-proxy]         end         subgraph cluster-network-operator pod             cluster-network-operator --&gt; konnectivity-socks5-proxy-cno[konnectivity-socks5-proxy]         end         subgraph ovnkube-control-plane pod             ovnkube-control-plane --&gt; konnectivity-socks5-proxy-ovn[konnectivity-socks5-proxy]         end         subgraph ingress-operator pod             ingress-operator --&gt; konnectivity-https-proxy-ingress[konnectivity-https-proxy]         end         subgraph packageserver pod             package-server --&gt; konnectivity-socks5-proxy-pkgsrv[konnectivity-socks5-proxy]         end         subgraph olm-operator pod             olm-operator --&gt; konnectivity-socks5-proxy-olm[konnectivity-socks5-proxy]         end         subgraph catalog-operator pod             catalog-operator --&gt; konnectivity-socks5-proxy-cat[konnectivity-socks5-proxy]         end         konnectivity-server -- HCP APIServices --&gt; konnectivity-agent-cp[konnectivity-agent]         konnectivity-agent-cp --&gt; openshift-apiserver         konnectivity-agent-cp --&gt; oauth-apiserver         konnectivity-agent-cp --&gt; package-server         konnectivity-agent-cp -.-&gt;|registration| konnectivity-server-service         konnectivity-https-proxy-oapi --&gt; konnectivity-server-local-service         konnectivity-socks5-proxy-oauth --&gt; konnectivity-server-local-service         konnectivity-https-proxy-oauth --&gt; konnectivity-server-local-service         konnectivity-socks5-proxy-cno --&gt; konnectivity-server-local-service         konnectivity-socks5-proxy-ovn --&gt; konnectivity-server-local-service         konnectivity-https-proxy-ingress --&gt; konnectivity-server-local-service         konnectivity-socks5-proxy-pkgsrv --&gt; konnectivity-server-local-service         konnectivity-socks5-proxy-olm --&gt; konnectivity-server-local-service         konnectivity-socks5-proxy-cat --&gt; konnectivity-server-local-service         konnectivity-server-local-service --&gt; konnectivity-server         konnectivity-server-service -..-&gt; konnectivity-server     end     subgraph guest node         konnectivity-server -- Default Route --&gt; konnectivity-agent-node[konnectivity-agent]         konnectivity-agent-node --&gt; kubelet         konnectivity-agent-node --&gt; guest-service-network[Guest Service Network]     end     konnectivity-agent-node --&gt; guest-machine-network[Guest Machine Network/Proxy]     konnectivity-agent-node -.-&gt;|registration| konnectivity-server-service"},{"location":"reference/konnectivity/#why-is-it-needed","title":"Why is it needed?","text":"<p>The Kubernetes API server needs to initiate requests to the kubelet running in worker nodes in order to:</p> <ul> <li>Obtain logs of running pods.</li> <li>Execute into running pods.</li> <li>Port forward ports of pods or services.</li> </ul> <p>In addition, the Kubernetes APIServer needs to access services running in the data plane when those services are configured as either aggregated API services (via <code>APIService</code> resource) or webhooks (via <code>ValidatingWebhookConfiguration</code> or <code>MutatingWebhookConfiguration</code> resources).</p> <p>In a hosted control plane architecture, the Kubernetes API server may not have direct network access to worker nodes. For example, in AWS, the control plane runs in the provider VPC, while customer worker nodes run in the customer's VPC. The worker nodes are not exposed externally. Therefore communication initiated from the Kubernetes API server must rely on a tunnel established by Konnectivity to enable this communication.</p> <p>In addition to the communication initiated by the Kubernetes API server, in HyperShift hosted clusters, konnectivity is used via a socks or https proxy (added as a sidecar) by the following components:</p> <ul> <li> <p>OpenShift APIServer</p> <ul> <li>Communicates with webhook services for resources served by the OpenShift APIServer</li> <li>Routes ImageStream connection to remote registries through the data plane.</li> </ul> </li> <li> <p>Ingress Operator</p> <ul> <li>Uses konnectivity for route health checks (routes in data plane are not necessarily accessible from the control plane)</li> </ul> </li> <li> <p>OAuth Server</p> <ul> <li>Enables communication with identity providers that potentially are only available to the data plane network.</li> </ul> </li> <li> <p>Cluster Network Operator</p> <ul> <li>Performs proxy readiness requests through the data plane network</li> </ul> </li> <li> <p>OVNKube Control Plane</p> <ul> <li>Used to enable OVN interconnect for hosted clusters</li> </ul> </li> <li> <p>OLM Operator</p> <ul> <li>Used for GRPC communication with in-cluster catalogs</li> </ul> </li> <li> <p>OLM Catalog Operator</p> <ul> <li>Used for GRPC communication with in-cluster catalogs</li> </ul> </li> <li> <p>OLM Package Server</p> <ul> <li>Used for GRPC communication with in-cluster catalogs</li> </ul> </li> </ul> <p>Konnectivity is not needed for traffic initiated from worker nodes to the Kubernetes API server.</p>"},{"location":"reference/konnectivity/#how-does-konnectivity-work","title":"How does Konnectivity work?","text":"<p>Konnectivity has 2 main components:</p> <ul> <li> <p>Konnectivity server - accepts requests from the Kube APIServer (or   other proxy clients) and routes them through an agent in the data   plane. It also accepts connections from agents in the data plane to   establish a tunnel.</p> </li> <li> <p>Konnectivity agent(s) - run as pods on the worker nodes and   establish a connection with the Konnectivity server so that they can   route traffic initiated by the Kube APIServer to the appropriate   destination in the data plane.</p> </li> </ul> <p>Konnectivity agents are deployed to the data plane as a Daemonset that runs in every worker node. When those agents first start up, they establish a connection to the konnectivity server(s) (this means that the control plane must be accessible to the worker nodes). The established connection can then be used by the konnectivity server to tunnel requests initiated by the Kubernetes API server.</p> <p>In HyperShift, the Konnectivity server runs as a sidecar (container <code>konnectivity-server</code>) of the Kubernetes API Server. This means that when the control plane runs in HA mode, there are 3 instances of the Konnectivity server in the control plane.</p> <p>Konnectivity agents run as the <code>konnectivity-agent</code> Daemonset in the <code>kube-system</code> namespace of the hosted cluster.</p> <p>The konnectivity server exposes 2 endpoints:</p> <ul> <li> <p>server endpoint - Exposed to clients of the tunnel such   as the Kubernetes API server, proxies running in OpenShift API   server, OAuth server, etc.   In a HyperShift control plane namespace, this is exposed via the   <code>konnectivity-server-local</code> service (port 8090).</p> </li> <li> <p>cluster endpoint - Exposed to agents that establish a connection   to the Konnectivity server. In a HyperShift control plane namespace,   this is exposed via the <code>konnectivity-server</code> service (port 8091) and   externally via Route or NodePort.</p> </li> </ul>"},{"location":"reference/konnectivity/#konnectivity-artifacts-in-hypershift","title":"Konnectivity Artifacts in HyperShift","text":"<p>HyperShift requires konnectivity artifacts in both the control plane namespace as well as the hosted cluster.</p>"},{"location":"reference/konnectivity/#in-the-control-plane-namespace","title":"In the Control Plane Namespace","text":""},{"location":"reference/konnectivity/#deployments","title":"Deployments","text":"<ul> <li> <p>kube-apiserver - Runs the konnectivity-server container as a     sidecar.</p> </li> <li> <p>konnectivity-agent - Runs an agent for konnectivity on the control     plane side (See Control Plane API services for more information)</p> </li> </ul>"},{"location":"reference/konnectivity/#configmaps","title":"ConfigMaps","text":"<ul> <li> <p>kas-egress-selector-config - Contains the configuration that tells     the Kubernetes API server how and when to use konnectivity. We only     configure konnectivity for cluster connections. This means that the     Kubernetes API server will route any connection to a service or a     kubelet through the konnectivity server.</p> </li> <li> <p>konnectivity-ca-bundle - Contains the self-signed CA bundle used to     verify connections to the Konnectivity server.</p> </li> </ul>"},{"location":"reference/konnectivity/#secrets","title":"Secrets","text":"<ul> <li> <p>konnectivity-signer - Contains the self-signed CA for the     konnectivity server</p> </li> <li> <p>konnectivity-server - Contains the serving certificate for the     konnectivity server endpoint (where Kube APIServer and socks5/http     proxies connect)</p> </li> <li> <p>konnectivity-cluster - Contains the serving certificate for the     konnectivity cluster endpoint (where konnectivity agents connect)</p> </li> <li> <p>konnectivity-client - Contains certificate and key to be used by     server clients (Kube APIServer and socks5/http proxies) to     authenticate with the konnectivity server.</p> </li> <li> <p>konnectivity-agent - Contains certificate and key to be used by     agents to authenticate with the konnectivity server.</p> </li> </ul>"},{"location":"reference/konnectivity/#services","title":"Services","text":"<ul> <li> <p>konnectivity-server - Exposes the cluster endpoint of the     konnectivity server. This is the service that agents use to connect     to the konnectivity server. It must be exposed externally so that     workers can reach it.</p> </li> <li> <p>konnectivity-server-local - The endpoint where clients connect to     the konnectivity server to send traffic to the data plane. This     endpoint is only exposed to control plane components and is     protected via network policy from access by components outside of     its own control plane namespace.</p> </li> </ul>"},{"location":"reference/konnectivity/#routes","title":"Routes","text":"<ul> <li>konnectivity-server - Exposes the konnectivity-server service     externally so that agents can connect to it.</li> </ul>"},{"location":"reference/konnectivity/#in-the-hosted-cluster","title":"In the Hosted Cluster","text":""},{"location":"reference/konnectivity/#daemonsets","title":"Daemonsets","text":"<ul> <li>konnectivity-agent (<code>kube-system</code> namespace) - Runs a pod in every     node that establishes a connection that allows clients in the     control plane namespace to communicate through it.</li> </ul>"},{"location":"reference/konnectivity/#configmaps_1","title":"ConfigMaps","text":"<ul> <li>konnectivity-ca-bundle (<code>kube-system</code> namespace) - Contains trust     bundle that agents use to verify connections to the konnectivity     server.</li> </ul>"},{"location":"reference/konnectivity/#secrets_1","title":"Secrets","text":"<ul> <li>konnectivity-agent (<code>kube-system</code> namespace) - Contains certificate     and key used by agents to authenticate with the konnectivity server.</li> </ul>"},{"location":"reference/konnectivity/#konnectivity-proxies-in-hypershift","title":"Konnectivity Proxies in HyperShift","text":"<p>The HyperShift code base contains 2 proxy commands that make use of the konnectivity tunnel to send traffic to the data plane. These are usually run in sidecar containers in the deployments of components that need to send their traffic through konnectivity.</p> <ul> <li> <p>konnectivity-socks5-proxy - Starts a socks5 proxy that allows TCP     connections to endpoints accessible from the data plane. Cannot     route traffic through customer-supplied HTTP/S proxy. Use cases for     this are:</p> <ul> <li> <p>Traffic only destined for the data plane without the need to     traverse a customer-supplied proxy.</p> </li> <li> <p>Traffic that is not HTTP/S. For example GRPC traffic to OLM     catalogs, or LDAP traffic to LDAP identity providers.</p> </li> </ul> </li> <li> <p>konnectivity-https-proxy - Starts a HTTP proxy that can send HTTP/S     traffic to endpoints accessible from the data plane. Can make use of     customer-supplied HTTP/S proxy to route traffic through that. Use     cases for this are:</p> <ul> <li>Traffic that can be sent through the data plane but must reach     external endpoints and must honor customer-supplied HTTP/S proxy     configuration. For example, openshift-apiserver traffic to     external registries to import ImageStreams or OAuth server     traffic to non-LDAP identity providers that rely on HTTP/S     protocol.</li> </ul> </li> </ul> <p>To summarize, there are 2 main use cases for these proxies:</p> <ul> <li> <p>Routing of traffic from the control plane with destination inside     the data plane: This is the case for services such as webhooks and     aggregated API servers that run in the data plane, as well as OLM     catalogs, or network checks. For this use case, customer-configured     HTTP/S proxies are not relevant, because traffic is only needed to     reach inside the data plane. The socks5 proxy is adequate for this.</p> </li> <li> <p>Routing of traffic from the control plane to destinations outside     the data plane but potentially only accessible from the data plane.     This is the case for ImageStream registries, and external identity     providers. For this use case, customer-configured HTTP/S proxy     configuration should be taken into consideration because traffic may     only be able to leave the data plane through those proxies. For this     use case, the HTTP proxy is needed.</p> </li> </ul>"},{"location":"reference/konnectivity/#konnectivity-and-control-plane-aggregated-api-servers","title":"Konnectivity and Control Plane Aggregated API Servers","text":"<p>For OpenShift clusters, servers such as openshift-apiserver, openshift-oauth-apiserver, and OLM packageserver are aggregated API servers that handle essential OpenShift resources such as ImageStreams, Users, and Groups.</p> <p>Because we consider these servers part of the control plane, they run in the control plane namespace along with the Kubernetes API server. They are configured via APIService resources which tell the Kubernetes API server how to reach them to serve up their resources. APIServices have the limitation of only allowing a Kubernetes Service to be configured as the endpoint of the APIService. This means that a Service resource must exist in the data plane for every APIService we want to configure for the HostedCluster. This Service in the data plane needs to send traffic to the components in the control plane.</p> <p>The way this is accomplished is that a Service without pod selector is created in the default namespace. Then a corresponding Endpoints resource is created in the same namespace that contains the IP of the service in the control plane namespace. Whenever the Kubernetes API Server wants to reach the aggregated API server, it ends up sending traffic to the service in the control plane.</p> <p>One problem with the way Konnectivity is configured is that when you configure the egress selector for the cluster to be proxied through Konnectivity, then all the traffic destined for services is sent through Konnectivity. For these services that live in the control plane namespace, the Konnectivity server needs to know to send traffic to the control plane and not the data plane.</p> <p>The way that the Konnectivity server can send traffic to the control plane and not the data plane, is that we run a Konnectivity agent pod in the control plane. This agent is configured with the --agent-identifiers flag to only route traffic destined for the control plane aggregated API services. This means that only traffic that is destined for those services will be routed to the control plane through the Konnectivity agent.</p>"},{"location":"reference/konnectivity/#troubleshooting-konnectivity-in-hypershift-clusters","title":"Troubleshooting Konnectivity in HyperShift clusters","text":""},{"location":"reference/konnectivity/#checking-whether-konnectivity-is-functional","title":"Checking whether Konnectivity is functional","text":"<p>To determine whether Konnectivity is functional for a particular hosted cluster an easy test is to get the logs of a pod running in the hosted cluster. If that works, then traffic is getting routed from the Kubernetes API server to the kubelet running in the nodes.</p>"},{"location":"reference/konnectivity/#testing-the-konnectivity-server-with-curl","title":"Testing the Konnectivity server with curl","text":"<p>It is possible to use curl to send traffic through the konnectivity server to a node. The konnectivity server acts like a regular proxy, however it requires authentication via client certificate/key. Curl allows specifying a proxy CA, certificate and key (<code>--proxy-cacert</code>, <code>--proxy-cert</code>, <code>--proxy-key</code>) in addition to the certs used for the final endpoint. The following script shows how the appropriate certificates can be extracted and used to make a request to the kubelet running in a worker node. The konnectivity server is made available locally via the port-forward command. NOTE: The konnectivity server does not resolve names, so it can only be tested with IP addresses.</p> <pre><code>#!/bin/bash\n\nset -euo pipefail\n\nworkdir=\"$(mktemp -d)\"\ncp_namespace=\"CONTROL_PLANE_NAMESPACE_HERE\"\n\necho \"work directory is: ${workdir}\"\n\n# Get the cert/CA required to use the konnectivity server as a proxy\noc get secret konnectivity-client -n ${cp_namespace} -o jsonpath='{ .data.tls\\.key }' | base64 -d &gt; \"${workdir}/client.key\"\noc get secret konnectivity-client -n ${cp_namespace} -o jsonpath='{ .data.tls\\.crt }' | base64 -d &gt; \"${workdir}/client.crt\"\noc get cm konnectivity-ca-bundle -n ${cp_namespace} -o jsonpath='{ .data.ca\\.crt }' &gt; \"${workdir}/konnectivity_ca.crt\"\n\n# Get the cert/CA required to access the kubelet endpoint\noc get cm kubelet-client-ca -n ${cp_namespace} -o jsonpath='{ .data.ca\\.crt }' &gt; ${workdir}/kubelet_ca.crt\noc get secret kas-kubelet-client-crt -n ${cp_namespace} -o jsonpath='{ .data.tls\\.crt }' | base64 -d &gt; ${workdir}/kubelet_client.crt\noc get secret kas-kubelet-client-crt -n ${cp_namespace} -o jsonpath='{ .data.tls\\.key }' | base64 -d &gt; ${workdir}/kubelet_client.key\n\n# Obtain a node IP from local machines\nnodeip=\"$(oc get machines -n ${cp_namespace} -o json | jq -r '.items[0].status.addresses[] | select(.type==\"InternalIP\") | .address')\"\n\n# Forward the konnectivity server endpdoint to the local machine\noc port-forward -n ${cp_namespace} svc/konnectivity-server-local 8090:8090 &amp;\n\n# Allow some time for the port-forwarding to start\nsleep 2\n\n# Perform the curl command with the localhost konnectivity endpoint\ncurl -x \"https://127.0.0.1:8090\" \\\n  --proxy-cacert ${workdir}/konnectivity_ca.crt \\\n  --proxy-cert ${workdir}/client.crt \\\n  --proxy-key ${workdir}/client.key \\\n  --cacert ${workdir}/kubelet_ca.crt \\\n  --cert ${workdir}/kubelet_client.crt \\\n  --key ${workdir}/kubelet_client.key \\\n  \"https://${nodeip}:10250/metrics\"\n\n# Kill the port-forward job\nkill %1\n</code></pre>"},{"location":"reference/konnectivity/#konnectivity-qa","title":"Konnectivity Q&amp;A","text":"<p>Upstream allows grpc or http-connect mode for the konnectivity protocol, why does HyperShift use http-connect? We need to use http-connect to allow clients other than the Kubernetes API server to send traffic through the konnectivity server. The socks5 and http proxies can use http-connect to establish a connection through konnectivity-server.</p> <p>Upstream allows connecting to the konnectivity server via unix domain socket or via tcp port. Why does HyperShift use tcp port? The konnectivity server only allows one way of serving (via UDS or TCP port). We need to use a TCP port so we can expose it as a service to clients that use the socks5 and http proxies.</p> <p>Why does Konnectivity server run as a sidecar of the Kubernetes API server and not in its own deployment? When running as a sidecar, the Kubernetes API server can communicate with the Konnectivity server very fast, since traffic happens over the loopback interface. It also allows the Kubernetes API server to communicate with a konnectivity-server as soon as it comes up.</p> <p>How is the Konnectivity server secured in HyperShift? We use mTLS to authenticate connections to Konnectivity either from clients like Kubernetes API server or from agents. The certificates used for authentication are in the konnectivity-client and konnectivity-agent secrets in the control plane.</p> <p>How does Konnectivity HA work? When a HyperShift control plane runs in HA mode, there are by default 3 replicas of the konnectivity server (1 sidecar per kube-apiserver pod). Each server generates an ID when starting up and the number of total servers is passed as a command line argument. When an agent dials the konnectivity server endpoint, the server's response includes its own ID and the total number of servers that the agent can expect. If greater than one, the agent keeps dialing the konnectivity server endpoint until it can establish connections with the same number of unique servers (identified by their unique id).</p> <p>What breaks if Konnectivity is not working? The first thing that breaks when Konnectivity is not working is any operation that requires that the Kubernetes API server access kubelets on nodes. This includes getting logs from pods, proxying to pods/services, port-forwarding and remote execution. The next thing that breaks is any aggregated API servers and webhooks that use data plane services.</p>"},{"location":"reference/multi-platform-support/","title":"Multi-platform support","text":"<p>A platform is an infrastructure environment where different HyperShift components can run enabling them to make a series of assumptions, e.g. AWS, Azure, Kubevirt, Agent, None. </p> <p>HyperShift provides semantics and support for platforms at different levels: HostedCluster, NodePools and management cluster. This document outlines the support matrix that involved these three entities.</p>"},{"location":"reference/multi-platform-support/#support-level","title":"Support level","text":"<ul> <li>A HostedCluster and its NodePools must all have the same platform.</li> <li>A single management cluster might have multiple HostedClusters with different platforms each of them.</li> <li>Some HostedCluster features might dictate coupling with the management cluster, e.g. an AWS private HostedCluster requires an AWS management cluster.</li> <li>For cloud provider platforms e.g. AWS, Azure, etc. HostedClusters are only tested with the same management cluster platform or a provider-agnostic platform e.g. Kubevirt, Agent, None. Mixed cloud providers e.g AWS management cluster and Azure HostedCluster is a best effort support level.</li> <li>Non OCP management is a best effort support level. The HyperShift Operator will try to auto-discover the management clusters features it has available.</li> </ul>"},{"location":"reference/versioning-support/","title":"Versioning Support","text":"<p>There are different components that might require independent versioning and support level:</p> <ul> <li>Management Cluster</li> <li>API <code>hypershift.openshift.io</code></li> <li>HyperShift Operator (HO)</li> <li>Control Plane Operator (CPO)</li> <li>HyperShift CLI</li> <li>Hosted Control Plane (HCP) CLI</li> </ul>"},{"location":"reference/versioning-support/#support-level","title":"Support Level","text":""},{"location":"reference/versioning-support/#managed-services","title":"Managed Services","text":"<p>Managed services, such as Red Hat OpenShift on IBM Cloud, control versioning of all components. Refer to the managed service documentation for the latest, authoritative support matrix.</p>"},{"location":"reference/versioning-support/#red-hat-openshift-on-ibm-cloud","title":"Red Hat OpenShift on IBM Cloud","text":"<p>Red Hat OpenShift on IBM Cloud may support OCP versions beyond standard OCP timelines. As of September 28, 2025: - Management cluster (where the HyperShift Operator runs): OpenShift 4.15 or later or Kubernetes 1.30 or later - HostedClusters created by HyperShift on IBM Cloud: OpenShift 4.14 or later</p> <p>Information above is subject to change; check IBM Cloud documentation or contact IBM development.</p>"},{"location":"reference/versioning-support/#management-cluster","title":"Management Cluster","text":"<p>In general, the upstream HyperShift project does not place strict requirements on the OpenShift version of your  management cluster. This may vary depending on the particular platform; for example, Kubevirt requires management  clusters with OCP 4.14 and higher.</p> <p>The HO determines what versions of OCP can be installed through the HostedCluster (HC); see the HO section for  more details. However, different versions of the HO are thoroughly tested only on a limited set of OpenShift versions,  and this should inform your deployment decisions.</p>"},{"location":"reference/versioning-support/#production-use-cases","title":"Production Use Cases","text":"<p>For production use &amp; support, it is required to use a downstream product which bundles a supported build of the  HyperShift Operator. This downstream product is called Multi-Cluster Engine (MCE) and it is available through  OpenShift's OperatorHub. </p> <p>MCE versions do require specific OCP versions for the Management Cluster to remain in a supported state.  Each version documents its own support matrix. For example, </p> <ul> <li>MCE 2.5</li> <li>MCE 2.4</li> </ul> <p>As a heuristic, a new release of MCE will run on:</p> <ul> <li>The latest, yet to be released version of OpenShift</li> <li>The latest GA version of OpenShift</li> <li>Two versions prior to the latest GA version</li> </ul> <p>Versions of MCE can also be obtained with the Advanced Cluster Management (ACM) offering. If you are running ACM, refer  to product documentation to determine the bundled MCE version.</p> <p>The full list of HostedCluster OCP versions that can be installed via the HO on a Management Cluster will depend on the  version of the installed HO. However, if you are running a tested configuration or MCE, this list will always include at  least (a) the same OCP version as the Management Cluster and (b) Two previous minor versions relative to the Management  Cluster. For example, if the Management Cluster is running 4.16 and a supported version of MCE, then the HO will at  least be able to install 4.16, 4.15, and 4.14 Hosted Clusters. See the Multi-Cluster Engine section, under the expanded  section titled \"OpenShift Advanced Cluster Management\" on this page for more details. </p>"},{"location":"reference/versioning-support/#api","title":"API","text":"<p>There are two user facing resources exposed by HyperShift: HostedClusters and NodePools.</p> <p>The HyperShift API version policy generally aligns with the Kubernetes API versioning.</p>"},{"location":"reference/versioning-support/#ho","title":"HO","text":"<p>The upstream HyperShift project does not release new versions aligned with the OpenShift release cadence. New versions  of the HO are periodically tagged from the <code>main</code> branch. These versions are tested and consumed by internal Red Hat  managed services, and you can use these versions directly. However, for supported production use, you should use a  supported version of MCE.</p> <p>The HO is tagged at particular commits as part of merging new HO versions for Red Hat managed services; there is no  particular tagging scheme for this effort.</p> <p>A list of the tags can be found here.</p> <p>Once installed, the HO creates a ConfigMap called <code>supported-versions</code> into the Hypershift namespace, which describes  the HostedClusters supported versions that could be deployed. </p> <p>Here is an example <code>supported-versions</code> ConfigMap: <pre><code>apiVersion: v1\ndata:\n    server-version: 2f6cfe21a0861dea3130f3bed0d3ae5553b8c28b\n    supported-versions: '{\"versions\":[\"4.17\",\"4.16\",\"4.15\",\"4.14\"]}'\nkind: ConfigMap\nmetadata:\n    creationTimestamp: \"2024-06-20T07:12:31Z\"\n    labels:\n        hypershift.openshift.io/supported-versions: \"true\"\n    name: supported-versions\n    namespace: hypershift\n    resourceVersion: \"927029\"\n    uid: f6336f91-33d3-472d-b747-94abae725f70\n</code></pre></p> <p>Important</p> <pre><code>You cannot install HCs higher than what the HO supports. In the example above, HCs using images greater than \n4.17 cannot be created.\n</code></pre>"},{"location":"reference/versioning-support/#cpo","title":"CPO","text":"<p>The CPO is released as part of each OCP payload release image. You can find those release images here:</p> <ul> <li>amd64</li> <li>arm64</li> <li>multi-arch</li> </ul>"},{"location":"reference/versioning-support/#hypershift-cli","title":"HyperShift CLI","text":"<p>The HyperShift CLI is a helper utility used only for development and testing purposes. No compatibility policies are  guaranteed.</p> <p>It helps create required infrastructure needed for a HostedCluster CR and NodePool CR to successfully install. </p>"},{"location":"reference/versioning-support/#showing-general-version-information","title":"Showing General Version Information","text":"<p>Running the following command will show what the latest OCP version the CLI supports against your KUBECONFIG: <pre><code>% hypershift version\nClient Version: openshift/hypershift: 1ed535a8d27c5a1546f1ff4cc71abf32dd1a26aa. Latest supported OCP: 4.17.0\nServer Version: 2f6cfe21a0861dea3130f3bed0d3ae5553b8c28b\nServer Supports OCP Versions: 4.17, 4.16, 4.15, 4.14\n</code></pre></p>"},{"location":"reference/versioning-support/#showing-hypershift-cli-version-information","title":"Showing HyperShift CLI Version Information","text":"<p>Running the following command will show the commit sha the HyperShift CLI was built from: <pre><code> % hypershift version --commit-only\n211a8536809aca94d6047c057866be54d96777c5\n</code></pre></p>"},{"location":"reference/versioning-support/#hcp-cli","title":"HCP CLI","text":"<p>The HCP CLI is the productized version of the HyperShift CLI. This CLI is available through download from MCE. </p> <p>Similar to the HyperShift CLI, running the <code>hcp version</code> command will show the latest OCP version the CLI supports  against your KUBECONFIG. Running the <code>hcp version --commit-only</code> will show the commit sha the HCP CLI was built from.</p>"},{"location":"reference/architecture/","title":"Diagrams","text":"<p>This section describes the architecture diagrams that is used by each platform.</p>"},{"location":"reference/architecture/mce-and-agent/","title":"Mce and agent","text":"<p>Introduction</p> <p>This section elucidates the collaboration between Multicluster Engine and Agent to facilitate in-house deployments. Detailed documentation for each of the network stacks can be found in the Self-Managed Laboratories section. If you intend to set up a self-managed environment, please proceed to that section and follow the provided steps.</p> <p>High-Level Overview</p> <p></p> <p>The diagram above provides an overview of the environment and how the workflow functions, along with labeled components for reference:</p> <ol> <li>(Applicable in Disconnected Environments Only): Create a ConfigMap with a specified name in the <code>openshift-config</code> namespace. In this example, we'll name it <code>registry-config</code>. The content of this ConfigMap should be the Registry CA certificate.</li> <li>(Applicable in Disconnected Environments Only): Modify the <code>images.config.openshift.io</code> Custom Resource (CR) and add a new field in the spec named <code>additionalTrustedCA</code>, with a value of <code>name: registry-config</code>.</li> <li>(Applicable in Disconnected Environments Only): Create a ConfigMap with a specified name containing two data fields. One field should contain the <code>registries.conf</code> file in RAW format, and the other should be named <code>ca-bundle.crt</code>, which should contain the Registry CA.</li> <li>Create the <code>multiclusterengine</code> CR, enabling both the Agent and Hypershift addons.</li> <li> <p>Create the HostedCluster Objects. This involves several components, including:</p> <ol> <li>Secrets: These contain the PullSecret, SSHKey, and ETCD Encryption Key.</li> <li>ConfigMap (Applicable in Disconnected Environments Only): This ConfigMap contains the CA certificate of the private registry.</li> <li>HostedCluster: Defines the configuration of the cluster you intend to create.</li> <li>NodePool: Identifies the pool that references the Machines to be used for the data plane.</li> </ol> </li> <li> <p>Following the creation of HostedCluster Objects, the Hypershift Operator will establish the HostedControlPlane Namespace to accommodate ControlPlane pods. This namespace will also host components like Agents, BareMetalHosts, Infraenv, and more. Subsequently, you need to create the InfraEnv and, after ISO creation, the BareMetalHosts along with their secrets containing BMC credentials.</p> </li> <li> <p>The Metal3 operator, within the <code>openshift-machine-api</code> namespace, will inspect the newly created BareMetalHosts. It will then attempt to connect to the BMCs to boot them up using the configured LiveISO and RootFS specified through the AgentServiceConfig CR in the MCE namespace.</p> </li> <li> <p>Once the worker nodes of the HostedCluster have successfully booted up, an Agent container will be initiated. This Agent will establish contact with the Assisted Service, which will orchestrate the necessary actions to complete the deployment. Initially, you will need to scale the NodePool to the desired number of worker nodes for your HostedCluster, after which the AssistedService will manage the remaining tasks.</p> </li> <li> <p>At this point, you need to patiently await the completion of the deployment process.</p> </li> </ol>"},{"location":"reference/architecture/managed-azure/secrets-csi/","title":"Secrets CSI Usage","text":"<p>The Secrets CSI driver is used in HyperShift's managed Azure architecture in order to read secrets from Azure Key Vault  and mount them as files in a pod. This allows for the secure storage of sensitive information such as credentials and  certificates.</p> <p>More information on Secrets CSI driver can be found in the official documentation.</p>"},{"location":"reference/architecture/managed-azure/secrets-csi/#overview","title":"Overview","text":"<p>A single managed identity is used to pull any secrets or certificates from Azure Key Vault. The managed identity is  created when the AKS cluster is created. For example, this happens when the flag  <code>enable-addons azure-keyvault-secrets-provider</code> is provided when creating the AKS cluster using the Azure CLI. </p> <p>Important</p> <p>The created managed identity is expected to have the <code>Key Vault Secrets User</code> role assigned to it so that it can read  secrets and credentials from the Azure Key Vault.</p> <p>Important</p> <p>This managed identity will be used by any HostedClusters managed by the HO to read secrets from the Azure Key Vault.</p> <p>This managed identity is passed in as a client ID to the HyperShift Operator during installation through the flag  <code>aro-hcp-key-vault-users-client-id</code>. This client ID will be passed in to every created SecretProviderClass CR and used  in the field called <code>userAssignedIdentityID</code>.</p>"},{"location":"reference/architecture/managed-azure/secrets-csi/#secretsproviderclass-crs","title":"SecretsProviderClass CRs","text":"<p>The HyperShift Operator creates a SecretProviderClass CR for:</p> <ul> <li>the control plane operator (CPO)</li> <li>the nodepool management provider (CAPZ)</li> </ul> <p>The CPO creates a SecretProviderClass CR for:</p> <ul> <li>key management service (KMS)</li> <li>cloud-network-config-controller (CNCC)</li> <li>cloud provider (CP, aka CCM)</li> <li>ingress</li> <li>image registry</li> <li>Azure disk CSI driver</li> <li>Azure file CSI driver</li> </ul> <p>Here is an example of a SecretProviderClass CR for the CPO:</p> <pre><code>apiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  annotations:\n    hypershift.openshift.io/cluster: e2e-clusters-example/example-cluster\nspec:\n  parameters:\n    keyvaultName: aks-e2e\n    objects: |2\n\n      array:\n        - |\n          objectName: cpo-secret\n          objectEncoding: utf-8\n          objectType: secret\n    tenantId: f47a1c3d-89ab-4def-b456-123456789abc\n    usePodIdentity: \"false\"\n    useVMManagedIdentity: \"true\"\n    userAssignedIdentityID: 92bd5e7a-3cfe-41a2-9f88-df0123456789\n  provider: azure\n</code></pre>"},{"location":"reference/architecture/managed-azure/secrets-csi/#how-secretproviderclass-cr-is-used","title":"How SecretProviderClass CR is Used","text":"<p>The SecretProviderClass CR is then used by the Secrets CSI driver to mount the secret, <code>objectName</code> in the above  example, into the pod as a file in a volume mount. Here is an example of a pod spec that mounts the secret:</p> <pre><code>  containers:\n...\n    name: control-plane-operator\n...\n    volumeMounts:\n      - mountPath: /mnt/certs\n        name: cpo-cert\n        readOnly: true\n...\n    volumes:\n      - csi:\n          driver: secrets-store.csi.k8s.io\n          readOnly: true\n          volumeAttributes:\n            secretProviderClass: managed-azure-cpo\n        name: cpo-cert\n</code></pre> <p>The mounted secret can be viewed in the pod by navigating to the <code>/mnt/certs</code> directory and catting the file. In this  example, something like:</p> <pre><code>k exec -it control-plane-operator -- /bin/bash\ncat /mnt/certs/cpo-secret\n</code></pre>"},{"location":"reference/architecture/managed-azure/secrets-csi/#how-secret-information-is-used-by-components","title":"How Secret Information is Used By Components","text":"<p>All the different components using the Secrets CSI driver will have their own way of consuming the secret.</p>"},{"location":"reference/architecture/managed-azure/secrets-csi/#consumed-directly-in-the-operatorcontroller","title":"Consumed directly in the operator/controller","text":"<p>The following components directly use the secret file mounted in the pod to authenticate with Azure cloud:</p> <ul> <li>CPO</li> <li>CNCC</li> <li>ingress</li> <li>image registry</li> </ul> <p>For example: <pre><code>    certPath := config.ManagedAzureCertificatePath + hcp.Spec.Platform.Azure.ManagedIdentities.ControlPlane.ControlPlaneOperator.CredentialsSecretName\n    creds, err := dataplane.NewUserAssignedIdentityCredential(ctx, certPath, dataplane.WithClientOpts(azcore.ClientOptions{Cloud: cloud.AzurePublic}))\n    if err != nil {\n        return fmt.Errorf(\"failed to create azure creds to verify resource group locations: %v\", err)\n    }\n</code></pre></p>"},{"location":"reference/architecture/managed-azure/secrets-csi/#consumed-by-a-configuration-file","title":"Consumed by a configuration file","text":"<p>The following components use a configuration file in order to know where to find the secret mounted in the pod:</p> <ul> <li>CP/CCM</li> <li>Azure disk CSI driver</li> <li>Azure file CSI driver</li> </ul> <p>For an example, see the official documentation.</p>"},{"location":"reference/architecture/managed-azure/secrets-csi/#consumed-through-a-cr","title":"Consumed through a CR","text":"<p>Finally, the nodepool management provider (CAPZ) uses a CR, AzureClusterIdentity, to identify where the secret is  mounted in the pod.</p> <p>For an example, see the official documentation.</p>"},{"location":"reference/architecture/managed-azure/shared-ingress/","title":"Shared Ingress","text":""},{"location":"reference/architecture/managed-azure/shared-ingress/#motivation","title":"Motivation","text":"<p>Today, exposing the apiserver of an hosted cluster to its nodes is possible only with two options: LoadBalancer and NodePort. The downside of the LoadBalancer method is that it would require a separate LB for every hosted cluster configured on the mgmt cluster. This can incur additional costs at the cloud provider and an additional, even if it small, spin up time to provision the LB service. The downside of the NodePort method is that it is bound to one of the nodes\u2019 IP of the mgmt cluster (typically the 1st one). Once this node is down, all of the hosted clusters\u2019 apiservers which are based on NodePort connectivity are no longer reachable. Shared Ingress presents another option, in which there would be a single LoadBalacer on the mgmt cluster, that will serve all of the hosted clusters that are configured to use this method. Connections to the hosted kube-apiservers from outside of the cluster will be made available through a Route.</p>"},{"location":"reference/architecture/managed-azure/shared-ingress/#overview","title":"Overview","text":"<p>Generally in kubernetes and openshift clusters, pods who wish to communicate with their cluster\u2019s api-server, are doing it through the kubernetes.default.svc.cluster.local master service, which is mapped to the cluster\u2019s kube-apiserver service. In HyperShift guest clusters, the master service (whose IP is 172.31.0.1) is mapped to a kube-apiserver-proxy pod, running on every node at the host level, which proxies the request to the apiserver on the HCP namespace at the mgmt cluster, 1:1 flat translation, in TLS passthrough.</p> <p>Shared Ingress (single LB) solution is able to differentiate between various hcp api-servers hosted on the mgmt cluster using the PROXY Protocol. The PROXY protocol enables adding additional info to a TLS connection, outside of the encrypted message, when both ends support this protocol. The fields that are added with the PROXY protocol are mainly: - Source IP - Source Port - Destination IP - Destination Port - TLVs (key-value pairs that carry arbitrary data)</p>"},{"location":"reference/architecture/managed-azure/shared-ingress/#guest-intermediate-proxy","title":"Guest Intermediate Proxy","text":"<p>On the guest cluster, the kube-apiserver-proxy (HAProxy) instance, will use the PROXY protocol when sending requests and includes a custom TLV which contains the <code>ClusterID</code> so the central proxy on the mgmt can see <code>ClusterID</code> and forward it to the respective kube-apiserver on the hcp. The default backend of this proxy will be the LB address of the central proxy.</p>"},{"location":"reference/architecture/managed-azure/shared-ingress/#mgmt-cluster-central-proxy","title":"MGMT Cluster Central Proxy","text":"<p>On the management cluster, we setup a single proxy server (HAProxy) in the <code>hypershift-sharedingress</code> namespace. This central proxy will accept connections through a LoadBalancer service, and with PROXY protocol. It will then examine the custom TLV field of the PROXY protocol, extract the attached <code>ClusterID</code> and using an ACL, will forward the connection to the respective hcp kube-apiserver. The destination IP in this case will be exactly the same as the ClusterIP of the hcp kube-apiserver.</p> <p>Note: the kube-apiserver will no longer be exposed through a dedicated LB service.</p>"},{"location":"reference/architecture/managed-azure/shared-ingress/#reference-diagram","title":"Reference Diagram","text":""},{"location":"reference/infrastructure/","title":"Infrastructure","text":"<p>This section describes the infrastructure that is used by each platform. This includes infrastructure that hypershift creates and manages, as well as prerequisites.</p>"},{"location":"reference/infrastructure/agent/","title":"Agent","text":"<p>The agent platform does not create any infrastructure but does have two kinds of prerequisites:</p> <ol> <li>Agents: An Agent represents a host booted with a discovery image and ready to be provisioned as an OpenShift node. For more information, see here.</li> <li>DNS: The API and ingress endpoints must be routable.</li> </ol> <p>You can find more details about the prerequisites in the how-to.</p>"},{"location":"reference/infrastructure/aws/","title":"Aws","text":"<p>In this section we want to dissect who creates what and what not. It contains 4 stages:</p> <ul> <li>Infra pre-required and unmanaged for hypershift operator in an arbitrary AWS account</li> <li>Infra pre-required and unmanaged in Hosted Cluster AWS account</li> <li>Infra managed by hypershift in Management AWS account</li> <li>Infra managed by hypershift in Hosted Cluster AWS account</li> <li>Infra managed by kubernetes in Hosted Cluster AWS account</li> </ul> <p>Note</p> <p>The arbitrary AWS account depends on who is providing the Hypershift service.</p> <ul> <li>Self Managed: It will be controlled by the Cluster Service Provider.</li> <li>SaaS: In this case the AWS Account will belong to Red Hat.</li> </ul>"},{"location":"reference/infrastructure/aws/#infra-pre-required-and-unmanaged-for-hypershift-operator-in-an-arbitrary-aws-account","title":"Infra pre-required and unmanaged for hypershift operator in an arbitrary AWS account","text":"Management Cluster AWS Account <ul> <li>1 S3 Bucket<ul> <li>OIDC</li> </ul> </li> <li>Route 53 Hosted zones<ul> <li>Domain to host Private and Public entries for HostedClusters</li> </ul> </li> </ul>"},{"location":"reference/infrastructure/aws/#infra-pre-required-and-unmanaged-in-hosted-cluster-aws-account","title":"Infra pre-required and unmanaged in Hosted Cluster AWS account","text":"All access Modes <ul> <li>1 VPC</li> <li>1 DHCP Options</li> <li>2 Subnets<ul> <li>Private subnet - internal data plane subnet</li> <li>Public subnet - enable access to the internet from the data plane</li> </ul> </li> <li>1 Internet Gateway</li> <li>1 Elastic IP</li> <li>1 NAT Gateway</li> <li>1 Security Group (Worker Nodes)</li> <li>2 Route Tables (1 Private, 1 Public)</li> <li>2 Route 53 Hosted Zones</li> <li>Enough quota for:<ul> <li>1 Ingress Service Load Balancer (for Public Hosted Clusters)</li> <li>1 Private Link Endpoint (for Private Hosted Clusters)</li> </ul> </li> </ul>"},{"location":"reference/infrastructure/aws/#aws-infra-managed-by-hypershift","title":"AWS Infra Managed by Hypershift","text":"<ul> <li>Public</li> </ul> Management Cluster AWS AccountHosted Cluster AWS account <ul> <li>NLB - Load Balancer Kube API Server<ul> <li>Kubernetes creates a Security Group</li> </ul> </li> <li>Volumes<ul> <li>For ETCD (1 or 3 depending on HA)</li> <li>For ovn-Kube</li> </ul> </li> </ul> <ul> <li>For NodePools:<ul> <li>EC2 Instances<ul> <li>Need the Role and RolePolicy</li> </ul> </li> </ul> </li> </ul> <ul> <li>Private</li> </ul> Management Cluster AWS AccountHosted Cluster AWS account <ul> <li>NLB - Load Balancer Private Router</li> <li>Endpoint Service (Private Link)</li> </ul> <ul> <li>Private Link Endpoints<ul> <li>1 Endpoint per Availability Zone</li> </ul> </li> <li>For NodePools:<ul> <li>EC2 Instances</li> </ul> </li> </ul> <ul> <li>PublicAndPrivate</li> </ul> Management Cluster AWS AccountHosted Cluster AWS account <ul> <li>1 NLB - Load Balancer Public Router</li> <li>1 NLB - Load Balancer Private Router</li> <li>Enpoint Service (Private Link)</li> <li>Volumes<ul> <li>For ETCD (1 or 3 depending on HA)</li> <li>For ovn-Kube</li> </ul> </li> </ul> <ul> <li>Private Link Endpoints<ul> <li>1 Endpoint per Availability Zone</li> </ul> </li> <li>For NodePools:<ul> <li>EC2 Instances</li> </ul> </li> </ul>"},{"location":"reference/infrastructure/aws/#aws-infra-managed-by-kubernetes","title":"AWS Infra Managed by Kubernetes","text":"Hosted Cluster AWS account <ul> <li>Network Load Balancer for default ingress</li> <li>S3 bucket for registry</li> </ul> <p>Note</p> <p>For the Private Link networking to work, we've observed that the Endpoint zone in the hosted cluster AWS account, must match the zone of the instance resolved by the Service Endpoint in the management cluster AWS account.</p> <p>In AWS the Zone names are just alias e.g. \"us-east-2b\" which do not necessarily map to the same zone in different accounts.</p> <p>Because of this for Private link to be guaranteed to work, the management cluster must have subnets/workers in all zones of its region.</p>"},{"location":"reference/infrastructure/aws/#iam","title":"IAM","text":"<p>In this section we want to clarify how the AWS IAM works in Hypershift context.</p> <p>Hypershift expects to have the ARN roles already created in AWS, so the entity responsible to create them lies in the consumer (CLI/OCM). Hypershift tries to enable granularity to honor principle of least privilege components, which means that every component will use their own role to operate or create AWS objects and the roles are limited to what is required for the normal functioning of the product.</p> <p>As an example, our CLI can handle the creation of these roles, you can follow this article to make this happen.</p> <p>The HostedCluster receives the ARN roles as input, then the CLI/OCM creates an AWS permission config for each component. That lets a component to auth via STS and preconfigured oidc idp.</p> <p>The roles created are consumed by some components from Hypershift, running on Control Plane and operating at Data Plane side:</p> <p>These are the different roles</p> <ul> <li>controlPlaneOperatorARN</li> <li>imageRegistryARN</li> <li>ingressARN</li> <li>kubeCloudControllerARN</li> <li>nodePoolManagementARN</li> <li>storageARN</li> <li>networkARN</li> </ul> <p>This is a sample of how this looks like the reference to the IAM Roles from the HostedCluster perspective</p> <pre><code>...\nendpointAccess: Public\n  region: us-east-2\n  resourceTags:\n  - key: kubernetes.io/cluster/cewong-dev-bz4j5\n    value: owned\nrolesRef:\n    controlPlaneOperatorARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-control-plane-operator\n    imageRegistryARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-openshift-image-registry\n    ingressARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-openshift-ingress\n    kubeCloudControllerARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-cloud-controller\n    networkARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-cloud-network-config-controller\n    nodePoolManagementARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-node-pool\n    storageARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-aws-ebs-csi-driver-controller\ntype: AWS\n...\n</code></pre> <p>And these are samples for each one of the roles Hypershift uses:</p> FullPermissionSetIngressARNImageRegistryARNStorageARNNetworkARNKubeCloudControllerARNNodePoolManagementARNControlPlaneOperatorARN <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Statement1\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:CreateBucket\",\n                \"s3:DeleteBucket\",\n                \"s3:PutBucketTagging\",\n                \"s3:GetBucketTagging\",\n                \"s3:PutBucketPublicAccessBlock\",\n                \"s3:GetBucketPublicAccessBlock\",\n                \"s3:PutEncryptionConfiguration\",\n                \"s3:GetEncryptionConfiguration\",\n                \"s3:PutLifecycleConfiguration\",\n                \"s3:GetLifecycleConfiguration\",\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\",\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\",\n                \"s3:ListBucketMultipartUploads\",\n                \"s3:AbortMultipartUpload\",\n                \"s3:ListMultipartUploadParts\",\n                \"elasticloadbalancing:DescribeLoadBalancers\",\n                \"tag:GetResources\",\n                \"route53:ListHostedZones\",\n                \"ec2:AttachVolume\",\n                \"ec2:CreateSnapshot\",\n                \"ec2:CreateTags\",\n                \"ec2:CreateVolume\",\n                \"ec2:DeleteSnapshot\",\n                \"ec2:DeleteTags\",\n                \"ec2:DeleteVolume\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeSnapshots\",\n                \"ec2:DescribeTags\",\n                \"ec2:DescribeVolumes\",\n                \"ec2:DescribeVolumesModifications\",\n                \"ec2:DetachVolume\",\n                \"ec2:ModifyVolume\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeInstanceStatus\",\n                \"ec2:DescribeInstanceTypes\",\n                \"ec2:UnassignPrivateIpAddresses\",\n                \"ec2:AssignPrivateIpAddresses\",\n                \"ec2:UnassignIpv6Addresses\",\n                \"ec2:AssignIpv6Addresses\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeNetworkInterfaces\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeImages\",\n                \"ec2:DescribeRegions\",\n                \"ec2:DescribeRouteTables\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeVolumes\",\n                \"ec2:CreateSecurityGroup\",\n                \"ec2:CreateTags\",\n                \"ec2:CreateVolume\",\n                \"ec2:ModifyInstanceAttribute\",\n                \"ec2:ModifyVolume\",\n                \"ec2:AttachVolume\",\n                \"ec2:AuthorizeSecurityGroupIngress\",\n                \"ec2:CreateRoute\",\n                \"ec2:DeleteRoute\",\n                \"ec2:DeleteSecurityGroup\",\n                \"ec2:DeleteVolume\",\n                \"ec2:DetachVolume\",\n                \"ec2:RevokeSecurityGroupIngress\",\n                \"ec2:DescribeVpcs\",\n                \"elasticloadbalancing:AddTags\",\n                \"elasticloadbalancing:AttachLoadBalancerToSubnets\",\n                \"elasticloadbalancing:ApplySecurityGroupsToLoadBalancer\",\n                \"elasticloadbalancing:CreateLoadBalancer\",\n                \"elasticloadbalancing:CreateLoadBalancerPolicy\",\n                \"elasticloadbalancing:CreateLoadBalancerListeners\",\n                \"elasticloadbalancing:ConfigureHealthCheck\",\n                \"elasticloadbalancing:DeleteLoadBalancer\",\n                \"elasticloadbalancing:DeleteLoadBalancerListeners\",\n                \"elasticloadbalancing:DescribeLoadBalancers\",\n                \"elasticloadbalancing:DescribeLoadBalancerAttributes\",\n                \"elasticloadbalancing:DetachLoadBalancerFromSubnets\",\n                \"elasticloadbalancing:DeregisterInstancesFromLoadBalancer\",\n                \"elasticloadbalancing:ModifyLoadBalancerAttributes\",\n                \"elasticloadbalancing:RegisterInstancesWithLoadBalancer\",\n                \"elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer\",\n                \"elasticloadbalancing:AddTags\",\n                \"elasticloadbalancing:CreateListener\",\n                \"elasticloadbalancing:CreateTargetGroup\",\n                \"elasticloadbalancing:DeleteListener\",\n                \"elasticloadbalancing:DeleteTargetGroup\",\n                \"elasticloadbalancing:DescribeListeners\",\n                \"elasticloadbalancing:DescribeLoadBalancerPolicies\",\n                \"elasticloadbalancing:DescribeTargetGroups\",\n                \"elasticloadbalancing:DescribeTargetHealth\",\n                \"elasticloadbalancing:ModifyListener\",\n                \"elasticloadbalancing:ModifyTargetGroup\",\n                \"elasticloadbalancing:RegisterTargets\",\n                \"elasticloadbalancing:SetLoadBalancerPoliciesOfListener\",\n                \"iam:CreateServiceLinkedRole\",\n                \"kms:DescribeKey\",\n                \"ec2:AllocateAddress\",\n                \"ec2:AssociateRouteTable\",\n                \"ec2:AttachInternetGateway\",\n                \"ec2:AuthorizeSecurityGroupIngress\",\n                \"ec2:CreateInternetGateway\",\n                \"ec2:CreateNatGateway\",\n                \"ec2:CreateRoute\",\n                \"ec2:CreateRouteTable\",\n                \"ec2:CreateSecurityGroup\",\n                \"ec2:CreateSubnet\",\n                \"ec2:CreateTags\",\n                \"ec2:DeleteInternetGateway\",\n                \"ec2:DeleteNatGateway\",\n                \"ec2:DeleteRouteTable\",\n                \"ec2:DeleteSecurityGroup\",\n                \"ec2:DeleteSubnet\",\n                \"ec2:DeleteTags\",\n                \"ec2:DescribeAccountAttributes\",\n                \"ec2:DescribeAddresses\",\n                \"ec2:DescribeAvailabilityZones\",\n                \"ec2:DescribeImages\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeInternetGateways\",\n                \"ec2:DescribeNatGateways\",\n                \"ec2:DescribeNetworkInterfaces\",\n                \"ec2:DescribeNetworkInterfaceAttribute\",\n                \"ec2:DescribeRouteTables\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeVpcs\",\n                \"ec2:DescribeVpcAttribute\",\n                \"ec2:DescribeVolumes\",\n                \"ec2:DetachInternetGateway\",\n                \"ec2:DisassociateRouteTable\",\n                \"ec2:DisassociateAddress\",\n                \"ec2:ModifyInstanceAttribute\",\n                \"ec2:ModifyNetworkInterfaceAttribute\",\n                \"ec2:ModifySubnetAttribute\",\n                \"ec2:ReleaseAddress\",\n                \"ec2:RevokeSecurityGroupIngress\",\n                \"ec2:RunInstances\",\n                \"ec2:TerminateInstances\",\n                \"tag:GetResources\",\n                \"ec2:CreateLaunchTemplate\",\n                \"ec2:CreateLaunchTemplateVersion\",\n                \"ec2:DescribeLaunchTemplates\",\n                \"ec2:DescribeLaunchTemplateVersions\",\n                \"ec2:DeleteLaunchTemplate\",\n                \"ec2:DeleteLaunchTemplateVersions\",\n                \"ec2:CreateVpcEndpoint\",\n                \"ec2:DescribeVpcEndpoints\",\n                \"ec2:ModifyVpcEndpoint\",\n                \"ec2:DeleteVpcEndpoints\",\n                \"ec2:CreateTags\",\n                \"route53:ListHostedZones\"\n                \"iam:GetRolePolicy\",\n                \"iam:PassRole\",\n                \"iam:AddRoleToInstanceProfile\",\n                \"iam:CreateInstanceProfile\",\n                \"iam:GetInstanceProfile\",\n                \"iam:PutRolePolicy\",\n                \"iam:CreateRole\",\n                \"iam:GetRole\",\n                \"iam:CreateOpenIDConnectProvider\",\n                \"iam:ListOpenIDConnectProviders\",\n                \"route53:ChangeResourceRecordSets\",\n                \"route53:ListResourceRecordSets\",\n                \"ec2:ReplaceRouteTableAssociation\",\n                \"ec2:AssociateDhcpOptions\",\n                \"ec2:CreateDhcpOptions\",\n                \"ec2:DescribeDhcpOptions\",\n                \"ec2:ModifyVpcAttribute\",\n                \"ec2:CreateVpc\",\n                \"route53:CreateHostedZone\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ]\n}\n</code></pre> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"elasticloadbalancing:DescribeLoadBalancers\",\n                \"tag:GetResources\",\n                \"route53:ListHostedZones\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"route53:ChangeResourceRecordSets\"\n            ],\n            \"Resource\": [\n                \"arn:aws:route53:::PUBLIC_ZONE_ID\",\n                \"arn:aws:route53:::PRIVATE_ZONE_ID\"\n            ]\n        }\n    ]\n}\n</code></pre> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:CreateBucket\",\n                \"s3:DeleteBucket\",\n                \"s3:PutBucketTagging\",\n                \"s3:GetBucketTagging\",\n                \"s3:PutBucketPublicAccessBlock\",\n                \"s3:GetBucketPublicAccessBlock\",\n                \"s3:PutEncryptionConfiguration\",\n                \"s3:GetEncryptionConfiguration\",\n                \"s3:PutLifecycleConfiguration\",\n                \"s3:GetLifecycleConfiguration\",\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\",\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\",\n                \"s3:ListBucketMultipartUploads\",\n                \"s3:AbortMultipartUpload\",\n                \"s3:ListMultipartUploadParts\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:AttachVolume\",\n                \"ec2:CreateSnapshot\",\n                \"ec2:CreateTags\",\n                \"ec2:CreateVolume\",\n                \"ec2:DeleteSnapshot\",\n                \"ec2:DeleteTags\",\n                \"ec2:DeleteVolume\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeSnapshots\",\n                \"ec2:DescribeTags\",\n                \"ec2:DescribeVolumes\",\n                \"ec2:DescribeVolumesModifications\",\n                \"ec2:DetachVolume\",\n                \"ec2:ModifyVolume\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeInstanceStatus\",\n                \"ec2:DescribeInstanceTypes\",\n                \"ec2:UnassignPrivateIpAddresses\",\n                \"ec2:AssignPrivateIpAddresses\",\n                \"ec2:UnassignIpv6Addresses\",\n                \"ec2:AssignIpv6Addresses\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeNetworkInterfaces\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeImages\",\n                \"ec2:DescribeRegions\",\n                \"ec2:DescribeRouteTables\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeVolumes\",\n                \"ec2:CreateSecurityGroup\",\n                \"ec2:CreateTags\",\n                \"ec2:CreateVolume\",\n                \"ec2:ModifyInstanceAttribute\",\n                \"ec2:ModifyVolume\",\n                \"ec2:AttachVolume\",\n                \"ec2:AuthorizeSecurityGroupIngress\",\n                \"ec2:CreateRoute\",\n                \"ec2:DeleteRoute\",\n                \"ec2:DeleteSecurityGroup\",\n                \"ec2:DeleteVolume\",\n                \"ec2:DetachVolume\",\n                \"ec2:RevokeSecurityGroupIngress\",\n                \"ec2:DescribeVpcs\",\n                \"elasticloadbalancing:AddTags\",\n                \"elasticloadbalancing:AttachLoadBalancerToSubnets\",\n                \"elasticloadbalancing:ApplySecurityGroupsToLoadBalancer\",\n                \"elasticloadbalancing:CreateLoadBalancer\",\n                \"elasticloadbalancing:CreateLoadBalancerPolicy\",\n                \"elasticloadbalancing:CreateLoadBalancerListeners\",\n                \"elasticloadbalancing:ConfigureHealthCheck\",\n                \"elasticloadbalancing:DeleteLoadBalancer\",\n                \"elasticloadbalancing:DeleteLoadBalancerListeners\",\n                \"elasticloadbalancing:DescribeLoadBalancers\",\n                \"elasticloadbalancing:DescribeLoadBalancerAttributes\",\n                \"elasticloadbalancing:DetachLoadBalancerFromSubnets\",\n                \"elasticloadbalancing:DeregisterInstancesFromLoadBalancer\",\n                \"elasticloadbalancing:ModifyLoadBalancerAttributes\",\n                \"elasticloadbalancing:RegisterInstancesWithLoadBalancer\",\n                \"elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer\",\n                \"elasticloadbalancing:AddTags\",\n                \"elasticloadbalancing:CreateListener\",\n                \"elasticloadbalancing:CreateTargetGroup\",\n                \"elasticloadbalancing:DeleteListener\",\n                \"elasticloadbalancing:DeleteTargetGroup\",\n                \"elasticloadbalancing:DescribeListeners\",\n                \"elasticloadbalancing:DescribeLoadBalancerPolicies\",\n                \"elasticloadbalancing:DescribeTargetGroups\",\n                \"elasticloadbalancing:DescribeTargetHealth\",\n                \"elasticloadbalancing:ModifyListener\",\n                \"elasticloadbalancing:ModifyTargetGroup\",\n                \"elasticloadbalancing:RegisterTargets\",\n                \"elasticloadbalancing:SetLoadBalancerPoliciesOfListener\",\n                \"iam:CreateServiceLinkedRole\",\n                \"kms:DescribeKey\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ],\n            \"Effect\": \"Allow\"\n        }\n    ]\n}\n</code></pre> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"ec2:AllocateAddress\",\n                \"ec2:AssociateRouteTable\",\n                \"ec2:AttachInternetGateway\",\n                \"ec2:AuthorizeSecurityGroupIngress\",\n                \"ec2:CreateInternetGateway\",\n                \"ec2:CreateNatGateway\",\n                \"ec2:CreateRoute\",\n                \"ec2:CreateRouteTable\",\n                \"ec2:CreateSecurityGroup\",\n                \"ec2:CreateSubnet\",\n                \"ec2:CreateTags\",\n                \"ec2:DeleteInternetGateway\",\n                \"ec2:DeleteNatGateway\",\n                \"ec2:DeleteRouteTable\",\n                \"ec2:DeleteSecurityGroup\",\n                \"ec2:DeleteSubnet\",\n                \"ec2:DeleteTags\",\n                \"ec2:DescribeAccountAttributes\",\n                \"ec2:DescribeAddresses\",\n                \"ec2:DescribeAvailabilityZones\",\n                \"ec2:DescribeImages\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeInternetGateways\",\n                \"ec2:DescribeNatGateways\",\n                \"ec2:DescribeNetworkInterfaces\",\n                \"ec2:DescribeNetworkInterfaceAttribute\",\n                \"ec2:DescribeRouteTables\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeVpcs\",\n                \"ec2:DescribeVpcAttribute\",\n                \"ec2:DescribeVolumes\",\n                \"ec2:DetachInternetGateway\",\n                \"ec2:DisassociateRouteTable\",\n                \"ec2:DisassociateAddress\",\n                \"ec2:ModifyInstanceAttribute\",\n                \"ec2:ModifyNetworkInterfaceAttribute\",\n                \"ec2:ModifySubnetAttribute\",\n                \"ec2:ReleaseAddress\",\n                \"ec2:RevokeSecurityGroupIngress\",\n                \"ec2:RunInstances\",\n                \"ec2:TerminateInstances\",\n                \"tag:GetResources\",\n                \"ec2:CreateLaunchTemplate\",\n                \"ec2:CreateLaunchTemplateVersion\",\n                \"ec2:DescribeLaunchTemplates\",\n                \"ec2:DescribeLaunchTemplateVersions\",\n                \"ec2:DeleteLaunchTemplate\",\n                \"ec2:DeleteLaunchTemplateVersions\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Condition\": {\n                \"StringLike\": {\n                    \"iam:AWSServiceName\": \"elasticloadbalancing.amazonaws.com\"\n                }\n            },\n            \"Action\": [\n                \"iam:CreateServiceLinkedRole\"\n            ],\n            \"Resource\": [\n                \"arn:*:iam::*:role/aws-service-role/elasticloadbalancing.amazonaws.com/AWSServiceRoleForElasticLoadBalancing\"\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Action\": [\n                \"iam:PassRole\"\n            ],\n            \"Resource\": [\n                \"arn:*:iam::*:role/*-worker-role\"\n            ],\n            \"Effect\": \"Allow\"\n        }\n    ]\n}\n</code></pre> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:CreateVpcEndpoint\",\n                \"ec2:DescribeVpcEndpoints\",\n                \"ec2:ModifyVpcEndpoint\",\n                \"ec2:DeleteVpcEndpoints\",\n                \"ec2:CreateTags\",\n                \"route53:ListHostedZones\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"route53:ChangeResourceRecordSets\",\n                \"route53:ListResourceRecordSets\"\n            ],\n            \"Resource\": \"arn:aws:route53:::%s\"\n        }\n    ]\n}\n</code></pre>"},{"location":"reference/manifests/","title":"Manifests","text":"<p>This section describes the manifests that are used for each platform.</p>"},{"location":"reference/manifests/ibmcloud/4.10/","title":"4.10","text":"<p>HostedCluster</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: HostedCluster\nmetadata:\n  annotations:\n    hypershift.openshift.io/capi-manager-image: us.icr.io/armada-master/hypershift-cluster-api@sha256:dd610ea557e88156df3c04b293698defa3a8378f922f9abb2b098bc7495564f2\n    hypershift.openshift.io/control-plane-operator-image: us.icr.io/armada-master/armada-hypershift-operator@sha256:70c345da5132c5a1cb82f759a1465c5448795e3a9c570b369714d6ecac263aa7\n    hypershift.openshift.io/disable-pki-reconciliation: \"true\"\n    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager\n    hypershift.openshift.io/force-upgrade-to: us.icr.io/armada-master/ocp-release:4.10.63-x86_64\n    hypershift.openshift.io/konnectivity-agent-image: us.icr.io/armada-master/rh-apiserver-network-proxy@sha256:3b5a71252f682ed75ba0d1854d058f44039661937abae4aa33a924d7f68a77c1\n    hypershift.openshift.io/konnectivity-server-image: us.icr.io/armada-master/rh-apiserver-network-proxy@sha256:3b5a71252f682ed75ba0d1854d058f44039661937abae4aa33a924d7f68a77c1\n    idpoverrides.hypershift.openshift.io/IAM: |\n      {\"urls\": {\"authorize\": \"https://iam.test.cloud.ibm.com/identity/authorize\", \"userInfo\": \"https://iam.test.cloud.ibm.com/identity/userinfo\", \"token\": \"https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token\"}, \"claims\": {\"id\": [\"iam_id\"], \"email\": [\"email\"], \"name\": [\"name\"], \"preferredUsername\": [\"preferred_username\"]}, \"challenge\": true}\n    oauth.hypershift.openshift.io/login-url-override: https://sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:31446\n  finalizers:\n  - hypershift.openshift.io/finalizer\n  labels:\n    clusterid: cismlo21050nmreb5nhg\n  name: cismlo21050nmreb5nhg\n  namespace: master\nspec:\n  autoscaling: {}\n  clusterID: 512f0876-573e-40b3-8a37-cb6f22b37e16\n  configuration:\n    apiServer:\n      audit:\n        profile: Default\n      clientCA:\n        name: \"\"\n      encryption: {}\n      servingCerts:\n        namedCertificates:\n        - names:\n          - sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n          servingCertificate:\n            name: ibm-named-certs\n      tlsSecurityProfile:\n        custom:\n          ciphers:\n          - ECDHE-ECDSA-AES128-GCM-SHA256\n          - ECDHE-RSA-AES128-GCM-SHA256\n          - ECDHE-ECDSA-AES256-GCM-SHA384\n          - ECDHE-RSA-AES256-GCM-SHA384\n          - ECDHE-ECDSA-CHACHA20-POLY1305\n          - ECDHE-RSA-CHACHA20-POLY1305\n          minTLSVersion: VersionTLS12\n        type: Custom\n    featureGate:\n      customNoUpgrade:\n        disabled:\n        - ServiceLBNodePortControl\n        - CSIMigrationAWS\n        - CSIMigrationOpenStack\n        - CSIMigrationGCE\n        - CSIMigrationAzureDisk\n        - CSIMigrationAzureFile\n        - CSIMigrationvSphere\n        enabled:\n        - ExpandInUsePersistentVolumes\n        - RotateKubeletServerCertificate\n        - DownwardAPIHugePages\n      featureSet: CustomNoUpgrade\n    ingress:\n      domain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n      loadBalancer:\n        platform:\n          type: \"\"\n    oauth:\n      identityProviders:\n      - mappingMethod: lookup\n        name: IAM\n        openID:\n          claims:\n            email:\n            - email\n            name:\n            - name\n            preferredUsername:\n            - preferred_username\n          clientID: CLIENTID\n          clientSecret:\n            name: hypershift-ibm-iam-clientsecret\n          issuer: https://iam.test.cloud.ibm.com/identity\n        type: OpenID\n      templates:\n        error:\n          name: \"\"\n        login:\n          name: \"\"\n        providerSelection:\n          name: \"\"\n      tokenConfig: {}\n  controllerAvailabilityPolicy: HighlyAvailable\n  dns:\n    baseDomain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n  etcd:\n    managementType: Unmanaged\n    unmanaged:\n      endpoint: https://etcd-cismlo21050nmreb5nhg-client:2379\n      tls:\n        clientSecret:\n          name: cismlo21050nmreb5nhg-etcd-client-tls\n  fips: false\n  imageContentSources:\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-release\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n  infraID: cismlo21050nmreb5nhg\n  infrastructureAvailabilityPolicy: HighlyAvailable\n  issuerURL: https://kubernetes.default.svc\n  networking:\n    apiServer:\n      advertiseAddress: 172.20.0.1\n      port: 2040\n    clusterNetwork:\n    - cidr: 172.30.0.0/16\n    machineNetwork:\n    - cidr: 172.30.0.0/16\n    networkType: Calico\n    serviceNetwork:\n    - cidr: 172.21.0.0/16\n  olmCatalogPlacement: guest\n  platform:\n    ibmcloud:\n      providerType: UPI\n    type: IBMCloud\n  pullSecret:\n    name: cismlo21050nmreb5nhg-pull-secret\n  release:\n    image: us.icr.io/armada-master/ocp-release:4.10.63-x86_64\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud\n        port: 31446\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 32167\n      type: NodePort\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 31938\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 30231\n      type: NodePort\n  sshKey: {}\n</code></pre> <p>HostedControlPlane</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: HostedControlPlane\nmetadata:\n  annotations:\n    hypershift.openshift.io/cluster: master/cismlo21050nmreb5nhg\n    hypershift.openshift.io/disable-pki-reconciliation: \"true\"\n    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager\n    hypershift.openshift.io/konnectivity-agent-image: us.icr.io/armada-master/rh-apiserver-network-proxy@sha256:3b5a71252f682ed75ba0d1854d058f44039661937abae4aa33a924d7f68a77c1\n    hypershift.openshift.io/konnectivity-server-image: us.icr.io/armada-master/rh-apiserver-network-proxy@sha256:3b5a71252f682ed75ba0d1854d058f44039661937abae4aa33a924d7f68a77c1\n    idpoverrides.hypershift.openshift.io/IAM: |\n      {\"urls\": {\"authorize\": \"https://iam.test.cloud.ibm.com/identity/authorize\", \"userInfo\": \"https://iam.test.cloud.ibm.com/identity/userinfo\", \"token\": \"https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token\"}, \"claims\": {\"id\": [\"iam_id\"], \"email\": [\"email\"], \"name\": [\"name\"], \"preferredUsername\": [\"preferred_username\"]}, \"challenge\": true}\n    oauth.hypershift.openshift.io/login-url-override: https://sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:31446\n  finalizers:\n  - hypershift.openshift.io/finalizer\n  name: cismlo21050nmreb5nhg\n  namespace: master-cismlo21050nmreb5nhg\nspec:\n  autoscaling: {}\n  clusterID: 512f0876-573e-40b3-8a37-cb6f22b37e16\n  configuration:\n    apiServer:\n      audit:\n        profile: Default\n      clientCA:\n        name: \"\"\n      encryption: {}\n      servingCerts:\n        namedCertificates:\n        - names:\n          - sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n          servingCertificate:\n            name: ibm-named-certs\n      tlsSecurityProfile:\n        custom:\n          ciphers:\n          - ECDHE-ECDSA-AES128-GCM-SHA256\n          - ECDHE-RSA-AES128-GCM-SHA256\n          - ECDHE-ECDSA-AES256-GCM-SHA384\n          - ECDHE-RSA-AES256-GCM-SHA384\n          - ECDHE-ECDSA-CHACHA20-POLY1305\n          - ECDHE-RSA-CHACHA20-POLY1305\n          minTLSVersion: VersionTLS12\n        type: Custom\n    featureGate:\n      customNoUpgrade:\n        disabled:\n        - ServiceLBNodePortControl\n        - CSIMigrationAWS\n        - CSIMigrationOpenStack\n        - CSIMigrationGCE\n        - CSIMigrationAzureDisk\n        - CSIMigrationAzureFile\n        - CSIMigrationvSphere\n        enabled:\n        - ExpandInUsePersistentVolumes\n        - RotateKubeletServerCertificate\n        - DownwardAPIHugePages\n      featureSet: CustomNoUpgrade\n    ingress:\n      domain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n      loadBalancer:\n        platform:\n          type: \"\"\n    oauth:\n      identityProviders:\n      - mappingMethod: lookup\n        name: IAM\n        openID:\n          ca:\n            name: \"\"\n          claims:\n            email:\n            - email\n            name:\n            - name\n            preferredUsername:\n            - preferred_username\n          clientID: CLIENTID\n          clientSecret:\n            name: hypershift-ibm-iam-clientsecret\n          issuer: https://iam.test.cloud.ibm.com/identity\n        type: OpenID\n      templates:\n        error:\n          name: \"\"\n        login:\n          name: \"\"\n        providerSelection:\n          name: \"\"\n      tokenConfig: {}\n  controllerAvailabilityPolicy: HighlyAvailable\n  dns:\n    baseDomain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n  etcd:\n    managementType: Unmanaged\n    unmanaged:\n      endpoint: https://etcd-cismlo21050nmreb5nhg-client:2379\n      tls:\n        clientSecret:\n          name: cismlo21050nmreb5nhg-etcd-client-tls\n  fips: false\n  imageContentSources:\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-release\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n  infraID: cismlo21050nmreb5nhg\n  infrastructureAvailabilityPolicy: HighlyAvailable\n  issuerURL: https://kubernetes.default.svc\n  networking:\n    apiServer:\n      advertiseAddress: 172.20.0.1\n      port: 2040\n    clusterNetwork:\n    - cidr: 172.30.0.0/16\n    machineNetwork:\n    - cidr: 172.30.0.0/16\n    networkType: Calico\n    serviceNetwork:\n    - cidr: 172.21.0.0/16\n  olmCatalogPlacement: guest\n  platform:\n    ibmcloud:\n      providerType: UPI\n    type: IBMCloud\n  pullSecret:\n    name: pull-secret\n  releaseImage: us.icr.io/armada-master/ocp-release:4.10.63-x86_64\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud\n        port: 31446\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 32167\n      type: NodePort\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 31938\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 30231\n      type: NodePort\n  sshKey: {}\n</code></pre>"},{"location":"reference/manifests/ibmcloud/4.11/","title":"4.11","text":"<p>HostedCluster</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: HostedCluster\nmetadata:\n  annotations:\n    hypershift.openshift.io/capi-manager-image: us.icr.io/armada-master/hypershift-cluster-api@sha256:dd610ea557e88156df3c04b293698defa3a8378f922f9abb2b098bc7495564f2\n    hypershift.openshift.io/disable-pki-reconciliation: \"true\"\n    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager\n    hypershift.openshift.io/force-upgrade-to: us.icr.io/armada-master/ocp-release:4.11.45-x86_64\n    idpoverrides.hypershift.openshift.io/IAM: |\n      {\"urls\": {\"authorize\": \"https://iam.test.cloud.ibm.com/identity/authorize\", \"userInfo\": \"https://iam.test.cloud.ibm.com/identity/userinfo\", \"token\": \"https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token\"}, \"claims\": {\"id\": [\"iam_id\"], \"email\": [\"email\"], \"name\": [\"name\"], \"preferredUsername\": [\"preferred_username\"]}, \"challenge\": true}\n    oauth.hypershift.openshift.io/login-url-override: https://sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:31446\n  finalizers:\n  - hypershift.openshift.io/finalizer\n  labels:\n    clusterid: cismlo21050nmreb5nhg\n  name: cismlo21050nmreb5nhg\n  namespace: master\nspec:\n  autoscaling: {}\n  clusterID: 512f0876-573e-40b3-8a37-cb6f22b37e16\n  configuration:\n    apiServer:\n      audit:\n        profile: Default\n      clientCA:\n        name: \"\"\n      encryption: {}\n      servingCerts:\n        namedCertificates:\n        - names:\n          - sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n          servingCertificate:\n            name: ibm-named-certs\n      tlsSecurityProfile:\n        custom:\n          ciphers:\n          - ECDHE-ECDSA-AES128-GCM-SHA256\n          - ECDHE-RSA-AES128-GCM-SHA256\n          - ECDHE-ECDSA-AES256-GCM-SHA384\n          - ECDHE-RSA-AES256-GCM-SHA384\n          - ECDHE-ECDSA-CHACHA20-POLY1305\n          - ECDHE-RSA-CHACHA20-POLY1305\n          minTLSVersion: VersionTLS12\n        type: Custom\n    featureGate:\n      customNoUpgrade:\n        disabled:\n        - CSIMigrationAWS\n        - CSIMigrationGCE\n        - CSIMigrationAzureFile\n        - CSIMigrationvSphere\n        enabled:\n        - ExpandInUsePersistentVolumes\n        - RotateKubeletServerCertificate\n        - DownwardAPIHugePages\n      featureSet: CustomNoUpgrade\n    ingress:\n      domain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n      loadBalancer:\n        platform:\n          type: \"\"\n    oauth:\n      identityProviders:\n      - mappingMethod: lookup\n        name: IAM\n        openID:\n          claims:\n            email:\n            - email\n            name:\n            - name\n            preferredUsername:\n            - preferred_username\n          clientID: CLIENTID\n          clientSecret:\n            name: hypershift-ibm-iam-clientsecret\n          issuer: https://iam.test.cloud.ibm.com/identity\n        type: OpenID\n      templates:\n        error:\n          name: \"\"\n        login:\n          name: \"\"\n        providerSelection:\n          name: \"\"\n      tokenConfig: {}\n  controllerAvailabilityPolicy: HighlyAvailable\n  dns:\n    baseDomain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n  etcd:\n    managementType: Unmanaged\n    unmanaged:\n      endpoint: https://etcd-cismlo21050nmreb5nhg-client:2379\n      tls:\n        clientSecret:\n          name: cismlo21050nmreb5nhg-etcd-client-tls\n  fips: false\n  imageContentSources:\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-release\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n  infraID: cismlo21050nmreb5nhg\n  infrastructureAvailabilityPolicy: HighlyAvailable\n  issuerURL: https://kubernetes.default.svc\n  networking:\n    apiServer:\n      advertiseAddress: 172.20.0.1\n      port: 2040\n    clusterNetwork:\n    - cidr: 172.30.0.0/16\n    machineNetwork:\n    - cidr: 172.30.0.0/16\n    networkType: Calico\n    serviceNetwork:\n    - cidr: 172.21.0.0/16\n  olmCatalogPlacement: guest\n  platform:\n    ibmcloud:\n      providerType: UPI\n    type: IBMCloud\n  pullSecret:\n    name: cismlo21050nmreb5nhg-pull-secret\n  release:\n    image: us.icr.io/armada-master/ocp-release:4.11.45-x86_64\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud\n        port: 31446\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 32167\n      type: NodePort\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 31938\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 30231\n      type: NodePort\n  sshKey: {}\n</code></pre> <p>HostedControlPlane</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: HostedControlPlane\nmetadata:\n  annotations:\n    hypershift.openshift.io/cluster: master/cismlo21050nmreb5nhg\n    hypershift.openshift.io/disable-pki-reconciliation: \"true\"\n    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager\n    idpoverrides.hypershift.openshift.io/IAM: |\n      {\"urls\": {\"authorize\": \"https://iam.test.cloud.ibm.com/identity/authorize\", \"userInfo\": \"https://iam.test.cloud.ibm.com/identity/userinfo\", \"token\": \"https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token\"}, \"claims\": {\"id\": [\"iam_id\"], \"email\": [\"email\"], \"name\": [\"name\"], \"preferredUsername\": [\"preferred_username\"]}, \"challenge\": true}\n    oauth.hypershift.openshift.io/login-url-override: https://sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:31446\n  finalizers:\n  - hypershift.openshift.io/finalizer\n  name: cismlo21050nmreb5nhg\n  namespace: master-cismlo21050nmreb5nhg\nspec:\n  autoscaling: {}\n  clusterID: 512f0876-573e-40b3-8a37-cb6f22b37e16\n  configuration:\n    apiServer:\n      audit:\n        profile: Default\n      clientCA:\n        name: \"\"\n      encryption: {}\n      servingCerts:\n        namedCertificates:\n        - names:\n          - sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n          servingCertificate:\n            name: ibm-named-certs\n      tlsSecurityProfile:\n        custom:\n          ciphers:\n          - ECDHE-ECDSA-AES128-GCM-SHA256\n          - ECDHE-RSA-AES128-GCM-SHA256\n          - ECDHE-ECDSA-AES256-GCM-SHA384\n          - ECDHE-RSA-AES256-GCM-SHA384\n          - ECDHE-ECDSA-CHACHA20-POLY1305\n          - ECDHE-RSA-CHACHA20-POLY1305\n          minTLSVersion: VersionTLS12\n        type: Custom\n    featureGate:\n      customNoUpgrade:\n        disabled:\n        - CSIMigrationAWS\n        - CSIMigrationGCE\n        - CSIMigrationAzureFile\n        - CSIMigrationvSphere\n        enabled:\n        - ExpandInUsePersistentVolumes\n        - RotateKubeletServerCertificate\n        - DownwardAPIHugePages\n      featureSet: CustomNoUpgrade\n    ingress:\n      domain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n      loadBalancer:\n        platform:\n          type: \"\"\n    oauth:\n      identityProviders:\n      - mappingMethod: lookup\n        name: IAM\n        openID:\n          ca:\n            name: \"\"\n          claims:\n            email:\n            - email\n            name:\n            - name\n            preferredUsername:\n            - preferred_username\n          clientID: CLIENTID\n          clientSecret:\n            name: hypershift-ibm-iam-clientsecret\n          issuer: https://iam.test.cloud.ibm.com/identity\n        type: OpenID\n      templates:\n        error:\n          name: \"\"\n        login:\n          name: \"\"\n        providerSelection:\n          name: \"\"\n      tokenConfig: {}\n  controllerAvailabilityPolicy: HighlyAvailable\n  dns:\n    baseDomain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n  etcd:\n    managementType: Unmanaged\n    unmanaged:\n      endpoint: https://etcd-cismlo21050nmreb5nhg-client:2379\n      tls:\n        clientSecret:\n          name: cismlo21050nmreb5nhg-etcd-client-tls\n  fips: false\n  imageContentSources:\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-release\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n  infraID: cismlo21050nmreb5nhg\n  infrastructureAvailabilityPolicy: HighlyAvailable\n  issuerURL: https://kubernetes.default.svc\n  networking:\n    apiServer:\n      advertiseAddress: 172.20.0.1\n      port: 2040\n    clusterNetwork:\n    - cidr: 172.30.0.0/16\n    machineNetwork:\n    - cidr: 172.30.0.0/16\n    networkType: Calico\n    serviceNetwork:\n    - cidr: 172.21.0.0/16\n  olmCatalogPlacement: guest\n  platform:\n    ibmcloud:\n      providerType: UPI\n    type: IBMCloud\n  pullSecret:\n    name: pull-secret\n  releaseImage: us.icr.io/armada-master/ocp-release:4.11.45-x86_64\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud\n        port: 31446\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 32167\n      type: NodePort\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 31938\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 30231\n      type: NodePort\n  sshKey: {}\n</code></pre>"},{"location":"reference/manifests/ibmcloud/4.12/","title":"4.12","text":"<p>HostedCluster</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: HostedCluster\nmetadata:\n  annotations:\n    hypershift.openshift.io/disable-pki-reconciliation: \"true\"\n    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager\n    hypershift.openshift.io/force-upgrade-to: us.icr.io/armada-master/ocp-release:4.12.25-x86_64\n    idpoverrides.hypershift.openshift.io/IAM: |\n      {\"urls\": {\"authorize\": \"https://iam.test.cloud.ibm.com/identity/authorize\", \"userInfo\": \"https://iam.test.cloud.ibm.com/identity/userinfo\", \"token\": \"https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token\"}, \"claims\": {\"id\": [\"iam_id\"], \"email\": [\"email\"], \"name\": [\"name\"], \"preferredUsername\": [\"preferred_username\"]}, \"challenge\": true}\n    oauth.hypershift.openshift.io/login-url-override: https://sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:31446\n  finalizers:\n  - hypershift.openshift.io/finalizer\n  labels:\n    clusterid: cismlo21050nmreb5nhg\n  name: cismlo21050nmreb5nhg\n  namespace: master\nspec:\n  autoscaling: {}\n  clusterID: 512f0876-573e-40b3-8a37-cb6f22b37e16\n  configuration:\n    apiServer:\n      audit:\n        profile: None\n      clientCA:\n        name: \"\"\n      encryption: {}\n      servingCerts:\n        namedCertificates:\n        - names:\n          - sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n          servingCertificate:\n            name: ibm-named-certs\n      tlsSecurityProfile:\n        custom:\n          ciphers:\n          - ECDHE-ECDSA-AES128-GCM-SHA256\n          - ECDHE-RSA-AES128-GCM-SHA256\n          - ECDHE-ECDSA-AES256-GCM-SHA384\n          - ECDHE-RSA-AES256-GCM-SHA384\n          - ECDHE-ECDSA-CHACHA20-POLY1305\n          - ECDHE-RSA-CHACHA20-POLY1305\n          minTLSVersion: VersionTLS12\n        type: Custom\n    featureGate:\n      customNoUpgrade:\n        disabled:\n        - CSIMigrationAzureFile\n        - CSIMigrationvSphere\n        enabled:\n        - ExpandInUsePersistentVolumes\n        - RotateKubeletServerCertificate\n      featureSet: CustomNoUpgrade\n    ingress:\n      domain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n      loadBalancer:\n        platform:\n          type: \"\"\n    oauth:\n      identityProviders:\n      - mappingMethod: lookup\n        name: IAM\n        openID:\n          claims:\n            email:\n            - email\n            name:\n            - name\n            preferredUsername:\n            - preferred_username\n          clientID: CLIENTID\n          clientSecret:\n            name: hypershift-ibm-iam-clientsecret\n          issuer: https://iam.test.cloud.ibm.com/identity\n        type: OpenID\n      templates:\n        error:\n          name: \"\"\n        login:\n          name: \"\"\n        providerSelection:\n          name: \"\"\n      tokenConfig: {}\n  controllerAvailabilityPolicy: HighlyAvailable\n  dns:\n    baseDomain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n  etcd:\n    managementType: Unmanaged\n    unmanaged:\n      endpoint: https://etcd-cismlo21050nmreb5nhg-client:2379\n      tls:\n        clientSecret:\n          name: cismlo21050nmreb5nhg-etcd-client-tls\n  fips: false\n  imageContentSources:\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-release\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n  infraID: cismlo21050nmreb5nhg\n  infrastructureAvailabilityPolicy: HighlyAvailable\n  issuerURL: https://kubernetes.default.svc\n  networking:\n    apiServer:\n      advertiseAddress: 172.20.0.1\n      port: 2040\n    clusterNetwork:\n    - cidr: 172.30.0.0/16\n    machineNetwork:\n    - cidr: 172.30.0.0/16\n    networkType: Calico\n    serviceNetwork:\n    - cidr: 172.21.0.0/16\n  olmCatalogPlacement: guest\n  platform:\n    ibmcloud:\n      providerType: UPI\n    type: IBMCloud\n  pullSecret:\n    name: cismlo21050nmreb5nhg-pull-secret\n  release:\n    image: us.icr.io/armada-master/ocp-release:4.12.25-x86_64\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud\n        port: 31446\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 32167\n      type: NodePort\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 31938\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 30231\n      type: NodePort\n  sshKey: {}\n</code></pre> <p>HostedControlPlane</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: HostedControlPlane\nmetadata:\n  annotations:\n    hypershift.openshift.io/cluster: master/cismlo21050nmreb5nhg\n    hypershift.openshift.io/disable-pki-reconciliation: \"true\"\n    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager\n    idpoverrides.hypershift.openshift.io/IAM: |\n      {\"urls\": {\"authorize\": \"https://iam.test.cloud.ibm.com/identity/authorize\", \"userInfo\": \"https://iam.test.cloud.ibm.com/identity/userinfo\", \"token\": \"https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token\"}, \"claims\": {\"id\": [\"iam_id\"], \"email\": [\"email\"], \"name\": [\"name\"], \"preferredUsername\": [\"preferred_username\"]}, \"challenge\": true}\n    oauth.hypershift.openshift.io/login-url-override: https://sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:31446\n  finalizers:\n  - hypershift.openshift.io/finalizer\n  name: cismlo21050nmreb5nhg\n  namespace: master-cismlo21050nmreb5nhg\nspec:\n  autoscaling: {}\n  clusterID: 512f0876-573e-40b3-8a37-cb6f22b37e16\n  configuration:\n    apiServer:\n      audit:\n        profile: None\n      clientCA:\n        name: \"\"\n      encryption: {}\n      servingCerts:\n        namedCertificates:\n        - names:\n          - sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n          servingCertificate:\n            name: ibm-named-certs\n      tlsSecurityProfile:\n        custom:\n          ciphers:\n          - ECDHE-ECDSA-AES128-GCM-SHA256\n          - ECDHE-RSA-AES128-GCM-SHA256\n          - ECDHE-ECDSA-AES256-GCM-SHA384\n          - ECDHE-RSA-AES256-GCM-SHA384\n          - ECDHE-ECDSA-CHACHA20-POLY1305\n          - ECDHE-RSA-CHACHA20-POLY1305\n          minTLSVersion: VersionTLS12\n        type: Custom\n    featureGate:\n      customNoUpgrade:\n        disabled:\n        - CSIMigrationAzureFile\n        - CSIMigrationvSphere\n        enabled:\n        - ExpandInUsePersistentVolumes\n        - RotateKubeletServerCertificate\n      featureSet: CustomNoUpgrade\n    ingress:\n      domain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n      loadBalancer:\n        platform:\n          type: \"\"\n    oauth:\n      identityProviders:\n      - mappingMethod: lookup\n        name: IAM\n        openID:\n          ca:\n            name: \"\"\n          claims:\n            email:\n            - email\n            name:\n            - name\n            preferredUsername:\n            - preferred_username\n          clientID: CLIENTID\n          clientSecret:\n            name: hypershift-ibm-iam-clientsecret\n          issuer: https://iam.test.cloud.ibm.com/identity\n        type: OpenID\n      templates:\n        error:\n          name: \"\"\n        login:\n          name: \"\"\n        providerSelection:\n          name: \"\"\n      tokenConfig: {}\n  controllerAvailabilityPolicy: HighlyAvailable\n  dns:\n    baseDomain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n  etcd:\n    managementType: Unmanaged\n    unmanaged:\n      endpoint: https://etcd-cismlo21050nmreb5nhg-client:2379\n      tls:\n        clientSecret:\n          name: cismlo21050nmreb5nhg-etcd-client-tls\n  fips: false\n  imageContentSources:\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-release\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n  infraID: cismlo21050nmreb5nhg\n  infrastructureAvailabilityPolicy: HighlyAvailable\n  issuerURL: https://kubernetes.default.svc\n  networking:\n    apiServer:\n      advertiseAddress: 172.20.0.1\n      port: 2040\n    clusterNetwork:\n    - cidr: 172.30.0.0/16\n    machineNetwork:\n    - cidr: 172.30.0.0/16\n    networkType: Calico\n    serviceNetwork:\n    - cidr: 172.21.0.0/16\n  olmCatalogPlacement: guest\n  platform:\n    ibmcloud:\n      providerType: UPI\n    type: IBMCloud\n  pullSecret:\n    name: pull-secret\n  releaseImage: us.icr.io/armada-master/ocp-release:4.12.25-x86_64\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud\n        port: 31446\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 32167\n      type: NodePort\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 31938\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 30231\n      type: NodePort\n  sshKey: {}\n</code></pre>"},{"location":"reference/manifests/ibmcloud/4.13/","title":"4.13","text":"<p>HostedCluster</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: HostedCluster\nmetadata:\n  annotations:\n    hypershift.openshift.io/disable-pki-reconciliation: \"true\"\n    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager\n    hypershift.openshift.io/force-upgrade-to: us.icr.io/armada-master/ocp-release:4.13.0-x86_64\n    idpoverrides.hypershift.openshift.io/IAM: |\n      {\"urls\": {\"authorize\": \"https://iam.test.cloud.ibm.com/identity/authorize\", \"userInfo\": \"https://iam.test.cloud.ibm.com/identity/userinfo\", \"token\": \"https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token\"}, \"claims\": {\"id\": [\"iam_id\"], \"email\": [\"email\"], \"name\": [\"name\"], \"preferredUsername\": [\"preferred_username\"]}, \"challenge\": true}\n    oauth.hypershift.openshift.io/login-url-override: https://s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:32750\n  finalizers:\n  - hypershift.openshift.io/finalizer\n  labels:\n    clusterid: civ8kn910u3bkslbt6lg\n  name: civ8kn910u3bkslbt6lg\n  namespace: master\nspec:\n  autoscaling: {}\n  clusterID: e54fe2bd-cc45-46e1-8272-f1053ffd4b71\n  configuration:\n    apiServer:\n      audit:\n        profile: None\n      clientCA:\n        name: \"\"\n      encryption: {}\n      servingCerts:\n        namedCertificates:\n        - names:\n          - s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n          servingCertificate:\n            name: ibm-named-certs\n      tlsSecurityProfile:\n        custom:\n          ciphers:\n          - ECDHE-ECDSA-AES128-GCM-SHA256\n          - ECDHE-RSA-AES128-GCM-SHA256\n          - ECDHE-ECDSA-AES256-GCM-SHA384\n          - ECDHE-RSA-AES256-GCM-SHA384\n          - ECDHE-ECDSA-CHACHA20-POLY1305\n          - ECDHE-RSA-CHACHA20-POLY1305\n          minTLSVersion: VersionTLS12\n        type: Custom\n    featureGate:\n      customNoUpgrade:\n        disabled:\n        - RetroactiveDefaultStorageClass\n        enabled:\n        - RotateKubeletServerCertificate\n      featureSet: CustomNoUpgrade\n    ingress:\n      domain: sat-e2e-16902088-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n      loadBalancer:\n        platform:\n          type: \"\"\n    oauth:\n      identityProviders:\n      - mappingMethod: lookup\n        name: IAM\n        openID:\n          claims:\n            email:\n            - email\n            name:\n            - name\n            preferredUsername:\n            - preferred_username\n          clientID: CLIENTID\n          clientSecret:\n            name: hypershift-ibm-iam-clientsecret\n          issuer: https://iam.test.cloud.ibm.com/identity\n        type: OpenID\n      templates:\n        error:\n          name: \"\"\n        login:\n          name: \"\"\n        providerSelection:\n          name: \"\"\n      tokenConfig: {}\n  controllerAvailabilityPolicy: HighlyAvailable\n  dns:\n    baseDomain: sat-e2e-16902088-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n  etcd:\n    managementType: Unmanaged\n    unmanaged:\n      endpoint: https://etcd-civ8kn910u3bkslbt6lg-client:2379\n      tls:\n        clientSecret:\n          name: civ8kn910u3bkslbt6lg-etcd-client-tls\n  fips: false\n  imageContentSources:\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-release\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n  infraID: civ8kn910u3bkslbt6lg\n  infrastructureAvailabilityPolicy: HighlyAvailable\n  issuerURL: https://kubernetes.default.svc\n  networking:\n    apiServer:\n      advertiseAddress: 172.20.0.1\n      port: 2040\n    clusterNetwork:\n    - cidr: 172.30.0.0/16\n    machineNetwork:\n    - cidr: 172.30.0.0/16\n    networkType: Calico\n    serviceNetwork:\n    - cidr: 172.21.0.0/16\n  olmCatalogPlacement: guest\n  platform:\n    ibmcloud:\n      providerType: UPI\n    type: IBMCloud\n  pullSecret:\n    name: civ8kn910u3bkslbt6lg-pull-secret\n  release:\n    image: us.icr.io/armada-master/ocp-release:4.13.0-x86_64\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud\n        port: 32750\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 32407\n      type: NodePort\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 32287\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 30284\n      type: NodePort\n  sshKey: {}\n</code></pre> <p>HostedControlPlane</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: HostedControlPlane\nmetadata:\n  annotations:\n    hypershift.openshift.io/cluster: master/civ8kn910u3bkslbt6lg\n    hypershift.openshift.io/disable-pki-reconciliation: \"true\"\n    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager\n    idpoverrides.hypershift.openshift.io/IAM: |\n      {\"urls\": {\"authorize\": \"https://iam.test.cloud.ibm.com/identity/authorize\", \"userInfo\": \"https://iam.test.cloud.ibm.com/identity/userinfo\", \"token\": \"https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token\"}, \"claims\": {\"id\": [\"iam_id\"], \"email\": [\"email\"], \"name\": [\"name\"], \"preferredUsername\": [\"preferred_username\"]}, \"challenge\": true}\n    oauth.hypershift.openshift.io/login-url-override: https://s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:32750\n  finalizers:\n  - hypershift.openshift.io/finalizer\n  name: civ8kn910u3bkslbt6lg\n  namespace: master-civ8kn910u3bkslbt6lg\nspec:\n  autoscaling: {}\n  clusterID: e54fe2bd-cc45-46e1-8272-f1053ffd4b71\n  configuration:\n    apiServer:\n      audit:\n        profile: None\n      clientCA:\n        name: \"\"\n      encryption: {}\n      servingCerts:\n        namedCertificates:\n        - names:\n          - s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n          servingCertificate:\n            name: ibm-named-certs\n      tlsSecurityProfile:\n        custom:\n          ciphers:\n          - ECDHE-ECDSA-AES128-GCM-SHA256\n          - ECDHE-RSA-AES128-GCM-SHA256\n          - ECDHE-ECDSA-AES256-GCM-SHA384\n          - ECDHE-RSA-AES256-GCM-SHA384\n          - ECDHE-ECDSA-CHACHA20-POLY1305\n          - ECDHE-RSA-CHACHA20-POLY1305\n          minTLSVersion: VersionTLS12\n        type: Custom\n    featureGate:\n      customNoUpgrade:\n        disabled:\n        - RetroactiveDefaultStorageClass\n        enabled:\n        - RotateKubeletServerCertificate\n      featureSet: CustomNoUpgrade\n    ingress:\n      domain: sat-e2e-16902088-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n      loadBalancer:\n        platform:\n          type: \"\"\n    oauth:\n      identityProviders:\n      - mappingMethod: lookup\n        name: IAM\n        openID:\n          ca:\n            name: \"\"\n          claims:\n            email:\n            - email\n            name:\n            - name\n            preferredUsername:\n            - preferred_username\n          clientID: CLIENTID\n          clientSecret:\n            name: hypershift-ibm-iam-clientsecret\n          issuer: https://iam.test.cloud.ibm.com/identity\n        type: OpenID\n      templates:\n        error:\n          name: \"\"\n        login:\n          name: \"\"\n        providerSelection:\n          name: \"\"\n      tokenConfig: {}\n  controllerAvailabilityPolicy: HighlyAvailable\n  dns:\n    baseDomain: sat-e2e-16902088-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n  etcd:\n    managementType: Unmanaged\n    unmanaged:\n      endpoint: https://etcd-civ8kn910u3bkslbt6lg-client:2379\n      tls:\n        clientSecret:\n          name: civ8kn910u3bkslbt6lg-etcd-client-tls\n  fips: false\n  imageContentSources:\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-release\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n  infraID: civ8kn910u3bkslbt6lg\n  infrastructureAvailabilityPolicy: HighlyAvailable\n  issuerURL: https://kubernetes.default.svc\n  networking:\n    apiServer:\n      advertiseAddress: 172.20.0.1\n      port: 2040\n    clusterNetwork:\n    - cidr: 172.30.0.0/16\n    machineNetwork:\n    - cidr: 172.30.0.0/16\n    networkType: Calico\n    serviceNetwork:\n    - cidr: 172.21.0.0/16\n  olmCatalogPlacement: guest\n  platform:\n    ibmcloud:\n      providerType: UPI\n    type: IBMCloud\n  pullSecret:\n    name: pull-secret\n  releaseImage: us.icr.io/armada-master/ocp-release:4.13.0-x86_64\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud\n        port: 32750\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 32407\n      type: NodePort\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 32287\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: s9bcaafaec11e1f50b3d0-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 30284\n      type: NodePort\n  sshKey: {}\n</code></pre>"},{"location":"reference/manifests/ibmcloud/4.9/","title":"4.9","text":"<p>HostedCluster</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: HostedCluster\nmetadata:\n  annotations:\n    hypershift.openshift.io/capi-manager-image: registry.ng.bluemix.net/armada-master/hypershift-cluster-api@sha256:7bde073326d99ce2008e2260671b8962cca44319438706ccce9e8e61fdd26a93\n    hypershift.openshift.io/control-plane-operator-image: registry.ng.bluemix.net/armada-master/armada-hypershift-operator@sha256:16982362b89be355f8db1e0bf59b9917f3b529e110364ef34e90f84b9e60ea10\n    hypershift.openshift.io/disable-pki-reconciliation: \"true\"\n    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager\n    hypershift.openshift.io/force-upgrade-to: registry.ng.bluemix.net/armada-master/ocp-release:4.9.53-x86_64\n    hypershift.openshift.io/konnectivity-agent-image: registry.ng.bluemix.net/armada-master/rh-apiserver-network-proxy@sha256:745511c3ed56aee521d018825b053c2310e3dd6d1af332e575bbd18789782c30\n    hypershift.openshift.io/konnectivity-server-image: registry.ng.bluemix.net/armada-master/rh-apiserver-network-proxy@sha256:745511c3ed56aee521d018825b053c2310e3dd6d1af332e575bbd18789782c30\n    hypershift.openshift.io/machine-approver-image: registry.ng.bluemix.net/armada-master/hypershift-machine-approver@sha256:19323007d6e2d2de9c3140bfc041d9d11cc3c17bcc3215eaae253dd94a06e5b7\n    idpoverrides.hypershift.openshift.io/IAM: |\n      {\"urls\": {\"authorize\": \"https://iam.test.cloud.ibm.com/identity/authorize\", \"userInfo\": \"https://iam.test.cloud.ibm.com/identity/userinfo\", \"token\": \"https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token\"}, \"claims\": {\"id\": [\"iam_id\"], \"email\": [\"email\"], \"name\": [\"name\"], \"preferredUsername\": [\"preferred_username\"]}, \"challenge\": true}\n    oauth.hypershift.openshift.io/login-url-override: https://sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:31446\n  finalizers:\n  - hypershift.openshift.io/finalizer\n  labels:\n    clusterid: cismlo21050nmreb5nhg\n  name: cismlo21050nmreb5nhg\n  namespace: master\nspec:\n  autoscaling: {}\n  clusterID: 512f0876-573e-40b3-8a37-cb6f22b37e16\n  configuration:\n    apiServer:\n      audit:\n        profile: Default\n      clientCA:\n        name: \"\"\n      encryption: {}\n      servingCerts:\n        namedCertificates:\n        - names:\n          - sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n          servingCertificate:\n            name: ibm-named-certs\n      tlsSecurityProfile:\n        custom:\n          ciphers:\n          - ECDHE-ECDSA-AES128-GCM-SHA256\n          - ECDHE-RSA-AES128-GCM-SHA256\n          - ECDHE-ECDSA-AES256-GCM-SHA384\n          - ECDHE-RSA-AES256-GCM-SHA384\n          - ECDHE-ECDSA-CHACHA20-POLY1305\n          - ECDHE-RSA-CHACHA20-POLY1305\n          minTLSVersion: VersionTLS12\n        type: Custom\n    featureGate:\n      customNoUpgrade:\n        disabled:\n        - ServiceLBNodePortControl\n        enabled:\n        - ExpandInUsePersistentVolumes\n        - RotateKubeletServerCertificate\n        - DownwardAPIHugePages\n      featureSet: CustomNoUpgrade\n    ingress:\n      domain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n      loadBalancer:\n        platform:\n          type: \"\"\n    oauth:\n      identityProviders:\n      - mappingMethod: lookup\n        name: IAM\n        openID:\n          ca:\n            name: \"\"\n          claims:\n            email:\n            - email\n            name:\n            - name\n            preferredUsername:\n            - preferred_username\n          clientID: CLIENTID\n          clientSecret:\n            name: hypershift-ibm-iam-clientsecret\n          issuer: https://iam.test.cloud.ibm.com/identity\n        type: OpenID\n      templates:\n        error:\n          name: \"\"\n        login:\n          name: \"\"\n        providerSelection:\n          name: \"\"\n      tokenConfig: {}\n  controllerAvailabilityPolicy: HighlyAvailable\n  dns:\n    baseDomain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n  etcd:\n    managementType: Unmanaged\n    unmanaged:\n      endpoint: https://etcd-cismlo21050nmreb5nhg-client:2379\n      tls:\n        clientSecret:\n          name: cismlo21050nmreb5nhg-etcd-client-tls\n  fips: false\n  imageContentSources:\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-release\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n  infraID: cismlo21050nmreb5nhg\n  infrastructureAvailabilityPolicy: HighlyAvailable\n  issuerURL: https://kubernetes.default.svc\n  networking:\n    apiServer:\n      advertiseAddress: 172.20.0.1\n      port: 2040\n    clusterNetwork:\n    - cidr: 172.30.0.0/16\n    machineNetwork:\n    - cidr: 172.30.0.0/16\n    networkType: Calico\n    serviceNetwork:\n    - cidr: 172.21.0.0/16\n  olmCatalogPlacement: guest\n  platform:\n    ibmcloud:\n      providerType: UPI\n    type: IBMCloud\n  pullSecret:\n    name: cismlo21050nmreb5nhg-pull-secret\n  release:\n    image: registry.ng.bluemix.net/armada-master/ocp-release:4.9.53-x86_64\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud\n        port: 31446\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 32167\n      type: NodePort\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 31938\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 30231\n      type: NodePort\n  sshKey: {}\n</code></pre> <p>HostedControlPlane</p> <pre><code>apiVersion: hypershift.openshift.io/v1beta1\nkind: HostedControlPlane\nmetadata:\n  annotations:\n    hypershift.openshift.io/cluster: master/cismlo21050nmreb5nhg\n    hypershift.openshift.io/disable-pki-reconciliation: \"true\"\n    hypershift.openshift.io/disable-profiling: kube-apiserver, kube-scheduler, kube-controller-manager\n    hypershift.openshift.io/konnectivity-agent-image: registry.ng.bluemix.net/armada-master/rh-apiserver-network-proxy@sha256:745511c3ed56aee521d018825b053c2310e3dd6d1af332e575bbd18789782c30\n    hypershift.openshift.io/konnectivity-server-image: registry.ng.bluemix.net/armada-master/rh-apiserver-network-proxy@sha256:745511c3ed56aee521d018825b053c2310e3dd6d1af332e575bbd18789782c30\n    idpoverrides.hypershift.openshift.io/IAM: |\n      {\"urls\": {\"authorize\": \"https://iam.test.cloud.ibm.com/identity/authorize\", \"userInfo\": \"https://iam.test.cloud.ibm.com/identity/userinfo\", \"token\": \"https://iam.test.cloud.ibm.com/identity/ACCOUNTID/token\"}, \"claims\": {\"id\": [\"iam_id\"], \"email\": [\"email\"], \"name\": [\"name\"], \"preferredUsername\": [\"preferred_username\"]}, \"challenge\": true}\n    oauth.hypershift.openshift.io/login-url-override: https://sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud:31446\n  finalizers:\n  - hypershift.openshift.io/finalizer\n  name: cismlo21050nmreb5nhg\n  namespace: master-cismlo21050nmreb5nhg\nspec:\n  autoscaling: {}\n  clusterID: 512f0876-573e-40b3-8a37-cb6f22b37e16\n  configuration:\n    apiServer:\n      audit:\n        profile: Default\n      clientCA:\n        name: \"\"\n      encryption: {}\n      servingCerts:\n        namedCertificates:\n        - names:\n          - sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n          servingCertificate:\n            name: ibm-named-certs\n      tlsSecurityProfile:\n        custom:\n          ciphers:\n          - ECDHE-ECDSA-AES128-GCM-SHA256\n          - ECDHE-RSA-AES128-GCM-SHA256\n          - ECDHE-ECDSA-AES256-GCM-SHA384\n          - ECDHE-RSA-AES256-GCM-SHA384\n          - ECDHE-ECDSA-CHACHA20-POLY1305\n          - ECDHE-RSA-CHACHA20-POLY1305\n          minTLSVersion: VersionTLS12\n        type: Custom\n    featureGate:\n      customNoUpgrade:\n        disabled:\n        - ServiceLBNodePortControl\n        enabled:\n        - ExpandInUsePersistentVolumes\n        - RotateKubeletServerCertificate\n        - DownwardAPIHugePages\n      featureSet: CustomNoUpgrade\n    ingress:\n      domain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n      loadBalancer:\n        platform:\n          type: \"\"\n    oauth:\n      identityProviders:\n      - mappingMethod: lookup\n        name: IAM\n        openID:\n          ca:\n            name: \"\"\n          claims:\n            email:\n            - email\n            name:\n            - name\n            preferredUsername:\n            - preferred_username\n          clientID: CLIENTID\n          clientSecret:\n            name: hypershift-ibm-iam-clientsecret\n          issuer: https://iam.test.cloud.ibm.com/identity\n        type: OpenID\n      templates:\n        error:\n          name: \"\"\n        login:\n          name: \"\"\n        providerSelection:\n          name: \"\"\n      tokenConfig: {}\n  controllerAvailabilityPolicy: HighlyAvailable\n  dns:\n    baseDomain: sat-e2e-16898731-9e37478581b5d9de33607f5926d1d18f-0000.us-south.prestg.stg.containers.appdomain.cloud\n  etcd:\n    managementType: Unmanaged\n    unmanaged:\n      endpoint: https://etcd-cismlo21050nmreb5nhg-client:2379\n      tls:\n        clientSecret:\n          name: cismlo21050nmreb5nhg-etcd-client-tls\n  fips: false\n  imageContentSources:\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-release\n  - mirrors:\n    - us.icr.io/armada-master/ocp-release\n    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n  infraID: cismlo21050nmreb5nhg\n  infrastructureAvailabilityPolicy: HighlyAvailable\n  issuerURL: https://kubernetes.default.svc\n  networking:\n    apiServer:\n      advertiseAddress: 172.20.0.1\n      port: 2040\n    clusterNetwork:\n    - cidr: 172.30.0.0/16\n    machineNetwork:\n    - cidr: 172.30.0.0/16\n    networkType: Calico\n    serviceNetwork:\n    - cidr: 172.21.0.0/16\n  olmCatalogPlacement: guest\n  platform:\n    ibmcloud:\n      providerType: UPI\n    type: IBMCloud\n  pullSecret:\n    name: pull-secret\n  releaseImage: registry.ng.bluemix.net/armada-master/ocp-release:4.9.53-x86_64\n  services:\n  - service: APIServer\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-c000.us-south.satellite.test.appdomain.cloud\n        port: 31446\n      type: NodePort\n  - service: OAuthServer\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 32167\n      type: NodePort\n  - service: Konnectivity\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 31938\n      type: NodePort\n  - service: Ignition\n    servicePublishingStrategy:\n      nodePort:\n        address: sab980c9cc8c17f3e13fa-d603ff82e51c94176a53d44566df9d79-ce00.us-south.satellite.test.appdomain.cloud\n        port: 30231\n      type: NodePort\n  sshKey: {}\n</code></pre>"},{"location":"reference/ocp-behaviour-deviations/","title":"OCP Standalone behaviour deviations","text":"<p>In this section, we will outline the behavioral differences between Hosted Control Planes/Hypershift and standalone OpenShift.</p>"},{"location":"reference/ocp-behaviour-deviations/upgrade-4.17-4.18/","title":"Upgrade Hypershift NodePool from 4.17 to 4.18","text":"<p>This section outlines the behavior and key differences between a Hosted Control Plane and a standalone OCP environment during an upgrade from version 4.17 to 4.18.</p>"},{"location":"reference/ocp-behaviour-deviations/upgrade-4.17-4.18/#runtime-migration-from-runc-to-crun","title":"Runtime Migration from <code>runc</code> to <code>crun</code>","text":"<p>In OpenShift 4.18, the MachineConfig Operator sets <code>crun</code> as the default runtime for worker nodes. This change also affects Hosted Control Planes, requiring us to assess and determine the best course of action, considering that OpenShift has some key differences in behavior.</p>"},{"location":"reference/ocp-behaviour-deviations/upgrade-4.17-4.18/#expected-behavior-for-ocp-standalone-upgrade-to-418","title":"Expected Behavior for OCP Standalone Upgrade to 4.18","text":"<p>The OpenShift team has established rules for this runtime migration, affecting both control planes and worker nodes:</p> <ul> <li>A new OCP cluster deployed with the 4.18 release will use <code>crun</code> as the default runtime.</li> <li>New nodes created in 4.18 will use <code>crun</code> by default.</li> <li>Existing nodes from 4.17 will not automatically migrate from <code>runc</code> to <code>crun</code> in 4.18 unless additional actions are taken by the customer.</li> </ul>"},{"location":"reference/ocp-behaviour-deviations/upgrade-4.17-4.18/#expected-behavior-for-hosted-control-planes-upgrade-to-418","title":"Expected Behavior for Hosted Control Planes Upgrade to 4.18","text":"<p>The Hypershift team has chosen a different approach from standalone OCP due to certain design decisions:</p> <ul> <li>A new Hosted Cluster from the 4.18 release will use <code>crun</code> as the default runtime.</li> <li>New NodePools in 4.18 will also use <code>crun</code> as the default runtime.</li> <li>Existing NodePools from 4.17 will migrate from <code>runc</code> to <code>crun</code> once they are upgraded to 4.18.</li> </ul>"},{"location":"reference/ocp-behaviour-deviations/upgrade-4.17-4.18/#detailed-overview","title":"Detailed Overview","text":""},{"location":"reference/ocp-behaviour-deviations/upgrade-4.17-4.18/#why","title":"Why","text":""},{"location":"reference/ocp-behaviour-deviations/upgrade-4.17-4.18/#context","title":"Context","text":"<p>In Hosted Control Planes, there are two different upgrade strategies: Replace and InPlace. The documentation can be found here, and these strategies operate differently at the code level.</p> <p>The Replace strategy creates new instances with the new version while removing old nodes in a rolling fashion. Internally, the NodePool controller will receive the new payload to roll out new nodes, which will eventually replace the current nodes once they join the NodePool. These new nodes will use <code>crun</code> as the default runtime, as the payload comes directly from the new version.</p> <p>The InPlace strategy performs updates to the operating system of existing instances. Internally, the Hosted Cluster Config Operator (HCCO) will run a pod to generate new ignition payloads, which will then be applied to reignite the current nodes. The worker nodes (usually one at a time, depending on the <code>maxUnavailable</code> and <code>maxSurge</code> settings) will be cordoned, drained, and rebooted. After reboot, these nodes will apply the new payload, load the new MCS templates, and use <code>crun</code> as the default container runtime.</p>"},{"location":"reference/ocp-behaviour-deviations/upgrade-4.17-4.18/#reasons","title":"Reasons","text":"<p>The rationale behind this design is as follows:</p> <ul> <li>Supporting retention of the <code>runc</code> runtime would introduce significant engineering complexity at an abstraction layer that would need to be maintained indefinitely.</li> <li>This would make it difficult to reproduce scenarios or bugs, as the state of long-running clusters would diverge significantly from newly created clusters, which presents a product management challenge.</li> <li>Extensive testing upstream and downstream within CRI-O and OpenShift has not indicated any significant risk associated with this change.</li> </ul>"},{"location":"reference/ocp-behaviour-deviations/upgrade-4.17-4.18/#safest-way-to-upgrade-your-hostedcluster","title":"Safest way to upgrade your HostedCluster","text":"<p>HyperShift allows customers to just leave their NodePools on 4.17, while creating a new NodePool on 4.18 to validate the workloads perform correctly with <code>crun</code> runtime. For that, the steps are clear:</p> <ul> <li>Upgrade your HostedCluster Control Plane to 4.18.Z. NodePools will still run 4.17.Z and runc. Workloads are not affected.</li> <li>Bring up a new small nodepool in 4.18, which will run <code>crun</code> as default runtime. Scale up the workload to spread the pods to the new nodepool.</li> <li>In the best case, all is fine and the nodepool with version 4.17.Z could be upgraded.</li> <li>In the worst case, you can report the issue as a bug to the team for the further investigation.</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/","title":"ARO HCP E2E Test Artifacts Navigation Guide","text":"<p>This document serves as a roadmap for understanding and navigating ARO HCP (Azure Red Hat OpenShift Hosted Control Planes) end-to-end test artifacts. Use this guide to quickly locate specific components, logs, and debugging information.</p>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#quick-navigation-index","title":"Quick Navigation Index","text":""},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#finding-hosted-control-plane-components","title":"\ud83d\udd0d Finding Hosted Control Plane Components","text":"<ul> <li>Control Plane Pod Deployments: <code>artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/e2e-clusters-*/apps/deployments/</code></li> <li>Control Plane Pod Logs: <code>artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/e2e-clusters-*/core/pods/logs/</code></li> <li>Control Plane Pod Manifests: <code>artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/e2e-clusters-*/core/pods/</code></li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#finding-hypershift-operator-components","title":"\ud83c\udf9b\ufe0f Finding HyperShift Operator Components","text":"<ul> <li>HyperShift Operator Deployment: <code>artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/hypershift/apps/deployments/operator.yaml</code></li> <li>External DNS Deployment: <code>artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/hypershift/apps/deployments/external-dns.yaml</code></li> <li>Operator Logs: <code>artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/hypershift/core/pods/logs/operator-*-operator.log</code></li> <li>External DNS Logs: <code>artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/hypershift/core/pods/logs/external-dns-*-external-dns.log</code></li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#key-control-plane-components-to-look-for","title":"\ud83c\udfaf Key Control Plane Components to Look For","text":""},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#core-kubernetes-control-plane-basic-cluster-functionality","title":"Core Kubernetes Control Plane (Basic cluster functionality)","text":"<p>These components form the foundation of any Kubernetes cluster. If these fail, the entire cluster is non-functional:</p> <ul> <li> <p>etcd: <code>etcd-0.yaml</code> and <code>etcd-0-*.log</code> files</p> <ul> <li>Why: Stores all cluster state and configuration. Failures here cause complete cluster outages</li> <li>Look for: Connection issues, disk space, quorum problems, backup/restore errors</li> </ul> </li> <li> <p>kube-apiserver: <code>kube-apiserver-*.yaml</code> and <code>kube-apiserver-*-*.log</code> files  </p> <ul> <li>Why: The central API endpoint for all cluster operations. Nothing works without it</li> <li>Look for: TLS certificate issues, etcd connectivity, authentication/authorization failures</li> </ul> </li> <li> <p>kube-controller-manager: <code>kube-controller-manager-*.yaml</code> and <code>kube-controller-manager-*-*.log</code> files</p> <ul> <li>Why: Runs core controllers (ReplicaSets, Deployments, etc.). Controls workload lifecycle</li> <li>Look for: Resource reconciliation failures, cloud provider integration issues</li> </ul> </li> <li> <p>kube-scheduler: <code>kube-scheduler-*.yaml</code> and <code>kube-scheduler-*-*.log</code> files</p> <ul> <li>Why: Places pods on nodes. Scheduling failures prevent workloads from running</li> <li>Look for: Resource constraints, node affinity issues, scheduling policy problems</li> </ul> </li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#openshift-specific-components-openshift-functionality","title":"OpenShift-Specific Components (OpenShift functionality)","text":"<p>These add OpenShift features on top of Kubernetes. Required for OpenShift-specific capabilities:</p> <ul> <li> <p>OpenShift API Server: <code>openshift-apiserver-*.yaml</code> and <code>openshift-apiserver-*-*.log</code> files</p> <ul> <li>Why: Handles OpenShift-specific APIs (Routes, BuildConfigs, etc.)</li> <li>Look for: Custom resource failures, OpenShift feature unavailability</li> </ul> </li> <li> <p>OAuth Server: <code>oauth-openshift-*.yaml</code> and <code>oauth-openshift-*-*.log</code> files</p> <ul> <li>Why: Manages authentication and user access to the cluster</li> <li>Look for: Login failures, identity provider issues, token problems</li> </ul> </li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#node-management-infrastructure-node-lifecycle-and-joining","title":"Node Management &amp; Infrastructure (Node lifecycle and joining)","text":"<p>These components manage the infrastructure and node lifecycle. Critical for scaling and node operations:</p> <ul> <li> <p>Control Plane Operator: <code>control-plane-operator-*.yaml</code> and <code>control-plane-operator-*-*.log</code> files</p> <ul> <li>Why: HyperShift-specific component that manages the hosted control plane lifecycle</li> <li>Look for: Control plane component deployment issues, version management problems</li> </ul> </li> <li> <p>Cluster API Manager: <code>cluster-api-*.yaml</code> and <code>cluster-api-*-*.log</code> files</p> <ul> <li>Why: Core CAPI controller that manages infrastructure resources and machine lifecycle</li> <li>Look for: Machine creation/deletion failures, cluster infrastructure issues</li> </ul> </li> <li> <p>CAPI Provider: <code>capi-provider-*.yaml</code> and <code>capi-provider-*-*.log</code> files</p> <ul> <li>Why: Azure-specific implementation that provisions VMs, networking, and storage</li> <li>Look for: Azure API failures, resource quota issues, networking problems</li> </ul> </li> <li> <p>Ignition Server: <code>ignition-server-*.yaml</code> and <code>ignition-server-*-*.log</code> files</p> <ul> <li>Why: Provides initial configuration to new nodes during bootstrap process</li> <li>Look for: Node configuration failures, network connectivity from nodes to control plane</li> </ul> </li> <li> <p>Machine Approver: <code>machine-approver-*.yaml</code> and <code>machine-approver-*-*.log</code> files</p> <ul> <li>Why: Automatically approves Certificate Signing Requests from nodes joining the cluster</li> <li>Look for: CSR approval failures, certificate issues preventing node join</li> </ul> </li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#start-here-primary-resources-for-debugging","title":"\ud83c\udfc1 Start Here: Primary Resources for Debugging","text":"<p>Always check these HyperShift custom resources first </p> <ul> <li>their status sections provide high-level cluster state:<ul> <li>HostedCluster: <code>artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/e2e-clusters-*/hypershift.openshift.io/hostedclusters/*.yaml</code></li> <li>HostedControlPlane: <code>artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/e2e-clusters-*-{test-name}-*/hypershift.openshift.io/hostedcontrolplanes/*.yaml</code></li> <li>NodePool: <code>artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/e2e-clusters-*/hypershift.openshift.io/nodepools/*.yaml</code></li> </ul> </li> </ul> <p>\ud83d\udca1 Why check these first: The <code>status</code> sections contain:</p> <ul> <li>Overall cluster readiness and conditions</li> <li>Infrastructure provisioning status  </li> <li>Control plane component health</li> <li>Node pool scaling and readiness</li> <li>Error messages and failure reasons</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#top-level-test-execution-files","title":"\ud83d\udcc2 Top-Level Test Execution Files","text":"<ul> <li><code>build-log.txt</code> - Main CI build execution log</li> <li><code>clone-log.txt</code> - Git repository cloning logs  </li> <li><code>finished.json</code> - Test completion status with success/failure details</li> <li><code>prowjob.json</code> - Complete Prow job specification (PR info, test configuration)</li> <li><code>podinfo.json</code> - CI pod execution information</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#understanding-test-results","title":"\ud83d\udccb Understanding Test Results","text":"<p>Each test directory (i.e. <code>artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*</code>) represents a different validation scenario. </p> <p>Check the <code>junit.xml</code> files for test pass/fail status (i.e. <code>artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/junit.xml</code>), and use the corresponding namespace directories to drill down into specific component failures.</p> <p>The artifact structure provides comprehensive debugging capabilities from high-level test orchestration down to individual container logs within the hosted control plane components.</p>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#available-test-scenarios","title":"\ud83e\uddea Available Test Scenarios","text":"<ul> <li><code>TestAutoscaling/</code> - Cluster autoscaling validation</li> <li><code>TestCreateCluster/</code> - Basic hosted cluster creation  </li> <li><code>TestCreateClusterCustomConfig/</code> - Custom configuration testing</li> <li><code>TestHAEtcdChaos/</code> - etcd high availability and chaos testing</li> <li><code>TestNodePool_HostedCluster*/</code> - Node pool lifecycle management</li> <li><code>TestUpgradeControlPlane/</code> - Control plane upgrade procedures</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#test-results-metrics","title":"\ud83d\udcc8 Test Results &amp; Metrics","text":"<ul> <li><code>e2e-metrics-raw.prometheus</code> - Raw Prometheus metrics from test execution</li> <li><code>junit.xml</code> - JUnit test results summary</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#artifactsrelease-release-management","title":"<code>/artifacts/release/</code> - Release Management","text":"<ul> <li><code>artifacts/release-images-n2minor</code> - Release image artifacts and metadata</li> <li><code>build-log.txt</code> - Release build logs</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#directory-structure-deep-dive","title":"Directory Structure Deep Dive","text":""},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#artifacts-main-test-artifacts-directory","title":"<code>/artifacts/</code> - Main Test Artifacts Directory","text":""},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#artifactsbuild-logs-component-build-logs","title":"<code>/artifacts/build-logs/</code> - Component Build Logs","text":"<p>Contains compilation logs for the core HyperShift components:</p> <ul> <li><code>hypershift-amd64.log</code> - Main HyperShift binary build</li> <li><code>hypershift-operator-amd64.log</code> - HyperShift operator build  </li> <li><code>hypershift-tests-amd64.log</code> - Test suite compilation</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#artifactsbuild-resources-build-time-kubernetes-resources","title":"<code>/artifacts/build-resources/</code> - Build-Time Kubernetes Resources","text":"<p>Resource manifests generated during the build process:</p> <ul> <li><code>builds.json</code> - OpenShift build configurations</li> <li><code>events.json</code> - Kubernetes events during build</li> <li><code>pods.json</code> - Pod specifications and status</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#artifactsci-operator-ci-pipeline-artifacts","title":"<code>/artifacts/ci-operator-*</code> - CI Pipeline Artifacts","text":"<p>CI operator execution details:</p> <ul> <li><code>ci-operator.log</code> - Main CI execution log (check here for high-level test failures)</li> <li><code>ci-operator-metrics.json</code> - Performance metrics and timing</li> <li><code>junit_operator.xml</code> - JUnit test results for CI integration</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#artifactse2e-aks-azure-e2e-test-execution","title":"<code>/artifacts/e2e-aks/</code> - Azure E2E Test Execution","text":"<p>This directory contains the complete AKS-based test execution pipeline:</p>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#infrastructure-management","title":"\ud83c\udfd7\ufe0f Infrastructure Management","text":"<ul> <li><code>aks-provision/</code> - AKS management cluster creation logs</li> <li><code>aks-deprovision/</code> - AKS cluster cleanup and teardown logs  </li> <li><code>azure-deprovision-resourcegroup/</code> - Azure resource group cleanup</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#security-and-access-setup","title":"\ud83d\udd10 Security and Access Setup","text":"<ul> <li><code>hypershift-azure-aks-attach-kv/</code> - Azure Key Vault integration for secrets</li> <li><code>hypershift-install/</code> - HyperShift operator installation on management cluster</li> <li><code>ipi-install-rbac/</code> - RBAC setup for infrastructure provisioning</li> <li><code>openshift-cluster-bot-rbac/</code> - Service account permissions for automation</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#main-test-execution","title":"\ud83e\uddea Main Test Execution","text":"<ul> <li><code>hypershift-azure-run-e2e/</code> - \u2b50 PRIMARY TEST DIRECTORY containing all hosted control plane tests</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#inside-hypershift-azure-run-e2eartifacts","title":"\ud83d\udcca Inside <code>hypershift-azure-run-e2e/artifacts/</code>","text":"<p>This is where you'll find the hosted control plane components and logs:</p>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#test-case-structure","title":"Test Case Structure","text":"<p>Each <code>Test*/</code> directory represents a different test scenario and contains:</p>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#per-test-artifacts","title":"\ud83d\udcc1 Per-Test Artifacts","text":"<ul> <li><code>create.log</code> - Hosted cluster creation logs (start here for provisioning issues)</li> <li><code>destroy.log</code> - Cluster cleanup logs</li> <li><code>dump.log</code> - Complete resource dump (comprehensive debugging info)</li> <li><code>infrastructure.log</code> - Azure infrastructure provisioning logs</li> <li><code>hostedcluster.tar</code> - Complete hosted cluster configuration archive</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#namespace-organization-namespaces-directory","title":"\ud83d\uddc2\ufe0f Namespace Organization (<code>namespaces/</code> directory)","text":"<p>Key namespace patterns to look for:</p>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#hypershift-hypershift-operator-namespace","title":"<code>hypershift/</code> - \ud83c\udf9b\ufe0f HyperShift Operator Namespace","text":"<p>This namespace contains the management cluster components that orchestrate hosted control planes:</p> <p>Core Components:</p> <ul> <li><code>apps/deployments/operator.yaml</code> - HyperShift operator deployment (manages HostedClusters)</li> <li><code>apps/deployments/external-dns.yaml</code> - External DNS deployment (Azure DNS integration)</li> </ul> <p>Pod Manifests &amp; Logs:</p> <ul> <li><code>core/pods/operator-*.yaml</code> - HyperShift operator pod specification</li> <li><code>core/pods/external-dns-*.yaml</code> - External DNS pod specification</li> <li><code>core/pods/logs/operator-*-operator.log</code> - \ud83d\udd25 Main HyperShift operator logs (HostedCluster reconciliation)</li> <li><code>core/pods/logs/operator-*-init-environment.log</code> - Operator initialization logs</li> <li><code>core/pods/logs/external-dns-*-external-dns.log</code> - DNS management logs (Azure DNS record creation/updates)</li> </ul> <p>Configuration &amp; Events:</p> <ul> <li><code>core/configmaps/</code> - Operator configuration (feature gates, supported versions)</li> <li><code>core/events/operator-*.yaml</code> - Operator lifecycle events</li> <li><code>core/events/external-dns-*.yaml</code> - DNS operation events</li> <li><code>core/services/operator.yaml</code> - Operator service definition</li> </ul> <p>\ud83d\udca1 Key Use Cases:</p> <ul> <li>HostedCluster issues: Check <code>operator-*-operator.log</code> for cluster provisioning/reconciliation errors</li> <li>DNS problems: Review <code>external-dns-*-external-dns.log</code> for Azure DNS integration issues</li> <li>Operator crashes: Look at operator events and initialization logs</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#hypershift-sharedingress-shared-ingress-infrastructure","title":"<code>hypershift-sharedingress/</code> - Shared Ingress Infrastructure","text":"<ul> <li>Router and ingress controller components</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#e2e-clusters-test-name-hosted-control-plane-namespace","title":"<code>e2e-clusters-*-{test-name}-*/</code> - \u2b50 HOSTED CONTROL PLANE NAMESPACE","text":"<p>This is where the actual control plane pods run. Look here for:</p> <ul> <li><code>core/pods/</code> - Control plane pod manifests (etcd, kube-apiserver, etc.)</li> <li><code>core/pods/logs/</code> - \ud83d\udd25 Control plane pod logs (primary debugging location)</li> <li><code>apps/deployments/</code> - Control plane component deployments</li> <li><code>hypershift.openshift.io/</code> - HyperShift custom resources (HostedControlPlane, etc.)</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#debugging-workflows","title":"\ud83d\udee0\ufe0f Debugging Workflows","text":""},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#finding-control-plane-issues","title":"Finding Control Plane Issues","text":"<ol> <li>Start with overall test status: Check <code>finished.json</code> for high-level failure info</li> <li>Check HyperShift custom resources: Examine HostedCluster, HostedControlPlane, and NodePool status sections for error conditions</li> <li>Check test execution: Look at <code>artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/create.log</code> </li> <li>Find control plane pods: Navigate to <code>artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/namespaces/e2e-clusters-*-{test-name}-*/core/pods/</code></li> <li>Review specific component logs: Check <code>core/pods/logs/{component-name}-*-{container}.log</code></li> </ol>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#node-joining-and-infrastructure-issues","title":"Node Joining and Infrastructure Issues","text":"<ol> <li>Check NodePool status: Look for scaling, readiness, and machine creation issues</li> <li>CAPI components: Review <code>cluster-api-*</code> and <code>capi-provider-*</code> logs for Azure infrastructure problems</li> <li>Node bootstrapping: Check <code>ignition-server-*</code> logs for node configuration issues</li> <li>CSR approval: Review <code>machine-approver-*</code> logs for certificate signing problems</li> <li>Control plane coordination: Check <code>control-plane-operator-*</code> logs for component management issues</li> </ol>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#common-log-locations","title":"Common Log Locations","text":""},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#hosted-control-plane-components-in-e2e-clusters-namespaces","title":"Hosted Control Plane Components (in <code>e2e-clusters-*</code> namespaces)","text":"<ul> <li>etcd logs: <code>etcd-0-etcd.log</code>, <code>etcd-0-healthz.log</code>, <code>etcd-0-etcd-metrics.log</code></li> <li>API Server logs: <code>kube-apiserver-*-kube-apiserver.log</code>, <code>kube-apiserver-*-audit-logs.log</code></li> <li>Controller Manager: <code>kube-controller-manager-*-kube-controller-manager.log</code></li> <li>Scheduler: <code>kube-scheduler-*-kube-scheduler.log</code></li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#management-cluster-components-in-hypershift-namespace","title":"Management Cluster Components (in <code>hypershift/</code> namespace)","text":"<ul> <li>HyperShift Operator: <code>operator-*-operator.log</code> (HostedCluster/HostedControlPlane reconciliation)</li> <li>External DNS: <code>external-dns-*-external-dns.log</code> (Azure DNS record management)</li> <li>Operator Initialization: <code>operator-*-init-environment.log</code> (startup and environment setup)</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#infrastructure-debugging","title":"Infrastructure Debugging","text":"<ul> <li>AKS provisioning: <code>artifacts/e2e-aks/aks-provision/build-log.txt</code></li> <li>Azure resources: <code>artifacts/e2e-aks/hypershift-azure-run-e2e/artifacts/Test*/infrastructure.log</code></li> <li>Network setup: Look for <code>cloud-network-config-controller</code> logs in hosted control plane namespace</li> </ul>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#hypershift-operator-debugging","title":"HyperShift Operator Debugging","text":"<ol> <li>HostedCluster creation issues: Check <code>hypershift/core/pods/logs/operator-*-operator.log</code> for reconciliation errors</li> <li>DNS resolution problems: Review <code>hypershift/core/pods/logs/external-dns-*-external-dns.log</code> for Azure DNS failures</li> <li>Operator startup issues: Examine <code>hypershift/core/pods/logs/operator-*-init-environment.log</code> and operator events</li> <li>Cross-reference with control plane: If operator reports success but control plane fails, check hosted control plane namespace logs</li> </ol>"},{"location":"reference/test-information-debugging/Azure/test-artifacts-directory-structure/#external-dns-specific-issues","title":"External DNS Specific Issues","text":"<ul> <li>Azure DNS integration: External DNS manages DNS records for hosted cluster API endpoints</li> <li>Common failure patterns: Permission issues with Azure DNS zones, rate limiting, DNS propagation delays</li> <li>Log format: Look for Azure API calls, DNS record operations, and error responses in external-dns logs</li> </ul>"},{"location":"videos/","title":"HyperShift Demos/Videos","text":""},{"location":"videos/#how-hostedcontrolplanes-work","title":"How HostedControlPlanes Work","text":"<p>This is a contribution from Srishti Miglani that explains how HostedControlPlanes work. The video provides an in-depth look at the KubeVirt provider.</p>"},{"location":"videos/#dualstack-agent-hosted-control-plane-deployment-connected","title":"Dualstack Agent Hosted Control Plane Deployment (Connected)","text":"<p>This video explains how to deploy a Hosted Control Plane using the Agent provider on bare metal. It uses a dual-stack configuration with IPv6 and IPv4. The IPv4 connectivity ensures that all required images are available for IPv6 deployment (this is why it works).</p>"},{"location":"videos/#dualstack-agent-hosted-control-plane-deployment-disconnected","title":"Dualstack Agent Hosted Control Plane Deployment (Disconnected)","text":"<p>This video demonstrates how to deploy a dual-stack, fully disconnected Hosted Control Plane using the Agent provider.</p>"},{"location":"videos/#hosted-control-plane-deployment-using-kubevirt-provider-and-multicluster-engine-ui","title":"Hosted Control Plane Deployment Using KubeVirt Provider and Multicluster Engine UI","text":"<p>This video demonstrates how to deploy a Hosted Control Plane using the Multicluster Engine (MCE) UI on a KubeVirt provider.</p>"},{"location":"videos/#ipv6-agent-hosted-control-plane-deployment-disconnected","title":"IPv6 Agent Hosted Control Plane Deployment (Disconnected)","text":"<p>This video demonstrates how to deploy an IPv6, fully disconnected Hosted Control Plane using the Agent provider.</p>"}]}